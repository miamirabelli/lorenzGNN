{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This onboarding notebook will demonstrate how to use this codebase. The different sections correspond to different parts of the overall pipeline and also roughly correspond to the different files in `utils`.\n",
    "\n",
    "Before running this notebook, ensure that your virtual environment is set up and that you have installed the necessary libraries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to set up the ipython autoreload extension, which autoreloads imported modules every time we run a cell in this notebook, so that any changes elsewhere in the codebase will be up to date when code in this notebook is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipython extension to autoreload imported modules so that any changes will be up to date before running code in this nb\n",
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.jraph_data import get_lorenz_graph_tuples, print_graph_fts\n",
    "from utils.lorenz import load_lorenz96_2coupled\n",
    "\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONFIGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO move this to top and reference it instead of magic numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configs are useful for keeping track of the parameters you are using for a particular run. Also, some of the parameters need to be reused between the data generation and the model/training setup, so it's convenient to be able to just reference a config file that contains everything in one place. Here is what an example config looks like and how to call attributes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml_collections\n",
    "\n",
    "config = ml_collections.ConfigDict()\n",
    "\n",
    "# Data params. \n",
    "config.n_samples=20\n",
    "config.input_steps=3\n",
    "config.output_delay=0\n",
    "config.output_steps=2\n",
    "config.timestep_duration=1\n",
    "config.sample_buffer=1\n",
    "config.time_resolution=100\n",
    "config.init_buffer_samples=0\n",
    "config.train_pct=0.7\n",
    "config.val_pct=0.2\n",
    "config.test_pct=0.1\n",
    "config.K=36\n",
    "config.F=8\n",
    "config.c=10\n",
    "config.b=10\n",
    "config.h=1\n",
    "config.seed=42\n",
    "config.normalize=True\n",
    "config.fully_connected_edges=True\n",
    "\n",
    "# Optimizer.\n",
    "config.optimizer = 'adam'\n",
    "config.learning_rate = 1e-3\n",
    "\n",
    "# Training hyperparameters.\n",
    "# config.batch_size = 3\n",
    "config.epochs = 2\n",
    "config.log_every_epochs = 1\n",
    "config.eval_every_epochs = 1\n",
    "config.checkpoint_every_epochs = 1\n",
    "config.max_checkpts_to_keep = None # None means keep all checkpoints\n",
    "\n",
    "# GNN hyperparameters.\n",
    "config.model = 'MLPGraphNetwork'\n",
    "config.n_blocks = 1\n",
    "config.activation = 'relu'\n",
    "config.dropout_rate = 0.1\n",
    "config.skip_connections = False # This was throwing a broadcast error in add_graphs_tuples_nodes when this was set to True\n",
    "config.layer_norm = False # TODO perhaps we want to turn on later\n",
    "config.edge_features = (4, 8) # the last feature size will be the number of features that the graph predicts\n",
    "config.node_features = (32, 2)\n",
    "config.global_features = None\n",
    "config.share_params = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can simply reference the config's attributes by calling, e.g. \n",
    "`some_param_you_want_to_set = config.param_name`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we go over how to run the Lorenz simulation, and how to generate the graph data objects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lorenz simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.lorenz import run_download_lorenz96_2coupled, load_lorenz96_2coupled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, we want to save the Lorenz simulations that we run so that we don't have to repeat the ODE integration if we want to reuse the data. \n",
    "\n",
    "The following code runs ODE integration over the coupled 2-layer Lorenz96 model and saves the data to an .npz file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_download_lorenz96_2coupled(\n",
    "        fname=\"data/demo\", \n",
    "        K=config.K,\n",
    "        F=config.F,\n",
    "        c=config.c,\n",
    "        b=config.b,\n",
    "        h=config.h,\n",
    "        n_steps=100,\n",
    "        resolution=config.time_resolution,\n",
    "        seed=config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the data folder contains a new .npz file, and that the data_directory.json file now contains a new entry for this simulation run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can load the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 72)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[8.        , 8.        , 8.        , ..., 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       [7.91069112, 7.91328858, 7.91054206, ..., 0.98064008, 0.98055162,\n",
       "        0.98020479],\n",
       "       [7.82329334, 7.83291207, 7.82210487, ..., 0.96272968, 0.9623841 ,\n",
       "        0.96101319],\n",
       "       ...,\n",
       "       [0.91615821, 1.21366249, 3.18265187, ..., 0.21614824, 0.20223427,\n",
       "        0.28321649],\n",
       "       [0.90460693, 1.22721173, 3.18752753, ..., 0.21261883, 0.23814942,\n",
       "        0.28273718],\n",
       "       [0.89409501, 1.24079363, 3.19048114, ..., 0.19468144, 0.25771688,\n",
       "        0.27600591]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t, X = load_lorenz96_2coupled(fname=\"data/demo.npz\")\n",
    "print(X.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play around with different ways to visualize this. (There are some existing functions in utils.visualization but they're kinda a mess right now.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GraphsTuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the pipeline to generate data and train the model, you won't actually need to call the Lorenz simulation directly, since it's wrapped in the calls to get the graph data. \n",
    "\n",
    "We are using the `GraphsTuple` object defined in the `jraph` library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here the difference between samples, timesteps, and (simulation) steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO insert a visualization of what these are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.jraph_data import get_lorenz_graph_tuples, print_graph_fts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate desired dataset with train/val split and subsampled windows\n",
    "graph_tuple_dict = get_lorenz_graph_tuples(\n",
    "    n_samples=config.n_samples,\n",
    "    input_steps=config.input_steps,\n",
    "    output_delay=config.output_delay,\n",
    "    output_steps=config.output_steps,\n",
    "    timestep_duration=config.timestep_duration,\n",
    "    sample_buffer=config.sample_buffer,\n",
    "    time_resolution=config.time_resolution,\n",
    "    init_buffer_samples=config.init_buffer_samples,\n",
    "    train_pct=config.train_pct,\n",
    "    val_pct=config.val_pct,\n",
    "    test_pct=config.test_pct,\n",
    "    K=config.K,\n",
    "    F=config.F,\n",
    "    c=config.c,\n",
    "    b=config.b,\n",
    "    h=config.h,\n",
    "    seed=config.seed,\n",
    "    normalize=config.normalize,\n",
    "    fully_connected_edges=config.fully_connected_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the additional parameters for `normalization` and `fully_connected_edges`. If `fully_connected_edges` is False, each node will only be connected to its two nearest neighbors and itself (5 total edges per node). There are some theoretical and practical considerations about which to choose (atm I am not sure what is best â€“ it seems like theoretically fully-connected could be better, but that performed way worse in hyperparameter tuning. Also, Keisler's Graph Weather paper only had edges between adjacent nodes (on a sphere), so it seems like in theory we may not actually need fully-connected?)\n",
    "\n",
    "Also, (not doing it here, but) it can be helpful to compare the normalized with non-normalized data to check that the pre-processing step is correct. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example of how to index the graph_tuple_dict object: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph_tuple_dict type: <class 'dict'>\n",
      "graph_tuple_dict keys: dict_keys(['train', 'val', 'test'])\n",
      "graph_tuple_dict value type: <class 'dict'>\n",
      "train dataset keys: dict_keys(['inputs', 'targets'])\n",
      "train dataset value type: <class 'list'>\n",
      "size of train inputs: 14\n",
      "size of train targets: 14\n",
      "size of val inputs: 4\n",
      "size of val targets: 4\n",
      "size of test inputs: 2\n",
      "size of test targets: 2\n",
      "train inputs window type: <class 'list'>\n",
      "train input window size (i.e. input steps per window): 3\n",
      "element type in window: <class 'jraph._src.graph.GraphsTuple'>\n"
     ]
    }
   ],
   "source": [
    "train_dataset = graph_tuple_dict['train']\n",
    "val_dataset = graph_tuple_dict['val']\n",
    "test_dataset = graph_tuple_dict['test']\n",
    "\n",
    "train_inputs = train_dataset['inputs']\n",
    "train_targets = train_dataset['targets']\n",
    "val_inputs = val_dataset['inputs']\n",
    "val_targets = val_dataset['targets']\n",
    "test_inputs = test_dataset['inputs']\n",
    "test_targets = test_dataset['targets']\n",
    "\n",
    "sample_input_window = train_inputs[0]\n",
    "sample_target_window = train_targets[0]\n",
    "sample_graph = sample_input_window[0]\n",
    "\n",
    "print(\"graph_tuple_dict type:\", type(graph_tuple_dict))\n",
    "print(\"graph_tuple_dict keys:\", graph_tuple_dict.keys())\n",
    "print(\"graph_tuple_dict value type:\", type(train_dataset))\n",
    "print(\"train dataset keys:\", train_dataset.keys())\n",
    "print(\"train dataset value type:\", type(train_inputs))\n",
    "\n",
    "print(\"size of train inputs:\", len(train_inputs))\n",
    "print(\"size of train targets:\", len(train_targets))\n",
    "print(\"size of val inputs:\", len(val_inputs))\n",
    "print(\"size of val targets:\", len(val_targets))\n",
    "print(\"size of test inputs:\", len(test_inputs))\n",
    "print(\"size of test targets:\", len(test_targets))\n",
    "\n",
    "print(\"train inputs window type:\", type(sample_input_window))\n",
    "print(\"train input window size (i.e. input steps per window):\", len(sample_input_window))\n",
    "print(\"element type in window:\", type(sample_graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of what a single GraphsTuple object looks like (representing the state of the system at a single instant in time). (It's terrible to scroll through, would recommend collapsing). Below are better ways to index through the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphsTuple(nodes=array([[0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25881994, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ],\n",
      "       [0.25872645, 1.344831  ]], dtype=float32), edges=Array([[ 0.],\n",
      "       [-1.],\n",
      "       [-2.],\n",
      "       ...,\n",
      "       [ 2.],\n",
      "       [ 1.],\n",
      "       [ 0.]], dtype=float32), receivers=Array([ 0,  1,  2, ..., 33, 34, 35], dtype=int32), senders=Array([ 0,  0,  0, ..., 35, 35, 35], dtype=int32), globals=Array([[1.]], dtype=float32), n_node=Array([36], dtype=int32), n_edge=Array([1296], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "print(sample_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 36\n",
      "Number of edges: 1296\n",
      "Node features shape: (36, 2)\n",
      "Edge features shape: (1296, 1)\n",
      "Global features shape: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "# helper function \n",
    "print_graph_fts(sample_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25881994, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ],\n",
       "       [0.25872645, 1.344831  ]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# node features (i.e. 36 nodes x 2 layers)\n",
    "print(graph_tuple_dict['train']['inputs'][0][0].nodes.shape)\n",
    "graph_tuple_dict['train']['inputs'][0][0].nodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1296, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([[ 0.],\n",
       "       [-1.],\n",
       "       [-2.],\n",
       "       ...,\n",
       "       [ 2.],\n",
       "       [ 1.],\n",
       "       [ 0.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# edge features \n",
    "print(graph_tuple_dict['train']['inputs'][0][0].edges.shape)\n",
    "graph_tuple_dict['train']['inputs'][0][0].edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "understanding edges: in a GraphsTuple, edges are encoded using three attributes: `GraphsTuple.senders`, `GraphsTuple.receivers`, and `GraphsTuple.edges`. \n",
    "\n",
    "The ith element of the `senders` attribute denotes the index of the source node; the ith element of the `receivers` attribute denotes the index of the receiving node. (The `senders` and `receivers` attributes are both 1d arrays.) The ith array of the `edges` attribute (a 2d array) are the encoded features for each node. In this case, each edge only has a single feature, which encodes the distance between the sender and receiver node. \n",
    "\n",
    "If the graph is fully connected, we expect 36\\*36 = 1296 edges. (If not, we default to 5 edges per node, so we would expect 36*5 = 180 edges.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side note, gathering node-level data into a time series is kind of a pain because it requires iterating over all GraphsTuples in the desired time frame... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side note 2: in jraph, there is an ability to create [batches](https://jraph.readthedocs.io/en/latest/api.html#batching-padding-utilities) of GraphsTuples, which basically means combining multiple GraphsTuples into a single GraphsTuple object. This actually makes gathering time series data easier/quicker; HOWEVER, it does not play nice with jax and is not compilable :( hence why some parts of the codebase do really slow and stupid indexing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side note 3: the `global_features` attribute in these graphs is just a dummy variable and is not used during training. I was trying to get rid of the variable but ran into some random bugs so I just left it as a placeholder since I didn't feel like wasting time dealing with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how to use a basic model. The process is essentially the same for any model you want to use/define, with the differences primarily being the parameters you pass in when initializing an instance of the model class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.jraph_models import MLPGraphNetwork\n",
    "import flax.linen as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPGraphNetwork(\n",
       "    # attributes\n",
       "    n_blocks = 1\n",
       "    share_params = False\n",
       "    dropout_rate = 0.1\n",
       "    skip_connections = False\n",
       "    layer_norm = False\n",
       "    deterministic = True\n",
       "    activation = relu\n",
       "    edge_features = (4, 8)\n",
       "    node_features = (32, 2)\n",
       "    global_features = None\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_funcs = {\n",
    "    \"relu\": nn.relu,\n",
    "    \"elu\": nn.elu,\n",
    "    \"leaky_relu\": nn.leaky_relu,\n",
    "}\n",
    "activation = activation_funcs[config.activation]\n",
    "\n",
    "model = MLPGraphNetwork(            \n",
    "    n_blocks=config.n_blocks,\n",
    "    share_params=config.share_params,\n",
    "    dropout_rate=config.dropout_rate,\n",
    "    skip_connections=config.skip_connections,\n",
    "    layer_norm=config.layer_norm,\n",
    "    deterministic=True,\n",
    "    activation=activation,\n",
    "    edge_features=config.edge_features,\n",
    "    node_features=config.node_features,\n",
    "    global_features=config.global_features,\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These models inherent from the flax nn.Module class.\n",
    "\n",
    "As of right now, models take in *lists* of GraphsTuples (i.e. windows of data). However, it currently can only process one timestep of data at a time (it can't process windows of data since there is no recurrent cell or anything to do that) so the model just uses the first GraphsTuple in a window and drops the rest. This is important to keep in mind when trying to call the model on data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually use a model, things rapidly get more complicated. At this point we bring in the `flax` library to help us keep track of model states, params, optimizers, etc. (If you've used pytorch or tensorflow you usually don't have to deal with this because it's taken care of behind the scenes!) \n",
    "\n",
    "Before initialization, a flax/jax model is sort of just nebulous emptiness. The model doesn't even know how big it needs to be - it figures this out when you give it some sample data during initialization. \n",
    "\n",
    "For more details, see the [flax documentation](https://flax.readthedocs.io/en/latest/guides/flax_fundamentals/flax_basics.html#model-parameters-initialization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from flax.training import train_state\n",
    "import optax # library with optimizer classes used with jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and initialize the parameters.\n",
    "rng = jax.random.key(0)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "params = jax.jit(model.init)(init_rng, sample_input_window)\n",
    "\n",
    "# Create an optimizer (you can create different types of optimizers, e.g. adam, SGD, etc.)\n",
    "tx = optax.adam(learning_rate=config.learning_rate)\n",
    "\n",
    "# Create the training state.\n",
    "state = train_state.TrainState.create(\n",
    "    apply_fn=model.apply, params=params, tx=tx\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A flax train_state is basically a convenient group of a bunch of things we need to call a forward pass, perform an update step for optimization, etc. These include the model `apply` function (i.e. forward pass/prediction function), the actual parameters, and the optimizer object. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally perform a forward pass. Note that this model is still totally untrained, so the outputs are going to be garbage. \n",
    "\n",
    "Since we need both the model apply function and the model's external params, the train_state is set up to allow us to do this more conveniently than if we used the raw jax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "garbage_output = state.apply_fn(state.params, sample_input_window, rngs={}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "1\n",
      "<class 'jraph._src.graph.GraphsTuple'>\n"
     ]
    }
   ],
   "source": [
    "print(type(garbage_output))\n",
    "print(len(garbage_output))\n",
    "print(type(garbage_output[0]))\n",
    "# print(garbage_output[0].nodes) # commented out because it's long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember how the input to the model is a window (even though we aren't currently using the whole window)? Well, we want the output type to match the input type (in case we want to chain multiple MLPBlocks together, like we do in the MLPGraphNetwork class); however, we only output one timestep at a time, hence why we have a list containing a single GraphsTuple object. (If we want to output a whole window of data, we'll have to call the model `apply` function autoregressively.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single training step consists of:\n",
    "* computing rollout and loss for a single forward pass \n",
    "* updating model params based on gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miamirabelli/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils.jraph_training import create_model, train_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for training, if we want dropout, we want to set the `deterministic` param for the model to False so that the dropout occurs randomly. (You should turn off `dropout` and `deterministic` for evaluation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and initialize the network.\n",
    "rng = jax.random.key(0)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "init_net = create_model(config, deterministic=True)\n",
    "params = jax.jit(init_net.init)(init_rng, sample_input_window)\n",
    "\n",
    "# Create the optimizer.\n",
    "tx = optax.adam(learning_rate=config.learning_rate)\n",
    "\n",
    "# Create the training state.\n",
    "net = create_model(config, deterministic=False)\n",
    "state = train_state.TrainState.create(\n",
    "    apply_fn=net.apply, params=params, tx=tx\n",
    ")\n",
    "\n",
    "rng, dropout_rng = jax.random.split(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, metrics_update, pred_nodes = train_step(\n",
    "    state=state,\n",
    "    n_rollout_steps=config.output_steps,\n",
    "    input_window_graphs=sample_input_window,\n",
    "    target_window_graphs=sample_target_window,\n",
    "    rngs={'dropout': dropout_rng})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Array([[ -98.677925 ,  -39.921608 ],\n",
       "        [-120.49141  ,  -50.34245  ],\n",
       "        [-104.10326  ,  -36.979153 ],\n",
       "        [ -91.72362  ,  -45.435158 ],\n",
       "        [-102.39267  ,  -23.706974 ],\n",
       "        [-105.23367  ,  -38.129932 ],\n",
       "        [-107.99461  ,  -34.88067  ],\n",
       "        [ -75.59826  ,   -2.6439323],\n",
       "        [-107.7919   ,  -55.576904 ],\n",
       "        [   0.       ,  -81.21098  ],\n",
       "        [-120.722084 ,  -47.333466 ],\n",
       "        [   0.       ,  -99.847534 ],\n",
       "        [ -61.948605 ,  -91.852066 ],\n",
       "        [   0.       ,  -19.734015 ],\n",
       "        [-114.468636 ,  -39.501007 ],\n",
       "        [-108.012985 ,   -7.4714293],\n",
       "        [ -91.07166  ,    0.       ],\n",
       "        [ -57.29462  ,    0.       ],\n",
       "        [   0.       ,  -39.664696 ],\n",
       "        [ -82.92592  ,  -11.086294 ],\n",
       "        [-102.702866 ,  -42.68212  ],\n",
       "        [ -93.784676 ,  -39.14112  ],\n",
       "        [-107.27273  ,  -63.723972 ],\n",
       "        [ -72.325096 ,  -66.53959  ],\n",
       "        [ -68.340866 ,  -87.95836  ],\n",
       "        [ -90.03129  ,  -48.926582 ],\n",
       "        [ -91.88275  ,   -0.9935209],\n",
       "        [-106.62933  ,   -7.7114897],\n",
       "        [ -62.931767 ,    2.6885633],\n",
       "        [-104.1441   ,  -30.487354 ],\n",
       "        [   0.       ,  -49.252693 ],\n",
       "        [   0.       ,    0.       ],\n",
       "        [-110.31584  ,  -34.4162   ],\n",
       "        [-108.63603  ,  -60.364872 ],\n",
       "        [-107.26558  ,  -34.46278  ],\n",
       "        [-105.96918  ,  -43.04252  ]], dtype=float32),\n",
       " Array([[ -98.7262   ,  -39.9499   ],\n",
       "        [-120.56743  ,  -50.402737 ],\n",
       "        [-104.130005 ,  -37.027817 ],\n",
       "        [ -91.75908  ,  -45.58351  ],\n",
       "        [-102.44264  ,  -23.787142 ],\n",
       "        [-105.27907  ,  -38.14805  ],\n",
       "        [-108.02981  ,  -34.922626 ],\n",
       "        [ -75.66555  ,   -2.6468148],\n",
       "        [-107.870674 ,  -55.57448  ],\n",
       "        [   0.       ,  -81.30731  ],\n",
       "        [-120.79009  ,  -47.406124 ],\n",
       "        [   0.       ,  -99.97928  ],\n",
       "        [ -61.978203 ,  -91.92596  ],\n",
       "        [   0.       ,  -19.811262 ],\n",
       "        [-114.52519  ,  -39.540695 ],\n",
       "        [-108.023895 ,   -7.419    ],\n",
       "        [ -91.14054  ,    0.       ],\n",
       "        [ -57.28967  ,    0.       ],\n",
       "        [   0.       ,  -39.669067 ],\n",
       "        [ -82.92543  ,  -11.086861 ],\n",
       "        [-102.75735  ,  -42.715668 ],\n",
       "        [ -93.85601  ,  -39.12049  ],\n",
       "        [-107.35241  ,  -63.757923 ],\n",
       "        [ -72.35126  ,  -66.661476 ],\n",
       "        [ -68.36243  ,  -88.126945 ],\n",
       "        [ -90.07001  ,  -48.960915 ],\n",
       "        [ -91.902756 ,   -1.018387 ],\n",
       "        [-106.68354  ,   -7.730284 ],\n",
       "        [ -62.993454 ,    2.7190173],\n",
       "        [-104.17475  ,  -30.479767 ],\n",
       "        [   0.       ,  -49.245052 ],\n",
       "        [   0.       ,    0.       ],\n",
       "        [-110.32677  ,  -34.450794 ],\n",
       "        [-108.71026  ,  -60.40383  ],\n",
       "        [-107.334496 ,  -34.52833  ],\n",
       "        [-105.91433  ,  -43.127934 ]], dtype=float32)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `metrics_update` variable is an object that holds some useful information about metrics from the training step, namely the loss. It can be accessed as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainMetrics(_reduction_counter=_ReductionCounter(value=Array(1, dtype=int32, weak_type=True)), loss=Metric.from_output.<locals>.FromOutput(total=Array(5125.2095, dtype=float32), count=Array(1., dtype=float32)))\n",
      "access loss like this: 5125.2095\n"
     ]
    }
   ],
   "source": [
    "print(metrics_update)\n",
    "print('access loss like this:', metrics_update.loss.total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATION (VALIDATION, TESTING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation step is interwoven throughout the training pipeline; it usually occurs every time you finish a training epoch, or if you want to reduce runtime, every X epochs. \n",
    "\n",
    "The testing dataset should not be used until the **VERY** end of model development, e.g. when you're about to present your results (preparing a poster, presentation, manuscript, etc.) It shouldn't influence your model development at all. That's what the validation dataset is for. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example of a single evaluation step: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.jraph_training import evaluate_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, during evaluation, we want to turn OFF the `dropout` and `deterministic` params in the model instantiation, so we make a new model; in our state we want to copy the params from the training state, keep the correct `apply` function from the new evaluatio model, and don't care about optimizers because we're just doing evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the evaluation state, corresponding to a deterministic model.\n",
    "eval_net = create_model(config, deterministic=True) # Note that dropout is deactivated if deterministic is True. \n",
    "eval_state = state.replace(apply_fn=eval_net.apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics, pred_nodes = evaluate_step(\n",
    "    state=eval_state,\n",
    "    n_rollout_steps=config.output_steps,\n",
    "    input_window_graphs=sample_input_window,\n",
    "    target_window_graphs=sample_target_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect the prediction to be different from above because the params should've been updated. Ideally the loss should also go down a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Array([[-83.19219 , -30.645147],\n",
       "        [-83.19219 , -30.645147],\n",
       "        [-83.19219 , -30.645155],\n",
       "        [-83.19219 , -30.645142],\n",
       "        [-83.19219 , -30.64515 ],\n",
       "        [-83.19219 , -30.645157],\n",
       "        [-83.19219 , -30.645155],\n",
       "        [-83.19219 , -30.64516 ],\n",
       "        [-83.192184, -30.645163],\n",
       "        [-83.19219 , -30.645164],\n",
       "        [-83.19219 , -30.645159],\n",
       "        [-83.19219 , -30.64516 ],\n",
       "        [-83.19219 , -30.645168],\n",
       "        [-83.19219 , -30.645157],\n",
       "        [-83.1922  , -30.64517 ],\n",
       "        [-83.1922  , -30.64519 ],\n",
       "        [-83.1923  , -30.64483 ],\n",
       "        [-83.19217 , -30.645159],\n",
       "        [-83.19216 , -30.645151],\n",
       "        [-83.19215 , -30.645138],\n",
       "        [-83.19215 , -30.645132],\n",
       "        [-83.19215 , -30.645138],\n",
       "        [-83.19215 , -30.645136],\n",
       "        [-83.19215 , -30.645136],\n",
       "        [-83.19215 , -30.645138],\n",
       "        [-83.19215 , -30.645123],\n",
       "        [-83.19214 , -30.645124],\n",
       "        [-83.19215 , -30.645138],\n",
       "        [-83.19215 , -30.645136],\n",
       "        [-83.19215 , -30.645128],\n",
       "        [-83.19215 , -30.645124],\n",
       "        [-83.19215 , -30.645128],\n",
       "        [-83.192154, -30.645136],\n",
       "        [-83.192154, -30.645134],\n",
       "        [-83.19219 , -30.645151],\n",
       "        [-83.19219 , -30.645142]], dtype=float32),\n",
       " Array([[-83.22858 , -30.676626],\n",
       "        [-83.23091 , -30.672771],\n",
       "        [-83.23177 , -30.67916 ],\n",
       "        [-83.23167 , -30.679142],\n",
       "        [-83.231606, -30.678928],\n",
       "        [-83.231606, -30.67891 ],\n",
       "        [-83.23162 , -30.678919],\n",
       "        [-83.23161 , -30.678917],\n",
       "        [-83.23163 , -30.678919],\n",
       "        [-83.231606, -30.67892 ],\n",
       "        [-83.23163 , -30.678919],\n",
       "        [-83.23161 , -30.678915],\n",
       "        [-83.23161 , -30.67891 ],\n",
       "        [-83.23163 , -30.678938],\n",
       "        [-83.23163 , -30.678951],\n",
       "        [-83.2316  , -30.678839],\n",
       "        [-83.23124 , -30.678196],\n",
       "        [-83.22104 , -30.669584],\n",
       "        [-83.22015 , -30.67171 ],\n",
       "        [-83.2206  , -30.671993],\n",
       "        [-83.220634, -30.671898],\n",
       "        [-83.22062 , -30.671888],\n",
       "        [-83.220634, -30.671892],\n",
       "        [-83.22062 , -30.67189 ],\n",
       "        [-83.22063 , -30.671885],\n",
       "        [-83.220634, -30.67188 ],\n",
       "        [-83.22062 , -30.671883],\n",
       "        [-83.22063 , -30.671904],\n",
       "        [-83.220634, -30.671892],\n",
       "        [-83.22061 , -30.671774],\n",
       "        [-83.22063 , -30.671843],\n",
       "        [-83.22066 , -30.672361],\n",
       "        [-83.22065 , -30.671919],\n",
       "        [-83.220856, -30.667856],\n",
       "        [-83.21882 , -30.670702],\n",
       "        [-83.1932  , -30.763962]], dtype=float32)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(3990.5767, dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics.loss.total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, as expected, we verify both are true. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `evaluate_model` does similar things except it computes loss for all windows in the dataset (not just a single window like what we passed in above) and it does so for multiple modes that you specify via the `splits` param (e.g. `splits = ['val', 'test']`). This is used in the `train_and_evaluate` pipeline and would also come in handy for evaluating over the testing dataset at the very end of model development. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FULL TRAIN PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `train_and_evaluate` runs the entire training pipeline, start to end for the number of epochs specified in the config, and logs a lot of stuff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.jraph_training import train_and_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Obtaining datasets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'relu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 1, 'dropout_rate': 0.1, 'edge_features': (4, 8), 'epochs': 2, 'eval_every_epochs': 1, 'fully_connected_edges': True, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 3, 'layer_norm': False, 'learning_rate': 0.001, 'log_every_epochs': 1, 'max_checkpts_to_keep': None, 'model': 'MLPGraphNetwork', 'n_blocks': 1, 'n_samples': 20, 'node_features': (32, 2), 'normalize': True, 'optimizer': 'adam', 'output_delay': 0, 'output_steps': 2, 'sample_buffer': 1, 'seed': 42, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 100, 'timestep_duration': 1, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| Name                                   | Shape    | Size | Mean     | Std   |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (4,)     | 4    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 4)   | 24   | -0.0819  | 0.36  |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (4, 8)   | 32   | -0.0639  | 0.439 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (32,)    | 32   | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (19, 32) | 608  | -0.00843 | 0.219 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)     | 2    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (32, 2)  | 64   | -0.00627 | 0.195 |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "Total: 774\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Restoring checkpoint: tests/outputs/demo/checkpoints/ckpt-3\n",
      "INFO:absl:Restored save_counter=3 restored_checkpoint=tests/outputs/demo/checkpoints/ckpt-3\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.01s.\n",
      "INFO:absl:Starting training.\n"
     ]
    }
   ],
   "source": [
    "workdir=\"tests/outputs/demo\"\n",
    "\n",
    "trained_state, train_metrics, eval_metrics_dict = train_and_evaluate(\n",
    "    config=config, workdir=workdir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you go to the workdir folder, you can see the checkpoints and tensorboard objects it created. Also notice in the logs that the loss gets better over the (two) epochs we trained for! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HYPERPARAMETER TUNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `optuna` library for hyperparameter tuning, which is a flexible framework that can work with any ML library we use. \n",
    "\n",
    "terminology:\n",
    "* objective function: a function that returns the value you want to optimize (for example, an entire training pipeline, which trains a model for X epochs and returns the final validation loss. We want to minimize the loss by the end of training, and we need to tune the hyperparameters to do so.)\n",
    "* Parameter: A variable whose value is to be optimized. can be continuous, discrete, categorical, etc. (e.g. learning rate, number of GNBlocks chained together, choice of activation function, etc.)\n",
    "* Trial: An object which is like a single run of the objective function; in each trial, optuna generates different parameters.\n",
    "* Study: An optimization session, which is a set of trials. parameters vary across trials, and we can choose the best set of hyperparams by the end of the study. \n",
    "\n",
    "This tutorial explains how things work in more depth: https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/guide4/Research_Projects_with_JAX.html#Automatic-hyperparameter-tuning-with-Optuna \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example run: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna \n",
    "from utils.jraph_training import train_and_evaluate_with_data, create_dataset\n",
    "import os \n",
    "from functools import partial\n",
    "\n",
    "CHECKPOINT_PATH = 'experiments/tuning'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the objective function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, study_name, datasets):\n",
    "    \"\"\" Defines the objective function to be optimized over, aka the validation loss of a model.\n",
    "    \n",
    "        Args:\n",
    "            trial: object which characterizes the current run \n",
    "            datasets: dictionary of data. we explicitly pass this in so that we don't have to waste runtime regenerating the same dataset over and over. \n",
    "    \"\"\"\n",
    "    # create config \n",
    "    config = ml_collections.ConfigDict()\n",
    "\n",
    "    # Data params. \n",
    "    config.n_samples=20\n",
    "    config.input_steps=3\n",
    "    config.output_delay=0\n",
    "    config.output_steps=2\n",
    "    config.timestep_duration=1\n",
    "    config.sample_buffer=1\n",
    "    config.time_resolution=100\n",
    "    config.init_buffer_samples=0\n",
    "    config.train_pct=0.7\n",
    "    config.val_pct=0.2\n",
    "    config.test_pct=0.1\n",
    "    config.K=36\n",
    "    config.F=8\n",
    "    config.c=10\n",
    "    config.b=10\n",
    "    config.h=1\n",
    "    config.seed=42\n",
    "    config.normalize=True\n",
    "    config.fully_connected_edges=True\n",
    "\n",
    "    config.max_checkpts_to_keep = 2\n",
    "\n",
    "    # Optimizer.\n",
    "    # config.optimizer = \"adam\"\n",
    "    config.optimizer = trial.suggest_categorical(\"optimizer\", [\"adam\", \"sgd\"])\n",
    "    config.learning_rate = trial.suggest_float('learning_rate', 3e-4, 3e-3, \n",
    "                                               log=True)\n",
    "    if config.optimizer == \"sgd\":\n",
    "        config.momentum = trial.suggest_float('momentum', 0, 0.999) # upper bound is inclusive, and we want to exclude a momentum of 1 because that would yield no decay \n",
    "\n",
    "    # Training hyperparameters.\n",
    "    config.batch_size = 1 # variable currently not used\n",
    "    config.epochs = 200\n",
    "    config.log_every_epochs = 5\n",
    "    config.eval_every_epochs = 5\n",
    "    config.checkpoint_every_epochs = 10\n",
    "\n",
    "    # GNN hyperparameters.\n",
    "    config.model = 'MLPGraphNetwork'\n",
    "    config.n_blocks = trial.suggest_int('n_blocks', 1, 3)\n",
    "    config.share_params = False\n",
    "    config.dropout_rate = trial.suggest_float('dropout_rate', 0, 0.2)\n",
    "    config.skip_connections = False # This was throwing a broadcast error in add_graphs_tuples_nodes when this was set to True\n",
    "    config.layer_norm = False # TODO perhaps we want to turn on later\n",
    "    config.activation = trial.suggest_categorical(\n",
    "        'activation', [\"relu\", \"elu\", \"leaky_relu\"])\n",
    "    # config.activation = \"leaky_relu\"\n",
    "    \n",
    "    config.pred_x1 = True\n",
    "    config.pred_x2 = True\n",
    "\n",
    "    if config.pred_x1 and config.pred_x2:\n",
    "        output_layer = 2\n",
    "    else:\n",
    "        output_layer = 1\n",
    "\n",
    "    # choose the hidden layer feature size using powers of 2 \n",
    "    config.edge_features = (\n",
    "        2**trial.suggest_int(\"edge_mlp_1_power\", 1, 3), # range 2 - 8; upper bound is inclusive\n",
    "        2**trial.suggest_int(\"edge_mlp_2_power\", 1, 3), # range 2 - 8\n",
    "    )\n",
    "    config.node_features = (\n",
    "        2**trial.suggest_int(\"node_mlp_1_power\", 1, 6), \n",
    "        # 2**trial.suggest_int(\"node_mlp_2_power\", 1, 6), \n",
    "        output_layer) \n",
    "    # note the last feature size will be the number of features that the graph predicts\n",
    "    config.global_features = None\n",
    "\n",
    "    # generate a workdir \n",
    "    # TODO: check if we actually care about referencing this in the future or if we can just create a temp dir \n",
    "    workdir=os.path.join(CHECKPOINT_PATH, study_name, f\"trial_{trial.number}\")\n",
    "\n",
    "    # run training \n",
    "    state, train_metrics, eval_metrics_dict = train_and_evaluate_with_data(config=config, workdir=workdir, datasets=datasets, trial=trial)\n",
    "    \n",
    "    # retrieve and return val loss (MSE)\n",
    "    return eval_metrics_dict['val'].compute()['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dataset \n",
    "datasets = create_dataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_study(study_name):\n",
    "    # get the objective function that reuses the pre-generated datasets \n",
    "    objective_partial = partial(objective, study_name=study_name, \n",
    "                                datasets=datasets)\n",
    "\n",
    "    # run optimization study\n",
    "    db_path = os.path.join(CHECKPOINT_PATH, study_name, \"optuna_hparam_search.db\")\n",
    "    if not os.path.exists(os.path.join(CHECKPOINT_PATH, study_name)):\n",
    "        os.makedirs(os.path.join(CHECKPOINT_PATH, study_name))\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name,\n",
    "        storage=f'sqlite:///{db_path}', # generates a new db if it doesn't exist\n",
    "        direction='minimize',\n",
    "        pruner=optuna.pruners.MedianPruner(\n",
    "            n_startup_trials=5, \n",
    "            n_warmup_steps=50,\n",
    "            ), \n",
    "        load_if_exists=True, \n",
    "    )\n",
    "    \n",
    "    return study, objective_partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the study. Then run the optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-20 15:32:57,100] Using an existing study with name 'demo' instead of creating a new one.\n"
     ]
    }
   ],
   "source": [
    "# create the study\n",
    "study, objective_partial = prepare_study(study_name=\"demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn off logging because it'll get annoying \n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the optimization \n",
    "study.optimize(objective_partial, \n",
    "                n_trials=5-len(study.trials), \n",
    "                n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can look at the different trials in the study and choose the best one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FrozenTrial(number=0, state=2, values=None, datetime_start=datetime.datetime(2024, 2, 12, 19, 55, 15, 475289), datetime_complete=datetime.datetime(2024, 2, 12, 19, 55, 17, 135231), params={'optimizer': 'sgd', 'learning_rate': 0.0009728772859336493, 'momentum': 0.8671627457759022, 'n_blocks': 2, 'dropout_rate': 0.1697621609916617, 'activation': 'relu', 'edge_mlp_1_power': 2, 'edge_mlp_2_power': 2, 'node_mlp_1_power': 6}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'optimizer': CategoricalDistribution(choices=('adam', 'sgd')), 'learning_rate': FloatDistribution(high=0.003, log=True, low=0.0003, step=None), 'momentum': FloatDistribution(high=0.999, log=False, low=0.0, step=None), 'n_blocks': IntDistribution(high=3, log=False, low=1, step=1), 'dropout_rate': FloatDistribution(high=0.2, log=False, low=0.0, step=None), 'activation': CategoricalDistribution(choices=('relu', 'elu', 'leaky_relu')), 'edge_mlp_1_power': IntDistribution(high=3, log=False, low=1, step=1), 'edge_mlp_2_power': IntDistribution(high=3, log=False, low=1, step=1), 'node_mlp_1_power': IntDistribution(high=6, log=False, low=1, step=1)}, trial_id=1, value=None),\n",
       " FrozenTrial(number=1, state=1, values=[7.0917510986328125], datetime_start=datetime.datetime(2024, 2, 12, 19, 55, 17, 139306), datetime_complete=datetime.datetime(2024, 2, 12, 19, 55, 18, 368273), params={'optimizer': 'adam', 'learning_rate': 0.0016255146777392404, 'n_blocks': 1, 'dropout_rate': 0.19430754462445998, 'activation': 'leaky_relu', 'edge_mlp_1_power': 2, 'edge_mlp_2_power': 3, 'node_mlp_1_power': 2}, user_attrs={}, system_attrs={}, intermediate_values={0: 91.2734375, 1: 14.511066436767578, 2: 8.264938354492188, 3: 7.0917510986328125}, distributions={'optimizer': CategoricalDistribution(choices=('adam', 'sgd')), 'learning_rate': FloatDistribution(high=0.003, log=True, low=0.0003, step=None), 'n_blocks': IntDistribution(high=3, log=False, low=1, step=1), 'dropout_rate': FloatDistribution(high=0.2, log=False, low=0.0, step=None), 'activation': CategoricalDistribution(choices=('relu', 'elu', 'leaky_relu')), 'edge_mlp_1_power': IntDistribution(high=3, log=False, low=1, step=1), 'edge_mlp_2_power': IntDistribution(high=3, log=False, low=1, step=1), 'node_mlp_1_power': IntDistribution(high=6, log=False, low=1, step=1)}, trial_id=2, value=None),\n",
       " FrozenTrial(number=2, state=2, values=None, datetime_start=datetime.datetime(2024, 2, 12, 19, 55, 18, 378008), datetime_complete=datetime.datetime(2024, 2, 12, 19, 55, 20, 951499), params={'optimizer': 'sgd', 'learning_rate': 0.00037519463234142865, 'momentum': 0.4681936787154971, 'n_blocks': 3, 'dropout_rate': 0.1546680551646304, 'activation': 'elu', 'edge_mlp_1_power': 2, 'edge_mlp_2_power': 1, 'node_mlp_1_power': 3}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'optimizer': CategoricalDistribution(choices=('adam', 'sgd')), 'learning_rate': FloatDistribution(high=0.003, log=True, low=0.0003, step=None), 'momentum': FloatDistribution(high=0.999, log=False, low=0.0, step=None), 'n_blocks': IntDistribution(high=3, log=False, low=1, step=1), 'dropout_rate': FloatDistribution(high=0.2, log=False, low=0.0, step=None), 'activation': CategoricalDistribution(choices=('relu', 'elu', 'leaky_relu')), 'edge_mlp_1_power': IntDistribution(high=3, log=False, low=1, step=1), 'edge_mlp_2_power': IntDistribution(high=3, log=False, low=1, step=1), 'node_mlp_1_power': IntDistribution(high=6, log=False, low=1, step=1)}, trial_id=3, value=None),\n",
       " FrozenTrial(number=3, state=2, values=None, datetime_start=datetime.datetime(2024, 2, 12, 19, 55, 20, 958056), datetime_complete=datetime.datetime(2024, 2, 12, 19, 55, 23, 395223), params={'optimizer': 'sgd', 'learning_rate': 0.0011230270487522771, 'momentum': 0.6779592918627833, 'n_blocks': 3, 'dropout_rate': 0.08356725498384746, 'activation': 'relu', 'edge_mlp_1_power': 1, 'edge_mlp_2_power': 1, 'node_mlp_1_power': 2}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'optimizer': CategoricalDistribution(choices=('adam', 'sgd')), 'learning_rate': FloatDistribution(high=0.003, log=True, low=0.0003, step=None), 'momentum': FloatDistribution(high=0.999, log=False, low=0.0, step=None), 'n_blocks': IntDistribution(high=3, log=False, low=1, step=1), 'dropout_rate': FloatDistribution(high=0.2, log=False, low=0.0, step=None), 'activation': CategoricalDistribution(choices=('relu', 'elu', 'leaky_relu')), 'edge_mlp_1_power': IntDistribution(high=3, log=False, low=1, step=1), 'edge_mlp_2_power': IntDistribution(high=3, log=False, low=1, step=1), 'node_mlp_1_power': IntDistribution(high=6, log=False, low=1, step=1)}, trial_id=4, value=None),\n",
       " FrozenTrial(number=4, state=1, values=[7.212490081787109], datetime_start=datetime.datetime(2024, 2, 12, 19, 55, 23, 401437), datetime_complete=datetime.datetime(2024, 2, 12, 19, 55, 26, 766023), params={'optimizer': 'adam', 'learning_rate': 0.0015430423185754743, 'n_blocks': 3, 'dropout_rate': 0.13720715640387193, 'activation': 'elu', 'edge_mlp_1_power': 2, 'edge_mlp_2_power': 3, 'node_mlp_1_power': 1}, user_attrs={}, system_attrs={}, intermediate_values={0: 8.622615814208984, 1: 7.058374404907227, 2: 7.1919379234313965, 3: 7.212490081787109}, distributions={'optimizer': CategoricalDistribution(choices=('adam', 'sgd')), 'learning_rate': FloatDistribution(high=0.003, log=True, low=0.0003, step=None), 'n_blocks': IntDistribution(high=3, log=False, low=1, step=1), 'dropout_rate': FloatDistribution(high=0.2, log=False, low=0.0, step=None), 'activation': CategoricalDistribution(choices=('relu', 'elu', 'leaky_relu')), 'edge_mlp_1_power': IntDistribution(high=3, log=False, low=1, step=1), 'edge_mlp_2_power': IntDistribution(high=3, log=False, low=1, step=1), 'node_mlp_1_power': IntDistribution(high=6, log=False, low=1, step=1)}, trial_id=5, value=None),\n",
       " FrozenTrial(number=5, state=1, values=[5.675701141357422], datetime_start=datetime.datetime(2024, 4, 17, 19, 42, 30, 737299), datetime_complete=datetime.datetime(2024, 4, 17, 19, 42, 36, 190572), params={'optimizer': 'adam', 'learning_rate': 0.0022191842993946613, 'n_blocks': 2, 'dropout_rate': 0.017341921943460136, 'activation': 'elu', 'edge_mlp_1_power': 3, 'edge_mlp_2_power': 2, 'node_mlp_1_power': 1}, user_attrs={}, system_attrs={}, intermediate_values={0: 18.896671295166016, 5: 5.985725402832031, 10: 5.970047950744629, 15: 5.950084209442139, 20: 5.928310871124268, 25: 5.905567169189453, 30: 5.882701873779297, 35: 5.861005783081055, 40: 5.83963680267334, 45: 5.818334579467773, 50: 5.798172473907471, 55: 5.778642654418945, 60: 5.762735843658447, 65: 5.74927282333374, 70: 5.739699840545654, 75: 5.728734970092773, 80: 5.719630241394043, 85: 5.7141194343566895, 90: 5.70828914642334, 95: 5.701236724853516, 100: 5.696618556976318, 105: 5.694576740264893, 110: 5.692327499389648, 115: 5.687554836273193, 120: 5.686495780944824, 125: 5.68486213684082, 130: 5.685214042663574, 135: 5.686432361602783, 140: 5.6840434074401855, 145: 5.680877685546875, 150: 5.680103778839111, 155: 5.680282115936279, 160: 5.680637836456299, 165: 5.679215908050537, 170: 5.6811113357543945, 175: 5.681423664093018, 180: 5.6826677322387695, 185: 5.679559707641602, 190: 5.680102825164795, 195: 5.679079055786133, 199: 5.675701141357422}, distributions={'optimizer': CategoricalDistribution(choices=('adam', 'sgd')), 'learning_rate': FloatDistribution(high=0.003, log=True, low=0.0003, step=None), 'n_blocks': IntDistribution(high=3, log=False, low=1, step=1), 'dropout_rate': FloatDistribution(high=0.2, log=False, low=0.0, step=None), 'activation': CategoricalDistribution(choices=('relu', 'elu', 'leaky_relu')), 'edge_mlp_1_power': IntDistribution(high=3, log=False, low=1, step=1), 'edge_mlp_2_power': IntDistribution(high=3, log=False, low=1, step=1), 'node_mlp_1_power': IntDistribution(high=6, log=False, low=1, step=1)}, trial_id=6, value=None),\n",
       " FrozenTrial(number=6, state=2, values=None, datetime_start=datetime.datetime(2024, 4, 17, 19, 42, 36, 201257), datetime_complete=datetime.datetime(2024, 4, 17, 19, 42, 37, 961327), params={'optimizer': 'sgd', 'learning_rate': 0.0015832208719263653, 'momentum': 0.25488423261894366, 'n_blocks': 2, 'dropout_rate': 0.05336724543453597, 'activation': 'leaky_relu', 'edge_mlp_1_power': 2, 'edge_mlp_2_power': 2, 'node_mlp_1_power': 1}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'optimizer': CategoricalDistribution(choices=('adam', 'sgd')), 'learning_rate': FloatDistribution(high=0.003, log=True, low=0.0003, step=None), 'momentum': FloatDistribution(high=0.999, log=False, low=0.0, step=None), 'n_blocks': IntDistribution(high=3, log=False, low=1, step=1), 'dropout_rate': FloatDistribution(high=0.2, log=False, low=0.0, step=None), 'activation': CategoricalDistribution(choices=('relu', 'elu', 'leaky_relu')), 'edge_mlp_1_power': IntDistribution(high=3, log=False, low=1, step=1), 'edge_mlp_2_power': IntDistribution(high=3, log=False, low=1, step=1), 'node_mlp_1_power': IntDistribution(high=6, log=False, low=1, step=1)}, trial_id=7, value=None),\n",
       " FrozenTrial(number=7, state=2, values=None, datetime_start=datetime.datetime(2024, 4, 17, 19, 42, 37, 968221), datetime_complete=datetime.datetime(2024, 4, 17, 19, 42, 38, 948373), params={'optimizer': 'sgd', 'learning_rate': 0.0008491770301799649, 'momentum': 0.4078208397466531, 'n_blocks': 1, 'dropout_rate': 0.056844393931194254, 'activation': 'elu', 'edge_mlp_1_power': 1, 'edge_mlp_2_power': 2, 'node_mlp_1_power': 2}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'optimizer': CategoricalDistribution(choices=('adam', 'sgd')), 'learning_rate': FloatDistribution(high=0.003, log=True, low=0.0003, step=None), 'momentum': FloatDistribution(high=0.999, log=False, low=0.0, step=None), 'n_blocks': IntDistribution(high=3, log=False, low=1, step=1), 'dropout_rate': FloatDistribution(high=0.2, log=False, low=0.0, step=None), 'activation': CategoricalDistribution(choices=('relu', 'elu', 'leaky_relu')), 'edge_mlp_1_power': IntDistribution(high=3, log=False, low=1, step=1), 'edge_mlp_2_power': IntDistribution(high=3, log=False, low=1, step=1), 'node_mlp_1_power': IntDistribution(high=6, log=False, low=1, step=1)}, trial_id=8, value=None),\n",
       " FrozenTrial(number=8, state=2, values=None, datetime_start=datetime.datetime(2024, 4, 17, 19, 42, 38, 953874), datetime_complete=datetime.datetime(2024, 4, 17, 19, 42, 39, 986570), params={'optimizer': 'sgd', 'learning_rate': 0.0012352324633728155, 'momentum': 0.49610673441578595, 'n_blocks': 1, 'dropout_rate': 0.16333216271077533, 'activation': 'elu', 'edge_mlp_1_power': 2, 'edge_mlp_2_power': 3, 'node_mlp_1_power': 1}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'optimizer': CategoricalDistribution(choices=('adam', 'sgd')), 'learning_rate': FloatDistribution(high=0.003, log=True, low=0.0003, step=None), 'momentum': FloatDistribution(high=0.999, log=False, low=0.0, step=None), 'n_blocks': IntDistribution(high=3, log=False, low=1, step=1), 'dropout_rate': FloatDistribution(high=0.2, log=False, low=0.0, step=None), 'activation': CategoricalDistribution(choices=('relu', 'elu', 'leaky_relu')), 'edge_mlp_1_power': IntDistribution(high=3, log=False, low=1, step=1), 'edge_mlp_2_power': IntDistribution(high=3, log=False, low=1, step=1), 'node_mlp_1_power': IntDistribution(high=6, log=False, low=1, step=1)}, trial_id=9, value=None),\n",
       " FrozenTrial(number=9, state=2, values=None, datetime_start=datetime.datetime(2024, 4, 17, 19, 42, 39, 996845), datetime_complete=datetime.datetime(2024, 4, 17, 19, 42, 41, 5186), params={'optimizer': 'sgd', 'learning_rate': 0.001969341571763617, 'momentum': 0.9124933978247781, 'n_blocks': 1, 'dropout_rate': 0.09481428258279015, 'activation': 'relu', 'edge_mlp_1_power': 3, 'edge_mlp_2_power': 1, 'node_mlp_1_power': 6}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'optimizer': CategoricalDistribution(choices=('adam', 'sgd')), 'learning_rate': FloatDistribution(high=0.003, log=True, low=0.0003, step=None), 'momentum': FloatDistribution(high=0.999, log=False, low=0.0, step=None), 'n_blocks': IntDistribution(high=3, log=False, low=1, step=1), 'dropout_rate': FloatDistribution(high=0.2, log=False, low=0.0, step=None), 'activation': CategoricalDistribution(choices=('relu', 'elu', 'leaky_relu')), 'edge_mlp_1_power': IntDistribution(high=3, log=False, low=1, step=1), 'edge_mlp_2_power': IntDistribution(high=3, log=False, low=1, step=1), 'node_mlp_1_power': IntDistribution(high=6, log=False, low=1, step=1)}, trial_id=10, value=None)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.hyperparam_tuning import get_best_trial_config, get_best_trial_workdir\n",
    "from utils.jraph_vis import plot_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint directory: /Users/miamirabelli/Desktop/GNN Research/lorenzGNN/experiments/tuning/demo/trial_5/checkpoints\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (480,) and (4,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/miamirabelli/Desktop/GNN Research/lorenzGNN/demo.ipynb Cell 94\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/demo.ipynb#Y162sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m plot_predictions(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/demo.ipynb#Y162sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     config\u001b[39m=\u001b[39;49mget_best_trial_config(study\u001b[39m=\u001b[39;49mstudy),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/demo.ipynb#Y162sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     workdir\u001b[39m=\u001b[39;49mget_best_trial_workdir(study\u001b[39m=\u001b[39;49mstudy), \u001b[39m# for loading checkpoints \u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/demo.ipynb#Y162sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     plot_ith_rollout_step\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, \u001b[39m# 0 indexed # for this study, we have a 4-step rollout \u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/demo.ipynb#Y162sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# dataset,\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/demo.ipynb#Y162sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# preds,\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/demo.ipynb#Y162sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# timestep_duration,\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/demo.ipynb#Y162sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# n_rollout_steps,\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/demo.ipynb#Y162sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m#  total_steps,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/demo.ipynb#Y162sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     node\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, \u001b[39m# 0-indexed \u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/demo.ipynb#Y162sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     plot_mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mval\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m# i.e. \"train\"/\"val\"/\"test\"\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/demo.ipynb#Y162sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     datasets\u001b[39m=\u001b[39;49mdatasets,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/demo.ipynb#Y162sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     plot_days\u001b[39m=\u001b[39;49m\u001b[39m60\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/demo.ipynb#Y162sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     title\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mVal Predictions, rollout step 0\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/demo.ipynb#Y162sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/utils/jraph_vis.py:233\u001b[0m, in \u001b[0;36mplot_predictions\u001b[0;34m(config, workdir, plot_ith_rollout_step, node, plot_mode, datasets, plot_days, title)\u001b[0m\n\u001b[1;32m    224\u001b[0m fig, (ax0, ax1) \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m15\u001b[39m, \u001b[39m10\u001b[39m), sharex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \n\u001b[1;32m    225\u001b[0m \u001b[39m# default dimensions are in units\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \n\u001b[1;32m    227\u001b[0m \u001b[39m# TODO do we even care about plotting inputs? \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    231\u001b[0m \n\u001b[1;32m    232\u001b[0m \u001b[39m# plot rollout targets\u001b[39;00m\n\u001b[0;32m--> 233\u001b[0m ax0\u001b[39m.\u001b[39;49mplot(\n\u001b[1;32m    234\u001b[0m     t_days,\n\u001b[1;32m    235\u001b[0m     node_targets[:, \u001b[39m0\u001b[39;49m],\n\u001b[1;32m    236\u001b[0m     \u001b[39m# s=5,\u001b[39;49;00m\n\u001b[1;32m    237\u001b[0m     alpha\u001b[39m=\u001b[39;49m\u001b[39m0.8\u001b[39;49m,\n\u001b[1;32m    238\u001b[0m     linewidth\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,\n\u001b[1;32m    239\u001b[0m     label\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mTarget\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    240\u001b[0m     c\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m#7170b5\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    241\u001b[0m ax1\u001b[39m.\u001b[39mplot(\n\u001b[1;32m    242\u001b[0m     t_days,\n\u001b[1;32m    243\u001b[0m     node_targets[:, \u001b[39m1\u001b[39m], \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    246\u001b[0m     label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTarget\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m    247\u001b[0m     c\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m#7170b5\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    249\u001b[0m \u001b[39m# plot predictions\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/matplotlib/axes/_axes.py:1662\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1421\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1659\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1660\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1661\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[0;32m-> 1662\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[1;32m   1663\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[1;32m   1664\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/matplotlib/axes/_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[1;32m    310\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 311\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(\n\u001b[1;32m    312\u001b[0m     this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39;49mambiguous_fmt_datakey)\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/matplotlib/axes/_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39mupdate_units(y)\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[0;32m--> 504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y must have same first dimension, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhave shapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    506\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    507\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y can be no greater than 2D, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (480,) and (4,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions(\n",
    "    config=get_best_trial_config(study=study),\n",
    "    workdir=get_best_trial_workdir(study=study), # for loading checkpoints \n",
    "    plot_ith_rollout_step=0, # 0 indexed # for this study, we have a 4-step rollout \n",
    "    # dataset,\n",
    "    # preds,\n",
    "    # timestep_duration,\n",
    "    # n_rollout_steps,\n",
    "    #  total_steps,\n",
    "    node=0, # 0-indexed \n",
    "    plot_mode=\"val\", # i.e. \"train\"/\"val\"/\"test\"\n",
    "    datasets=datasets,\n",
    "    #plot_days=60,\n",
    "    title=\"Val Predictions, rollout step 0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library documentation that I had to reference a lot: \n",
    "\n",
    "* [jraph](https://jraph.readthedocs.io/en/latest/index.html)\n",
    "* [jax](https://jax.readthedocs.io/en/latest/index.html)\n",
    "* [flax](https://flax.readthedocs.io/en/latest/)\n",
    "* [optuna](https://optuna.readthedocs.io/en/stable/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lorenzvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
