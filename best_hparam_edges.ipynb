{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDGES STATISTICAL SIGNIFICANCE TEST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will determine whether including edges in the graph structure is statistically significant. I hope it is.\n",
    "\n",
    "First, we must generate our data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create and train a total of 20 models -- 10 of which with a total of 180 edges (ie 5 edegs connecting each node), 10 of which with 36 edges (no edges connecting each node, only a self-loop). For each epoch in 100 epochs, we will record the training loss. Then, we will take the average of the last 10 epochs of the list of losses for each model. We will create a list of the average epoch loss for every model to use in our t-test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a note, all configs will have the same hyperparameters, as determined in chem_talk_figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe Jupyter notebook server failed to launch in time. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ipython extension to autoreload imported modules so that any changes will be up to date before running code in this nb\n",
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe Jupyter notebook server failed to launch in time. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import ml_collections\n",
    "\n",
    "from utils.jraph_training import train_and_evaluate_with_data, create_dataset, evaluate_model, rollout\n",
    "# from utils.jraph_models import MLPGraphNetwork\n",
    "from utils.jraph_data import print_graph_fts\n",
    "from utils.jraph_vis import plot_predictions\n",
    "import optuna \n",
    "from flax import linen as nn\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "import os \n",
    "import tempfile\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import jax.numpy as jnp\n",
    "import jraph\n",
    "\n",
    "import jax\n",
    "from utils.jraph_training import create_dataset, create_model, create_optimizer\n",
    "from clu import checkpoint\n",
    "from clu import parameter_overview\n",
    "from flax.training import train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_seeds = [65, 66, 67, 68, 69, 70, 80, 81, 83, 84]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_config5(seed, connected=None):\n",
    "    config = ml_collections.ConfigDict()\n",
    "\n",
    "    # Data params. \n",
    "    config.n_samples=10000\n",
    "    config.input_steps=1\n",
    "    config.output_delay=0 # predict 0 hours into the future\n",
    "    config.output_steps=6\n",
    "    config.timestep_duration=3\n",
    "    config.sample_buffer=-1 * (config.input_steps + config.output_delay + config.output_steps - 1) # negative buffer so that our sample input are continuous (i.e. the first sample would overlap a bit with consecutive samples) \n",
    "    config.time_resolution=120\n",
    "    config.init_buffer_samples=0\n",
    "    config.train_pct=0.7\n",
    "    config.val_pct=0.2\n",
    "    config.test_pct=0.1\n",
    "    config.K=36\n",
    "    config.F=8\n",
    "    config.c=10\n",
    "    config.b=10\n",
    "    config.h=1\n",
    "    config.seed=good_seeds[seed]\n",
    "    config.normalize=True\n",
    "    config.fully_connected_edges=connected\n",
    "\n",
    "    # Optimizer.\n",
    "    config.optimizer = 'sgd'\n",
    "    config.learning_rate = 0.00045346796177033903\n",
    "    config.momentum = 0.8712873602503628\n",
    "\n",
    "    # Training hyperparameters.\n",
    "    # config.batch_size = 3\n",
    "    config.epochs = 200\n",
    "    config.log_every_epochs = 1\n",
    "    config.eval_every_epochs = 1\n",
    "    config.checkpoint_every_epochs = 10\n",
    "    config.max_checkpts_to_keep = None # None means keep all checkpoints\n",
    "\n",
    "    # GNN hyperparameters.\n",
    "    config.model = 'MLPGraphNetwork'\n",
    "    config.n_blocks = 1\n",
    "    config.activation = 'relu'\n",
    "    config.dropout_rate = 0.013287043114620523\n",
    "    config.skip_connections = False # This was throwing a broadcast error in add_graphs_tuples_nodes when this was set to True\n",
    "    config.layer_norm = False # TODO perhaps we want to turn on later\n",
    "    config.edge_features = (8, 8) # the last feature size will be the number of features that the graph predicts\n",
    "    config.node_features = (32, 2)\n",
    "    config.global_features = None\n",
    "    config.share_params = False\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_config3(seed, connected=None):\n",
    "    config = ml_collections.ConfigDict()\n",
    "\n",
    "    # Data params. \n",
    "    config.n_samples=10000\n",
    "    config.input_steps=1\n",
    "    config.output_delay=0 # predict 0 hours into the future\n",
    "    config.output_steps=6\n",
    "    config.timestep_duration=3\n",
    "    config.sample_buffer=-1 * (config.input_steps + config.output_delay + config.output_steps - 1) # negative buffer so that our sample input are continuous (i.e. the first sample would overlap a bit with consecutive samples) \n",
    "    config.time_resolution=120\n",
    "    config.init_buffer_samples=0\n",
    "    config.train_pct=0.7\n",
    "    config.val_pct=0.2\n",
    "    config.test_pct=0.1\n",
    "    config.K=36\n",
    "    config.F=8\n",
    "    config.c=10\n",
    "    config.b=10\n",
    "    config.h=1\n",
    "    config.seed=good_seeds[seed]\n",
    "    config.normalize=True\n",
    "    config.fully_connected_edges=connected\n",
    "\n",
    "    # Optimizer.\n",
    "    config.optimizer = 'sgd'\n",
    "    config.learning_rate = 0.00045346796177033903\n",
    "    config.momentum = 0.8712873602503628\n",
    "\n",
    "    # Training hyperparameters.\n",
    "    # config.batch_size = 3\n",
    "    config.epochs = 200\n",
    "    config.log_every_epochs = 1\n",
    "    config.eval_every_epochs = 1\n",
    "    config.checkpoint_every_epochs = 10\n",
    "    config.max_checkpts_to_keep = 2 # None means keep all checkpoints\n",
    "\n",
    "    # GNN hyperparameters.\n",
    "    config.model = 'MLPGraphNetwork'\n",
    "    config.n_blocks = 1\n",
    "    config.activation = 'relu'\n",
    "    config.dropout_rate = 0.013287043114620523\n",
    "    config.skip_connections = False # This was throwing a broadcast error in add_graphs_tuples_nodes when this was set to True\n",
    "    config.layer_norm = False # TODO perhaps we want to turn on later\n",
    "    config.edge_features = (4, 4) # the last feature size will be the number of features that the graph predicts\n",
    "    config.node_features = (64, 2)\n",
    "    config.global_features = None\n",
    "    config.share_params = False\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_config7(seed, connected=None):\n",
    "    config = ml_collections.ConfigDict()\n",
    "\n",
    "    # Data params. \n",
    "    config.n_samples=10000\n",
    "    config.input_steps=1\n",
    "    config.output_delay=0 # predict 0 hours into the future\n",
    "    config.output_steps=6\n",
    "    config.timestep_duration=3\n",
    "    config.sample_buffer=-1 * (config.input_steps + config.output_delay + config.output_steps - 1) # negative buffer so that our sample input are continuous (i.e. the first sample would overlap a bit with consecutive samples) \n",
    "    config.time_resolution=120\n",
    "    config.init_buffer_samples=0\n",
    "    config.train_pct=0.7\n",
    "    config.val_pct=0.2\n",
    "    config.test_pct=0.1\n",
    "    config.K=36\n",
    "    config.F=8\n",
    "    config.c=10\n",
    "    config.b=10\n",
    "    config.h=1\n",
    "    config.seed=good_seeds[seed]\n",
    "    config.normalize=True\n",
    "    config.fully_connected_edges=connected\n",
    "\n",
    "    # Optimizer.\n",
    "    config.optimizer = 'sgd'\n",
    "    config.learning_rate = 0.00045346796177033903\n",
    "    config.momentum = 0.8712873602503628\n",
    "\n",
    "    # Training hyperparameters.\n",
    "    # config.batch_size = 3\n",
    "    config.epochs = 200\n",
    "    config.log_every_epochs = 1\n",
    "    config.eval_every_epochs = 1\n",
    "    config.checkpoint_every_epochs = 10\n",
    "    config.max_checkpts_to_keep = 2 # None means keep all checkpoints\n",
    "\n",
    "    # GNN hyperparameters.\n",
    "    config.model = 'MLPGraphNetwork'\n",
    "    config.n_blocks = 1\n",
    "    config.activation = 'relu'\n",
    "    config.dropout_rate = 0.013287043114620523\n",
    "    config.skip_connections = False # This was throwing a broadcast error in add_graphs_tuples_nodes when this was set to True\n",
    "    config.layer_norm = False # TODO perhaps we want to turn on later\n",
    "    config.edge_features = (4, 2) # the last feature size will be the number of features that the graph predicts\n",
    "    config.node_features = (16, 2)\n",
    "    config.global_features = None\n",
    "    config.share_params = False\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_config1(seed, connected=None):\n",
    "    config = ml_collections.ConfigDict()\n",
    "\n",
    "    # Data params. \n",
    "    config.n_samples=10000\n",
    "    config.input_steps=1\n",
    "    config.output_delay=0 # predict 0 hours into the future\n",
    "    config.output_steps=6\n",
    "    config.timestep_duration=3\n",
    "    config.sample_buffer=-1 * (config.input_steps + config.output_delay + config.output_steps - 1) # negative buffer so that our sample input are continuous (i.e. the first sample would overlap a bit with consecutive samples) \n",
    "    config.time_resolution=120\n",
    "    config.init_buffer_samples=0\n",
    "    config.train_pct=0.7\n",
    "    config.val_pct=0.2\n",
    "    config.test_pct=0.1\n",
    "    config.K=36\n",
    "    config.F=8\n",
    "    config.c=10\n",
    "    config.b=10\n",
    "    config.h=1\n",
    "    config.seed=good_seeds[seed]\n",
    "    config.normalize=True\n",
    "    config.fully_connected_edges=connected\n",
    "\n",
    "    # Optimizer.\n",
    "    config.optimizer = 'sgd'\n",
    "    config.learning_rate = 0.00045346796177033903\n",
    "    config.momentum = 0.8712873602503628\n",
    "\n",
    "    # Training hyperparameters.\n",
    "    # config.batch_size = 3\n",
    "    config.epochs = 200\n",
    "    config.log_every_epochs = 1\n",
    "    config.eval_every_epochs = 1\n",
    "    config.checkpoint_every_epochs = 10\n",
    "    config.max_checkpts_to_keep = 2 # None means keep all checkpoints\n",
    "\n",
    "    # GNN hyperparameters.\n",
    "    config.model = 'MLPGraphNetwork'\n",
    "    config.n_blocks = 1\n",
    "    config.activation = 'relu'\n",
    "    config.dropout_rate = 0.013287043114620523\n",
    "    config.skip_connections = False # This was throwing a broadcast error in add_graphs_tuples_nodes when this was set to True\n",
    "    config.layer_norm = False # TODO perhaps we want to turn on later\n",
    "    config.edge_features = (8, 2) # the last feature size will be the number of features that the graph predicts\n",
    "    config.node_features = (16, 2)\n",
    "    config.global_features = None\n",
    "    config.share_params = False\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_configs_7 = []\n",
    "connected_configs_5 = []\n",
    "connected_configs_3 = []\n",
    "solo_configs = []\n",
    "\n",
    "for trial in range(num_trials):\n",
    "    connected_config_7 = create_config7(trial, connected=7)\n",
    "    connected_config_5 = create_config5(trial, connected=5)\n",
    "    connected_config_3 = create_config3(trial, connected=3)\n",
    "    solo_config = create_config1(trial, connected=1)\n",
    "\n",
    "    connected_configs_7.append(connected_config_7)\n",
    "    connected_configs_5.append(connected_config_5)\n",
    "    connected_configs_3.append(connected_config_3)\n",
    "    solo_configs.append(solo_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.jraph_data import get_lorenz_graph_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_connected_7_datasets = []\n",
    "all_connected_5_datasets = []\n",
    "all_connected_3_datasets = []\n",
    "all_solo_datasets = []\n",
    "\n",
    "for connected_config_7 in connected_configs_7:\n",
    "    connected_7_datasets = get_lorenz_graph_tuples(\n",
    "        n_samples=connected_config_7.n_samples,\n",
    "        input_steps=connected_config_7.input_steps,\n",
    "        output_delay=connected_config_7.output_delay,\n",
    "        output_steps=connected_config_7.output_steps,\n",
    "        timestep_duration=connected_config_7.timestep_duration,\n",
    "        sample_buffer=connected_config_7.sample_buffer,\n",
    "        time_resolution=connected_config_7.time_resolution,\n",
    "        init_buffer_samples=connected_config_7.init_buffer_samples,\n",
    "        train_pct=connected_config_7.train_pct,\n",
    "        val_pct=connected_config_7.val_pct,\n",
    "        test_pct=connected_config_7.test_pct,\n",
    "        K=connected_config_7.K,\n",
    "        F=connected_config_7.F,\n",
    "        c=connected_config_7.c,\n",
    "        b=connected_config_7.b,\n",
    "        h=connected_config_7.h,\n",
    "        seed=connected_config_7.seed,\n",
    "        normalize=connected_config_7.normalize,\n",
    "        fully_connected_edges=connected_config_7.fully_connected_edges)\n",
    "    all_connected_7_datasets.append(connected_7_datasets)\n",
    "    \n",
    "for connected_config_5 in connected_configs_5:\n",
    "    connected_5_datasets = get_lorenz_graph_tuples(\n",
    "        n_samples=connected_config_5.n_samples,\n",
    "        input_steps=connected_config_5.input_steps,\n",
    "        output_delay=connected_config_5.output_delay,\n",
    "        output_steps=connected_config_5.output_steps,\n",
    "        timestep_duration=connected_config_5.timestep_duration,\n",
    "        sample_buffer=connected_config_5.sample_buffer,\n",
    "        time_resolution=connected_config_5.time_resolution,\n",
    "        init_buffer_samples=connected_config_5.init_buffer_samples,\n",
    "        train_pct=connected_config_5.train_pct,\n",
    "        val_pct=connected_config_5.val_pct,\n",
    "        test_pct=connected_config_5.test_pct,\n",
    "        K=connected_config_5.K,\n",
    "        F=connected_config_5.F,\n",
    "        c=connected_config_5.c,\n",
    "        b=connected_config_5.b,\n",
    "        h=connected_config_5.h,\n",
    "        seed=connected_config_5.seed,\n",
    "        normalize=connected_config_5.normalize,\n",
    "        fully_connected_edges=connected_config_5.fully_connected_edges)\n",
    "    all_connected_5_datasets.append(connected_5_datasets)\n",
    "\n",
    "for connected_config_3 in connected_configs_3:\n",
    "    connected_3_datasets = get_lorenz_graph_tuples(\n",
    "        n_samples=connected_config_3.n_samples,\n",
    "        input_steps=connected_config_3.input_steps,\n",
    "        output_delay=connected_config_3.output_delay,\n",
    "        output_steps=connected_config_3.output_steps,\n",
    "        timestep_duration=connected_config_3.timestep_duration,\n",
    "        sample_buffer=connected_config_3.sample_buffer,\n",
    "        time_resolution=connected_config_3.time_resolution,\n",
    "        init_buffer_samples=connected_config_3.init_buffer_samples,\n",
    "        train_pct=connected_config_3.train_pct,\n",
    "        val_pct=connected_config_3.val_pct,\n",
    "        test_pct=connected_config_3.test_pct,\n",
    "        K=connected_config_3.K,\n",
    "        F=connected_config_3.F,\n",
    "        c=connected_config_3.c,\n",
    "        b=connected_config_3.b,\n",
    "        h=connected_config_3.h,\n",
    "        seed=connected_config_3.seed,\n",
    "        normalize=connected_config_3.normalize,\n",
    "        fully_connected_edges=connected_config_3.fully_connected_edges)\n",
    "    all_connected_3_datasets.append(connected_3_datasets)\n",
    "\n",
    "for solo_config in solo_configs:\n",
    "    solo_datasets = get_lorenz_graph_tuples(\n",
    "        n_samples=solo_config.n_samples,\n",
    "        input_steps=solo_config.input_steps,\n",
    "        output_delay=solo_config.output_delay,\n",
    "        output_steps=solo_config.output_steps,\n",
    "        timestep_duration=solo_config.timestep_duration,\n",
    "        sample_buffer=solo_config.sample_buffer,\n",
    "        time_resolution=solo_config.time_resolution,\n",
    "        init_buffer_samples=solo_config.init_buffer_samples,\n",
    "        train_pct=solo_config.train_pct,\n",
    "        val_pct=solo_config.val_pct,\n",
    "        test_pct=solo_config.test_pct,\n",
    "        K=solo_config.K,\n",
    "        F=solo_config.F,\n",
    "        c=solo_config.c,\n",
    "        b=solo_config.b,\n",
    "        h=solo_config.h,\n",
    "        seed=solo_config.seed,\n",
    "        normalize=solo_config.normalize,\n",
    "        fully_connected_edges=solo_config.fully_connected_edges)\n",
    "    all_solo_datasets.append(solo_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.jraph_vis import draw_jraph_graph_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_graph = all_connected_7_datasets[0]['train']['inputs'][0][0]\n",
    "draw_jraph_graph_structure(sample_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_graph = all_connected_5_datasets[0]['train']['inputs'][0][0]\n",
    "draw_jraph_graph_structure(sample_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_graph = all_connected_3_datasets[0]['train']['inputs'][0][0]\n",
    "draw_jraph_graph_structure(sample_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_graph = all_solo_datasets[0]['train']['inputs'][0][0]\n",
    "draw_jraph_graph_structure(sample_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_workdirs(trials):\n",
    "    connected_7_workdir_start = \"tests/outputs/connected_7\"\n",
    "    connected_5_workdir_start = \"tests/outputs/connected_5\"\n",
    "    connected_3_workdir_start = \"tests/outputs/connected_3\"\n",
    "    solo_workdir_start = \"tests/outputs/solo\"\n",
    "    \n",
    "    connected_7_workdirs = []\n",
    "    connected_5_workdirs = []\n",
    "    connected_3_workdirs = []\n",
    "    solo_workdirs = []\n",
    "    for i in range(trials):\n",
    "        connected_7_trial_workdir = os.path.join(connected_7_workdir_start, f\"trial-{i}\")\n",
    "        connected_5_trial_workdir = os.path.join(connected_5_workdir_start, f\"trial-{i}\")\n",
    "        connected_3_trial_workdir = os.path.join(connected_3_workdir_start, f\"trial-{i}\")\n",
    "        solo_trial_workdir = os.path.join(solo_workdir_start, f\"trial-{i}\")\n",
    "\n",
    "        connected_7_workdirs.append(connected_7_trial_workdir)\n",
    "        connected_5_workdirs.append(connected_5_trial_workdir)\n",
    "        connected_3_workdirs.append(connected_3_trial_workdir)\n",
    "        solo_workdirs.append(solo_trial_workdir)\n",
    "\n",
    "    return connected_7_workdirs, connected_5_workdirs, connected_3_workdirs, solo_workdirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_workdirs_7, connected_workdirs_5, connected_workdirs_3, solo_workdirs = create_workdirs(num_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_7_eval_metrics = []\n",
    "connected_5_eval_metrics = []\n",
    "connected_3_eval_metrics = []\n",
    "solo_eval_metrics = []\n",
    "\n",
    "all_connected_7_epoch_losses = []\n",
    "all_connected_5_epoch_losses = []\n",
    "all_connected_3_epoch_losses = []\n",
    "all_solo_epoch_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'relu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 10, 'dropout_rate': 0.013287043114620523, 'edge_features': (4, 2), 'epochs': 200, 'eval_every_epochs': 1, 'fully_connected_edges': 7, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 1, 'layer_norm': False, 'learning_rate': 0.00045346796177033903, 'log_every_epochs': 1, 'max_checkpts_to_keep': 2, 'model': 'MLPGraphNetwork', 'momentum': 0.8712873602503628, 'n_blocks': 1, 'n_samples': 10000, 'node_features': (16, 2), 'normalize': True, 'optimizer': 'sgd', 'output_delay': 0, 'output_steps': 6, 'sample_buffer': -6, 'seed': 66, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 120, 'timestep_duration': 3, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+---------+------+---------+-------+\n",
      "| Name                                   | Shape   | Size | Mean    | Std   |\n",
      "+----------------------------------------+---------+------+---------+-------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (4,)    | 4    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 4)  | 24   | -0.0819 | 0.36  |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (2,)    | 2    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (4, 2)  | 8    | 0.289   | 0.469 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (16,)   | 16   | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (7, 16) | 112  | 0.0206  | 0.365 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)    | 2    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (16, 2) | 32   | -0.0397 | 0.242 |\n",
      "+----------------------------------------+---------+------+---------+-------+\n",
      "Total: 200\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Checkpoint None does not exist.\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.01s.\n",
      "INFO:absl:Starting training.\n",
      "INFO:absl:Created artifact [10] Profile of type ArtifactType.URL and value None.\n",
      "INFO:absl:[0] train_loss=2.029505491256714, train_x1_loss=1.0122774839401245, train_x2_loss=1.0172277688980103\n",
      "INFO:absl:[0] val_loss=3.047861337661743\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[0] test_loss=3.15024733543396\n",
      "INFO:absl:Checkpoint.save() finished after 0.02s.\n",
      "INFO:absl:[1] train_loss=2.000286817550659, train_x1_loss=0.9999845623970032, train_x2_loss=1.0003050565719604\n",
      "INFO:absl:[1] val_loss=3.047914743423462\n",
      "INFO:absl:[1] test_loss=3.1502981185913086\n",
      "INFO:absl:[2] train_loss=1.998591661453247, train_x1_loss=0.9982759356498718, train_x2_loss=1.0003137588500977\n",
      "INFO:absl:[2] val_loss=3.047884464263916\n",
      "INFO:absl:[2] test_loss=3.1486990451812744\n",
      "INFO:absl:[3] train_loss=2.0002918243408203, train_x1_loss=0.9999804496765137, train_x2_loss=1.0003103017807007\n",
      "INFO:absl:[3] val_loss=3.0478663444519043\n",
      "INFO:absl:[3] test_loss=3.1486518383026123\n",
      "INFO:absl:[4] train_loss=2.000271797180176, train_x1_loss=0.9999938011169434, train_x2_loss=1.000280499458313\n",
      "INFO:absl:[4] val_loss=3.048018217086792\n",
      "INFO:absl:[4] test_loss=3.1487889289855957\n",
      "INFO:absl:[5] train_loss=2.0002708435058594, train_x1_loss=0.9999635815620422, train_x2_loss=1.0003050565719604\n",
      "INFO:absl:[5] val_loss=3.047684669494629\n",
      "INFO:absl:[5] test_loss=3.1483941078186035\n",
      "INFO:absl:[6] train_loss=2.0002849102020264, train_x1_loss=0.9999800324440002, train_x2_loss=1.0003066062927246\n",
      "INFO:absl:[6] val_loss=3.04746413230896\n",
      "INFO:absl:[6] test_loss=3.148247003555298\n",
      "INFO:absl:[7] train_loss=2.0002763271331787, train_x1_loss=0.9999754428863525, train_x2_loss=1.0003002882003784\n",
      "INFO:absl:[7] val_loss=3.0475780963897705\n",
      "INFO:absl:[7] test_loss=3.148327112197876\n",
      "INFO:absl:[8] train_loss=2.0002987384796143, train_x1_loss=0.9999916553497314, train_x2_loss=1.000304102897644\n",
      "INFO:absl:[8] val_loss=3.047701120376587\n",
      "INFO:absl:[8] test_loss=3.148383617401123\n",
      "INFO:absl:[9] train_loss=2.000310182571411, train_x1_loss=1.0000053644180298, train_x2_loss=1.0003070831298828\n",
      "INFO:absl:[9] val_loss=3.048043966293335\n",
      "INFO:absl:[9] test_loss=3.1487669944763184\n",
      "INFO:absl:[10] train_loss=2.0002732276916504, train_x1_loss=0.9999718070030212, train_x2_loss=1.0003037452697754\n",
      "INFO:absl:[10] val_loss=3.0476832389831543\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[10] test_loss=3.1484034061431885\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[11] train_loss=2.0002999305725098, train_x1_loss=0.9999855756759644, train_x2_loss=1.0003132820129395\n",
      "INFO:absl:[11] val_loss=3.0476222038269043\n",
      "INFO:absl:[11] test_loss=3.1482744216918945\n",
      "INFO:absl:[12] train_loss=2.000290870666504, train_x1_loss=0.9999804496765137, train_x2_loss=1.0003095865249634\n",
      "INFO:absl:[12] val_loss=3.0477616786956787\n",
      "INFO:absl:[12] test_loss=3.1484715938568115\n",
      "INFO:absl:[13] train_loss=2.000283718109131, train_x1_loss=0.9999896287918091, train_x2_loss=1.0003008842468262\n",
      "INFO:absl:[13] val_loss=3.0477349758148193\n",
      "INFO:absl:[13] test_loss=3.1484203338623047\n",
      "INFO:absl:[14] train_loss=2.0002875328063965, train_x1_loss=0.999979555606842, train_x2_loss=1.0003094673156738\n",
      "INFO:absl:[14] val_loss=3.047637701034546\n",
      "INFO:absl:[14] test_loss=3.148350715637207\n",
      "INFO:absl:[15] train_loss=2.0002870559692383, train_x1_loss=0.999986469745636, train_x2_loss=1.0003010034561157\n",
      "INFO:absl:[15] val_loss=3.0479576587677\n",
      "INFO:absl:[15] test_loss=3.148604393005371\n",
      "INFO:absl:[16] train_loss=2.0002810955047607, train_x1_loss=0.9999809861183167, train_x2_loss=1.0003044605255127\n",
      "INFO:absl:[16] val_loss=3.0479588508605957\n",
      "INFO:absl:[16] test_loss=3.148543357849121\n",
      "INFO:absl:[17] train_loss=2.0002987384796143, train_x1_loss=0.9999869465827942, train_x2_loss=1.0003083944320679\n",
      "INFO:absl:[17] val_loss=3.0480692386627197\n",
      "INFO:absl:[17] test_loss=3.148714303970337\n",
      "INFO:absl:[18] train_loss=2.0003058910369873, train_x1_loss=1.0000033378601074, train_x2_loss=1.000300407409668\n",
      "INFO:absl:[18] val_loss=3.0478198528289795\n",
      "INFO:absl:[18] test_loss=3.14841628074646\n",
      "INFO:absl:[19] train_loss=2.0002803802490234, train_x1_loss=0.9999790191650391, train_x2_loss=1.000306248664856\n",
      "INFO:absl:[19] val_loss=3.047635793685913\n",
      "INFO:absl:[19] test_loss=3.14829158782959\n",
      "INFO:absl:[20] train_loss=2.0002756118774414, train_x1_loss=0.9999740719795227, train_x2_loss=1.0003045797348022\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[20] val_loss=3.047865390777588\n",
      "INFO:absl:[20] test_loss=3.1484551429748535\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[21] train_loss=2.0002665519714355, train_x1_loss=0.9999698400497437, train_x2_loss=1.0002974271774292\n",
      "INFO:absl:[21] val_loss=3.0479013919830322\n",
      "INFO:absl:[21] test_loss=3.1484570503234863\n",
      "INFO:absl:[22] train_loss=2.000288963317871, train_x1_loss=0.9999949932098389, train_x2_loss=1.0002920627593994\n",
      "INFO:absl:[22] val_loss=3.0478408336639404\n",
      "INFO:absl:Setting work unit notes: 2672.8 steps/s, 11.5% (161000/1400000), ETA: 7m (1m : 0.1% checkpoint, 14.6% eval)\n",
      "INFO:absl:[22] test_loss=3.148437738418579\n",
      "INFO:absl:[161000] steps_per_sec=2672.751379\n",
      "INFO:absl:[23] train_loss=2.0003294944763184, train_x1_loss=1.0000112056732178, train_x2_loss=1.0003209114074707\n",
      "INFO:absl:[23] val_loss=3.0481863021850586\n",
      "INFO:absl:[23] test_loss=3.1487209796905518\n",
      "INFO:absl:[24] train_loss=2.000288486480713, train_x1_loss=0.9999750256538391, train_x2_loss=1.0003185272216797\n",
      "INFO:absl:[24] val_loss=3.0479791164398193\n",
      "INFO:absl:[24] test_loss=3.148538589477539\n",
      "INFO:absl:[25] train_loss=2.0003020763397217, train_x1_loss=0.9999953508377075, train_x2_loss=1.000306487083435\n",
      "INFO:absl:[25] val_loss=3.0479629039764404\n",
      "INFO:absl:[25] test_loss=3.1484434604644775\n",
      "INFO:absl:[26] train_loss=2.000293493270874, train_x1_loss=0.9999876618385315, train_x2_loss=1.0003074407577515\n",
      "INFO:absl:[26] val_loss=3.0478415489196777\n",
      "INFO:absl:[26] test_loss=3.1482925415039062\n",
      "INFO:absl:[27] train_loss=2.000275135040283, train_x1_loss=0.9999830722808838, train_x2_loss=1.0002979040145874\n",
      "INFO:absl:[27] val_loss=3.0479342937469482\n",
      "INFO:absl:[27] test_loss=3.1484665870666504\n",
      "INFO:absl:[28] train_loss=2.0003068447113037, train_x1_loss=0.9999946355819702, train_x2_loss=1.0003129243850708\n",
      "INFO:absl:[28] val_loss=3.047762870788574\n",
      "INFO:absl:[28] test_loss=3.148261308670044\n",
      "INFO:absl:[29] train_loss=2.0002822875976562, train_x1_loss=0.9999892115592957, train_x2_loss=1.000294804573059\n",
      "INFO:absl:[29] val_loss=3.047832489013672\n",
      "INFO:absl:[29] test_loss=3.1482198238372803\n",
      "INFO:absl:[30] train_loss=2.0003035068511963, train_x1_loss=0.9999927282333374, train_x2_loss=1.0003119707107544\n",
      "INFO:absl:[30] val_loss=3.047729015350342\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[30] test_loss=3.1481943130493164\n",
      "INFO:absl:Checkpoint.save() finished after 0.02s.\n",
      "INFO:absl:[31] train_loss=2.000288963317871, train_x1_loss=0.9999827742576599, train_x2_loss=1.0003045797348022\n",
      "INFO:absl:[31] val_loss=3.0477612018585205\n",
      "INFO:absl:[31] test_loss=3.1482861042022705\n",
      "INFO:absl:[32] train_loss=2.0002763271331787, train_x1_loss=0.999976396560669, train_x2_loss=1.0002931356430054\n",
      "INFO:absl:[32] val_loss=3.0479848384857178\n",
      "INFO:absl:[32] test_loss=3.1483218669891357\n",
      "INFO:absl:[33] train_loss=2.0003044605255127, train_x1_loss=0.999991238117218, train_x2_loss=1.000313639640808\n",
      "INFO:absl:[33] val_loss=3.047889232635498\n",
      "INFO:absl:[33] test_loss=3.14829158782959\n",
      "INFO:absl:[34] train_loss=2.0002846717834473, train_x1_loss=0.9999894499778748, train_x2_loss=1.000295877456665\n",
      "INFO:absl:[34] val_loss=3.047851800918579\n",
      "INFO:absl:[34] test_loss=3.1482393741607666\n",
      "INFO:absl:[35] train_loss=2.000277280807495, train_x1_loss=0.999979555606842, train_x2_loss=1.0002988576889038\n",
      "INFO:absl:[35] val_loss=3.0478782653808594\n",
      "INFO:absl:[35] test_loss=3.1483161449432373\n",
      "INFO:absl:[36] train_loss=2.0002806186676025, train_x1_loss=0.9999790191650391, train_x2_loss=1.000307321548462\n",
      "INFO:absl:[36] val_loss=3.047677516937256\n",
      "INFO:absl:[36] test_loss=3.1481237411499023\n",
      "INFO:absl:[37] train_loss=2.0002758502960205, train_x1_loss=0.999986469745636, train_x2_loss=1.0002893209457397\n",
      "INFO:absl:[37] val_loss=3.0478901863098145\n",
      "INFO:absl:[37] test_loss=3.1483664512634277\n",
      "INFO:absl:[38] train_loss=2.0002665519714355, train_x1_loss=0.9999769330024719, train_x2_loss=1.0002890825271606\n",
      "INFO:absl:[38] val_loss=3.0479538440704346\n",
      "INFO:absl:[38] test_loss=3.148374557495117\n",
      "INFO:absl:[39] train_loss=2.0002665519714355, train_x1_loss=0.9999626874923706, train_x2_loss=1.0003008842468262\n",
      "INFO:absl:[39] val_loss=3.047905683517456\n",
      "INFO:absl:[39] test_loss=3.1483983993530273\n",
      "INFO:absl:[40] train_loss=2.000293254852295, train_x1_loss=0.9999849796295166, train_x2_loss=1.0003132820129395\n",
      "INFO:absl:[40] val_loss=3.0476958751678467\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[40] test_loss=3.148047685623169\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[41] train_loss=2.000298261642456, train_x1_loss=0.9999967217445374, train_x2_loss=1.0003039836883545\n",
      "INFO:absl:[41] val_loss=3.0478460788726807\n",
      "INFO:absl:[41] test_loss=3.1482579708099365\n",
      "INFO:absl:[42] train_loss=2.000271797180176, train_x1_loss=0.9999769330024719, train_x2_loss=1.0002939701080322\n",
      "INFO:absl:[42] val_loss=3.0478568077087402\n",
      "INFO:absl:[42] test_loss=3.1482479572296143\n",
      "INFO:absl:[43] train_loss=2.000283718109131, train_x1_loss=0.9999745488166809, train_x2_loss=1.000304937362671\n",
      "INFO:absl:[43] val_loss=3.047819137573242\n",
      "INFO:absl:[43] test_loss=3.1481971740722656\n",
      "INFO:absl:[44] train_loss=2.0002641677856445, train_x1_loss=0.9999725222587585, train_x2_loss=1.000292420387268\n",
      "INFO:absl:[44] val_loss=3.048016309738159\n",
      "INFO:absl:[44] test_loss=3.148397922515869\n",
      "INFO:absl:[45] train_loss=2.0002994537353516, train_x1_loss=1.000001072883606, train_x2_loss=1.0002998113632202\n",
      "INFO:absl:[45] val_loss=3.0480077266693115\n",
      "INFO:absl:[45] test_loss=3.148364543914795\n",
      "INFO:absl:[46] train_loss=2.000276565551758, train_x1_loss=0.9999863505363464, train_x2_loss=1.0002886056900024\n",
      "INFO:absl:[46] val_loss=3.048055410385132\n",
      "INFO:absl:[46] test_loss=3.148310899734497\n",
      "INFO:absl:[47] train_loss=2.000293731689453, train_x1_loss=0.9999828934669495, train_x2_loss=1.0003135204315186\n",
      "INFO:absl:[47] val_loss=3.0478532314300537\n",
      "INFO:absl:[47] test_loss=3.1481330394744873\n",
      "INFO:absl:Setting work unit notes: 2937.5 steps/s, 24.1% (337248/1400000), ETA: 6m (2m : 0.1% checkpoint, 14.7% eval)\n",
      "INFO:absl:[337248] steps_per_sec=2937.464892\n",
      "INFO:absl:[48] train_loss=2.0002853870391846, train_x1_loss=0.9999825358390808, train_x2_loss=1.0002998113632202\n",
      "INFO:absl:[48] val_loss=3.0479443073272705\n",
      "INFO:absl:[48] test_loss=3.1482760906219482\n",
      "INFO:absl:[49] train_loss=2.0002996921539307, train_x1_loss=0.9999960064888, train_x2_loss=1.000306487083435\n",
      "INFO:absl:[49] val_loss=3.0478172302246094\n",
      "INFO:absl:[49] test_loss=3.1481330394744873\n",
      "INFO:absl:[50] train_loss=2.000288486480713, train_x1_loss=0.9999884963035583, train_x2_loss=1.0002999305725098\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[50] val_loss=3.047924041748047\n",
      "INFO:absl:[50] test_loss=3.148268938064575\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[51] train_loss=2.0002732276916504, train_x1_loss=0.9999732971191406, train_x2_loss=1.000300645828247\n",
      "INFO:absl:[51] val_loss=3.0476720333099365\n",
      "INFO:absl:[51] test_loss=3.1479387283325195\n",
      "INFO:absl:[52] train_loss=2.0002763271331787, train_x1_loss=0.9999819397926331, train_x2_loss=1.000295877456665\n",
      "INFO:absl:[52] val_loss=3.0480422973632812\n",
      "INFO:absl:[52] test_loss=3.1483333110809326\n",
      "INFO:absl:[53] train_loss=2.0002870559692383, train_x1_loss=0.9999905824661255, train_x2_loss=1.0002968311309814\n",
      "INFO:absl:[53] val_loss=3.0480213165283203\n",
      "INFO:absl:[53] test_loss=3.148287296295166\n",
      "INFO:absl:[54] train_loss=2.000291109085083, train_x1_loss=0.9999833106994629, train_x2_loss=1.0003061294555664\n",
      "INFO:absl:[54] val_loss=3.0480291843414307\n",
      "INFO:absl:[54] test_loss=3.1483547687530518\n",
      "INFO:absl:[55] train_loss=2.000295877456665, train_x1_loss=0.9999938607215881, train_x2_loss=1.0003010034561157\n",
      "INFO:absl:[55] val_loss=3.0478155612945557\n",
      "INFO:absl:[55] test_loss=3.1480658054351807\n",
      "INFO:absl:[56] train_loss=2.000300884246826, train_x1_loss=0.9999829530715942, train_x2_loss=1.0003160238265991\n",
      "INFO:absl:[56] val_loss=3.0478484630584717\n",
      "INFO:absl:[56] test_loss=3.1480743885040283\n",
      "INFO:absl:[57] train_loss=2.0002894401550293, train_x1_loss=0.9999830722808838, train_x2_loss=1.0003082752227783\n",
      "INFO:absl:[57] val_loss=3.0482254028320312\n",
      "INFO:absl:[57] test_loss=3.148470878601074\n",
      "INFO:absl:[58] train_loss=2.0003247261047363, train_x1_loss=1.0000100135803223, train_x2_loss=1.0003163814544678\n",
      "INFO:absl:[58] val_loss=3.0480988025665283\n",
      "INFO:absl:[58] test_loss=3.1483092308044434\n",
      "INFO:absl:[59] train_loss=2.0002849102020264, train_x1_loss=0.99997878074646, train_x2_loss=1.0003046989440918\n",
      "INFO:absl:[59] val_loss=3.0478625297546387\n",
      "INFO:absl:[59] test_loss=3.147998809814453\n",
      "INFO:absl:[60] train_loss=2.000265121459961, train_x1_loss=0.9999713897705078, train_x2_loss=1.000298023223877\n",
      "INFO:absl:[60] val_loss=3.04781436920166\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[60] test_loss=3.1479876041412354\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[61] train_loss=2.0002732276916504, train_x1_loss=0.999984860420227, train_x2_loss=1.0002938508987427\n",
      "INFO:absl:[61] val_loss=3.0479462146759033\n",
      "INFO:absl:[61] test_loss=3.148125648498535\n",
      "INFO:absl:[62] train_loss=2.000279426574707, train_x1_loss=0.9999750852584839, train_x2_loss=1.0003010034561157\n",
      "INFO:absl:[62] val_loss=3.0479257106781006\n",
      "INFO:absl:[62] test_loss=3.1480767726898193\n",
      "INFO:absl:[63] train_loss=2.0002994537353516, train_x1_loss=0.9999911189079285, train_x2_loss=1.000311017036438\n",
      "INFO:absl:[63] val_loss=3.04780650138855\n",
      "INFO:absl:[63] test_loss=3.1480274200439453\n",
      "INFO:absl:[64] train_loss=2.0002753734588623, train_x1_loss=0.9999812245368958, train_x2_loss=1.0002968311309814\n",
      "INFO:absl:[64] val_loss=3.0478434562683105\n",
      "INFO:absl:[64] test_loss=3.1480276584625244\n",
      "INFO:absl:[65] train_loss=2.0002689361572266, train_x1_loss=0.9999790191650391, train_x2_loss=1.000295639038086\n",
      "INFO:absl:[65] val_loss=3.0478475093841553\n",
      "INFO:absl:[65] test_loss=3.147899866104126\n",
      "INFO:absl:[66] train_loss=2.000298261642456, train_x1_loss=0.999971866607666, train_x2_loss=1.0003244876861572\n",
      "INFO:absl:[66] val_loss=3.0479273796081543\n",
      "INFO:absl:[66] test_loss=3.1480135917663574\n",
      "INFO:absl:[67] train_loss=2.000272512435913, train_x1_loss=0.9999757409095764, train_x2_loss=1.000299334526062\n",
      "INFO:absl:[67] val_loss=3.0479910373687744\n",
      "INFO:absl:[67] test_loss=3.1480555534362793\n",
      "INFO:absl:[68] train_loss=2.000302314758301, train_x1_loss=0.9999951720237732, train_x2_loss=1.0003138780593872\n",
      "INFO:absl:[68] val_loss=3.0480096340179443\n",
      "INFO:absl:[68] test_loss=3.1481568813323975\n",
      "INFO:absl:[69] train_loss=2.0002810955047607, train_x1_loss=0.9999772310256958, train_x2_loss=1.0003008842468262\n",
      "INFO:absl:[69] val_loss=3.0479683876037598\n",
      "INFO:absl:[69] test_loss=3.1479289531707764\n",
      "INFO:absl:[70] train_loss=2.0002729892730713, train_x1_loss=0.9999683499336243, train_x2_loss=1.0003082752227783\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[70] val_loss=3.048092842102051\n",
      "INFO:absl:[70] test_loss=3.148153305053711\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[71] train_loss=2.000304698944092, train_x1_loss=0.9999926686286926, train_x2_loss=1.0003106594085693\n",
      "INFO:absl:[71] val_loss=3.04803729057312\n",
      "INFO:absl:[71] test_loss=3.148066520690918\n",
      "INFO:absl:[72] train_loss=2.0003037452697754, train_x1_loss=0.9999978542327881, train_x2_loss=1.0003015995025635\n",
      "INFO:absl:[72] val_loss=3.0478477478027344\n",
      "INFO:absl:[72] test_loss=3.1479878425598145\n",
      "INFO:absl:[73] train_loss=2.0002803802490234, train_x1_loss=0.9999717473983765, train_x2_loss=1.00031316280365\n",
      "INFO:absl:[73] val_loss=3.0478529930114746\n",
      "INFO:absl:[73] test_loss=3.1478021144866943\n",
      "INFO:absl:Setting work unit notes: 2997.3 steps/s, 37.0% (518000/1400000), ETA: 4m (3m : 0.1% checkpoint, 14.6% eval)\n",
      "INFO:absl:[518000] steps_per_sec=2997.342764\n",
      "INFO:absl:[74] train_loss=2.0002715587615967, train_x1_loss=0.9999759793281555, train_x2_loss=1.0002996921539307\n",
      "INFO:absl:[74] val_loss=3.047823429107666\n",
      "INFO:absl:[74] test_loss=3.1478238105773926\n",
      "INFO:absl:[75] train_loss=2.000272035598755, train_x1_loss=0.9999809861183167, train_x2_loss=1.000291109085083\n",
      "INFO:absl:[75] val_loss=3.047926187515259\n",
      "INFO:absl:[75] test_loss=3.147904396057129\n",
      "INFO:absl:[76] train_loss=2.0002787113189697, train_x1_loss=0.9999841451644897, train_x2_loss=1.0002999305725098\n",
      "INFO:absl:[76] val_loss=3.047931432723999\n",
      "INFO:absl:[76] test_loss=3.1479556560516357\n",
      "INFO:absl:[77] train_loss=2.00028395652771, train_x1_loss=0.9999803900718689, train_x2_loss=1.0002988576889038\n",
      "INFO:absl:[77] val_loss=3.0481395721435547\n",
      "INFO:absl:[77] test_loss=3.1481311321258545\n",
      "INFO:absl:[78] train_loss=2.0002808570861816, train_x1_loss=0.9999803900718689, train_x2_loss=1.0002989768981934\n",
      "INFO:absl:[78] val_loss=3.047886610031128\n",
      "INFO:absl:[78] test_loss=3.1478021144866943\n",
      "INFO:absl:[79] train_loss=2.0002686977386475, train_x1_loss=0.9999786019325256, train_x2_loss=1.0002890825271606\n",
      "INFO:absl:[79] val_loss=3.0478458404541016\n",
      "INFO:absl:[79] test_loss=3.1478307247161865\n",
      "INFO:absl:[80] train_loss=2.000293254852295, train_x1_loss=0.9999901056289673, train_x2_loss=1.0003070831298828\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[80] val_loss=3.047755718231201\n",
      "INFO:absl:[80] test_loss=3.1475954055786133\n",
      "INFO:absl:Checkpoint.save() finished after 0.02s.\n",
      "INFO:absl:[81] train_loss=2.000279664993286, train_x1_loss=0.9999770522117615, train_x2_loss=1.0002988576889038\n",
      "INFO:absl:[81] val_loss=3.0477442741394043\n",
      "INFO:absl:[81] test_loss=3.1476075649261475\n",
      "INFO:absl:[82] train_loss=2.000265598297119, train_x1_loss=0.9999733567237854, train_x2_loss=1.0002901554107666\n",
      "INFO:absl:[82] val_loss=3.047830581665039\n",
      "INFO:absl:[82] test_loss=3.1477532386779785\n",
      "INFO:absl:[83] train_loss=2.000271797180176, train_x1_loss=0.9999746084213257, train_x2_loss=1.000300407409668\n",
      "INFO:absl:[83] val_loss=3.047776937484741\n",
      "INFO:absl:[83] test_loss=3.147718906402588\n",
      "INFO:absl:[84] train_loss=2.0002706050872803, train_x1_loss=0.999977707862854, train_x2_loss=1.0002942085266113\n",
      "INFO:absl:[84] val_loss=3.0478103160858154\n",
      "INFO:absl:[84] test_loss=3.1476049423217773\n",
      "INFO:absl:[85] train_loss=2.0002853870391846, train_x1_loss=0.9999777674674988, train_x2_loss=1.000311017036438\n",
      "INFO:absl:[85] val_loss=3.0479376316070557\n",
      "INFO:absl:[85] test_loss=3.1477606296539307\n",
      "INFO:absl:[86] train_loss=2.0002896785736084, train_x1_loss=0.9999908804893494, train_x2_loss=1.0002988576889038\n",
      "INFO:absl:[86] val_loss=3.047844171524048\n",
      "INFO:absl:[86] test_loss=3.147665500640869\n",
      "INFO:absl:[87] train_loss=2.0002784729003906, train_x1_loss=0.9999800324440002, train_x2_loss=1.0003007650375366\n",
      "INFO:absl:[87] val_loss=3.047760486602783\n",
      "INFO:absl:[87] test_loss=3.147646903991699\n",
      "INFO:absl:[88] train_loss=2.0002996921539307, train_x1_loss=0.9999963045120239, train_x2_loss=1.0003026723861694\n",
      "INFO:absl:[88] val_loss=3.047759771347046\n",
      "INFO:absl:[88] test_loss=3.147554397583008\n",
      "INFO:absl:[89] train_loss=2.0002753734588623, train_x1_loss=0.9999738931655884, train_x2_loss=1.0003031492233276\n",
      "INFO:absl:[89] val_loss=3.0478570461273193\n",
      "INFO:absl:[89] test_loss=3.147610902786255\n",
      "INFO:absl:[90] train_loss=2.0002851486206055, train_x1_loss=0.9999924898147583, train_x2_loss=1.0002944469451904\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[90] val_loss=3.047677516937256\n",
      "INFO:absl:[90] test_loss=3.147505283355713\n",
      "INFO:absl:Checkpoint.save() finished after 0.02s.\n",
      "INFO:absl:[91] train_loss=2.0003106594085693, train_x1_loss=1.0000046491622925, train_x2_loss=1.0003111362457275\n",
      "INFO:absl:[91] val_loss=3.0478110313415527\n",
      "INFO:absl:[91] test_loss=3.147566556930542\n",
      "INFO:absl:[92] train_loss=2.000305414199829, train_x1_loss=1.0000041723251343, train_x2_loss=1.0003037452697754\n",
      "INFO:absl:[92] val_loss=3.0476436614990234\n",
      "INFO:absl:[92] test_loss=3.1474266052246094\n",
      "INFO:absl:[93] train_loss=2.0002803802490234, train_x1_loss=0.9999921321868896, train_x2_loss=1.0002899169921875\n",
      "INFO:absl:[93] val_loss=3.0479273796081543\n",
      "INFO:absl:[93] test_loss=3.1476027965545654\n",
      "INFO:absl:[94] train_loss=2.0002694129943848, train_x1_loss=0.9999679327011108, train_x2_loss=1.0003024339675903\n",
      "INFO:absl:[94] val_loss=3.048076868057251\n",
      "INFO:absl:[94] test_loss=3.147792339324951\n",
      "INFO:absl:[95] train_loss=2.0002787113189697, train_x1_loss=0.9999828934669495, train_x2_loss=1.0002975463867188\n",
      "INFO:absl:[95] val_loss=3.04772686958313\n",
      "INFO:absl:[95] test_loss=3.1474103927612305\n",
      "INFO:absl:[96] train_loss=2.000286102294922, train_x1_loss=0.9999954104423523, train_x2_loss=1.0002914667129517\n",
      "INFO:absl:[96] val_loss=3.047792434692383\n",
      "INFO:absl:[96] test_loss=3.1474199295043945\n",
      "INFO:absl:[97] train_loss=2.000286102294922, train_x1_loss=0.999992311000824, train_x2_loss=1.000294804573059\n",
      "INFO:absl:[97] val_loss=3.0479729175567627\n",
      "INFO:absl:[97] test_loss=3.1475472450256348\n",
      "INFO:absl:[98] train_loss=2.0002942085266113, train_x1_loss=1.0000005960464478, train_x2_loss=1.0002983808517456\n",
      "INFO:absl:[98] val_loss=3.047910213470459\n",
      "INFO:absl:[98] test_loss=3.147493600845337\n",
      "INFO:absl:[99] train_loss=2.0002853870391846, train_x1_loss=0.9999779462814331, train_x2_loss=1.000308632850647\n",
      "INFO:absl:[99] val_loss=3.0477893352508545\n",
      "INFO:absl:[99] test_loss=3.1473565101623535\n",
      "INFO:absl:Setting work unit notes: 3129.9 steps/s, 50.4% (705793/1400000), ETA: 3m (4m : 0.1% checkpoint, 14.3% eval)\n",
      "INFO:absl:[705793] steps_per_sec=3129.880299\n",
      "INFO:absl:[100] train_loss=2.0002951622009277, train_x1_loss=0.9999880194664001, train_x2_loss=1.0003056526184082\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[100] val_loss=3.0478768348693848\n",
      "INFO:absl:[100] test_loss=3.147430896759033\n",
      "INFO:absl:Checkpoint.save() finished after 0.02s.\n",
      "INFO:absl:[101] train_loss=2.0002708435058594, train_x1_loss=0.9999796152114868, train_x2_loss=1.00028657913208\n",
      "INFO:absl:[101] val_loss=3.0479073524475098\n",
      "INFO:absl:[101] test_loss=3.1474053859710693\n",
      "INFO:absl:[102] train_loss=2.000277280807495, train_x1_loss=0.9999767541885376, train_x2_loss=1.0003039836883545\n",
      "INFO:absl:[102] val_loss=3.047628879547119\n",
      "INFO:absl:[102] test_loss=3.1471285820007324\n",
      "INFO:absl:[103] train_loss=2.0002787113189697, train_x1_loss=0.9999884963035583, train_x2_loss=1.000291347503662\n",
      "INFO:absl:[103] val_loss=3.047774076461792\n",
      "INFO:absl:[103] test_loss=3.147172212600708\n",
      "INFO:absl:[104] train_loss=2.0002784729003906, train_x1_loss=0.9999791383743286, train_x2_loss=1.0002986192703247\n",
      "INFO:absl:[104] val_loss=3.0479843616485596\n",
      "INFO:absl:[104] test_loss=3.147357940673828\n",
      "INFO:absl:[105] train_loss=2.000291585922241, train_x1_loss=0.9999943375587463, train_x2_loss=1.0002989768981934\n",
      "INFO:absl:[105] val_loss=3.047672986984253\n",
      "INFO:absl:[105] test_loss=3.1471474170684814\n",
      "INFO:absl:[106] train_loss=2.000267267227173, train_x1_loss=0.9999691843986511, train_x2_loss=1.000299334526062\n",
      "INFO:absl:[106] val_loss=3.0477821826934814\n",
      "INFO:absl:[106] test_loss=3.1473028659820557\n",
      "INFO:absl:[107] train_loss=2.0002777576446533, train_x1_loss=0.9999885559082031, train_x2_loss=1.0002856254577637\n",
      "INFO:absl:[107] val_loss=3.047971248626709\n",
      "INFO:absl:[107] test_loss=3.1472220420837402\n",
      "INFO:absl:[108] train_loss=2.0002756118774414, train_x1_loss=0.9999751448631287, train_x2_loss=1.0003046989440918\n",
      "INFO:absl:[108] val_loss=3.047658681869507\n",
      "INFO:absl:[108] test_loss=3.147045135498047\n",
      "INFO:absl:[109] train_loss=2.000291347503662, train_x1_loss=0.9999884366989136, train_x2_loss=1.0003039836883545\n",
      "INFO:absl:[109] val_loss=3.047927141189575\n",
      "INFO:absl:[109] test_loss=3.1472339630126953\n",
      "INFO:absl:[110] train_loss=2.000269889831543, train_x1_loss=0.9999822378158569, train_x2_loss=1.0002893209457397\n",
      "INFO:absl:[110] val_loss=3.0480539798736572\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[110] test_loss=3.147325038909912\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[111] train_loss=2.000284194946289, train_x1_loss=0.9999935030937195, train_x2_loss=1.0002914667129517\n",
      "INFO:absl:[111] val_loss=3.0477468967437744\n",
      "INFO:absl:[111] test_loss=3.1470181941986084\n",
      "INFO:absl:[112] train_loss=2.0002620220184326, train_x1_loss=0.9999655485153198, train_x2_loss=1.0002989768981934\n",
      "INFO:absl:[112] val_loss=3.0477967262268066\n",
      "INFO:absl:[112] test_loss=3.14697527885437\n",
      "INFO:absl:[113] train_loss=2.00026273727417, train_x1_loss=0.9999646544456482, train_x2_loss=1.0002979040145874\n",
      "INFO:absl:[113] val_loss=3.0480175018310547\n",
      "INFO:absl:[113] test_loss=3.1471822261810303\n",
      "INFO:absl:[114] train_loss=2.0002763271331787, train_x1_loss=0.9999804496765137, train_x2_loss=1.000295639038086\n",
      "INFO:absl:[114] val_loss=3.0478789806365967\n",
      "INFO:absl:[114] test_loss=3.146934986114502\n",
      "INFO:absl:[115] train_loss=2.0002944469451904, train_x1_loss=0.999981164932251, train_x2_loss=1.0003069639205933\n",
      "INFO:absl:[115] val_loss=3.047628402709961\n",
      "INFO:absl:[115] test_loss=3.1466712951660156\n",
      "INFO:absl:[116] train_loss=2.000275135040283, train_x1_loss=0.9999733567237854, train_x2_loss=1.0003037452697754\n",
      "INFO:absl:[116] val_loss=3.0475993156433105\n",
      "INFO:absl:[116] test_loss=3.146616220474243\n",
      "INFO:absl:[117] train_loss=2.0002601146698, train_x1_loss=0.9999688267707825, train_x2_loss=1.0002917051315308\n",
      "INFO:absl:[117] val_loss=3.0476975440979004\n",
      "INFO:absl:[117] test_loss=3.146634817123413\n",
      "INFO:absl:[118] train_loss=2.0002801418304443, train_x1_loss=0.9999894499778748, train_x2_loss=1.000291109085083\n",
      "INFO:absl:[118] val_loss=3.0476911067962646\n",
      "INFO:absl:[118] test_loss=3.146615505218506\n",
      "INFO:absl:[119] train_loss=2.000258445739746, train_x1_loss=0.9999773502349854, train_x2_loss=1.0002845525741577\n",
      "INFO:absl:[119] val_loss=3.0480730533599854\n",
      "INFO:absl:[119] test_loss=3.146918296813965\n",
      "INFO:absl:[120] train_loss=2.0002896785736084, train_x1_loss=0.9999919533729553, train_x2_loss=1.0003019571304321\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[120] val_loss=3.0478172302246094\n",
      "INFO:absl:[120] test_loss=3.146531581878662\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[121] train_loss=2.0002732276916504, train_x1_loss=0.9999818205833435, train_x2_loss=1.000293254852295\n",
      "INFO:absl:[121] val_loss=3.047761917114258\n",
      "INFO:absl:[121] test_loss=3.1464481353759766\n",
      "INFO:absl:[122] train_loss=2.000290870666504, train_x1_loss=0.9999803304672241, train_x2_loss=1.000313639640808\n",
      "INFO:absl:[122] val_loss=3.0479249954223633\n",
      "INFO:absl:[122] test_loss=3.146531820297241\n",
      "INFO:absl:[123] train_loss=2.0002939701080322, train_x1_loss=0.9999877214431763, train_x2_loss=1.0003083944320679\n",
      "INFO:absl:[123] val_loss=3.0476255416870117\n",
      "INFO:absl:[123] test_loss=3.14613676071167\n",
      "INFO:absl:[124] train_loss=2.0002810955047607, train_x1_loss=0.9999894499778748, train_x2_loss=1.000291347503662\n",
      "INFO:absl:[124] val_loss=3.0477540493011475\n",
      "INFO:absl:[124] test_loss=3.1462488174438477\n",
      "INFO:absl:[125] train_loss=2.0002787113189697, train_x1_loss=0.999985933303833, train_x2_loss=1.0002931356430054\n",
      "INFO:absl:[125] val_loss=3.0476603507995605\n",
      "INFO:absl:[125] test_loss=3.1461293697357178\n",
      "INFO:absl:[126] train_loss=2.0002782344818115, train_x1_loss=0.9999775886535645, train_x2_loss=1.0003012418746948\n",
      "INFO:absl:[126] val_loss=3.0478079319000244\n",
      "INFO:absl:[126] test_loss=3.146242380142212\n",
      "INFO:absl:[127] train_loss=2.000277519226074, train_x1_loss=0.9999803900718689, train_x2_loss=1.000292181968689\n",
      "INFO:absl:[127] val_loss=3.047793388366699\n",
      "INFO:absl:Setting work unit notes: 3156.7 steps/s, 64.0% (896000/1400000), ETA: 2m (5m : 0.1% checkpoint, 14.3% eval)\n",
      "INFO:absl:[127] test_loss=3.1460001468658447\n",
      "INFO:absl:[896000] steps_per_sec=3156.682347\n",
      "INFO:absl:[128] train_loss=2.000272274017334, train_x1_loss=0.9999740719795227, train_x2_loss=1.0002951622009277\n",
      "INFO:absl:[128] val_loss=3.0478365421295166\n",
      "INFO:absl:[128] test_loss=3.1460516452789307\n",
      "INFO:absl:[129] train_loss=2.000272035598755, train_x1_loss=0.9999786019325256, train_x2_loss=1.0002909898757935\n",
      "INFO:absl:[129] val_loss=3.04774808883667\n",
      "INFO:absl:[129] test_loss=3.1458160877227783\n",
      "INFO:absl:[130] train_loss=2.000265598297119, train_x1_loss=0.9999683499336243, train_x2_loss=1.0002995729446411\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[130] val_loss=3.047952890396118\n",
      "INFO:absl:[130] test_loss=3.1458442211151123\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[131] train_loss=2.000244617462158, train_x1_loss=0.9999657273292542, train_x2_loss=1.0002812147140503\n",
      "INFO:absl:[131] val_loss=3.0476653575897217\n",
      "INFO:absl:[131] test_loss=3.145019054412842\n",
      "INFO:absl:[132] train_loss=1.9067556858062744, train_x1_loss=0.9319902062416077, train_x2_loss=0.9747673869132996\n",
      "INFO:absl:[132] val_loss=3.0534121990203857\n",
      "INFO:absl:[132] test_loss=3.1504578590393066\n",
      "INFO:absl:[133] train_loss=2.0002970695495605, train_x1_loss=1.0000076293945312, train_x2_loss=1.0002936124801636\n",
      "INFO:absl:[133] val_loss=3.04760479927063\n",
      "INFO:absl:[133] test_loss=3.147797107696533\n",
      "INFO:absl:[134] train_loss=2.0002996921539307, train_x1_loss=0.9999938011169434, train_x2_loss=1.0003103017807007\n",
      "INFO:absl:[134] val_loss=3.0478312969207764\n",
      "INFO:absl:[134] test_loss=3.148021936416626\n",
      "INFO:absl:[135] train_loss=2.000304937362671, train_x1_loss=0.9999873638153076, train_x2_loss=1.0003156661987305\n",
      "INFO:absl:[135] val_loss=3.0477614402770996\n",
      "INFO:absl:[135] test_loss=3.1479878425598145\n",
      "INFO:absl:[136] train_loss=2.0002787113189697, train_x1_loss=0.999968409538269, train_x2_loss=1.0003104209899902\n",
      "INFO:absl:[136] val_loss=3.0480313301086426\n",
      "INFO:absl:[136] test_loss=3.1482350826263428\n",
      "INFO:absl:[137] train_loss=2.000283718109131, train_x1_loss=0.9999805092811584, train_x2_loss=1.0003031492233276\n",
      "INFO:absl:[137] val_loss=3.0480353832244873\n",
      "INFO:absl:[137] test_loss=3.148181676864624\n",
      "INFO:absl:[138] train_loss=2.0002830028533936, train_x1_loss=0.9999819993972778, train_x2_loss=1.000303030014038\n",
      "INFO:absl:[138] val_loss=3.0480892658233643\n",
      "INFO:absl:[138] test_loss=3.1482508182525635\n",
      "INFO:absl:[139] train_loss=2.0003013610839844, train_x1_loss=0.9999895095825195, train_x2_loss=1.0003104209899902\n",
      "INFO:absl:[139] val_loss=3.047860622406006\n",
      "INFO:absl:[139] test_loss=3.147998809814453\n",
      "INFO:absl:[140] train_loss=2.00028395652771, train_x1_loss=0.9999755024909973, train_x2_loss=1.000314474105835\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[140] val_loss=3.047788381576538\n",
      "INFO:absl:[140] test_loss=3.1479523181915283\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[141] train_loss=2.000267505645752, train_x1_loss=0.999974250793457, train_x2_loss=1.0002959966659546\n",
      "INFO:absl:[141] val_loss=3.0479259490966797\n",
      "INFO:absl:[141] test_loss=3.14805006980896\n",
      "INFO:absl:[142] train_loss=2.0002782344818115, train_x1_loss=0.9999685287475586, train_x2_loss=1.0003098249435425\n",
      "INFO:absl:[142] val_loss=3.0479633808135986\n",
      "INFO:absl:[142] test_loss=3.148082971572876\n",
      "INFO:absl:[143] train_loss=2.000282049179077, train_x1_loss=0.9999648332595825, train_x2_loss=1.0003160238265991\n",
      "INFO:absl:[143] val_loss=3.0477659702301025\n",
      "INFO:absl:[143] test_loss=3.147925853729248\n",
      "INFO:absl:[144] train_loss=2.0002684593200684, train_x1_loss=0.9999364018440247, train_x2_loss=1.0003283023834229\n",
      "INFO:absl:[144] val_loss=3.0478761196136475\n",
      "INFO:absl:[144] test_loss=3.1479108333587646\n",
      "INFO:absl:[145] train_loss=1.9997107982635498, train_x1_loss=0.9981561303138733, train_x2_loss=1.0015586614608765\n",
      "INFO:absl:[145] val_loss=3.050424337387085\n",
      "INFO:absl:[145] test_loss=3.149585485458374\n",
      "INFO:absl:[146] train_loss=1.244458794593811, train_x1_loss=0.42636504769325256, train_x2_loss=0.8180919885635376\n",
      "INFO:absl:[146] val_loss=1.967756986618042\n",
      "INFO:absl:[146] test_loss=2.096745252609253\n",
      "INFO:absl:[147] train_loss=1.0561466217041016, train_x1_loss=0.2686028778553009, train_x2_loss=0.7875453233718872\n",
      "INFO:absl:[147] val_loss=1.817991018295288\n",
      "INFO:absl:[147] test_loss=1.9402469396591187\n",
      "INFO:absl:[148] train_loss=1.0350393056869507, train_x1_loss=0.25325530767440796, train_x2_loss=0.7817826271057129\n",
      "INFO:absl:[148] val_loss=1.7952697277069092\n",
      "INFO:absl:[148] test_loss=1.8886134624481201\n",
      "INFO:absl:[149] train_loss=0.9980871677398682, train_x1_loss=0.2427011877298355, train_x2_loss=0.7553871870040894\n",
      "INFO:absl:[149] val_loss=1.638375997543335\n",
      "INFO:absl:[149] test_loss=1.7185202836990356\n",
      "INFO:absl:[150] train_loss=0.92530757188797, train_x1_loss=0.225115105509758, train_x2_loss=0.7001949548721313\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[150] val_loss=1.5569523572921753\n",
      "INFO:absl:[150] test_loss=1.6284006834030151\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[151] train_loss=0.9158048033714294, train_x1_loss=0.2227982133626938, train_x2_loss=0.693006157875061\n",
      "INFO:absl:[151] val_loss=1.549249529838562\n",
      "INFO:absl:[151] test_loss=1.624674677848816\n",
      "INFO:absl:[152] train_loss=0.9091170430183411, train_x1_loss=0.21991173923015594, train_x2_loss=0.6892070174217224\n",
      "INFO:absl:[152] val_loss=1.5440503358840942\n",
      "INFO:absl:[152] test_loss=1.6201504468917847\n",
      "WARNING:absl:loss is nan for step 1072919 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072920 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072921 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072922 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072923 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072924 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072925 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072926 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072927 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072928 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072929 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072930 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072931 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072932 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072933 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072934 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072935 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072936 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072937 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072938 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072939 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072940 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072941 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072942 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072943 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072944 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072945 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072946 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072947 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072948 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072949 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072950 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072951 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072952 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072953 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072954 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072955 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072956 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072957 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072958 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072959 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072960 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072961 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072962 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072963 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072964 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072965 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072966 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072967 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072968 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072969 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072970 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072971 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072972 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072973 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072974 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072975 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072976 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072977 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072978 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072979 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072980 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072981 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072982 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072983 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072984 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072985 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072986 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072987 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072988 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072989 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072990 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072991 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072992 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072993 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072994 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072995 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072996 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072997 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072998 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1072999 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073000 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073001 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073002 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073003 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073004 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073005 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073006 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073007 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073008 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073009 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073010 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073011 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073012 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073013 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073014 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073015 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073016 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073017 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073018 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073019 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073020 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073021 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073022 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073023 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073024 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073025 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073026 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073027 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073028 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073029 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073030 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073031 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073032 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073033 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073034 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073035 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073036 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073037 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073038 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073039 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073040 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073041 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073042 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073043 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073044 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073045 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073046 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073047 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073048 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073049 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073050 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073051 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073052 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073053 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073054 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073055 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073056 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073057 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073058 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073059 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073060 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073061 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073062 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073063 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073064 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073065 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073066 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073067 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073068 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073069 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073070 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073071 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073072 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073073 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073074 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073075 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073076 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073077 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073078 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073079 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073080 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073081 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073082 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073083 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073084 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073085 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073086 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073087 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073088 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073089 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073090 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073091 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073092 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073093 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073094 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073095 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073096 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073097 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073098 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073099 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073100 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073101 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073102 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073103 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073104 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073105 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073106 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073107 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073108 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073109 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073110 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073111 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073112 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073113 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073114 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073115 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073116 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073117 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073118 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073119 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073120 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073121 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073122 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073123 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073124 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073125 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073126 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073127 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073128 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073129 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073130 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073131 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073132 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073133 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073134 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073135 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073136 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073137 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073138 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073139 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073140 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073141 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073142 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073143 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073144 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073145 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073146 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073147 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073148 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073149 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073150 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073151 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073152 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073153 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073154 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073155 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073156 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073157 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073158 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073159 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073160 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073161 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073162 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073163 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073164 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073165 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073166 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073167 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073168 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073169 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073170 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073171 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073172 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073173 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073174 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073175 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073176 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073177 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073178 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073179 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073180 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073181 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073182 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073183 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073184 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073185 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073186 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073187 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073188 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073189 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073190 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073191 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073192 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073193 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073194 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073195 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073196 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073197 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073198 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073199 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073200 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073201 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073202 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073203 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073204 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073205 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073206 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073207 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073208 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073209 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073210 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073211 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073212 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073213 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073214 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073215 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073216 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073217 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073218 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073219 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073220 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073221 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073222 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073223 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073224 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073225 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073226 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073227 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073228 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073229 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073230 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073231 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073232 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073233 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073234 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073235 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073236 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073237 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073238 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073239 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073240 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073241 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073242 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073243 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073244 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073245 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073246 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073247 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073248 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073249 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073250 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073251 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073252 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073253 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073254 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073255 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073256 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073257 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073258 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073259 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073260 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073261 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073262 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073263 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073264 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073265 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073266 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073267 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073268 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073269 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073270 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073271 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073272 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073273 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073274 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073275 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073276 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073277 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073278 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073279 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073280 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073281 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073282 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073283 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073284 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073285 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073286 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073287 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073288 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073289 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073290 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073291 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073292 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073293 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073294 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073295 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073296 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073297 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073298 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073299 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073300 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073301 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073302 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073303 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073304 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073305 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073306 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073307 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073308 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073309 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073310 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073311 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073312 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073313 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073314 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073315 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073316 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073317 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073318 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073319 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073320 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073321 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073322 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073323 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073324 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073325 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073326 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073327 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073328 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073329 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073330 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073331 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073332 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073333 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073334 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073335 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073336 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073337 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073338 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073339 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073340 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073341 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073342 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073343 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073344 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073345 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073346 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073347 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073348 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073349 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073350 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073351 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073352 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073353 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073354 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073355 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073356 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073357 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073358 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073359 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073360 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073361 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073362 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073363 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073364 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073365 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073366 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073367 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073368 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073369 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073370 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073371 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073372 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073373 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073374 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073375 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073376 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073377 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073378 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073379 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073380 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073381 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073382 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073383 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073384 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073385 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073386 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073387 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073388 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073389 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073390 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073391 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073392 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073393 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073394 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073395 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073396 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073397 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073398 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073399 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073400 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073401 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073402 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073403 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073404 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073405 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073406 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073407 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073408 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073409 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073410 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073411 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073412 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073413 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073414 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073415 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073416 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073417 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073418 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073419 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073420 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073421 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073422 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073423 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073424 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073425 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073426 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073427 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073428 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073429 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073430 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073431 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073432 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073433 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073434 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073435 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073436 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073437 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073438 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073439 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073440 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073441 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073442 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073443 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073444 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073445 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073446 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073447 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073448 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073449 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073450 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073451 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073452 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073453 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073454 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073455 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073456 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073457 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073458 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073459 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073460 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073461 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073462 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073463 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073464 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073465 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073466 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073467 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073468 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073469 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073470 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073471 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073472 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073473 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073474 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073475 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073476 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073477 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073478 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073479 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073480 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073481 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073482 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073483 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073484 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073485 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073486 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073487 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073488 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073489 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073490 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073491 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073492 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073493 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073494 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073495 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073496 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073497 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073498 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073499 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073500 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073501 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073502 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073503 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073504 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073505 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073506 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073507 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073508 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073509 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073510 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073511 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073512 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073513 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073514 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073515 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073516 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073517 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073518 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073519 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073520 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073521 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073522 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073523 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073524 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073525 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073526 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073527 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073528 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073529 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073530 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073531 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073532 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073533 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073534 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073535 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073536 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073537 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073538 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073539 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073540 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073541 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073542 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073543 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073544 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073545 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073546 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073547 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073548 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073549 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073550 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073551 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073552 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073553 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073554 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073555 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073556 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073557 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073558 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073559 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073560 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073561 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073562 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073563 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073564 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073565 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073566 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073567 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073568 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073569 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073570 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073571 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073572 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073573 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073574 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073575 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073576 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073577 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073578 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073579 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073580 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073581 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073582 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073583 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073584 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073585 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073586 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073587 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073588 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073589 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073590 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073591 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073592 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073593 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073594 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073595 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073596 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073597 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073598 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073599 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073600 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073601 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073602 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073603 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073604 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073605 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073606 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073607 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073608 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073609 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073610 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073611 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073612 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073613 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073614 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073615 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073616 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073617 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073618 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073619 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073620 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073621 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073622 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073623 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073624 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073625 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073626 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073627 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073628 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073629 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073630 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073631 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073632 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073633 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073634 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073635 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073636 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073637 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073638 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073639 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073640 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073641 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073642 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073643 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073644 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073645 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073646 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073647 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073648 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073649 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073650 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073651 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073652 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073653 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073654 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073655 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073656 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073657 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073658 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073659 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073660 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073661 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073662 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073663 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073664 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073665 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073666 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073667 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073668 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073669 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073670 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073671 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073672 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073673 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073674 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073675 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073676 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073677 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073678 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073679 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073680 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073681 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073682 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073683 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073684 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073685 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073686 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073687 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073688 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073689 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073690 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073691 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073692 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073693 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073694 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073695 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073696 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073697 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073698 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073699 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073700 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073701 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073702 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073703 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073704 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073705 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073706 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073707 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073708 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073709 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073710 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073711 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073712 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073713 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073714 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073715 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073716 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073717 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073718 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073719 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073720 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073721 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073722 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073723 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073724 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073725 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073726 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073727 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073728 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073729 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073730 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073731 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073732 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073733 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073734 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073735 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073736 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073737 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073738 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073739 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073740 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073741 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073742 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073743 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073744 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073745 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073746 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073747 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073748 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073749 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073750 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073751 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073752 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073753 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073754 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073755 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073756 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073757 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073758 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073759 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073760 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073761 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073762 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073763 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073764 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073765 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073766 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073767 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073768 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073769 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073770 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073771 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073772 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073773 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073774 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073775 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073776 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073777 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073778 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073779 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073780 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073781 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073782 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073783 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073784 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073785 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073786 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073787 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073788 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073789 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073790 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073791 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073792 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073793 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073794 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073795 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073796 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073797 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073798 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073799 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073800 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073801 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073802 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073803 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073804 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073805 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073806 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073807 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073808 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073809 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073810 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073811 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073812 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073813 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073814 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073815 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073816 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073817 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073818 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073819 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073820 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073821 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073822 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073823 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073824 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073825 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073826 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073827 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073828 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073829 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073830 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073831 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073832 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073833 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073834 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073835 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073836 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073837 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073838 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073839 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073840 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073841 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073842 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073843 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073844 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073845 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073846 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073847 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073848 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073849 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073850 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073851 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073852 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073853 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073854 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073855 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073856 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073857 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073858 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073859 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073860 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073861 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073862 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073863 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073864 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073865 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073866 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073867 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073868 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073869 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073870 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073871 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073872 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073873 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073874 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073875 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073876 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073877 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073878 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073879 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073880 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073881 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073882 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073883 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073884 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073885 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073886 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073887 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073888 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073889 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073890 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073891 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073892 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073893 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073894 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073895 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073896 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073897 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073898 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073899 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073900 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073901 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073902 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073903 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073904 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073905 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073906 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073907 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073908 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073909 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073910 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073911 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073912 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073913 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073914 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073915 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073916 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073917 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073918 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073919 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073920 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073921 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073922 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073923 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073924 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073925 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073926 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073927 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073928 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073929 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073930 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073931 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073932 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073933 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073934 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073935 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073936 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073937 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073938 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073939 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073940 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073941 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073942 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073943 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073944 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073945 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073946 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073947 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073948 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073949 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073950 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073951 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073952 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073953 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073954 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073955 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073956 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073957 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073958 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073959 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073960 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073961 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073962 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073963 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073964 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073965 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073966 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073967 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073968 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073969 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073970 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073971 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073972 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073973 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073974 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073975 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073976 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073977 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073978 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073979 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073980 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073981 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073982 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073983 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073984 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073985 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073986 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073987 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073988 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073989 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073990 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073991 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073992 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073993 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073994 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073995 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073996 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073997 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073998 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1073999 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074000 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074001 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074002 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074003 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074004 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074005 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074006 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074007 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074008 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074009 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074010 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074011 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074012 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074013 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074014 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074015 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074016 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074017 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074018 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074019 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074020 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074021 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074022 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074023 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074024 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074025 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074026 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074027 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074028 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074029 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074030 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074031 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074032 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074033 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074034 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074035 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074036 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074037 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074038 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074039 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074040 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074041 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074042 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074043 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074044 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074045 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074046 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074047 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074048 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074049 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074050 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074051 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074052 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074053 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074054 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074055 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074056 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074057 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074058 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074059 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074060 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074061 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074062 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074063 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074064 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074065 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074066 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074067 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074068 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074069 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074070 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074071 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074072 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074073 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074074 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074075 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074076 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074077 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074078 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074079 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074080 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074081 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074082 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074083 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074084 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074085 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074086 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074087 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074088 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074089 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074090 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074091 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074092 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074093 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074094 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074095 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074096 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074097 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074098 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074099 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074100 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074101 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074102 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074103 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074104 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074105 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074106 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074107 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074108 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074109 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074110 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074111 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074112 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074113 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074114 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074115 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074116 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074117 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074118 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074119 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074120 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074121 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074122 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074123 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074124 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074125 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074126 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074127 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074128 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074129 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074130 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074131 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074132 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074133 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074134 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074135 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074136 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074137 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074138 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074139 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074140 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074141 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074142 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074143 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074144 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074145 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074146 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074147 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074148 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074149 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074150 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074151 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074152 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074153 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074154 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074155 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074156 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074157 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074158 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074159 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074160 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074161 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074162 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074163 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074164 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074165 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074166 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074167 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074168 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074169 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074170 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074171 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074172 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074173 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074174 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074175 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074176 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074177 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074178 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074179 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074180 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074181 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074182 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074183 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074184 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074185 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074186 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074187 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074188 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074189 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074190 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074191 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074192 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074193 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074194 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074195 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074196 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074197 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074198 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074199 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074200 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074201 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074202 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074203 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074204 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074205 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074206 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074207 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074208 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074209 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074210 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074211 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074212 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074213 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074214 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074215 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074216 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074217 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074218 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074219 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074220 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074221 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074222 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074223 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074224 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074225 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074226 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074227 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074228 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074229 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074230 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074231 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074232 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074233 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074234 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074235 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074236 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074237 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074238 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074239 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074240 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074241 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074242 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074243 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074244 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074245 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074246 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074247 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074248 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074249 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074250 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074251 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074252 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074253 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074254 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074255 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074256 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074257 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074258 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074259 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074260 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074261 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074262 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074263 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074264 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074265 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074266 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074267 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074268 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074269 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074270 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074271 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074272 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074273 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074274 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074275 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074276 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074277 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074278 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074279 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074280 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074281 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074282 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074283 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074284 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074285 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074286 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074287 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074288 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074289 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074290 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074291 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074292 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074293 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074294 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074295 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074296 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074297 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074298 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074299 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074300 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074301 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074302 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074303 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074304 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074305 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074306 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074307 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074308 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074309 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074310 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074311 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074312 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074313 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074314 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074315 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074316 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074317 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074318 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074319 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074320 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074321 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074322 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074323 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074324 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074325 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074326 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074327 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074328 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074329 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074330 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074331 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074332 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074333 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074334 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074335 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074336 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074337 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074338 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074339 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074340 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074341 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074342 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074343 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074344 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074345 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074346 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074347 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074348 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074349 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074350 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074351 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074352 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074353 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074354 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074355 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074356 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074357 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074358 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074359 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074360 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074361 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074362 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074363 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074364 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074365 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074366 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074367 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074368 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074369 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074370 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074371 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074372 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074373 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074374 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074375 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074376 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074377 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074378 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074379 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074380 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074381 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074382 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074383 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074384 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074385 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074386 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074387 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074388 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074389 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074390 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074391 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074392 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074393 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074394 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074395 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074396 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074397 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074398 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074399 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074400 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074401 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074402 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074403 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074404 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074405 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074406 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074407 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074408 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074409 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074410 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074411 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074412 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074413 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074414 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074415 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074416 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074417 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074418 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074419 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074420 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074421 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074422 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074423 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074424 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074425 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074426 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074427 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074428 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074429 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074430 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074431 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074432 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074433 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074434 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074435 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074436 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074437 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074438 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074439 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074440 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074441 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074442 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074443 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074444 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074445 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074446 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074447 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074448 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074449 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074450 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074451 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074452 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074453 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074454 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074455 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074456 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074457 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074458 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074459 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074460 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074461 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074462 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074463 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074464 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074465 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074466 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074467 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074468 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074469 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074470 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074471 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074472 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074473 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074474 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074475 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074476 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074477 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074478 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074479 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074480 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074481 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074482 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074483 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074484 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074485 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074486 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074487 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074488 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074489 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074490 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074491 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074492 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074493 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074494 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074495 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074496 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074497 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074498 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074499 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074500 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074501 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074502 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074503 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074504 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074505 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074506 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074507 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074508 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074509 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074510 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074511 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074512 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074513 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074514 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074515 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074516 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074517 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074518 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074519 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074520 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074521 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074522 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074523 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074524 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074525 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074526 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074527 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074528 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074529 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074530 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074531 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074532 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074533 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074534 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074535 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074536 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074537 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074538 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074539 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074540 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074541 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074542 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074543 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074544 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074545 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074546 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074547 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074548 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074549 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074550 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074551 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074552 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074553 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074554 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074555 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074556 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074557 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074558 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074559 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074560 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074561 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074562 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074563 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074564 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074565 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074566 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074567 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074568 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074569 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074570 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074571 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074572 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074573 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074574 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074575 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074576 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074577 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074578 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074579 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074580 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074581 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074582 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074583 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074584 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074585 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074586 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074587 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074588 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074589 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074590 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074591 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074592 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074593 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074594 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074595 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074596 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074597 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074598 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074599 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074600 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074601 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074602 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074603 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074604 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074605 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074606 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074607 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074608 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074609 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074610 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074611 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074612 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074613 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074614 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074615 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074616 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074617 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074618 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074619 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074620 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074621 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074622 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074623 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074624 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074625 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074626 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074627 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074628 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074629 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074630 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074631 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074632 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074633 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074634 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074635 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074636 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074637 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074638 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074639 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074640 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074641 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074642 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074643 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074644 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074645 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074646 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074647 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074648 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074649 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074650 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074651 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074652 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074653 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074654 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074655 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074656 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074657 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074658 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074659 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074660 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074661 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074662 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074663 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074664 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074665 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074666 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074667 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074668 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074669 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074670 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074671 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074672 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074673 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074674 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074675 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074676 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074677 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074678 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074679 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074680 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074681 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074682 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074683 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074684 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074685 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074686 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074687 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074688 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074689 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074690 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074691 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074692 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074693 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074694 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074695 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074696 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074697 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074698 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074699 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074700 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074701 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074702 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074703 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074704 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074705 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074706 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074707 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074708 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074709 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074710 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074711 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074712 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074713 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074714 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074715 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074716 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074717 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074718 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074719 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074720 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074721 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074722 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074723 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074724 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074725 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074726 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074727 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074728 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074729 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074730 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074731 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074732 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074733 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074734 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074735 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074736 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074737 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074738 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074739 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074740 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074741 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074742 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074743 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074744 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074745 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074746 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074747 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074748 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074749 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074750 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074751 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074752 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074753 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074754 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074755 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074756 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074757 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074758 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074759 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074760 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074761 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074762 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074763 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074764 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074765 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074766 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074767 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074768 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074769 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074770 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074771 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074772 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074773 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074774 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074775 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074776 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074777 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074778 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074779 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074780 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074781 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074782 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074783 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074784 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074785 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074786 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074787 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074788 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074789 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074790 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074791 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074792 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074793 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074794 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074795 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074796 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074797 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074798 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074799 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074800 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074801 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074802 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074803 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074804 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074805 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074806 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074807 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074808 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074809 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074810 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074811 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074812 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074813 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074814 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074815 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074816 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074817 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074818 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074819 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074820 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074821 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074822 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074823 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074824 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074825 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074826 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074827 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074828 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074829 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074830 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074831 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074832 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074833 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074834 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074835 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074836 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074837 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074838 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074839 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074840 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074841 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074842 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074843 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074844 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074845 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074846 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074847 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074848 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074849 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074850 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074851 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074852 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074853 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074854 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074855 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074856 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074857 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074858 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074859 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074860 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074861 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074862 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074863 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074864 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074865 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074866 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074867 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074868 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074869 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074870 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074871 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074872 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074873 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074874 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074875 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074876 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074877 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074878 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074879 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074880 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074881 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074882 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074883 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074884 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074885 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074886 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074887 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074888 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074889 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074890 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074891 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074892 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074893 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074894 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074895 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074896 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074897 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074898 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074899 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074900 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074901 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074902 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074903 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074904 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074905 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074906 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074907 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074908 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074909 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074910 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074911 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074912 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074913 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074914 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074915 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074916 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074917 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074918 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074919 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074920 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074921 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074922 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074923 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074924 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074925 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074926 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074927 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074928 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074929 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074930 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074931 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074932 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074933 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074934 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074935 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074936 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074937 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074938 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074939 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074940 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074941 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074942 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074943 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074944 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074945 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074946 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074947 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074948 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074949 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074950 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074951 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074952 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074953 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074954 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074955 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074956 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074957 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074958 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074959 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074960 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074961 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074962 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074963 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074964 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074965 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074966 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074967 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074968 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074969 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074970 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074971 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074972 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074973 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074974 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074975 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074976 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074977 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074978 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074979 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074980 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074981 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074982 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074983 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074984 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074985 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074986 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074987 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074988 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074989 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074990 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074991 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074992 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074993 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074994 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074995 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074996 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074997 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074998 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1074999 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075000 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075001 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075002 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075003 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075004 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075005 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075006 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075007 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075008 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075009 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075010 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075011 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075012 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075013 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075014 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075015 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075016 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075017 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075018 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075019 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075020 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075021 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075022 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075023 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075024 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075025 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075026 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075027 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075028 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075029 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075030 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075031 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075032 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075033 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075034 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075035 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075036 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075037 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075038 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075039 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075040 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075041 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075042 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075043 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075044 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075045 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075046 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075047 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075048 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075049 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075050 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075051 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075052 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075053 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075054 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075055 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075056 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075057 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075058 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075059 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075060 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075061 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075062 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075063 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075064 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075065 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075066 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075067 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075068 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075069 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075070 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075071 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075072 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075073 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075074 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075075 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075076 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075077 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075078 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075079 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075080 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075081 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075082 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075083 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075084 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075085 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075086 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075087 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075088 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075089 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075090 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075091 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075092 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075093 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075094 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075095 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075096 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075097 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075098 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075099 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075100 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075101 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075102 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075103 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075104 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075105 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075106 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075107 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075108 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075109 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075110 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075111 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075112 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075113 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075114 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075115 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075116 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075117 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075118 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075119 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075120 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075121 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075122 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075123 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075124 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075125 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075126 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075127 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075128 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075129 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075130 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075131 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075132 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075133 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075134 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075135 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075136 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075137 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075138 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075139 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075140 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075141 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075142 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075143 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075144 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075145 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075146 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075147 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075148 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075149 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075150 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075151 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075152 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075153 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075154 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075155 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075156 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075157 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075158 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075159 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075160 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075161 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075162 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075163 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075164 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075165 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075166 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075167 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075168 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075169 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075170 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075171 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075172 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075173 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075174 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075175 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075176 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075177 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075178 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075179 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075180 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075181 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075182 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075183 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075184 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075185 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075186 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075187 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075188 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075189 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075190 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075191 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075192 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075193 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075194 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075195 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075196 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075197 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075198 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075199 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075200 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075201 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075202 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075203 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075204 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075205 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075206 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075207 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075208 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075209 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075210 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075211 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075212 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075213 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075214 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075215 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075216 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075217 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075218 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075219 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075220 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075221 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075222 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075223 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075224 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075225 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075226 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075227 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075228 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075229 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075230 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075231 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075232 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075233 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075234 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075235 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075236 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075237 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075238 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075239 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075240 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075241 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075242 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075243 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075244 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075245 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075246 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075247 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075248 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075249 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075250 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075251 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075252 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075253 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075254 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075255 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075256 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075257 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075258 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075259 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075260 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075261 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075262 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075263 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075264 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075265 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075266 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075267 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075268 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075269 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075270 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075271 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075272 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075273 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075274 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075275 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075276 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075277 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075278 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075279 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075280 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075281 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075282 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075283 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075284 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075285 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075286 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075287 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075288 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075289 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075290 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075291 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075292 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075293 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075294 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075295 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075296 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075297 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075298 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075299 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075300 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075301 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075302 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075303 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075304 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075305 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075306 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075307 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075308 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075309 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075310 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075311 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075312 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075313 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075314 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075315 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075316 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075317 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075318 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075319 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075320 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075321 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075322 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075323 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075324 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075325 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075326 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075327 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075328 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075329 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075330 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075331 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075332 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075333 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075334 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075335 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075336 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075337 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075338 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075339 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075340 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075341 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075342 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075343 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075344 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075345 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075346 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075347 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075348 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075349 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075350 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075351 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075352 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075353 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075354 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075355 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075356 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075357 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075358 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075359 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075360 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075361 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075362 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075363 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075364 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075365 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075366 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075367 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075368 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075369 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075370 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075371 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075372 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075373 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075374 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075375 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075376 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075377 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075378 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075379 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075380 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075381 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075382 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075383 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075384 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075385 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075386 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075387 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075388 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075389 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075390 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075391 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075392 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075393 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075394 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075395 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075396 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075397 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075398 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075399 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075400 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075401 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075402 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075403 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075404 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075405 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075406 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075407 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075408 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075409 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075410 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075411 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075412 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075413 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075414 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075415 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075416 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075417 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075418 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075419 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075420 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075421 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075422 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075423 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075424 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075425 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075426 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075427 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075428 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075429 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075430 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075431 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075432 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075433 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075434 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075435 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075436 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075437 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075438 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075439 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075440 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075441 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075442 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075443 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075444 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075445 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075446 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075447 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075448 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075449 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075450 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075451 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075452 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075453 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075454 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075455 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075456 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075457 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075458 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075459 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075460 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075461 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075462 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075463 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075464 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075465 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075466 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075467 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075468 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075469 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075470 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075471 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075472 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075473 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075474 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075475 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075476 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075477 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075478 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075479 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075480 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075481 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075482 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075483 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075484 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075485 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075486 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075487 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075488 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075489 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075490 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075491 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075492 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075493 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075494 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075495 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075496 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075497 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075498 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075499 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075500 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075501 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075502 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075503 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075504 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075505 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075506 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075507 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075508 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075509 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075510 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075511 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075512 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075513 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075514 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075515 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075516 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075517 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075518 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075519 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075520 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075521 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075522 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075523 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075524 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075525 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075526 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075527 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075528 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075529 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075530 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075531 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075532 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075533 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075534 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075535 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075536 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075537 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075538 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075539 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075540 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075541 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075542 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075543 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075544 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075545 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075546 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075547 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075548 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075549 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075550 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075551 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075552 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075553 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075554 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075555 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075556 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075557 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075558 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075559 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075560 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075561 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075562 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075563 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075564 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075565 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075566 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075567 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075568 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075569 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075570 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075571 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075572 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075573 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075574 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075575 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075576 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075577 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075578 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075579 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075580 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075581 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075582 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075583 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075584 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075585 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075586 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075587 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075588 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075589 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075590 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075591 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075592 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075593 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075594 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075595 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075596 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075597 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075598 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075599 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075600 (in epoch 153)\n",
      "INFO:absl:Setting work unit notes: 2993.3 steps/s, 76.8% (1075600/1400000), ETA: 1m (6m : 0.1% checkpoint, 14.2% eval)\n",
      "INFO:absl:[1075600] steps_per_sec=2993.276872\n",
      "WARNING:absl:loss is nan for step 1075601 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075602 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075603 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075604 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075605 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075606 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075607 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075608 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075609 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075610 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075611 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075612 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075613 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075614 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075615 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075616 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075617 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075618 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075619 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075620 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075621 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075622 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075623 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075624 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075625 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075626 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075627 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075628 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075629 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075630 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075631 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075632 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075633 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075634 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075635 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075636 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075637 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075638 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075639 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075640 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075641 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075642 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075643 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075644 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075645 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075646 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075647 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075648 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075649 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075650 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075651 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075652 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075653 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075654 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075655 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075656 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075657 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075658 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075659 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075660 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075661 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075662 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075663 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075664 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075665 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075666 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075667 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075668 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075669 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075670 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075671 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075672 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075673 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075674 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075675 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075676 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075677 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075678 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075679 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075680 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075681 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075682 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075683 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075684 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075685 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075686 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075687 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075688 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075689 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075690 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075691 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075692 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075693 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075694 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075695 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075696 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075697 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075698 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075699 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075700 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075701 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075702 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075703 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075704 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075705 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075706 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075707 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075708 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075709 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075710 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075711 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075712 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075713 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075714 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075715 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075716 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075717 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075718 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075719 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075720 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075721 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075722 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075723 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075724 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075725 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075726 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075727 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075728 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075729 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075730 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075731 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075732 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075733 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075734 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075735 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075736 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075737 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075738 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075739 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075740 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075741 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075742 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075743 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075744 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075745 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075746 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075747 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075748 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075749 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075750 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075751 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075752 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075753 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075754 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075755 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075756 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075757 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075758 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075759 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075760 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075761 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075762 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075763 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075764 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075765 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075766 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075767 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075768 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075769 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075770 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075771 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075772 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075773 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075774 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075775 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075776 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075777 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075778 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075779 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075780 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075781 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075782 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075783 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075784 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075785 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075786 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075787 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075788 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075789 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075790 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075791 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075792 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075793 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075794 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075795 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075796 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075797 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075798 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075799 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075800 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075801 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075802 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075803 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075804 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075805 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075806 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075807 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075808 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075809 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075810 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075811 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075812 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075813 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075814 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075815 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075816 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075817 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075818 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075819 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075820 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075821 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075822 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075823 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075824 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075825 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075826 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075827 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075828 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075829 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075830 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075831 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075832 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075833 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075834 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075835 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075836 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075837 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075838 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075839 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075840 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075841 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075842 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075843 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075844 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075845 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075846 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075847 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075848 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075849 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075850 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075851 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075852 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075853 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075854 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075855 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075856 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075857 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075858 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075859 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075860 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075861 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075862 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075863 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075864 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075865 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075866 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075867 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075868 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075869 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075870 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075871 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075872 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075873 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075874 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075875 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075876 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075877 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075878 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075879 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075880 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075881 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075882 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075883 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075884 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075885 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075886 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075887 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075888 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075889 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075890 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075891 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075892 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075893 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075894 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075895 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075896 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075897 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075898 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075899 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075900 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075901 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075902 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075903 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075904 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075905 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075906 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075907 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075908 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075909 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075910 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075911 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075912 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075913 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075914 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075915 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075916 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075917 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075918 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075919 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075920 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075921 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075922 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075923 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075924 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075925 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075926 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075927 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075928 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075929 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075930 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075931 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075932 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075933 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075934 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075935 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075936 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075937 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075938 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075939 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075940 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075941 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075942 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075943 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075944 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075945 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075946 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075947 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075948 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075949 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075950 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075951 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075952 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075953 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075954 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075955 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075956 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075957 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075958 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075959 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075960 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075961 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075962 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075963 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075964 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075965 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075966 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075967 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075968 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075969 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075970 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075971 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075972 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075973 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075974 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075975 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075976 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075977 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075978 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075979 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075980 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075981 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075982 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075983 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075984 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075985 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075986 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075987 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075988 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075989 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075990 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075991 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075992 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075993 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075994 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075995 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075996 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075997 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075998 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1075999 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076000 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076001 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076002 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076003 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076004 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076005 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076006 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076007 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076008 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076009 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076010 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076011 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076012 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076013 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076014 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076015 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076016 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076017 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076018 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076019 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076020 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076021 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076022 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076023 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076024 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076025 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076026 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076027 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076028 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076029 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076030 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076031 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076032 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076033 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076034 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076035 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076036 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076037 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076038 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076039 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076040 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076041 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076042 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076043 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076044 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076045 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076046 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076047 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076048 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076049 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076050 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076051 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076052 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076053 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076054 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076055 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076056 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076057 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076058 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076059 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076060 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076061 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076062 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076063 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076064 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076065 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076066 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076067 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076068 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076069 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076070 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076071 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076072 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076073 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076074 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076075 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076076 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076077 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076078 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076079 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076080 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076081 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076082 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076083 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076084 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076085 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076086 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076087 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076088 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076089 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076090 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076091 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076092 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076093 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076094 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076095 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076096 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076097 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076098 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076099 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076100 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076101 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076102 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076103 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076104 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076105 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076106 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076107 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076108 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076109 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076110 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076111 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076112 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076113 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076114 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076115 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076116 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076117 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076118 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076119 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076120 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076121 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076122 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076123 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076124 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076125 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076126 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076127 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076128 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076129 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076130 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076131 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076132 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076133 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076134 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076135 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076136 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076137 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076138 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076139 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076140 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076141 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076142 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076143 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076144 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076145 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076146 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076147 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076148 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076149 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076150 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076151 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076152 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076153 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076154 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076155 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076156 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076157 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076158 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076159 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076160 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076161 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076162 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076163 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076164 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076165 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076166 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076167 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076168 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076169 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076170 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076171 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076172 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076173 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076174 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076175 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076176 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076177 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076178 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076179 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076180 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076181 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076182 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076183 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076184 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076185 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076186 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076187 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076188 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076189 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076190 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076191 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076192 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076193 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076194 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076195 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076196 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076197 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076198 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076199 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076200 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076201 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076202 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076203 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076204 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076205 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076206 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076207 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076208 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076209 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076210 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076211 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076212 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076213 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076214 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076215 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076216 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076217 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076218 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076219 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076220 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076221 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076222 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076223 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076224 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076225 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076226 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076227 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076228 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076229 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076230 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076231 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076232 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076233 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076234 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076235 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076236 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076237 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076238 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076239 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076240 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076241 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076242 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076243 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076244 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076245 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076246 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076247 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076248 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076249 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076250 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076251 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076252 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076253 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076254 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076255 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076256 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076257 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076258 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076259 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076260 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076261 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076262 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076263 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076264 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076265 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076266 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076267 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076268 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076269 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076270 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076271 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076272 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076273 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076274 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076275 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076276 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076277 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076278 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076279 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076280 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076281 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076282 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076283 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076284 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076285 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076286 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076287 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076288 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076289 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076290 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076291 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076292 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076293 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076294 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076295 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076296 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076297 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076298 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076299 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076300 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076301 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076302 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076303 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076304 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076305 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076306 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076307 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076308 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076309 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076310 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076311 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076312 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076313 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076314 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076315 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076316 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076317 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076318 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076319 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076320 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076321 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076322 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076323 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076324 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076325 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076326 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076327 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076328 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076329 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076330 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076331 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076332 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076333 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076334 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076335 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076336 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076337 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076338 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076339 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076340 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076341 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076342 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076343 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076344 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076345 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076346 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076347 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076348 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076349 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076350 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076351 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076352 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076353 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076354 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076355 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076356 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076357 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076358 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076359 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076360 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076361 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076362 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076363 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076364 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076365 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076366 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076367 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076368 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076369 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076370 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076371 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076372 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076373 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076374 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076375 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076376 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076377 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076378 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076379 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076380 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076381 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076382 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076383 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076384 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076385 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076386 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076387 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076388 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076389 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076390 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076391 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076392 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076393 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076394 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076395 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076396 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076397 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076398 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076399 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076400 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076401 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076402 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076403 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076404 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076405 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076406 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076407 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076408 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076409 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076410 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076411 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076412 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076413 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076414 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076415 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076416 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076417 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076418 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076419 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076420 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076421 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076422 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076423 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076424 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076425 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076426 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076427 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076428 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076429 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076430 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076431 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076432 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076433 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076434 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076435 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076436 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076437 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076438 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076439 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076440 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076441 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076442 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076443 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076444 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076445 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076446 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076447 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076448 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076449 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076450 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076451 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076452 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076453 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076454 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076455 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076456 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076457 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076458 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076459 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076460 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076461 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076462 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076463 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076464 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076465 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076466 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076467 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076468 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076469 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076470 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076471 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076472 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076473 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076474 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076475 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076476 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076477 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076478 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076479 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076480 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076481 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076482 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076483 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076484 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076485 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076486 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076487 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076488 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076489 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076490 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076491 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076492 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076493 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076494 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076495 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076496 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076497 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076498 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076499 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076500 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076501 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076502 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076503 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076504 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076505 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076506 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076507 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076508 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076509 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076510 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076511 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076512 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076513 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076514 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076515 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076516 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076517 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076518 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076519 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076520 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076521 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076522 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076523 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076524 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076525 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076526 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076527 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076528 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076529 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076530 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076531 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076532 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076533 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076534 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076535 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076536 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076537 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076538 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076539 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076540 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076541 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076542 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076543 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076544 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076545 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076546 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076547 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076548 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076549 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076550 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076551 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076552 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076553 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076554 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076555 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076556 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076557 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076558 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076559 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076560 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076561 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076562 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076563 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076564 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076565 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076566 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076567 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076568 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076569 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076570 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076571 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076572 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076573 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076574 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076575 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076576 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076577 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076578 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076579 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076580 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076581 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076582 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076583 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076584 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076585 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076586 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076587 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076588 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076589 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076590 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076591 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076592 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076593 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076594 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076595 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076596 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076597 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076598 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076599 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076600 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076601 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076602 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076603 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076604 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076605 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076606 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076607 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076608 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076609 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076610 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076611 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076612 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076613 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076614 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076615 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076616 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076617 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076618 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076619 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076620 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076621 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076622 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076623 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076624 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076625 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076626 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076627 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076628 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076629 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076630 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076631 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076632 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076633 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076634 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076635 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076636 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076637 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076638 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076639 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076640 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076641 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076642 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076643 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076644 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076645 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076646 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076647 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076648 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076649 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076650 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076651 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076652 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076653 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076654 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076655 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076656 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076657 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076658 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076659 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076660 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076661 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076662 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076663 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076664 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076665 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076666 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076667 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076668 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076669 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076670 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076671 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076672 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076673 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076674 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076675 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076676 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076677 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076678 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076679 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076680 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076681 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076682 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076683 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076684 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076685 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076686 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076687 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076688 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076689 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076690 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076691 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076692 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076693 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076694 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076695 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076696 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076697 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076698 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076699 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076700 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076701 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076702 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076703 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076704 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076705 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076706 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076707 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076708 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076709 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076710 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076711 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076712 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076713 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076714 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076715 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076716 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076717 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076718 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076719 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076720 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076721 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076722 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076723 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076724 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076725 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076726 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076727 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076728 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076729 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076730 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076731 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076732 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076733 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076734 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076735 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076736 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076737 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076738 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076739 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076740 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076741 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076742 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076743 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076744 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076745 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076746 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076747 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076748 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076749 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076750 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076751 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076752 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076753 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076754 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076755 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076756 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076757 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076758 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076759 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076760 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076761 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076762 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076763 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076764 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076765 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076766 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076767 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076768 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076769 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076770 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076771 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076772 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076773 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076774 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076775 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076776 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076777 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076778 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076779 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076780 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076781 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076782 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076783 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076784 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076785 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076786 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076787 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076788 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076789 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076790 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076791 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076792 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076793 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076794 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076795 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076796 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076797 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076798 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076799 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076800 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076801 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076802 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076803 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076804 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076805 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076806 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076807 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076808 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076809 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076810 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076811 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076812 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076813 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076814 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076815 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076816 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076817 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076818 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076819 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076820 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076821 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076822 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076823 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076824 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076825 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076826 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076827 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076828 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076829 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076830 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076831 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076832 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076833 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076834 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076835 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076836 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076837 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076838 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076839 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076840 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076841 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076842 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076843 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076844 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076845 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076846 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076847 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076848 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076849 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076850 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076851 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076852 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076853 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076854 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076855 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076856 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076857 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076858 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076859 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076860 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076861 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076862 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076863 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076864 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076865 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076866 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076867 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076868 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076869 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076870 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076871 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076872 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076873 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076874 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076875 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076876 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076877 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076878 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076879 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076880 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076881 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076882 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076883 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076884 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076885 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076886 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076887 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076888 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076889 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076890 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076891 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076892 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076893 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076894 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076895 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076896 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076897 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076898 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076899 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076900 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076901 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076902 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076903 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076904 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076905 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076906 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076907 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076908 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076909 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076910 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076911 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076912 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076913 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076914 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076915 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076916 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076917 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076918 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076919 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076920 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076921 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076922 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076923 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076924 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076925 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076926 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076927 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076928 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076929 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076930 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076931 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076932 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076933 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076934 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076935 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076936 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076937 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076938 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076939 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076940 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076941 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076942 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076943 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076944 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076945 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076946 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076947 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076948 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076949 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076950 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076951 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076952 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076953 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076954 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076955 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076956 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076957 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076958 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076959 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076960 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076961 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076962 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076963 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076964 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076965 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076966 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076967 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076968 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076969 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076970 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076971 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076972 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076973 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076974 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076975 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076976 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076977 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076978 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076979 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076980 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076981 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076982 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076983 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076984 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076985 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076986 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076987 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076988 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076989 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076990 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076991 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076992 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076993 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076994 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076995 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076996 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076997 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076998 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1076999 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077000 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077001 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077002 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077003 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077004 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077005 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077006 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077007 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077008 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077009 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077010 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077011 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077012 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077013 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077014 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077015 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077016 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077017 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077018 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077019 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077020 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077021 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077022 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077023 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077024 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077025 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077026 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077027 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077028 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077029 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077030 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077031 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077032 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077033 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077034 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077035 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077036 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077037 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077038 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077039 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077040 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077041 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077042 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077043 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077044 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077045 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077046 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077047 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077048 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077049 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077050 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077051 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077052 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077053 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077054 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077055 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077056 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077057 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077058 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077059 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077060 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077061 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077062 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077063 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077064 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077065 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077066 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077067 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077068 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077069 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077070 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077071 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077072 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077073 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077074 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077075 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077076 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077077 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077078 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077079 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077080 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077081 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077082 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077083 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077084 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077085 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077086 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077087 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077088 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077089 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077090 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077091 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077092 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077093 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077094 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077095 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077096 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077097 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077098 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077099 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077100 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077101 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077102 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077103 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077104 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077105 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077106 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077107 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077108 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077109 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077110 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077111 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077112 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077113 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077114 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077115 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077116 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077117 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077118 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077119 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077120 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077121 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077122 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077123 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077124 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077125 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077126 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077127 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077128 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077129 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077130 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077131 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077132 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077133 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077134 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077135 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077136 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077137 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077138 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077139 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077140 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077141 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077142 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077143 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077144 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077145 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077146 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077147 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077148 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077149 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077150 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077151 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077152 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077153 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077154 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077155 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077156 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077157 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077158 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077159 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077160 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077161 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077162 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077163 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077164 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077165 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077166 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077167 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077168 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077169 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077170 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077171 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077172 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077173 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077174 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077175 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077176 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077177 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077178 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077179 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077180 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077181 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077182 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077183 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077184 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077185 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077186 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077187 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077188 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077189 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077190 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077191 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077192 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077193 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077194 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077195 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077196 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077197 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077198 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077199 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077200 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077201 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077202 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077203 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077204 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077205 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077206 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077207 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077208 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077209 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077210 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077211 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077212 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077213 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077214 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077215 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077216 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077217 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077218 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077219 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077220 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077221 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077222 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077223 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077224 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077225 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077226 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077227 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077228 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077229 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077230 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077231 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077232 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077233 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077234 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077235 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077236 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077237 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077238 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077239 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077240 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077241 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077242 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077243 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077244 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077245 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077246 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077247 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077248 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077249 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077250 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077251 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077252 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077253 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077254 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077255 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077256 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077257 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077258 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077259 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077260 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077261 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077262 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077263 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077264 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077265 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077266 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077267 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077268 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077269 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077270 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077271 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077272 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077273 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077274 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077275 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077276 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077277 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077278 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077279 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077280 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077281 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077282 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077283 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077284 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077285 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077286 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077287 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077288 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077289 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077290 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077291 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077292 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077293 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077294 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077295 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077296 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077297 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077298 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077299 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077300 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077301 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077302 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077303 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077304 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077305 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077306 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077307 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077308 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077309 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077310 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077311 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077312 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077313 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077314 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077315 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077316 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077317 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077318 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077319 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077320 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077321 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077322 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077323 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077324 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077325 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077326 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077327 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077328 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077329 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077330 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077331 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077332 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077333 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077334 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077335 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077336 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077337 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077338 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077339 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077340 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077341 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077342 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077343 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077344 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077345 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077346 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077347 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077348 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077349 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077350 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077351 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077352 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077353 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077354 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077355 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077356 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077357 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077358 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077359 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077360 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077361 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077362 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077363 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077364 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077365 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077366 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077367 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077368 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077369 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077370 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077371 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077372 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077373 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077374 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077375 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077376 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077377 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077378 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077379 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077380 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077381 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077382 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077383 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077384 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077385 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077386 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077387 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077388 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077389 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077390 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077391 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077392 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077393 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077394 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077395 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077396 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077397 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077398 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077399 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077400 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077401 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077402 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077403 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077404 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077405 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077406 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077407 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077408 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077409 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077410 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077411 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077412 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077413 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077414 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077415 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077416 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077417 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077418 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077419 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077420 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077421 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077422 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077423 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077424 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077425 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077426 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077427 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077428 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077429 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077430 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077431 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077432 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077433 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077434 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077435 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077436 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077437 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077438 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077439 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077440 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077441 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077442 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077443 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077444 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077445 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077446 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077447 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077448 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077449 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077450 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077451 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077452 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077453 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077454 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077455 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077456 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077457 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077458 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077459 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077460 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077461 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077462 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077463 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077464 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077465 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077466 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077467 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077468 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077469 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077470 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077471 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077472 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077473 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077474 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077475 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077476 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077477 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077478 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077479 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077480 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077481 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077482 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077483 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077484 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077485 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077486 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077487 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077488 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077489 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077490 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077491 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077492 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077493 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077494 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077495 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077496 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077497 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077498 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077499 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077500 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077501 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077502 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077503 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077504 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077505 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077506 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077507 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077508 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077509 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077510 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077511 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077512 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077513 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077514 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077515 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077516 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077517 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077518 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077519 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077520 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077521 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077522 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077523 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077524 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077525 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077526 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077527 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077528 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077529 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077530 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077531 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077532 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077533 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077534 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077535 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077536 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077537 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077538 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077539 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077540 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077541 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077542 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077543 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077544 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077545 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077546 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077547 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077548 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077549 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077550 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077551 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077552 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077553 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077554 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077555 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077556 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077557 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077558 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077559 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077560 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077561 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077562 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077563 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077564 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077565 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077566 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077567 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077568 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077569 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077570 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077571 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077572 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077573 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077574 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077575 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077576 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077577 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077578 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077579 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077580 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077581 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077582 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077583 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077584 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077585 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077586 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077587 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077588 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077589 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077590 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077591 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077592 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077593 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077594 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077595 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077596 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077597 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077598 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077599 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077600 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077601 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077602 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077603 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077604 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077605 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077606 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077607 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077608 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077609 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077610 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077611 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077612 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077613 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077614 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077615 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077616 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077617 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077618 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077619 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077620 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077621 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077622 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077623 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077624 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077625 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077626 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077627 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077628 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077629 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077630 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077631 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077632 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077633 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077634 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077635 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077636 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077637 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077638 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077639 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077640 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077641 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077642 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077643 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077644 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077645 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077646 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077647 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077648 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077649 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077650 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077651 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077652 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077653 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077654 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077655 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077656 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077657 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077658 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077659 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077660 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077661 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077662 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077663 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077664 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077665 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077666 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077667 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077668 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077669 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077670 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077671 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077672 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077673 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077674 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077675 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077676 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077677 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077678 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077679 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077680 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077681 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077682 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077683 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077684 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077685 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077686 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077687 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077688 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077689 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077690 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077691 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077692 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077693 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077694 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077695 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077696 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077697 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077698 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077699 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077700 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077701 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077702 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077703 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077704 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077705 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077706 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077707 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077708 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077709 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077710 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077711 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077712 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077713 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077714 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077715 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077716 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077717 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077718 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077719 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077720 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077721 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077722 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077723 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077724 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077725 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077726 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077727 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077728 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077729 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077730 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077731 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077732 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077733 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077734 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077735 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077736 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077737 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077738 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077739 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077740 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077741 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077742 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077743 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077744 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077745 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077746 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077747 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077748 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077749 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077750 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077751 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077752 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077753 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077754 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077755 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077756 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077757 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077758 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077759 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077760 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077761 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077762 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077763 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077764 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077765 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077766 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077767 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077768 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077769 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077770 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077771 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077772 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077773 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077774 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077775 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077776 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077777 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077778 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077779 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077780 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077781 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077782 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077783 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077784 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077785 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077786 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077787 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077788 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077789 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077790 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077791 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077792 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077793 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077794 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077795 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077796 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077797 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077798 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077799 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077800 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077801 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077802 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077803 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077804 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077805 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077806 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077807 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077808 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077809 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077810 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077811 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077812 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077813 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077814 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077815 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077816 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077817 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077818 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077819 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077820 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077821 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077822 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077823 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077824 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077825 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077826 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077827 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077828 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077829 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077830 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077831 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077832 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077833 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077834 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077835 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077836 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077837 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077838 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077839 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077840 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077841 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077842 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077843 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077844 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077845 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077846 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077847 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077848 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077849 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077850 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077851 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077852 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077853 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077854 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077855 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077856 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077857 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077858 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077859 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077860 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077861 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077862 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077863 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077864 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077865 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077866 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077867 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077868 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077869 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077870 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077871 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077872 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077873 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077874 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077875 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077876 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077877 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077878 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077879 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077880 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077881 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077882 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077883 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077884 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077885 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077886 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077887 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077888 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077889 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077890 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077891 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077892 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077893 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077894 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077895 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077896 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077897 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077898 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077899 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077900 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077901 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077902 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077903 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077904 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077905 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077906 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077907 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077908 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077909 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077910 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077911 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077912 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077913 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077914 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077915 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077916 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077917 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077918 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077919 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077920 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077921 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077922 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077923 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077924 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077925 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077926 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077927 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077928 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077929 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077930 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077931 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077932 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077933 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077934 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077935 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077936 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077937 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077938 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077939 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077940 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077941 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077942 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077943 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077944 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077945 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077946 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077947 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077948 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077949 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077950 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077951 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077952 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077953 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077954 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077955 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077956 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077957 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077958 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077959 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077960 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077961 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077962 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077963 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077964 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077965 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077966 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077967 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077968 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077969 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077970 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077971 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077972 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077973 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077974 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077975 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077976 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077977 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077978 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077979 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077980 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077981 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077982 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077983 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077984 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077985 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077986 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077987 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077988 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077989 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077990 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077991 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077992 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077993 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077994 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077995 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077996 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077997 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077998 (in epoch 153)\n",
      "WARNING:absl:loss is nan for step 1077999 (in epoch 153)\n",
      "INFO:absl:[153] train_loss=nan, train_x1_loss=nan, train_x2_loss=nan\n",
      "INFO:absl:[153] val_loss=nan\n",
      "INFO:absl:[153] test_loss=nan\n",
      "WARNING:absl:loss is nan for step 1078000 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078001 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078002 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078003 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078004 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078005 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078006 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078007 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078008 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078009 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078010 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078011 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078012 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078013 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078014 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078015 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078016 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078017 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078018 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078019 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078020 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078021 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078022 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078023 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078024 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078025 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078026 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078027 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078028 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078029 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078030 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078031 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078032 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078033 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078034 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078035 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078036 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078037 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078038 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078039 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078040 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078041 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078042 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078043 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078044 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078045 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078046 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078047 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078048 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078049 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078050 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078051 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078052 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078053 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078054 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078055 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078056 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078057 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078058 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078059 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078060 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078061 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078062 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078063 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078064 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078065 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078066 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078067 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078068 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078069 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078070 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078071 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078072 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078073 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078074 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078075 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078076 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078077 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078078 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078079 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078080 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078081 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078082 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078083 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078084 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078085 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078086 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078087 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078088 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078089 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078090 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078091 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078092 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078093 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078094 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078095 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078096 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078097 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078098 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078099 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078100 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078101 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078102 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078103 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078104 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078105 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078106 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078107 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078108 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078109 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078110 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078111 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078112 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078113 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078114 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078115 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078116 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078117 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078118 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078119 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078120 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078121 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078122 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078123 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078124 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078125 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078126 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078127 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078128 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078129 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078130 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078131 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078132 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078133 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078134 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078135 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078136 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078137 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078138 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078139 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078140 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078141 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078142 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078143 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078144 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078145 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078146 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078147 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078148 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078149 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078150 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078151 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078152 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078153 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078154 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078155 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078156 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078157 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078158 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078159 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078160 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078161 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078162 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078163 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078164 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078165 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078166 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078167 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078168 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078169 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078170 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078171 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078172 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078173 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078174 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078175 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078176 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078177 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078178 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078179 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078180 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078181 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078182 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078183 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078184 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078185 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078186 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078187 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078188 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078189 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078190 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078191 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078192 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078193 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078194 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078195 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078196 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078197 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078198 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078199 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078200 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078201 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078202 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078203 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078204 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078205 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078206 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078207 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078208 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078209 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078210 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078211 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078212 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078213 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078214 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078215 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078216 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078217 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078218 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078219 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078220 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078221 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078222 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078223 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078224 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078225 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078226 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078227 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078228 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078229 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078230 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078231 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078232 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078233 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078234 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078235 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078236 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078237 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078238 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078239 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078240 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078241 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078242 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078243 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078244 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078245 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078246 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078247 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078248 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078249 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078250 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078251 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078252 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078253 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078254 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078255 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078256 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078257 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078258 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078259 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078260 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078261 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078262 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078263 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078264 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078265 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078266 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078267 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078268 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078269 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078270 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078271 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078272 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078273 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078274 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078275 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078276 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078277 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078278 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078279 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078280 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078281 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078282 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078283 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078284 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078285 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078286 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078287 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078288 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078289 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078290 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078291 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078292 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078293 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078294 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078295 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078296 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078297 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078298 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078299 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078300 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078301 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078302 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078303 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078304 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078305 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078306 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078307 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078308 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078309 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078310 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078311 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078312 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078313 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078314 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078315 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078316 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078317 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078318 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078319 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078320 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078321 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078322 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078323 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078324 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078325 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078326 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078327 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078328 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078329 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078330 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078331 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078332 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078333 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078334 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078335 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078336 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078337 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078338 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078339 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078340 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078341 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078342 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078343 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078344 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078345 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078346 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078347 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078348 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078349 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078350 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078351 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078352 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078353 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078354 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078355 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078356 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078357 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078358 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078359 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078360 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078361 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078362 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078363 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078364 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078365 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078366 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078367 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078368 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078369 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078370 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078371 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078372 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078373 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078374 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078375 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078376 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078377 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078378 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078379 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078380 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078381 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078382 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078383 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078384 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078385 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078386 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078387 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078388 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078389 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078390 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078391 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078392 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078393 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078394 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078395 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078396 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078397 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078398 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078399 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078400 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078401 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078402 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078403 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078404 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078405 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078406 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078407 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078408 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078409 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078410 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078411 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078412 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078413 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078414 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078415 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078416 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078417 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078418 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078419 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078420 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078421 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078422 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078423 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078424 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078425 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078426 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078427 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078428 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078429 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078430 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078431 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078432 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078433 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078434 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078435 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078436 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078437 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078438 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078439 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078440 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078441 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078442 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078443 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078444 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078445 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078446 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078447 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078448 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078449 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078450 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078451 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078452 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078453 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078454 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078455 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078456 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078457 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078458 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078459 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078460 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078461 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078462 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078463 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078464 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078465 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078466 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078467 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078468 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078469 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078470 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078471 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078472 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078473 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078474 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078475 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078476 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078477 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078478 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078479 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078480 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078481 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078482 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078483 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078484 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078485 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078486 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078487 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078488 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078489 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078490 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078491 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078492 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078493 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078494 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078495 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078496 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078497 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078498 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078499 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078500 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078501 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078502 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078503 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078504 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078505 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078506 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078507 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078508 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078509 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078510 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078511 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078512 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078513 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078514 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078515 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078516 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078517 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078518 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078519 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078520 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078521 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078522 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078523 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078524 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078525 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078526 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078527 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078528 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078529 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078530 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078531 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078532 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078533 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078534 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078535 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078536 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078537 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078538 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078539 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078540 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078541 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078542 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078543 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078544 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078545 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078546 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078547 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078548 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078549 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078550 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078551 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078552 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078553 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078554 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078555 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078556 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078557 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078558 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078559 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078560 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078561 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078562 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078563 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078564 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078565 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078566 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078567 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078568 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078569 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078570 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078571 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078572 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078573 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078574 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078575 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078576 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078577 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078578 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078579 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078580 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078581 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078582 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078583 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078584 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078585 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078586 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078587 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078588 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078589 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078590 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078591 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078592 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078593 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078594 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078595 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078596 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078597 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078598 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078599 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078600 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078601 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078602 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078603 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078604 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078605 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078606 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078607 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078608 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078609 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078610 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078611 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078612 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078613 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078614 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078615 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078616 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078617 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078618 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078619 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078620 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078621 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078622 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078623 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078624 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078625 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078626 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078627 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078628 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078629 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078630 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078631 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078632 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078633 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078634 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078635 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078636 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078637 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078638 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078639 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078640 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078641 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078642 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078643 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078644 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078645 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078646 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078647 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078648 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078649 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078650 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078651 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078652 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078653 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078654 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078655 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078656 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078657 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078658 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078659 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078660 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078661 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078662 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078663 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078664 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078665 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078666 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078667 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078668 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078669 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078670 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078671 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078672 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078673 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078674 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078675 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078676 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078677 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078678 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078679 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078680 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078681 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078682 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078683 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078684 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078685 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078686 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078687 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078688 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078689 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078690 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078691 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078692 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078693 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078694 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078695 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078696 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078697 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078698 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078699 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078700 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078701 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078702 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078703 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078704 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078705 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078706 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078707 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078708 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078709 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078710 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078711 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078712 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078713 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078714 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078715 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078716 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078717 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078718 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078719 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078720 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078721 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078722 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078723 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078724 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078725 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078726 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078727 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078728 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078729 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078730 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078731 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078732 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078733 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078734 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078735 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078736 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078737 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078738 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078739 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078740 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078741 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078742 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078743 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078744 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078745 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078746 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078747 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078748 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078749 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078750 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078751 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078752 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078753 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078754 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078755 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078756 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078757 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078758 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078759 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078760 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078761 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078762 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078763 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078764 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078765 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078766 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078767 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078768 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078769 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078770 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078771 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078772 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078773 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078774 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078775 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078776 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078777 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078778 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078779 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078780 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078781 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078782 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078783 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078784 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078785 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078786 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078787 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078788 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078789 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078790 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078791 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078792 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078793 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078794 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078795 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078796 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078797 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078798 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078799 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078800 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078801 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078802 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078803 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078804 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078805 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078806 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078807 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078808 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078809 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078810 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078811 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078812 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078813 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078814 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078815 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078816 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078817 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078818 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078819 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078820 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078821 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078822 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078823 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078824 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078825 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078826 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078827 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078828 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078829 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078830 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078831 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078832 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078833 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078834 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078835 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078836 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078837 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078838 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078839 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078840 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078841 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078842 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078843 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078844 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078845 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078846 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078847 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078848 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078849 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078850 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078851 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078852 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078853 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078854 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078855 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078856 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078857 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078858 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078859 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078860 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078861 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078862 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078863 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078864 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078865 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078866 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078867 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078868 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078869 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078870 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078871 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078872 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078873 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078874 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078875 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078876 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078877 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078878 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078879 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078880 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078881 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078882 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078883 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078884 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078885 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078886 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078887 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078888 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078889 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078890 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078891 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078892 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078893 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078894 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078895 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078896 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078897 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078898 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078899 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078900 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078901 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078902 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078903 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078904 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078905 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078906 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078907 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078908 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078909 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078910 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078911 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078912 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078913 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078914 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078915 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078916 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078917 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078918 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078919 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078920 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078921 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078922 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078923 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078924 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078925 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078926 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078927 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078928 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078929 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078930 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078931 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078932 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078933 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078934 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078935 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078936 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078937 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078938 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078939 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078940 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078941 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078942 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078943 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078944 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078945 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078946 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078947 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078948 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078949 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078950 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078951 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078952 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078953 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078954 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078955 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078956 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078957 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078958 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078959 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078960 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078961 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078962 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078963 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078964 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078965 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078966 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078967 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078968 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078969 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078970 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078971 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078972 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078973 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078974 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078975 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078976 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078977 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078978 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078979 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078980 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078981 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078982 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078983 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078984 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078985 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078986 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078987 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078988 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078989 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078990 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078991 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078992 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078993 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078994 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078995 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078996 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078997 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078998 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1078999 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079000 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079001 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079002 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079003 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079004 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079005 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079006 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079007 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079008 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079009 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079010 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079011 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079012 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079013 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079014 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079015 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079016 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079017 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079018 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079019 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079020 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079021 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079022 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079023 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079024 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079025 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079026 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079027 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079028 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079029 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079030 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079031 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079032 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079033 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079034 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079035 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079036 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079037 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079038 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079039 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079040 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079041 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079042 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079043 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079044 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079045 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079046 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079047 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079048 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079049 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079050 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079051 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079052 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079053 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079054 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079055 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079056 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079057 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079058 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079059 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079060 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079061 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079062 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079063 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079064 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079065 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079066 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079067 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079068 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079069 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079070 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079071 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079072 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079073 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079074 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079075 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079076 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079077 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079078 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079079 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079080 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079081 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079082 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079083 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079084 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079085 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079086 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079087 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079088 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079089 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079090 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079091 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079092 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079093 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079094 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079095 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079096 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079097 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079098 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079099 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079100 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079101 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079102 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079103 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079104 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079105 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079106 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079107 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079108 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079109 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079110 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079111 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079112 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079113 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079114 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079115 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079116 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079117 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079118 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079119 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079120 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079121 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079122 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079123 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079124 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079125 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079126 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079127 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079128 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079129 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079130 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079131 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079132 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079133 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079134 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079135 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079136 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079137 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079138 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079139 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079140 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079141 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079142 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079143 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079144 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079145 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079146 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079147 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079148 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079149 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079150 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079151 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079152 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079153 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079154 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079155 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079156 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079157 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079158 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079159 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079160 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079161 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079162 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079163 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079164 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079165 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079166 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079167 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079168 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079169 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079170 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079171 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079172 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079173 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079174 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079175 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079176 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079177 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079178 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079179 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079180 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079181 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079182 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079183 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079184 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079185 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079186 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079187 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079188 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079189 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079190 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079191 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079192 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079193 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079194 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079195 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079196 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079197 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079198 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079199 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079200 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079201 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079202 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079203 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079204 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079205 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079206 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079207 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079208 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079209 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079210 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079211 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079212 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079213 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079214 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079215 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079216 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079217 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079218 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079219 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079220 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079221 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079222 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079223 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079224 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079225 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079226 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079227 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079228 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079229 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079230 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079231 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079232 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079233 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079234 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079235 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079236 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079237 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079238 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079239 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079240 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079241 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079242 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079243 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079244 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079245 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079246 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079247 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079248 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079249 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079250 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079251 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079252 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079253 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079254 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079255 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079256 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079257 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079258 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079259 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079260 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079261 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079262 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079263 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079264 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079265 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079266 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079267 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079268 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079269 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079270 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079271 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079272 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079273 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079274 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079275 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079276 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079277 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079278 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079279 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079280 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079281 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079282 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079283 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079284 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079285 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079286 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079287 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079288 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079289 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079290 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079291 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079292 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079293 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079294 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079295 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079296 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079297 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079298 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079299 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079300 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079301 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079302 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079303 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079304 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079305 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079306 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079307 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079308 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079309 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079310 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079311 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079312 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079313 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079314 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079315 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079316 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079317 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079318 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079319 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079320 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079321 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079322 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079323 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079324 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079325 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079326 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079327 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079328 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079329 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079330 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079331 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079332 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079333 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079334 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079335 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079336 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079337 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079338 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079339 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079340 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079341 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079342 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079343 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079344 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079345 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079346 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079347 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079348 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079349 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079350 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079351 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079352 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079353 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079354 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079355 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079356 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079357 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079358 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079359 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079360 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079361 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079362 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079363 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079364 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079365 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079366 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079367 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079368 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079369 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079370 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079371 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079372 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079373 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079374 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079375 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079376 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079377 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079378 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079379 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079380 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079381 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079382 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079383 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079384 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079385 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079386 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079387 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079388 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079389 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079390 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079391 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079392 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079393 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079394 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079395 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079396 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079397 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079398 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079399 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079400 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079401 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079402 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079403 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079404 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079405 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079406 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079407 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079408 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079409 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079410 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079411 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079412 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079413 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079414 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079415 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079416 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079417 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079418 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079419 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079420 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079421 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079422 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079423 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079424 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079425 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079426 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079427 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079428 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079429 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079430 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079431 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079432 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079433 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079434 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079435 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079436 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079437 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079438 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079439 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079440 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079441 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079442 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079443 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079444 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079445 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079446 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079447 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079448 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079449 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079450 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079451 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079452 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079453 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079454 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079455 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079456 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079457 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079458 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079459 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079460 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079461 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079462 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079463 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079464 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079465 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079466 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079467 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079468 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079469 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079470 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079471 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079472 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079473 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079474 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079475 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079476 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079477 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079478 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079479 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079480 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079481 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079482 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079483 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079484 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079485 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079486 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079487 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079488 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079489 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079490 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079491 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079492 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079493 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079494 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079495 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079496 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079497 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079498 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079499 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079500 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079501 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079502 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079503 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079504 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079505 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079506 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079507 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079508 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079509 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079510 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079511 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079512 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079513 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079514 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079515 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079516 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079517 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079518 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079519 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079520 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079521 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079522 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079523 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079524 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079525 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079526 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079527 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079528 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079529 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079530 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079531 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079532 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079533 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079534 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079535 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079536 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079537 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079538 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079539 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079540 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079541 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079542 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079543 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079544 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079545 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079546 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079547 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079548 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079549 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079550 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079551 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079552 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079553 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079554 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079555 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079556 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079557 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079558 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079559 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079560 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079561 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079562 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079563 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079564 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079565 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079566 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079567 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079568 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079569 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079570 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079571 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079572 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079573 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079574 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079575 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079576 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079577 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079578 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079579 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079580 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079581 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079582 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079583 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079584 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079585 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079586 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079587 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079588 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079589 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079590 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079591 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079592 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079593 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079594 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079595 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079596 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079597 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079598 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079599 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079600 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079601 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079602 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079603 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079604 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079605 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079606 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079607 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079608 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079609 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079610 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079611 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079612 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079613 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079614 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079615 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079616 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079617 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079618 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079619 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079620 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079621 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079622 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079623 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079624 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079625 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079626 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079627 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079628 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079629 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079630 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079631 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079632 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079633 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079634 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079635 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079636 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079637 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079638 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079639 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079640 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079641 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079642 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079643 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079644 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079645 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079646 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079647 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079648 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079649 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079650 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079651 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079652 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079653 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079654 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079655 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079656 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079657 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079658 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079659 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079660 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079661 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079662 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079663 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079664 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079665 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079666 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079667 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079668 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079669 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079670 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079671 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079672 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079673 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079674 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079675 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079676 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079677 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079678 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079679 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079680 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079681 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079682 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079683 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079684 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079685 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079686 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079687 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079688 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079689 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079690 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079691 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079692 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079693 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079694 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079695 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079696 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079697 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079698 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079699 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079700 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079701 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079702 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079703 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079704 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079705 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079706 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079707 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079708 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079709 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079710 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079711 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079712 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079713 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079714 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079715 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079716 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079717 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079718 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079719 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079720 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079721 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079722 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079723 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079724 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079725 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079726 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079727 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079728 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079729 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079730 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079731 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079732 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079733 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079734 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079735 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079736 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079737 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079738 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079739 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079740 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079741 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079742 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079743 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079744 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079745 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079746 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079747 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079748 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079749 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079750 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079751 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079752 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079753 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079754 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079755 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079756 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079757 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079758 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079759 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079760 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079761 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079762 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079763 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079764 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079765 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079766 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079767 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079768 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079769 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079770 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079771 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079772 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079773 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079774 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079775 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079776 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079777 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079778 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079779 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079780 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079781 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079782 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079783 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079784 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079785 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079786 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079787 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079788 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079789 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079790 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079791 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079792 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079793 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079794 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079795 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079796 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079797 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079798 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079799 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079800 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079801 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079802 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079803 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079804 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079805 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079806 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079807 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079808 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079809 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079810 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079811 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079812 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079813 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079814 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079815 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079816 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079817 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079818 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079819 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079820 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079821 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079822 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079823 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079824 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079825 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079826 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079827 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079828 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079829 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079830 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079831 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079832 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079833 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079834 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079835 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079836 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079837 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079838 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079839 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079840 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079841 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079842 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079843 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079844 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079845 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079846 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079847 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079848 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079849 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079850 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079851 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079852 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079853 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079854 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079855 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079856 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079857 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079858 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079859 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079860 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079861 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079862 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079863 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079864 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079865 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079866 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079867 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079868 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079869 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079870 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079871 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079872 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079873 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079874 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079875 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079876 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079877 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079878 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079879 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079880 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079881 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079882 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079883 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079884 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079885 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079886 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079887 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079888 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079889 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079890 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079891 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079892 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079893 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079894 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079895 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079896 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079897 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079898 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079899 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079900 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079901 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079902 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079903 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079904 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079905 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079906 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079907 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079908 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079909 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079910 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079911 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079912 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079913 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079914 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079915 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079916 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079917 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079918 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079919 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079920 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079921 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079922 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079923 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079924 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079925 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079926 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079927 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079928 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079929 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079930 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079931 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079932 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079933 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079934 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079935 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079936 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079937 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079938 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079939 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079940 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079941 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079942 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079943 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079944 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079945 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079946 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079947 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079948 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079949 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079950 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079951 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079952 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079953 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079954 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079955 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079956 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079957 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079958 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079959 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079960 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079961 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079962 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079963 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079964 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079965 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079966 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079967 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079968 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079969 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079970 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079971 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079972 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079973 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079974 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079975 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079976 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079977 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079978 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079979 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079980 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079981 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079982 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079983 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079984 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079985 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079986 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079987 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079988 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079989 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079990 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079991 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079992 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079993 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079994 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079995 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079996 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079997 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079998 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1079999 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080000 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080001 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080002 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080003 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080004 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080005 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080006 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080007 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080008 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080009 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080010 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080011 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080012 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080013 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080014 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080015 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080016 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080017 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080018 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080019 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080020 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080021 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080022 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080023 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080024 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080025 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080026 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080027 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080028 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080029 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080030 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080031 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080032 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080033 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080034 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080035 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080036 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080037 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080038 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080039 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080040 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080041 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080042 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080043 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080044 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080045 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080046 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080047 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080048 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080049 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080050 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080051 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080052 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080053 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080054 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080055 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080056 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080057 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080058 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080059 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080060 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080061 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080062 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080063 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080064 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080065 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080066 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080067 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080068 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080069 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080070 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080071 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080072 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080073 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080074 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080075 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080076 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080077 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080078 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080079 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080080 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080081 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080082 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080083 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080084 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080085 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080086 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080087 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080088 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080089 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080090 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080091 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080092 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080093 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080094 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080095 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080096 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080097 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080098 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080099 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080100 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080101 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080102 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080103 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080104 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080105 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080106 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080107 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080108 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080109 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080110 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080111 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080112 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080113 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080114 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080115 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080116 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080117 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080118 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080119 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080120 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080121 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080122 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080123 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080124 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080125 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080126 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080127 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080128 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080129 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080130 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080131 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080132 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080133 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080134 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080135 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080136 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080137 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080138 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080139 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080140 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080141 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080142 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080143 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080144 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080145 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080146 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080147 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080148 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080149 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080150 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080151 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080152 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080153 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080154 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080155 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080156 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080157 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080158 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080159 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080160 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080161 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080162 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080163 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080164 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080165 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080166 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080167 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080168 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080169 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080170 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080171 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080172 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080173 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080174 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080175 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080176 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080177 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080178 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080179 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080180 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080181 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080182 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080183 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080184 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080185 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080186 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080187 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080188 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080189 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080190 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080191 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080192 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080193 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080194 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080195 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080196 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080197 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080198 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080199 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080200 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080201 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080202 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080203 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080204 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080205 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080206 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080207 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080208 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080209 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080210 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080211 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080212 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080213 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080214 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080215 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080216 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080217 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080218 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080219 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080220 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080221 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080222 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080223 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080224 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080225 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080226 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080227 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080228 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080229 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080230 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080231 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080232 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080233 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080234 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080235 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080236 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080237 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080238 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080239 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080240 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080241 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080242 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080243 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080244 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080245 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080246 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080247 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080248 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080249 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080250 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080251 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080252 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080253 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080254 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080255 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080256 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080257 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080258 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080259 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080260 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080261 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080262 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080263 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080264 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080265 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080266 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080267 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080268 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080269 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080270 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080271 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080272 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080273 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080274 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080275 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080276 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080277 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080278 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080279 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080280 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080281 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080282 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080283 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080284 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080285 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080286 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080287 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080288 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080289 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080290 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080291 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080292 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080293 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080294 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080295 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080296 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080297 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080298 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080299 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080300 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080301 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080302 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080303 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080304 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080305 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080306 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080307 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080308 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080309 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080310 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080311 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080312 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080313 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080314 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080315 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080316 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080317 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080318 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080319 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080320 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080321 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080322 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080323 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080324 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080325 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080326 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080327 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080328 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080329 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080330 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080331 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080332 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080333 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080334 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080335 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080336 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080337 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080338 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080339 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080340 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080341 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080342 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080343 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080344 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080345 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080346 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080347 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080348 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080349 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080350 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080351 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080352 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080353 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080354 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080355 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080356 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080357 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080358 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080359 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080360 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080361 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080362 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080363 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080364 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080365 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080366 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080367 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080368 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080369 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080370 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080371 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080372 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080373 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080374 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080375 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080376 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080377 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080378 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080379 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080380 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080381 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080382 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080383 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080384 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080385 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080386 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080387 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080388 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080389 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080390 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080391 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080392 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080393 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080394 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080395 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080396 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080397 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080398 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080399 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080400 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080401 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080402 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080403 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080404 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080405 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080406 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080407 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080408 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080409 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080410 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080411 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080412 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080413 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080414 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080415 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080416 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080417 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080418 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080419 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080420 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080421 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080422 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080423 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080424 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080425 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080426 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080427 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080428 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080429 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080430 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080431 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080432 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080433 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080434 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080435 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080436 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080437 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080438 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080439 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080440 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080441 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080442 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080443 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080444 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080445 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080446 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080447 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080448 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080449 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080450 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080451 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080452 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080453 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080454 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080455 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080456 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080457 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080458 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080459 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080460 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080461 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080462 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080463 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080464 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080465 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080466 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080467 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080468 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080469 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080470 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080471 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080472 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080473 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080474 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080475 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080476 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080477 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080478 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080479 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080480 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080481 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080482 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080483 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080484 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080485 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080486 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080487 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080488 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080489 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080490 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080491 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080492 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080493 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080494 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080495 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080496 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080497 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080498 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080499 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080500 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080501 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080502 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080503 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080504 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080505 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080506 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080507 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080508 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080509 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080510 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080511 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080512 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080513 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080514 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080515 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080516 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080517 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080518 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080519 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080520 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080521 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080522 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080523 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080524 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080525 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080526 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080527 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080528 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080529 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080530 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080531 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080532 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080533 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080534 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080535 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080536 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080537 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080538 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080539 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080540 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080541 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080542 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080543 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080544 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080545 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080546 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080547 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080548 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080549 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080550 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080551 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080552 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080553 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080554 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080555 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080556 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080557 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080558 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080559 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080560 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080561 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080562 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080563 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080564 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080565 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080566 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080567 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080568 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080569 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080570 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080571 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080572 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080573 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080574 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080575 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080576 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080577 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080578 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080579 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080580 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080581 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080582 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080583 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080584 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080585 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080586 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080587 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080588 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080589 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080590 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080591 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080592 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080593 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080594 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080595 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080596 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080597 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080598 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080599 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080600 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080601 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080602 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080603 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080604 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080605 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080606 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080607 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080608 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080609 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080610 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080611 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080612 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080613 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080614 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080615 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080616 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080617 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080618 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080619 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080620 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080621 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080622 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080623 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080624 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080625 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080626 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080627 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080628 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080629 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080630 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080631 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080632 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080633 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080634 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080635 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080636 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080637 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080638 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080639 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080640 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080641 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080642 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080643 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080644 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080645 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080646 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080647 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080648 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080649 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080650 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080651 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080652 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080653 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080654 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080655 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080656 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080657 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080658 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080659 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080660 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080661 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080662 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080663 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080664 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080665 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080666 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080667 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080668 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080669 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080670 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080671 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080672 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080673 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080674 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080675 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080676 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080677 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080678 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080679 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080680 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080681 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080682 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080683 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080684 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080685 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080686 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080687 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080688 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080689 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080690 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080691 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080692 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080693 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080694 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080695 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080696 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080697 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080698 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080699 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080700 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080701 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080702 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080703 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080704 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080705 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080706 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080707 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080708 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080709 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080710 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080711 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080712 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080713 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080714 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080715 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080716 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080717 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080718 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080719 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080720 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080721 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080722 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080723 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080724 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080725 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080726 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080727 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080728 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080729 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080730 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080731 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080732 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080733 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080734 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080735 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080736 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080737 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080738 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080739 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080740 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080741 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080742 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080743 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080744 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080745 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080746 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080747 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080748 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080749 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080750 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080751 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080752 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080753 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080754 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080755 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080756 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080757 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080758 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080759 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080760 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080761 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080762 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080763 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080764 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080765 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080766 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080767 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080768 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080769 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080770 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080771 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080772 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080773 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080774 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080775 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080776 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080777 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080778 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080779 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080780 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080781 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080782 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080783 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080784 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080785 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080786 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080787 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080788 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080789 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080790 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080791 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080792 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080793 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080794 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080795 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080796 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080797 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080798 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080799 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080800 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080801 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080802 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080803 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080804 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080805 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080806 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080807 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080808 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080809 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080810 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080811 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080812 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080813 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080814 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080815 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080816 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080817 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080818 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080819 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080820 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080821 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080822 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080823 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080824 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080825 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080826 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080827 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080828 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080829 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080830 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080831 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080832 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080833 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080834 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080835 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080836 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080837 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080838 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080839 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080840 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080841 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080842 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080843 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080844 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080845 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080846 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080847 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080848 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080849 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080850 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080851 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080852 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080853 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080854 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080855 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080856 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080857 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080858 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080859 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080860 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080861 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080862 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080863 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080864 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080865 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080866 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080867 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080868 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080869 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080870 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080871 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080872 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080873 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080874 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080875 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080876 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080877 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080878 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080879 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080880 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080881 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080882 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080883 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080884 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080885 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080886 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080887 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080888 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080889 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080890 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080891 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080892 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080893 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080894 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080895 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080896 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080897 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080898 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080899 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080900 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080901 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080902 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080903 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080904 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080905 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080906 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080907 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080908 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080909 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080910 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080911 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080912 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080913 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080914 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080915 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080916 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080917 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080918 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080919 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080920 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080921 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080922 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080923 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080924 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080925 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080926 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080927 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080928 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080929 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080930 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080931 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080932 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080933 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080934 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080935 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080936 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080937 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080938 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080939 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080940 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080941 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080942 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080943 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080944 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080945 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080946 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080947 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080948 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080949 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080950 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080951 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080952 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080953 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080954 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080955 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080956 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080957 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080958 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080959 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080960 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080961 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080962 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080963 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080964 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080965 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080966 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080967 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080968 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080969 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080970 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080971 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080972 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080973 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080974 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080975 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080976 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080977 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080978 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080979 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080980 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080981 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080982 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080983 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080984 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080985 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080986 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080987 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080988 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080989 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080990 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080991 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080992 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080993 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080994 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080995 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080996 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080997 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080998 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1080999 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081000 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081001 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081002 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081003 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081004 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081005 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081006 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081007 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081008 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081009 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081010 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081011 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081012 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081013 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081014 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081015 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081016 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081017 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081018 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081019 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081020 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081021 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081022 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081023 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081024 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081025 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081026 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081027 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081028 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081029 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081030 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081031 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081032 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081033 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081034 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081035 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081036 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081037 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081038 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081039 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081040 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081041 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081042 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081043 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081044 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081045 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081046 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081047 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081048 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081049 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081050 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081051 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081052 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081053 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081054 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081055 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081056 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081057 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081058 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081059 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081060 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081061 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081062 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081063 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081064 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081065 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081066 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081067 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081068 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081069 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081070 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081071 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081072 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081073 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081074 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081075 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081076 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081077 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081078 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081079 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081080 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081081 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081082 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081083 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081084 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081085 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081086 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081087 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081088 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081089 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081090 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081091 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081092 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081093 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081094 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081095 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081096 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081097 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081098 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081099 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081100 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081101 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081102 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081103 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081104 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081105 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081106 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081107 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081108 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081109 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081110 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081111 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081112 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081113 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081114 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081115 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081116 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081117 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081118 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081119 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081120 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081121 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081122 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081123 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081124 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081125 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081126 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081127 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081128 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081129 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081130 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081131 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081132 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081133 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081134 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081135 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081136 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081137 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081138 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081139 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081140 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081141 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081142 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081143 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081144 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081145 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081146 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081147 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081148 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081149 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081150 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081151 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081152 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081153 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081154 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081155 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081156 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081157 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081158 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081159 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081160 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081161 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081162 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081163 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081164 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081165 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081166 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081167 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081168 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081169 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081170 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081171 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081172 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081173 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081174 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081175 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081176 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081177 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081178 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081179 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081180 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081181 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081182 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081183 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081184 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081185 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081186 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081187 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081188 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081189 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081190 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081191 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081192 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081193 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081194 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081195 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081196 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081197 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081198 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081199 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081200 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081201 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081202 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081203 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081204 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081205 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081206 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081207 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081208 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081209 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081210 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081211 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081212 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081213 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081214 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081215 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081216 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081217 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081218 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081219 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081220 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081221 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081222 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081223 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081224 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081225 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081226 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081227 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081228 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081229 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081230 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081231 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081232 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081233 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081234 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081235 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081236 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081237 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081238 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081239 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081240 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081241 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081242 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081243 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081244 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081245 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081246 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081247 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081248 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081249 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081250 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081251 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081252 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081253 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081254 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081255 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081256 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081257 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081258 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081259 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081260 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081261 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081262 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081263 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081264 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081265 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081266 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081267 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081268 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081269 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081270 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081271 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081272 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081273 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081274 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081275 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081276 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081277 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081278 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081279 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081280 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081281 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081282 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081283 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081284 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081285 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081286 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081287 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081288 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081289 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081290 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081291 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081292 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081293 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081294 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081295 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081296 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081297 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081298 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081299 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081300 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081301 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081302 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081303 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081304 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081305 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081306 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081307 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081308 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081309 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081310 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081311 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081312 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081313 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081314 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081315 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081316 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081317 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081318 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081319 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081320 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081321 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081322 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081323 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081324 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081325 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081326 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081327 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081328 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081329 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081330 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081331 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081332 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081333 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081334 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081335 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081336 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081337 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081338 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081339 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081340 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081341 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081342 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081343 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081344 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081345 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081346 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081347 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081348 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081349 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081350 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081351 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081352 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081353 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081354 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081355 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081356 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081357 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081358 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081359 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081360 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081361 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081362 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081363 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081364 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081365 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081366 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081367 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081368 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081369 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081370 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081371 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081372 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081373 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081374 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081375 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081376 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081377 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081378 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081379 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081380 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081381 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081382 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081383 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081384 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081385 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081386 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081387 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081388 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081389 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081390 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081391 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081392 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081393 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081394 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081395 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081396 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081397 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081398 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081399 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081400 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081401 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081402 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081403 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081404 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081405 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081406 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081407 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081408 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081409 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081410 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081411 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081412 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081413 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081414 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081415 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081416 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081417 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081418 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081419 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081420 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081421 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081422 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081423 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081424 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081425 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081426 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081427 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081428 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081429 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081430 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081431 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081432 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081433 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081434 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081435 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081436 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081437 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081438 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081439 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081440 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081441 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081442 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081443 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081444 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081445 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081446 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081447 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081448 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081449 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081450 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081451 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081452 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081453 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081454 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081455 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081456 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081457 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081458 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081459 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081460 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081461 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081462 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081463 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081464 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081465 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081466 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081467 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081468 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081469 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081470 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081471 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081472 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081473 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081474 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081475 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081476 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081477 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081478 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081479 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081480 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081481 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081482 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081483 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081484 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081485 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081486 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081487 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081488 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081489 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081490 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081491 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081492 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081493 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081494 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081495 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081496 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081497 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081498 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081499 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081500 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081501 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081502 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081503 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081504 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081505 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081506 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081507 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081508 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081509 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081510 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081511 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081512 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081513 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081514 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081515 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081516 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081517 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081518 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081519 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081520 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081521 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081522 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081523 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081524 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081525 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081526 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081527 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081528 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081529 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081530 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081531 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081532 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081533 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081534 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081535 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081536 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081537 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081538 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081539 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081540 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081541 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081542 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081543 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081544 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081545 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081546 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081547 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081548 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081549 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081550 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081551 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081552 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081553 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081554 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081555 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081556 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081557 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081558 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081559 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081560 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081561 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081562 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081563 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081564 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081565 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081566 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081567 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081568 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081569 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081570 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081571 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081572 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081573 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081574 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081575 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081576 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081577 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081578 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081579 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081580 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081581 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081582 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081583 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081584 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081585 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081586 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081587 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081588 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081589 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081590 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081591 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081592 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081593 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081594 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081595 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081596 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081597 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081598 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081599 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081600 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081601 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081602 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081603 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081604 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081605 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081606 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081607 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081608 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081609 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081610 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081611 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081612 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081613 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081614 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081615 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081616 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081617 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081618 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081619 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081620 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081621 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081622 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081623 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081624 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081625 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081626 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081627 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081628 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081629 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081630 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081631 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081632 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081633 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081634 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081635 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081636 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081637 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081638 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081639 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081640 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081641 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081642 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081643 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081644 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081645 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081646 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081647 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081648 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081649 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081650 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081651 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081652 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081653 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081654 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081655 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081656 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081657 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081658 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081659 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081660 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081661 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081662 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081663 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081664 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081665 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081666 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081667 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081668 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081669 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081670 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081671 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081672 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081673 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081674 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081675 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081676 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081677 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081678 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081679 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081680 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081681 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081682 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081683 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081684 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081685 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081686 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081687 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081688 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081689 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081690 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081691 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081692 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081693 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081694 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081695 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081696 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081697 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081698 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081699 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081700 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081701 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081702 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081703 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081704 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081705 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081706 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081707 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081708 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081709 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081710 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081711 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081712 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081713 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081714 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081715 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081716 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081717 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081718 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081719 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081720 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081721 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081722 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081723 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081724 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081725 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081726 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081727 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081728 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081729 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081730 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081731 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081732 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081733 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081734 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081735 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081736 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081737 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081738 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081739 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081740 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081741 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081742 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081743 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081744 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081745 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081746 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081747 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081748 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081749 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081750 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081751 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081752 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081753 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081754 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081755 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081756 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081757 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081758 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081759 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081760 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081761 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081762 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081763 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081764 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081765 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081766 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081767 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081768 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081769 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081770 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081771 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081772 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081773 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081774 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081775 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081776 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081777 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081778 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081779 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081780 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081781 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081782 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081783 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081784 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081785 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081786 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081787 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081788 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081789 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081790 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081791 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081792 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081793 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081794 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081795 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081796 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081797 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081798 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081799 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081800 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081801 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081802 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081803 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081804 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081805 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081806 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081807 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081808 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081809 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081810 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081811 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081812 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081813 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081814 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081815 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081816 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081817 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081818 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081819 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081820 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081821 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081822 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081823 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081824 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081825 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081826 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081827 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081828 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081829 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081830 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081831 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081832 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081833 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081834 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081835 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081836 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081837 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081838 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081839 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081840 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081841 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081842 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081843 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081844 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081845 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081846 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081847 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081848 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081849 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081850 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081851 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081852 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081853 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081854 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081855 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081856 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081857 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081858 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081859 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081860 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081861 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081862 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081863 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081864 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081865 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081866 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081867 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081868 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081869 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081870 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081871 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081872 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081873 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081874 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081875 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081876 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081877 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081878 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081879 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081880 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081881 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081882 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081883 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081884 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081885 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081886 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081887 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081888 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081889 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081890 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081891 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081892 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081893 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081894 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081895 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081896 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081897 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081898 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081899 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081900 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081901 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081902 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081903 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081904 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081905 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081906 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081907 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081908 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081909 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081910 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081911 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081912 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081913 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081914 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081915 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081916 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081917 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081918 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081919 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081920 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081921 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081922 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081923 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081924 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081925 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081926 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081927 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081928 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081929 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081930 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081931 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081932 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081933 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081934 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081935 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081936 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081937 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081938 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081939 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081940 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081941 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081942 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081943 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081944 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081945 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081946 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081947 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081948 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081949 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081950 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081951 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081952 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081953 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081954 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081955 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081956 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081957 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081958 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081959 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081960 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081961 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081962 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081963 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081964 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081965 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081966 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081967 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081968 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081969 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081970 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081971 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081972 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081973 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081974 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081975 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081976 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081977 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081978 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081979 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081980 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081981 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081982 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081983 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081984 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081985 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081986 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081987 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081988 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081989 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081990 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081991 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081992 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081993 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081994 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081995 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081996 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081997 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081998 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1081999 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082000 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082001 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082002 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082003 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082004 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082005 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082006 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082007 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082008 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082009 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082010 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082011 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082012 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082013 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082014 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082015 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082016 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082017 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082018 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082019 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082020 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082021 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082022 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082023 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082024 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082025 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082026 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082027 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082028 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082029 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082030 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082031 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082032 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082033 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082034 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082035 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082036 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082037 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082038 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082039 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082040 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082041 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082042 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082043 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082044 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082045 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082046 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082047 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082048 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082049 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082050 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082051 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082052 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082053 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082054 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082055 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082056 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082057 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082058 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082059 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082060 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082061 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082062 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082063 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082064 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082065 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082066 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082067 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082068 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082069 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082070 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082071 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082072 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082073 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082074 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082075 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082076 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082077 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082078 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082079 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082080 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082081 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082082 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082083 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082084 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082085 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082086 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082087 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082088 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082089 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082090 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082091 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082092 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082093 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082094 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082095 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082096 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082097 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082098 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082099 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082100 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082101 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082102 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082103 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082104 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082105 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082106 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082107 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082108 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082109 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082110 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082111 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082112 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082113 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082114 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082115 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082116 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082117 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082118 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082119 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082120 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082121 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082122 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082123 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082124 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082125 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082126 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082127 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082128 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082129 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082130 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082131 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082132 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082133 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082134 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082135 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082136 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082137 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082138 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082139 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082140 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082141 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082142 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082143 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082144 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082145 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082146 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082147 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082148 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082149 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082150 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082151 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082152 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082153 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082154 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082155 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082156 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082157 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082158 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082159 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082160 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082161 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082162 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082163 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082164 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082165 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082166 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082167 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082168 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082169 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082170 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082171 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082172 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082173 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082174 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082175 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082176 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082177 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082178 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082179 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082180 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082181 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082182 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082183 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082184 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082185 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082186 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082187 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082188 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082189 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082190 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082191 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082192 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082193 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082194 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082195 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082196 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082197 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082198 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082199 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082200 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082201 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082202 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082203 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082204 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082205 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082206 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082207 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082208 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082209 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082210 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082211 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082212 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082213 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082214 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082215 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082216 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082217 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082218 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082219 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082220 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082221 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082222 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082223 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082224 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082225 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082226 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082227 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082228 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082229 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082230 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082231 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082232 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082233 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082234 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082235 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082236 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082237 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082238 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082239 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082240 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082241 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082242 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082243 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082244 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082245 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082246 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082247 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082248 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082249 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082250 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082251 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082252 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082253 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082254 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082255 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082256 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082257 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082258 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082259 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082260 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082261 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082262 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082263 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082264 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082265 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082266 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082267 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082268 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082269 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082270 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082271 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082272 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082273 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082274 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082275 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082276 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082277 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082278 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082279 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082280 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082281 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082282 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082283 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082284 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082285 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082286 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082287 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082288 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082289 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082290 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082291 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082292 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082293 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082294 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082295 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082296 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082297 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082298 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082299 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082300 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082301 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082302 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082303 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082304 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082305 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082306 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082307 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082308 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082309 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082310 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082311 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082312 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082313 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082314 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082315 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082316 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082317 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082318 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082319 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082320 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082321 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082322 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082323 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082324 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082325 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082326 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082327 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082328 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082329 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082330 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082331 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082332 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082333 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082334 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082335 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082336 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082337 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082338 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082339 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082340 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082341 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082342 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082343 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082344 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082345 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082346 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082347 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082348 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082349 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082350 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082351 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082352 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082353 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082354 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082355 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082356 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082357 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082358 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082359 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082360 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082361 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082362 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082363 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082364 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082365 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082366 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082367 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082368 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082369 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082370 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082371 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082372 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082373 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082374 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082375 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082376 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082377 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082378 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082379 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082380 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082381 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082382 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082383 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082384 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082385 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082386 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082387 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082388 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082389 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082390 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082391 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082392 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082393 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082394 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082395 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082396 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082397 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082398 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082399 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082400 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082401 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082402 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082403 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082404 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082405 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082406 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082407 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082408 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082409 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082410 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082411 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082412 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082413 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082414 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082415 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082416 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082417 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082418 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082419 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082420 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082421 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082422 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082423 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082424 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082425 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082426 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082427 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082428 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082429 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082430 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082431 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082432 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082433 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082434 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082435 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082436 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082437 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082438 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082439 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082440 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082441 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082442 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082443 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082444 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082445 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082446 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082447 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082448 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082449 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082450 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082451 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082452 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082453 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082454 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082455 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082456 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082457 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082458 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082459 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082460 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082461 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082462 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082463 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082464 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082465 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082466 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082467 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082468 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082469 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082470 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082471 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082472 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082473 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082474 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082475 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082476 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082477 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082478 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082479 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082480 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082481 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082482 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082483 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082484 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082485 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082486 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082487 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082488 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082489 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082490 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082491 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082492 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082493 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082494 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082495 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082496 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082497 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082498 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082499 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082500 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082501 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082502 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082503 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082504 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082505 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082506 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082507 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082508 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082509 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082510 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082511 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082512 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082513 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082514 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082515 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082516 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082517 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082518 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082519 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082520 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082521 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082522 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082523 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082524 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082525 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082526 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082527 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082528 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082529 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082530 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082531 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082532 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082533 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082534 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082535 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082536 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082537 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082538 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082539 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082540 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082541 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082542 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082543 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082544 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082545 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082546 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082547 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082548 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082549 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082550 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082551 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082552 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082553 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082554 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082555 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082556 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082557 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082558 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082559 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082560 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082561 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082562 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082563 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082564 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082565 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082566 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082567 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082568 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082569 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082570 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082571 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082572 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082573 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082574 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082575 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082576 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082577 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082578 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082579 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082580 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082581 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082582 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082583 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082584 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082585 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082586 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082587 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082588 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082589 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082590 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082591 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082592 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082593 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082594 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082595 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082596 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082597 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082598 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082599 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082600 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082601 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082602 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082603 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082604 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082605 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082606 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082607 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082608 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082609 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082610 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082611 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082612 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082613 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082614 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082615 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082616 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082617 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082618 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082619 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082620 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082621 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082622 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082623 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082624 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082625 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082626 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082627 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082628 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082629 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082630 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082631 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082632 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082633 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082634 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082635 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082636 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082637 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082638 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082639 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082640 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082641 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082642 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082643 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082644 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082645 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082646 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082647 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082648 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082649 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082650 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082651 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082652 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082653 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082654 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082655 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082656 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082657 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082658 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082659 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082660 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082661 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082662 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082663 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082664 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082665 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082666 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082667 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082668 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082669 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082670 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082671 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082672 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082673 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082674 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082675 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082676 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082677 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082678 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082679 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082680 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082681 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082682 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082683 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082684 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082685 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082686 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082687 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082688 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082689 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082690 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082691 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082692 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082693 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082694 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082695 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082696 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082697 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082698 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082699 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082700 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082701 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082702 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082703 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082704 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082705 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082706 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082707 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082708 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082709 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082710 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082711 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082712 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082713 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082714 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082715 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082716 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082717 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082718 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082719 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082720 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082721 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082722 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082723 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082724 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082725 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082726 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082727 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082728 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082729 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082730 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082731 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082732 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082733 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082734 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082735 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082736 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082737 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082738 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082739 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082740 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082741 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082742 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082743 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082744 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082745 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082746 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082747 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082748 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082749 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082750 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082751 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082752 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082753 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082754 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082755 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082756 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082757 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082758 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082759 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082760 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082761 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082762 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082763 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082764 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082765 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082766 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082767 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082768 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082769 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082770 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082771 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082772 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082773 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082774 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082775 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082776 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082777 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082778 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082779 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082780 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082781 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082782 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082783 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082784 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082785 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082786 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082787 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082788 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082789 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082790 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082791 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082792 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082793 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082794 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082795 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082796 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082797 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082798 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082799 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082800 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082801 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082802 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082803 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082804 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082805 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082806 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082807 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082808 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082809 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082810 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082811 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082812 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082813 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082814 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082815 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082816 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082817 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082818 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082819 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082820 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082821 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082822 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082823 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082824 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082825 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082826 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082827 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082828 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082829 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082830 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082831 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082832 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082833 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082834 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082835 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082836 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082837 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082838 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082839 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082840 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082841 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082842 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082843 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082844 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082845 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082846 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082847 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082848 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082849 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082850 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082851 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082852 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082853 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082854 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082855 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082856 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082857 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082858 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082859 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082860 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082861 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082862 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082863 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082864 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082865 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082866 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082867 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082868 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082869 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082870 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082871 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082872 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082873 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082874 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082875 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082876 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082877 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082878 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082879 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082880 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082881 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082882 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082883 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082884 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082885 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082886 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082887 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082888 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082889 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082890 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082891 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082892 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082893 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082894 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082895 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082896 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082897 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082898 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082899 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082900 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082901 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082902 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082903 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082904 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082905 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082906 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082907 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082908 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082909 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082910 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082911 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082912 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082913 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082914 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082915 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082916 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082917 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082918 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082919 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082920 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082921 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082922 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082923 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082924 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082925 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082926 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082927 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082928 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082929 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082930 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082931 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082932 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082933 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082934 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082935 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082936 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082937 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082938 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082939 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082940 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082941 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082942 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082943 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082944 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082945 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082946 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082947 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082948 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082949 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082950 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082951 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082952 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082953 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082954 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082955 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082956 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082957 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082958 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082959 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082960 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082961 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082962 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082963 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082964 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082965 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082966 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082967 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082968 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082969 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082970 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082971 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082972 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082973 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082974 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082975 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082976 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082977 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082978 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082979 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082980 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082981 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082982 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082983 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082984 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082985 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082986 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082987 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082988 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082989 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082990 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082991 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082992 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082993 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082994 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082995 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082996 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082997 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082998 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1082999 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083000 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083001 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083002 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083003 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083004 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083005 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083006 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083007 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083008 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083009 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083010 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083011 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083012 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083013 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083014 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083015 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083016 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083017 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083018 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083019 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083020 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083021 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083022 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083023 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083024 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083025 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083026 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083027 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083028 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083029 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083030 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083031 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083032 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083033 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083034 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083035 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083036 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083037 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083038 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083039 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083040 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083041 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083042 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083043 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083044 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083045 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083046 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083047 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083048 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083049 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083050 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083051 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083052 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083053 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083054 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083055 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083056 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083057 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083058 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083059 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083060 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083061 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083062 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083063 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083064 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083065 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083066 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083067 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083068 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083069 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083070 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083071 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083072 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083073 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083074 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083075 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083076 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083077 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083078 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083079 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083080 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083081 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083082 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083083 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083084 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083085 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083086 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083087 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083088 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083089 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083090 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083091 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083092 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083093 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083094 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083095 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083096 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083097 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083098 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083099 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083100 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083101 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083102 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083103 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083104 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083105 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083106 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083107 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083108 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083109 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083110 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083111 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083112 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083113 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083114 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083115 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083116 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083117 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083118 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083119 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083120 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083121 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083122 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083123 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083124 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083125 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083126 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083127 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083128 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083129 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083130 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083131 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083132 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083133 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083134 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083135 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083136 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083137 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083138 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083139 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083140 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083141 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083142 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083143 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083144 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083145 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083146 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083147 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083148 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083149 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083150 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083151 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083152 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083153 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083154 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083155 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083156 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083157 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083158 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083159 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083160 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083161 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083162 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083163 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083164 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083165 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083166 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083167 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083168 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083169 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083170 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083171 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083172 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083173 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083174 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083175 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083176 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083177 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083178 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083179 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083180 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083181 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083182 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083183 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083184 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083185 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083186 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083187 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083188 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083189 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083190 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083191 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083192 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083193 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083194 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083195 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083196 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083197 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083198 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083199 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083200 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083201 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083202 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083203 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083204 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083205 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083206 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083207 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083208 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083209 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083210 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083211 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083212 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083213 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083214 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083215 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083216 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083217 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083218 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083219 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083220 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083221 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083222 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083223 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083224 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083225 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083226 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083227 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083228 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083229 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083230 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083231 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083232 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083233 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083234 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083235 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083236 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083237 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083238 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083239 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083240 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083241 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083242 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083243 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083244 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083245 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083246 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083247 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083248 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083249 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083250 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083251 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083252 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083253 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083254 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083255 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083256 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083257 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083258 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083259 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083260 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083261 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083262 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083263 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083264 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083265 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083266 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083267 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083268 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083269 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083270 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083271 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083272 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083273 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083274 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083275 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083276 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083277 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083278 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083279 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083280 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083281 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083282 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083283 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083284 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083285 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083286 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083287 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083288 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083289 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083290 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083291 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083292 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083293 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083294 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083295 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083296 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083297 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083298 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083299 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083300 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083301 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083302 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083303 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083304 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083305 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083306 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083307 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083308 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083309 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083310 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083311 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083312 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083313 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083314 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083315 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083316 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083317 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083318 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083319 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083320 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083321 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083322 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083323 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083324 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083325 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083326 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083327 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083328 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083329 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083330 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083331 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083332 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083333 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083334 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083335 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083336 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083337 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083338 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083339 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083340 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083341 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083342 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083343 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083344 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083345 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083346 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083347 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083348 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083349 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083350 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083351 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083352 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083353 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083354 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083355 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083356 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083357 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083358 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083359 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083360 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083361 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083362 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083363 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083364 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083365 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083366 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083367 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083368 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083369 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083370 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083371 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083372 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083373 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083374 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083375 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083376 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083377 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083378 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083379 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083380 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083381 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083382 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083383 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083384 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083385 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083386 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083387 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083388 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083389 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083390 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083391 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083392 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083393 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083394 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083395 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083396 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083397 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083398 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083399 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083400 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083401 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083402 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083403 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083404 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083405 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083406 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083407 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083408 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083409 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083410 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083411 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083412 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083413 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083414 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083415 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083416 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083417 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083418 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083419 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083420 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083421 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083422 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083423 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083424 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083425 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083426 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083427 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083428 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083429 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083430 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083431 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083432 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083433 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083434 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083435 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083436 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083437 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083438 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083439 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083440 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083441 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083442 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083443 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083444 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083445 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083446 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083447 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083448 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083449 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083450 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083451 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083452 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083453 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083454 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083455 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083456 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083457 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083458 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083459 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083460 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083461 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083462 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083463 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083464 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083465 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083466 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083467 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083468 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083469 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083470 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083471 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083472 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083473 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083474 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083475 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083476 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083477 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083478 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083479 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083480 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083481 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083482 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083483 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083484 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083485 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083486 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083487 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083488 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083489 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083490 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083491 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083492 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083493 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083494 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083495 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083496 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083497 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083498 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083499 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083500 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083501 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083502 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083503 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083504 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083505 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083506 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083507 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083508 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083509 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083510 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083511 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083512 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083513 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083514 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083515 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083516 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083517 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083518 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083519 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083520 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083521 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083522 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083523 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083524 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083525 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083526 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083527 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083528 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083529 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083530 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083531 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083532 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083533 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083534 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083535 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083536 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083537 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083538 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083539 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083540 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083541 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083542 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083543 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083544 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083545 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083546 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083547 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083548 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083549 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083550 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083551 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083552 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083553 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083554 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083555 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083556 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083557 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083558 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083559 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083560 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083561 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083562 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083563 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083564 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083565 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083566 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083567 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083568 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083569 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083570 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083571 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083572 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083573 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083574 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083575 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083576 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083577 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083578 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083579 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083580 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083581 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083582 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083583 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083584 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083585 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083586 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083587 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083588 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083589 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083590 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083591 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083592 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083593 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083594 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083595 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083596 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083597 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083598 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083599 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083600 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083601 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083602 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083603 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083604 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083605 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083606 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083607 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083608 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083609 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083610 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083611 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083612 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083613 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083614 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083615 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083616 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083617 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083618 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083619 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083620 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083621 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083622 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083623 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083624 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083625 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083626 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083627 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083628 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083629 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083630 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083631 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083632 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083633 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083634 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083635 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083636 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083637 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083638 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083639 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083640 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083641 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083642 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083643 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083644 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083645 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083646 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083647 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083648 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083649 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083650 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083651 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083652 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083653 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083654 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083655 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083656 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083657 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083658 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083659 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083660 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083661 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083662 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083663 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083664 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083665 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083666 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083667 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083668 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083669 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083670 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083671 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083672 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083673 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083674 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083675 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083676 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083677 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083678 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083679 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083680 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083681 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083682 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083683 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083684 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083685 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083686 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083687 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083688 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083689 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083690 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083691 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083692 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083693 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083694 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083695 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083696 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083697 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083698 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083699 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083700 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083701 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083702 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083703 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083704 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083705 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083706 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083707 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083708 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083709 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083710 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083711 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083712 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083713 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083714 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083715 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083716 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083717 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083718 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083719 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083720 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083721 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083722 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083723 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083724 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083725 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083726 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083727 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083728 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083729 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083730 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083731 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083732 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083733 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083734 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083735 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083736 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083737 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083738 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083739 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083740 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083741 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083742 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083743 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083744 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083745 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083746 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083747 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083748 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083749 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083750 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083751 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083752 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083753 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083754 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083755 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083756 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083757 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083758 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083759 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083760 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083761 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083762 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083763 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083764 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083765 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083766 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083767 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083768 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083769 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083770 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083771 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083772 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083773 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083774 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083775 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083776 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083777 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083778 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083779 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083780 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083781 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083782 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083783 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083784 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083785 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083786 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083787 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083788 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083789 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083790 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083791 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083792 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083793 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083794 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083795 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083796 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083797 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083798 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083799 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083800 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083801 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083802 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083803 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083804 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083805 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083806 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083807 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083808 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083809 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083810 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083811 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083812 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083813 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083814 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083815 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083816 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083817 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083818 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083819 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083820 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083821 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083822 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083823 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083824 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083825 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083826 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083827 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083828 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083829 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083830 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083831 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083832 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083833 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083834 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083835 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083836 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083837 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083838 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083839 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083840 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083841 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083842 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083843 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083844 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083845 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083846 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083847 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083848 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083849 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083850 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083851 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083852 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083853 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083854 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083855 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083856 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083857 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083858 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083859 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083860 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083861 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083862 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083863 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083864 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083865 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083866 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083867 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083868 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083869 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083870 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083871 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083872 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083873 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083874 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083875 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083876 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083877 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083878 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083879 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083880 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083881 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083882 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083883 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083884 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083885 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083886 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083887 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083888 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083889 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083890 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083891 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083892 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083893 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083894 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083895 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083896 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083897 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083898 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083899 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083900 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083901 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083902 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083903 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083904 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083905 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083906 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083907 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083908 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083909 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083910 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083911 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083912 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083913 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083914 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083915 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083916 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083917 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083918 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083919 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083920 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083921 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083922 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083923 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083924 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083925 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083926 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083927 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083928 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083929 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083930 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083931 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083932 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083933 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083934 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083935 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083936 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083937 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083938 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083939 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083940 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083941 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083942 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083943 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083944 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083945 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083946 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083947 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083948 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083949 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083950 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083951 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083952 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083953 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083954 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083955 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083956 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083957 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083958 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083959 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083960 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083961 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083962 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083963 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083964 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083965 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083966 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083967 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083968 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083969 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083970 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083971 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083972 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083973 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083974 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083975 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083976 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083977 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083978 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083979 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083980 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083981 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083982 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083983 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083984 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083985 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083986 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083987 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083988 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083989 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083990 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083991 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083992 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083993 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083994 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083995 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083996 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083997 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083998 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1083999 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084000 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084001 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084002 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084003 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084004 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084005 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084006 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084007 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084008 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084009 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084010 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084011 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084012 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084013 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084014 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084015 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084016 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084017 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084018 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084019 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084020 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084021 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084022 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084023 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084024 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084025 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084026 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084027 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084028 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084029 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084030 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084031 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084032 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084033 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084034 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084035 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084036 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084037 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084038 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084039 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084040 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084041 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084042 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084043 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084044 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084045 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084046 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084047 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084048 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084049 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084050 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084051 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084052 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084053 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084054 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084055 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084056 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084057 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084058 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084059 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084060 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084061 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084062 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084063 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084064 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084065 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084066 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084067 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084068 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084069 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084070 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084071 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084072 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084073 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084074 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084075 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084076 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084077 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084078 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084079 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084080 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084081 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084082 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084083 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084084 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084085 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084086 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084087 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084088 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084089 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084090 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084091 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084092 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084093 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084094 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084095 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084096 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084097 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084098 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084099 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084100 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084101 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084102 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084103 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084104 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084105 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084106 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084107 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084108 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084109 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084110 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084111 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084112 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084113 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084114 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084115 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084116 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084117 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084118 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084119 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084120 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084121 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084122 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084123 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084124 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084125 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084126 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084127 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084128 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084129 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084130 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084131 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084132 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084133 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084134 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084135 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084136 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084137 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084138 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084139 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084140 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084141 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084142 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084143 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084144 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084145 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084146 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084147 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084148 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084149 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084150 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084151 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084152 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084153 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084154 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084155 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084156 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084157 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084158 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084159 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084160 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084161 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084162 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084163 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084164 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084165 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084166 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084167 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084168 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084169 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084170 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084171 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084172 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084173 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084174 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084175 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084176 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084177 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084178 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084179 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084180 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084181 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084182 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084183 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084184 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084185 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084186 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084187 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084188 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084189 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084190 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084191 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084192 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084193 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084194 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084195 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084196 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084197 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084198 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084199 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084200 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084201 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084202 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084203 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084204 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084205 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084206 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084207 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084208 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084209 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084210 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084211 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084212 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084213 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084214 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084215 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084216 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084217 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084218 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084219 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084220 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084221 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084222 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084223 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084224 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084225 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084226 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084227 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084228 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084229 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084230 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084231 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084232 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084233 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084234 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084235 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084236 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084237 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084238 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084239 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084240 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084241 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084242 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084243 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084244 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084245 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084246 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084247 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084248 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084249 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084250 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084251 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084252 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084253 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084254 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084255 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084256 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084257 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084258 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084259 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084260 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084261 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084262 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084263 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084264 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084265 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084266 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084267 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084268 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084269 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084270 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084271 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084272 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084273 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084274 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084275 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084276 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084277 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084278 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084279 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084280 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084281 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084282 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084283 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084284 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084285 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084286 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084287 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084288 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084289 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084290 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084291 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084292 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084293 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084294 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084295 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084296 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084297 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084298 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084299 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084300 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084301 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084302 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084303 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084304 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084305 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084306 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084307 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084308 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084309 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084310 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084311 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084312 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084313 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084314 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084315 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084316 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084317 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084318 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084319 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084320 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084321 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084322 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084323 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084324 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084325 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084326 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084327 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084328 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084329 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084330 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084331 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084332 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084333 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084334 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084335 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084336 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084337 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084338 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084339 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084340 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084341 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084342 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084343 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084344 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084345 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084346 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084347 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084348 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084349 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084350 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084351 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084352 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084353 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084354 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084355 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084356 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084357 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084358 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084359 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084360 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084361 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084362 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084363 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084364 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084365 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084366 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084367 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084368 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084369 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084370 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084371 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084372 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084373 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084374 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084375 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084376 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084377 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084378 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084379 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084380 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084381 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084382 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084383 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084384 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084385 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084386 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084387 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084388 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084389 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084390 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084391 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084392 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084393 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084394 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084395 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084396 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084397 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084398 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084399 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084400 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084401 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084402 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084403 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084404 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084405 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084406 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084407 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084408 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084409 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084410 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084411 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084412 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084413 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084414 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084415 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084416 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084417 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084418 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084419 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084420 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084421 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084422 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084423 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084424 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084425 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084426 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084427 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084428 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084429 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084430 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084431 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084432 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084433 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084434 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084435 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084436 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084437 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084438 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084439 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084440 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084441 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084442 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084443 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084444 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084445 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084446 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084447 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084448 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084449 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084450 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084451 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084452 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084453 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084454 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084455 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084456 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084457 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084458 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084459 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084460 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084461 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084462 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084463 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084464 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084465 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084466 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084467 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084468 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084469 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084470 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084471 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084472 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084473 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084474 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084475 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084476 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084477 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084478 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084479 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084480 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084481 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084482 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084483 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084484 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084485 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084486 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084487 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084488 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084489 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084490 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084491 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084492 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084493 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084494 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084495 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084496 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084497 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084498 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084499 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084500 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084501 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084502 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084503 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084504 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084505 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084506 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084507 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084508 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084509 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084510 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084511 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084512 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084513 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084514 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084515 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084516 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084517 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084518 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084519 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084520 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084521 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084522 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084523 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084524 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084525 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084526 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084527 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084528 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084529 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084530 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084531 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084532 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084533 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084534 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084535 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084536 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084537 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084538 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084539 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084540 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084541 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084542 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084543 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084544 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084545 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084546 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084547 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084548 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084549 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084550 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084551 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084552 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084553 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084554 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084555 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084556 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084557 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084558 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084559 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084560 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084561 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084562 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084563 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084564 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084565 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084566 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084567 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084568 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084569 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084570 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084571 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084572 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084573 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084574 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084575 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084576 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084577 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084578 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084579 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084580 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084581 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084582 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084583 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084584 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084585 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084586 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084587 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084588 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084589 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084590 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084591 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084592 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084593 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084594 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084595 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084596 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084597 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084598 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084599 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084600 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084601 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084602 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084603 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084604 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084605 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084606 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084607 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084608 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084609 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084610 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084611 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084612 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084613 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084614 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084615 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084616 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084617 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084618 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084619 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084620 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084621 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084622 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084623 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084624 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084625 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084626 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084627 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084628 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084629 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084630 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084631 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084632 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084633 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084634 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084635 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084636 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084637 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084638 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084639 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084640 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084641 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084642 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084643 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084644 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084645 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084646 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084647 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084648 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084649 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084650 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084651 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084652 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084653 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084654 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084655 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084656 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084657 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084658 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084659 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084660 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084661 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084662 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084663 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084664 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084665 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084666 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084667 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084668 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084669 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084670 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084671 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084672 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084673 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084674 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084675 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084676 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084677 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084678 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084679 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084680 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084681 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084682 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084683 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084684 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084685 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084686 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084687 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084688 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084689 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084690 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084691 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084692 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084693 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084694 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084695 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084696 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084697 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084698 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084699 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084700 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084701 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084702 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084703 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084704 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084705 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084706 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084707 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084708 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084709 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084710 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084711 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084712 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084713 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084714 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084715 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084716 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084717 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084718 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084719 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084720 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084721 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084722 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084723 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084724 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084725 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084726 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084727 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084728 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084729 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084730 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084731 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084732 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084733 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084734 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084735 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084736 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084737 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084738 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084739 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084740 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084741 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084742 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084743 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084744 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084745 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084746 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084747 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084748 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084749 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084750 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084751 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084752 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084753 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084754 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084755 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084756 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084757 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084758 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084759 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084760 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084761 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084762 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084763 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084764 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084765 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084766 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084767 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084768 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084769 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084770 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084771 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084772 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084773 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084774 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084775 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084776 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084777 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084778 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084779 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084780 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084781 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084782 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084783 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084784 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084785 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084786 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084787 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084788 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084789 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084790 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084791 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084792 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084793 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084794 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084795 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084796 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084797 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084798 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084799 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084800 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084801 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084802 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084803 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084804 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084805 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084806 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084807 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084808 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084809 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084810 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084811 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084812 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084813 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084814 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084815 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084816 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084817 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084818 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084819 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084820 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084821 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084822 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084823 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084824 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084825 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084826 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084827 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084828 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084829 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084830 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084831 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084832 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084833 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084834 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084835 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084836 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084837 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084838 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084839 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084840 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084841 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084842 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084843 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084844 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084845 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084846 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084847 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084848 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084849 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084850 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084851 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084852 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084853 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084854 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084855 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084856 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084857 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084858 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084859 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084860 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084861 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084862 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084863 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084864 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084865 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084866 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084867 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084868 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084869 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084870 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084871 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084872 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084873 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084874 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084875 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084876 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084877 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084878 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084879 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084880 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084881 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084882 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084883 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084884 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084885 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084886 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084887 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084888 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084889 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084890 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084891 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084892 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084893 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084894 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084895 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084896 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084897 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084898 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084899 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084900 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084901 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084902 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084903 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084904 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084905 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084906 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084907 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084908 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084909 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084910 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084911 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084912 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084913 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084914 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084915 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084916 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084917 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084918 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084919 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084920 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084921 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084922 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084923 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084924 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084925 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084926 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084927 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084928 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084929 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084930 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084931 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084932 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084933 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084934 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084935 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084936 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084937 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084938 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084939 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084940 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084941 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084942 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084943 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084944 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084945 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084946 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084947 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084948 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084949 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084950 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084951 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084952 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084953 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084954 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084955 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084956 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084957 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084958 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084959 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084960 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084961 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084962 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084963 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084964 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084965 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084966 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084967 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084968 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084969 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084970 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084971 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084972 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084973 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084974 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084975 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084976 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084977 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084978 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084979 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084980 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084981 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084982 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084983 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084984 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084985 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084986 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084987 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084988 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084989 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084990 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084991 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084992 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084993 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084994 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084995 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084996 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084997 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084998 (in epoch 154)\n",
      "WARNING:absl:loss is nan for step 1084999 (in epoch 154)\n",
      "INFO:absl:[154] train_loss=nan, train_x1_loss=nan, train_x2_loss=nan\n",
      "INFO:absl:[154] val_loss=nan\n",
      "INFO:absl:[154] test_loss=nan\n",
      "WARNING:absl:loss is nan for step 1085000 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085001 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085002 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085003 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085004 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085005 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085006 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085007 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085008 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085009 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085010 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085011 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085012 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085013 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085014 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085015 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085016 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085017 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085018 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085019 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085020 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085021 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085022 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085023 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085024 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085025 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085026 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085027 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085028 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085029 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085030 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085031 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085032 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085033 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085034 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085035 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085036 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085037 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085038 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085039 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085040 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085041 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085042 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085043 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085044 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085045 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085046 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085047 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085048 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085049 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085050 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085051 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085052 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085053 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085054 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085055 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085056 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085057 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085058 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085059 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085060 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085061 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085062 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085063 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085064 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085065 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085066 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085067 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085068 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085069 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085070 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085071 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085072 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085073 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085074 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085075 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085076 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085077 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085078 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085079 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085080 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085081 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085082 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085083 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085084 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085085 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085086 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085087 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085088 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085089 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085090 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085091 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085092 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085093 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085094 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085095 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085096 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085097 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085098 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085099 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085100 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085101 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085102 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085103 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085104 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085105 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085106 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085107 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085108 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085109 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085110 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085111 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085112 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085113 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085114 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085115 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085116 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085117 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085118 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085119 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085120 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085121 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085122 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085123 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085124 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085125 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085126 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085127 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085128 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085129 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085130 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085131 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085132 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085133 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085134 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085135 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085136 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085137 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085138 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085139 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085140 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085141 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085142 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085143 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085144 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085145 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085146 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085147 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085148 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085149 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085150 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085151 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085152 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085153 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085154 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085155 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085156 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085157 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085158 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085159 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085160 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085161 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085162 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085163 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085164 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085165 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085166 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085167 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085168 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085169 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085170 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085171 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085172 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085173 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085174 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085175 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085176 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085177 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085178 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085179 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085180 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085181 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085182 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085183 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085184 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085185 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085186 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085187 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085188 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085189 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085190 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085191 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085192 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085193 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085194 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085195 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085196 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085197 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085198 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085199 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085200 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085201 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085202 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085203 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085204 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085205 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085206 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085207 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085208 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085209 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085210 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085211 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085212 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085213 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085214 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085215 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085216 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085217 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085218 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085219 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085220 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085221 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085222 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085223 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085224 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085225 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085226 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085227 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085228 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085229 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085230 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085231 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085232 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085233 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085234 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085235 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085236 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085237 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085238 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085239 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085240 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085241 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085242 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085243 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085244 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085245 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085246 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085247 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085248 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085249 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085250 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085251 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085252 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085253 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085254 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085255 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085256 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085257 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085258 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085259 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085260 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085261 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085262 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085263 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085264 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085265 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085266 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085267 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085268 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085269 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085270 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085271 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085272 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085273 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085274 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085275 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085276 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085277 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085278 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085279 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085280 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085281 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085282 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085283 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085284 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085285 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085286 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085287 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085288 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085289 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085290 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085291 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085292 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085293 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085294 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085295 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085296 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085297 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085298 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085299 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085300 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085301 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085302 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085303 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085304 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085305 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085306 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085307 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085308 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085309 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085310 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085311 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085312 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085313 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085314 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085315 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085316 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085317 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085318 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085319 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085320 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085321 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085322 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085323 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085324 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085325 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085326 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085327 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085328 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085329 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085330 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085331 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085332 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085333 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085334 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085335 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085336 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085337 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085338 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085339 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085340 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085341 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085342 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085343 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085344 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085345 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085346 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085347 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085348 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085349 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085350 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085351 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085352 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085353 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085354 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085355 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085356 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085357 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085358 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085359 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085360 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085361 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085362 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085363 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085364 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085365 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085366 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085367 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085368 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085369 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085370 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085371 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085372 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085373 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085374 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085375 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085376 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085377 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085378 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085379 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085380 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085381 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085382 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085383 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085384 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085385 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085386 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085387 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085388 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085389 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085390 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085391 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085392 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085393 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085394 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085395 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085396 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085397 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085398 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085399 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085400 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085401 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085402 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085403 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085404 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085405 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085406 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085407 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085408 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085409 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085410 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085411 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085412 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085413 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085414 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085415 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085416 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085417 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085418 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085419 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085420 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085421 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085422 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085423 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085424 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085425 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085426 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085427 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085428 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085429 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085430 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085431 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085432 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085433 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085434 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085435 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085436 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085437 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085438 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085439 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085440 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085441 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085442 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085443 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085444 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085445 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085446 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085447 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085448 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085449 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085450 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085451 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085452 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085453 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085454 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085455 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085456 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085457 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085458 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085459 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085460 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085461 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085462 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085463 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085464 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085465 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085466 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085467 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085468 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085469 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085470 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085471 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085472 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085473 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085474 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085475 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085476 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085477 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085478 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085479 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085480 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085481 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085482 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085483 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085484 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085485 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085486 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085487 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085488 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085489 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085490 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085491 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085492 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085493 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085494 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085495 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085496 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085497 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085498 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085499 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085500 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085501 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085502 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085503 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085504 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085505 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085506 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085507 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085508 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085509 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085510 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085511 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085512 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085513 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085514 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085515 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085516 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085517 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085518 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085519 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085520 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085521 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085522 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085523 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085524 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085525 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085526 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085527 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085528 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085529 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085530 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085531 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085532 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085533 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085534 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085535 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085536 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085537 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085538 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085539 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085540 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085541 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085542 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085543 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085544 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085545 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085546 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085547 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085548 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085549 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085550 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085551 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085552 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085553 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085554 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085555 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085556 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085557 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085558 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085559 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085560 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085561 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085562 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085563 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085564 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085565 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085566 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085567 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085568 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085569 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085570 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085571 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085572 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085573 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085574 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085575 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085576 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085577 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085578 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085579 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085580 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085581 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085582 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085583 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085584 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085585 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085586 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085587 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085588 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085589 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085590 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085591 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085592 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085593 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085594 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085595 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085596 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085597 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085598 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085599 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085600 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085601 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085602 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085603 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085604 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085605 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085606 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085607 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085608 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085609 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085610 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085611 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085612 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085613 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085614 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085615 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085616 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085617 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085618 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085619 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085620 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085621 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085622 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085623 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085624 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085625 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085626 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085627 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085628 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085629 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085630 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085631 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085632 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085633 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085634 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085635 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085636 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085637 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085638 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085639 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085640 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085641 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085642 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085643 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085644 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085645 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085646 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085647 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085648 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085649 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085650 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085651 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085652 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085653 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085654 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085655 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085656 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085657 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085658 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085659 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085660 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085661 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085662 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085663 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085664 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085665 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085666 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085667 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085668 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085669 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085670 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085671 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085672 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085673 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085674 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085675 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085676 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085677 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085678 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085679 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085680 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085681 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085682 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085683 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085684 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085685 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085686 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085687 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085688 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085689 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085690 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085691 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085692 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085693 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085694 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085695 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085696 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085697 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085698 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085699 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085700 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085701 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085702 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085703 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085704 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085705 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085706 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085707 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085708 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085709 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085710 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085711 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085712 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085713 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085714 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085715 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085716 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085717 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085718 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085719 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085720 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085721 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085722 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085723 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085724 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085725 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085726 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085727 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085728 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085729 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085730 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085731 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085732 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085733 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085734 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085735 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085736 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085737 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085738 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085739 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085740 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085741 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085742 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085743 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085744 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085745 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085746 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085747 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085748 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085749 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085750 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085751 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085752 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085753 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085754 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085755 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085756 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085757 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085758 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085759 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085760 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085761 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085762 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085763 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085764 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085765 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085766 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085767 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085768 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085769 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085770 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085771 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085772 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085773 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085774 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085775 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085776 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085777 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085778 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085779 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085780 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085781 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085782 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085783 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085784 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085785 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085786 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085787 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085788 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085789 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085790 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085791 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085792 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085793 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085794 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085795 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085796 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085797 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085798 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085799 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085800 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085801 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085802 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085803 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085804 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085805 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085806 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085807 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085808 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085809 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085810 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085811 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085812 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085813 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085814 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085815 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085816 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085817 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085818 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085819 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085820 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085821 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085822 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085823 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085824 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085825 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085826 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085827 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085828 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085829 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085830 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085831 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085832 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085833 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085834 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085835 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085836 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085837 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085838 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085839 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085840 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085841 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085842 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085843 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085844 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085845 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085846 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085847 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085848 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085849 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085850 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085851 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085852 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085853 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085854 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085855 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085856 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085857 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085858 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085859 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085860 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085861 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085862 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085863 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085864 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085865 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085866 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085867 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085868 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085869 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085870 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085871 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085872 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085873 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085874 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085875 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085876 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085877 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085878 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085879 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085880 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085881 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085882 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085883 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085884 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085885 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085886 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085887 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085888 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085889 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085890 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085891 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085892 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085893 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085894 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085895 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085896 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085897 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085898 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085899 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085900 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085901 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085902 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085903 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085904 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085905 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085906 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085907 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085908 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085909 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085910 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085911 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085912 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085913 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085914 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085915 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085916 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085917 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085918 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085919 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085920 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085921 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085922 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085923 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085924 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085925 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085926 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085927 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085928 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085929 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085930 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085931 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085932 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085933 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085934 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085935 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085936 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085937 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085938 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085939 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085940 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085941 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085942 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085943 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085944 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085945 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085946 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085947 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085948 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085949 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085950 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085951 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085952 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085953 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085954 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085955 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085956 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085957 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085958 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085959 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085960 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085961 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085962 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085963 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085964 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085965 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085966 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085967 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085968 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085969 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085970 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085971 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085972 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085973 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085974 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085975 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085976 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085977 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085978 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085979 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085980 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085981 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085982 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085983 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085984 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085985 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085986 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085987 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085988 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085989 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085990 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085991 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085992 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085993 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085994 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085995 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085996 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085997 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085998 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1085999 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086000 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086001 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086002 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086003 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086004 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086005 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086006 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086007 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086008 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086009 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086010 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086011 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086012 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086013 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086014 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086015 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086016 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086017 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086018 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086019 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086020 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086021 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086022 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086023 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086024 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086025 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086026 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086027 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086028 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086029 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086030 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086031 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086032 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086033 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086034 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086035 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086036 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086037 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086038 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086039 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086040 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086041 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086042 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086043 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086044 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086045 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086046 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086047 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086048 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086049 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086050 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086051 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086052 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086053 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086054 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086055 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086056 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086057 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086058 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086059 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086060 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086061 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086062 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086063 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086064 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086065 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086066 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086067 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086068 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086069 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086070 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086071 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086072 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086073 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086074 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086075 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086076 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086077 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086078 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086079 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086080 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086081 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086082 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086083 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086084 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086085 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086086 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086087 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086088 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086089 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086090 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086091 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086092 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086093 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086094 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086095 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086096 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086097 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086098 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086099 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086100 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086101 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086102 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086103 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086104 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086105 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086106 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086107 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086108 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086109 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086110 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086111 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086112 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086113 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086114 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086115 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086116 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086117 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086118 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086119 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086120 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086121 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086122 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086123 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086124 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086125 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086126 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086127 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086128 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086129 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086130 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086131 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086132 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086133 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086134 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086135 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086136 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086137 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086138 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086139 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086140 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086141 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086142 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086143 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086144 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086145 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086146 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086147 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086148 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086149 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086150 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086151 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086152 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086153 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086154 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086155 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086156 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086157 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086158 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086159 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086160 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086161 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086162 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086163 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086164 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086165 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086166 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086167 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086168 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086169 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086170 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086171 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086172 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086173 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086174 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086175 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086176 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086177 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086178 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086179 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086180 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086181 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086182 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086183 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086184 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086185 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086186 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086187 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086188 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086189 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086190 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086191 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086192 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086193 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086194 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086195 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086196 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086197 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086198 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086199 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086200 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086201 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086202 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086203 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086204 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086205 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086206 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086207 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086208 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086209 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086210 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086211 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086212 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086213 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086214 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086215 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086216 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086217 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086218 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086219 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086220 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086221 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086222 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086223 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086224 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086225 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086226 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086227 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086228 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086229 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086230 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086231 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086232 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086233 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086234 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086235 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086236 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086237 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086238 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086239 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086240 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086241 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086242 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086243 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086244 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086245 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086246 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086247 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086248 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086249 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086250 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086251 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086252 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086253 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086254 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086255 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086256 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086257 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086258 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086259 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086260 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086261 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086262 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086263 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086264 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086265 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086266 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086267 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086268 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086269 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086270 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086271 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086272 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086273 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086274 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086275 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086276 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086277 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086278 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086279 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086280 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086281 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086282 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086283 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086284 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086285 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086286 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086287 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086288 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086289 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086290 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086291 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086292 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086293 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086294 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086295 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086296 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086297 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086298 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086299 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086300 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086301 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086302 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086303 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086304 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086305 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086306 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086307 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086308 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086309 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086310 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086311 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086312 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086313 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086314 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086315 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086316 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086317 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086318 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086319 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086320 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086321 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086322 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086323 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086324 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086325 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086326 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086327 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086328 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086329 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086330 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086331 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086332 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086333 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086334 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086335 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086336 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086337 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086338 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086339 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086340 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086341 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086342 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086343 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086344 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086345 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086346 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086347 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086348 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086349 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086350 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086351 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086352 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086353 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086354 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086355 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086356 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086357 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086358 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086359 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086360 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086361 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086362 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086363 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086364 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086365 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086366 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086367 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086368 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086369 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086370 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086371 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086372 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086373 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086374 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086375 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086376 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086377 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086378 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086379 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086380 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086381 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086382 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086383 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086384 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086385 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086386 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086387 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086388 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086389 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086390 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086391 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086392 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086393 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086394 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086395 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086396 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086397 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086398 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086399 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086400 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086401 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086402 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086403 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086404 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086405 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086406 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086407 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086408 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086409 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086410 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086411 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086412 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086413 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086414 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086415 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086416 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086417 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086418 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086419 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086420 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086421 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086422 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086423 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086424 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086425 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086426 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086427 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086428 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086429 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086430 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086431 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086432 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086433 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086434 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086435 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086436 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086437 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086438 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086439 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086440 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086441 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086442 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086443 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086444 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086445 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086446 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086447 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086448 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086449 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086450 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086451 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086452 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086453 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086454 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086455 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086456 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086457 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086458 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086459 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086460 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086461 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086462 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086463 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086464 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086465 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086466 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086467 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086468 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086469 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086470 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086471 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086472 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086473 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086474 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086475 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086476 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086477 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086478 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086479 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086480 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086481 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086482 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086483 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086484 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086485 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086486 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086487 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086488 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086489 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086490 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086491 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086492 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086493 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086494 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086495 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086496 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086497 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086498 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086499 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086500 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086501 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086502 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086503 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086504 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086505 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086506 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086507 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086508 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086509 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086510 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086511 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086512 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086513 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086514 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086515 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086516 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086517 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086518 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086519 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086520 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086521 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086522 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086523 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086524 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086525 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086526 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086527 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086528 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086529 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086530 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086531 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086532 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086533 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086534 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086535 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086536 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086537 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086538 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086539 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086540 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086541 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086542 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086543 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086544 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086545 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086546 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086547 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086548 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086549 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086550 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086551 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086552 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086553 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086554 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086555 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086556 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086557 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086558 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086559 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086560 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086561 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086562 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086563 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086564 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086565 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086566 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086567 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086568 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086569 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086570 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086571 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086572 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086573 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086574 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086575 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086576 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086577 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086578 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086579 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086580 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086581 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086582 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086583 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086584 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086585 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086586 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086587 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086588 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086589 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086590 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086591 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086592 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086593 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086594 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086595 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086596 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086597 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086598 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086599 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086600 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086601 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086602 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086603 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086604 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086605 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086606 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086607 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086608 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086609 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086610 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086611 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086612 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086613 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086614 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086615 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086616 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086617 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086618 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086619 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086620 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086621 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086622 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086623 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086624 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086625 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086626 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086627 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086628 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086629 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086630 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086631 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086632 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086633 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086634 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086635 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086636 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086637 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086638 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086639 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086640 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086641 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086642 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086643 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086644 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086645 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086646 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086647 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086648 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086649 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086650 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086651 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086652 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086653 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086654 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086655 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086656 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086657 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086658 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086659 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086660 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086661 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086662 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086663 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086664 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086665 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086666 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086667 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086668 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086669 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086670 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086671 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086672 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086673 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086674 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086675 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086676 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086677 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086678 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086679 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086680 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086681 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086682 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086683 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086684 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086685 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086686 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086687 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086688 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086689 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086690 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086691 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086692 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086693 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086694 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086695 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086696 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086697 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086698 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086699 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086700 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086701 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086702 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086703 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086704 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086705 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086706 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086707 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086708 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086709 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086710 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086711 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086712 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086713 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086714 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086715 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086716 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086717 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086718 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086719 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086720 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086721 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086722 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086723 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086724 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086725 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086726 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086727 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086728 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086729 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086730 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086731 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086732 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086733 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086734 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086735 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086736 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086737 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086738 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086739 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086740 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086741 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086742 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086743 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086744 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086745 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086746 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086747 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086748 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086749 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086750 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086751 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086752 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086753 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086754 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086755 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086756 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086757 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086758 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086759 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086760 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086761 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086762 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086763 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086764 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086765 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086766 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086767 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086768 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086769 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086770 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086771 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086772 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086773 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086774 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086775 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086776 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086777 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086778 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086779 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086780 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086781 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086782 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086783 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086784 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086785 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086786 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086787 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086788 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086789 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086790 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086791 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086792 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086793 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086794 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086795 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086796 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086797 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086798 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086799 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086800 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086801 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086802 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086803 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086804 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086805 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086806 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086807 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086808 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086809 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086810 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086811 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086812 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086813 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086814 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086815 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086816 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086817 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086818 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086819 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086820 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086821 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086822 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086823 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086824 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086825 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086826 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086827 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086828 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086829 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086830 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086831 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086832 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086833 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086834 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086835 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086836 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086837 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086838 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086839 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086840 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086841 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086842 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086843 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086844 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086845 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086846 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086847 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086848 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086849 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086850 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086851 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086852 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086853 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086854 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086855 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086856 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086857 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086858 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086859 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086860 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086861 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086862 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086863 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086864 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086865 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086866 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086867 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086868 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086869 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086870 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086871 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086872 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086873 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086874 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086875 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086876 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086877 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086878 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086879 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086880 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086881 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086882 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086883 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086884 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086885 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086886 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086887 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086888 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086889 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086890 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086891 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086892 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086893 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086894 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086895 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086896 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086897 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086898 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086899 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086900 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086901 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086902 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086903 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086904 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086905 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086906 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086907 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086908 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086909 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086910 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086911 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086912 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086913 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086914 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086915 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086916 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086917 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086918 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086919 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086920 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086921 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086922 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086923 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086924 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086925 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086926 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086927 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086928 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086929 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086930 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086931 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086932 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086933 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086934 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086935 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086936 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086937 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086938 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086939 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086940 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086941 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086942 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086943 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086944 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086945 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086946 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086947 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086948 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086949 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086950 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086951 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086952 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086953 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086954 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086955 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086956 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086957 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086958 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086959 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086960 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086961 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086962 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086963 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086964 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086965 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086966 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086967 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086968 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086969 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086970 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086971 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086972 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086973 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086974 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086975 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086976 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086977 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086978 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086979 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086980 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086981 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086982 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086983 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086984 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086985 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086986 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086987 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086988 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086989 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086990 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086991 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086992 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086993 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086994 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086995 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086996 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086997 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086998 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1086999 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087000 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087001 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087002 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087003 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087004 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087005 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087006 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087007 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087008 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087009 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087010 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087011 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087012 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087013 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087014 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087015 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087016 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087017 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087018 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087019 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087020 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087021 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087022 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087023 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087024 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087025 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087026 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087027 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087028 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087029 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087030 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087031 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087032 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087033 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087034 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087035 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087036 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087037 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087038 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087039 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087040 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087041 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087042 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087043 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087044 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087045 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087046 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087047 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087048 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087049 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087050 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087051 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087052 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087053 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087054 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087055 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087056 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087057 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087058 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087059 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087060 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087061 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087062 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087063 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087064 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087065 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087066 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087067 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087068 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087069 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087070 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087071 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087072 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087073 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087074 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087075 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087076 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087077 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087078 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087079 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087080 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087081 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087082 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087083 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087084 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087085 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087086 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087087 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087088 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087089 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087090 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087091 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087092 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087093 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087094 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087095 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087096 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087097 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087098 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087099 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087100 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087101 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087102 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087103 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087104 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087105 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087106 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087107 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087108 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087109 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087110 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087111 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087112 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087113 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087114 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087115 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087116 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087117 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087118 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087119 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087120 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087121 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087122 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087123 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087124 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087125 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087126 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087127 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087128 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087129 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087130 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087131 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087132 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087133 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087134 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087135 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087136 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087137 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087138 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087139 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087140 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087141 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087142 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087143 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087144 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087145 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087146 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087147 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087148 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087149 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087150 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087151 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087152 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087153 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087154 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087155 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087156 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087157 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087158 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087159 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087160 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087161 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087162 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087163 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087164 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087165 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087166 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087167 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087168 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087169 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087170 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087171 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087172 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087173 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087174 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087175 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087176 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087177 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087178 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087179 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087180 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087181 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087182 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087183 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087184 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087185 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087186 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087187 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087188 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087189 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087190 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087191 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087192 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087193 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087194 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087195 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087196 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087197 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087198 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087199 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087200 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087201 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087202 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087203 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087204 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087205 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087206 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087207 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087208 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087209 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087210 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087211 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087212 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087213 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087214 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087215 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087216 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087217 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087218 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087219 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087220 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087221 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087222 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087223 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087224 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087225 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087226 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087227 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087228 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087229 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087230 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087231 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087232 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087233 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087234 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087235 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087236 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087237 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087238 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087239 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087240 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087241 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087242 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087243 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087244 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087245 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087246 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087247 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087248 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087249 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087250 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087251 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087252 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087253 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087254 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087255 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087256 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087257 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087258 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087259 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087260 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087261 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087262 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087263 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087264 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087265 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087266 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087267 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087268 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087269 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087270 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087271 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087272 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087273 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087274 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087275 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087276 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087277 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087278 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087279 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087280 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087281 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087282 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087283 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087284 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087285 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087286 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087287 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087288 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087289 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087290 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087291 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087292 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087293 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087294 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087295 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087296 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087297 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087298 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087299 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087300 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087301 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087302 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087303 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087304 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087305 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087306 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087307 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087308 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087309 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087310 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087311 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087312 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087313 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087314 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087315 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087316 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087317 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087318 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087319 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087320 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087321 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087322 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087323 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087324 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087325 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087326 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087327 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087328 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087329 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087330 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087331 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087332 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087333 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087334 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087335 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087336 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087337 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087338 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087339 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087340 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087341 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087342 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087343 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087344 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087345 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087346 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087347 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087348 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087349 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087350 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087351 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087352 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087353 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087354 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087355 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087356 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087357 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087358 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087359 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087360 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087361 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087362 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087363 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087364 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087365 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087366 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087367 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087368 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087369 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087370 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087371 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087372 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087373 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087374 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087375 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087376 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087377 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087378 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087379 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087380 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087381 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087382 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087383 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087384 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087385 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087386 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087387 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087388 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087389 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087390 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087391 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087392 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087393 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087394 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087395 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087396 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087397 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087398 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087399 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087400 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087401 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087402 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087403 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087404 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087405 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087406 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087407 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087408 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087409 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087410 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087411 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087412 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087413 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087414 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087415 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087416 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087417 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087418 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087419 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087420 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087421 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087422 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087423 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087424 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087425 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087426 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087427 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087428 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087429 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087430 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087431 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087432 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087433 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087434 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087435 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087436 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087437 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087438 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087439 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087440 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087441 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087442 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087443 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087444 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087445 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087446 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087447 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087448 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087449 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087450 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087451 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087452 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087453 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087454 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087455 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087456 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087457 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087458 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087459 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087460 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087461 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087462 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087463 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087464 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087465 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087466 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087467 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087468 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087469 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087470 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087471 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087472 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087473 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087474 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087475 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087476 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087477 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087478 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087479 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087480 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087481 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087482 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087483 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087484 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087485 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087486 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087487 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087488 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087489 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087490 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087491 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087492 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087493 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087494 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087495 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087496 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087497 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087498 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087499 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087500 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087501 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087502 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087503 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087504 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087505 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087506 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087507 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087508 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087509 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087510 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087511 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087512 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087513 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087514 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087515 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087516 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087517 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087518 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087519 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087520 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087521 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087522 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087523 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087524 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087525 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087526 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087527 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087528 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087529 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087530 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087531 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087532 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087533 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087534 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087535 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087536 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087537 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087538 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087539 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087540 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087541 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087542 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087543 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087544 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087545 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087546 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087547 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087548 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087549 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087550 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087551 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087552 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087553 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087554 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087555 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087556 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087557 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087558 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087559 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087560 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087561 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087562 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087563 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087564 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087565 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087566 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087567 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087568 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087569 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087570 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087571 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087572 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087573 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087574 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087575 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087576 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087577 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087578 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087579 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087580 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087581 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087582 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087583 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087584 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087585 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087586 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087587 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087588 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087589 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087590 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087591 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087592 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087593 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087594 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087595 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087596 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087597 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087598 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087599 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087600 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087601 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087602 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087603 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087604 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087605 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087606 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087607 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087608 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087609 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087610 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087611 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087612 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087613 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087614 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087615 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087616 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087617 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087618 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087619 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087620 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087621 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087622 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087623 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087624 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087625 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087626 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087627 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087628 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087629 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087630 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087631 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087632 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087633 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087634 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087635 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087636 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087637 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087638 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087639 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087640 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087641 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087642 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087643 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087644 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087645 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087646 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087647 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087648 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087649 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087650 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087651 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087652 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087653 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087654 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087655 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087656 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087657 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087658 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087659 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087660 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087661 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087662 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087663 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087664 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087665 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087666 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087667 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087668 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087669 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087670 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087671 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087672 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087673 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087674 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087675 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087676 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087677 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087678 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087679 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087680 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087681 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087682 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087683 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087684 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087685 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087686 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087687 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087688 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087689 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087690 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087691 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087692 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087693 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087694 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087695 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087696 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087697 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087698 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087699 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087700 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087701 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087702 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087703 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087704 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087705 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087706 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087707 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087708 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087709 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087710 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087711 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087712 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087713 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087714 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087715 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087716 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087717 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087718 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087719 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087720 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087721 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087722 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087723 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087724 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087725 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087726 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087727 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087728 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087729 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087730 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087731 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087732 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087733 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087734 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087735 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087736 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087737 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087738 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087739 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087740 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087741 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087742 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087743 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087744 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087745 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087746 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087747 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087748 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087749 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087750 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087751 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087752 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087753 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087754 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087755 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087756 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087757 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087758 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087759 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087760 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087761 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087762 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087763 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087764 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087765 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087766 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087767 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087768 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087769 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087770 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087771 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087772 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087773 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087774 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087775 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087776 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087777 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087778 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087779 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087780 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087781 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087782 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087783 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087784 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087785 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087786 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087787 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087788 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087789 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087790 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087791 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087792 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087793 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087794 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087795 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087796 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087797 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087798 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087799 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087800 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087801 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087802 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087803 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087804 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087805 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087806 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087807 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087808 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087809 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087810 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087811 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087812 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087813 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087814 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087815 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087816 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087817 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087818 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087819 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087820 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087821 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087822 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087823 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087824 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087825 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087826 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087827 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087828 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087829 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087830 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087831 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087832 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087833 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087834 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087835 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087836 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087837 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087838 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087839 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087840 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087841 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087842 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087843 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087844 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087845 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087846 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087847 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087848 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087849 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087850 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087851 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087852 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087853 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087854 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087855 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087856 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087857 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087858 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087859 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087860 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087861 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087862 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087863 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087864 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087865 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087866 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087867 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087868 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087869 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087870 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087871 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087872 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087873 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087874 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087875 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087876 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087877 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087878 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087879 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087880 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087881 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087882 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087883 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087884 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087885 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087886 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087887 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087888 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087889 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087890 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087891 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087892 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087893 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087894 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087895 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087896 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087897 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087898 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087899 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087900 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087901 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087902 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087903 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087904 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087905 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087906 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087907 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087908 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087909 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087910 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087911 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087912 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087913 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087914 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087915 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087916 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087917 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087918 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087919 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087920 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087921 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087922 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087923 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087924 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087925 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087926 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087927 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087928 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087929 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087930 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087931 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087932 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087933 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087934 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087935 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087936 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087937 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087938 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087939 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087940 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087941 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087942 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087943 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087944 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087945 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087946 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087947 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087948 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087949 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087950 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087951 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087952 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087953 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087954 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087955 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087956 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087957 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087958 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087959 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087960 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087961 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087962 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087963 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087964 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087965 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087966 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087967 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087968 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087969 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087970 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087971 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087972 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087973 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087974 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087975 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087976 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087977 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087978 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087979 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087980 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087981 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087982 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087983 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087984 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087985 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087986 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087987 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087988 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087989 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087990 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087991 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087992 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087993 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087994 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087995 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087996 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1087997 (in epoch 155)\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "WARNING:absl:loss is nan for step 1088765 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088766 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088767 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088768 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088769 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088770 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088771 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088772 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088773 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088774 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088775 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088776 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088777 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088778 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088779 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088780 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088781 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088782 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088783 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088784 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088785 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088786 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088787 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088788 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088789 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088790 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088791 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088792 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088793 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088794 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088795 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088796 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088797 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088798 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088799 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088800 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088801 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088802 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088803 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088804 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088805 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088806 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088807 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088808 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088809 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088810 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088811 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088812 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088813 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088814 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088815 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088816 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088817 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088818 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088819 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088820 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088821 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088822 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088823 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088824 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088825 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088826 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088827 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088828 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088829 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088830 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088831 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088832 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088833 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088834 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088835 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088836 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088837 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088838 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088839 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088840 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088841 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088842 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088843 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088844 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088845 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088846 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088847 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088848 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088849 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088850 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088851 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088852 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088853 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088854 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088855 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088856 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088857 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088858 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088859 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088860 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088861 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088862 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088863 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088864 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088865 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088866 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088867 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088868 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088869 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088870 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088871 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088872 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088873 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088874 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088875 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088876 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088877 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088878 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088879 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088880 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088881 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088882 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088883 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088884 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088885 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088886 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088887 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088888 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088889 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088890 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088891 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088892 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088893 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088894 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088895 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088896 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088897 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088898 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088899 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088900 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088901 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088902 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088903 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088904 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088905 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088906 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088907 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088908 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088909 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088910 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088911 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088912 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088913 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088914 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088915 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088916 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088917 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088918 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088919 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088920 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088921 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088922 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088923 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088924 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088925 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088926 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088927 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088928 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088929 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088930 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088931 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088932 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088933 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088934 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088935 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088936 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088937 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088938 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088939 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088940 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088941 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088942 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088943 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088944 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088945 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088946 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088947 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088948 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088949 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088950 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088951 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088952 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088953 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088954 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088955 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088956 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088957 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088958 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088959 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088960 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088961 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088962 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088963 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088964 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088965 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088966 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088967 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088968 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088969 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088970 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088971 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088972 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088973 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088974 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088975 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088976 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088977 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088978 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088979 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088980 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088981 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088982 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088983 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088984 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088985 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088986 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088987 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088988 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088989 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088990 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088991 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088992 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088993 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088994 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088995 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088996 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088997 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088998 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1088999 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089000 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089001 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089002 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089003 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089004 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089005 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089006 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089007 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089008 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089009 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089010 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089011 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089012 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089013 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089014 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089015 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089016 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089017 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089018 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089019 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089020 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089021 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089022 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089023 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089024 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089025 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089026 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089027 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089028 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089029 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089030 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089031 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089032 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089033 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089034 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089035 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089036 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089037 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089038 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089039 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089040 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089041 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089042 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089043 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089044 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089045 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089046 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089047 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089048 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089049 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089050 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089051 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089052 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089053 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089054 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089055 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089056 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089057 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089058 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089059 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089060 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089061 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089062 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089063 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089064 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089065 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089066 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089067 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089068 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089069 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089070 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089071 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089072 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089073 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089074 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089075 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089076 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089077 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089078 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089079 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089080 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089081 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089082 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089083 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089084 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089085 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089086 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089087 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089088 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089089 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089090 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089091 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089092 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089093 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089094 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089095 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089096 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089097 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089098 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089099 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089100 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089101 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089102 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089103 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089104 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089105 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089106 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089107 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089108 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089109 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089110 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089111 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089112 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089113 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089114 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089115 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089116 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089117 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089118 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089119 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089120 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089121 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089122 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089123 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089124 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089125 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089126 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089127 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089128 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089129 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089130 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089131 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089132 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089133 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089134 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089135 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089136 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089137 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089138 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089139 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089140 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089141 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089142 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089143 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089144 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089145 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089146 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089147 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089148 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089149 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089150 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089151 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089152 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089153 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089154 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089155 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089156 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089157 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089158 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089159 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089160 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089161 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089162 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089163 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089164 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089165 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089166 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089167 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089168 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089169 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089170 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089171 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089172 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089173 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089174 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089175 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089176 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089177 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089178 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089179 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089180 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089181 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089182 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089183 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089184 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089185 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089186 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089187 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089188 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089189 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089190 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089191 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089192 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089193 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089194 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089195 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089196 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089197 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089198 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089199 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089200 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089201 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089202 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089203 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089204 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089205 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089206 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089207 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089208 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089209 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089210 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089211 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089212 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089213 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089214 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089215 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089216 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089217 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089218 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089219 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089220 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089221 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089222 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089223 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089224 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089225 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089226 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089227 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089228 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089229 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089230 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089231 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089232 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089233 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089234 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089235 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089236 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089237 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089238 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089239 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089240 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089241 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089242 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089243 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089244 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089245 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089246 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089247 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089248 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089249 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089250 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089251 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089252 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089253 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089254 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089255 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089256 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089257 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089258 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089259 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089260 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089261 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089262 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089263 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089264 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089265 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089266 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089267 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089268 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089269 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089270 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089271 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089272 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089273 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089274 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089275 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089276 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089277 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089278 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089279 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089280 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089281 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089282 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089283 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089284 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089285 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089286 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089287 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089288 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089289 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089290 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089291 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089292 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089293 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089294 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089295 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089296 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089297 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089298 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089299 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089300 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089301 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089302 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089303 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089304 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089305 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089306 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089307 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089308 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089309 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089310 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089311 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089312 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089313 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089314 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089315 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089316 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089317 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089318 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089319 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089320 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089321 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089322 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089323 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089324 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089325 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089326 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089327 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089328 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089329 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089330 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089331 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089332 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089333 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089334 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089335 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089336 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089337 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089338 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089339 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089340 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089341 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089342 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089343 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089344 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089345 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089346 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089347 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089348 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089349 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089350 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089351 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089352 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089353 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089354 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089355 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089356 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089357 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089358 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089359 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089360 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089361 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089362 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089363 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089364 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089365 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089366 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089367 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089368 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089369 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089370 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089371 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089372 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089373 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089374 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089375 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089376 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089377 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089378 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089379 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089380 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089381 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089382 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089383 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089384 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089385 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089386 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089387 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089388 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089389 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089390 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089391 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089392 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089393 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089394 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089395 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089396 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089397 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089398 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089399 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089400 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089401 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089402 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089403 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089404 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089405 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089406 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089407 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089408 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089409 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089410 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089411 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089412 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089413 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089414 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089415 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089416 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089417 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089418 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089419 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089420 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089421 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089422 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089423 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089424 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089425 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089426 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089427 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089428 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089429 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089430 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089431 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089432 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089433 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089434 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089435 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089436 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089437 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089438 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089439 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089440 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089441 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089442 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089443 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089444 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089445 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089446 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089447 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089448 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089449 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089450 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089451 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089452 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089453 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089454 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089455 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089456 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089457 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089458 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089459 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089460 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089461 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089462 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089463 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089464 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089465 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089466 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089467 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089468 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089469 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089470 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089471 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089472 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089473 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089474 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089475 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089476 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089477 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089478 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089479 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089480 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089481 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089482 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089483 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089484 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089485 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089486 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089487 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089488 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089489 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089490 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089491 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089492 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089493 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089494 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089495 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089496 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089497 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089498 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089499 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089500 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089501 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089502 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089503 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089504 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089505 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089506 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089507 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089508 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089509 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089510 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089511 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089512 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089513 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089514 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089515 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089516 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089517 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089518 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089519 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089520 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089521 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089522 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089523 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089524 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089525 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089526 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089527 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089528 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089529 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089530 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089531 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089532 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089533 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089534 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089535 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089536 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089537 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089538 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089539 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089540 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089541 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089542 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089543 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089544 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089545 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089546 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089547 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089548 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089549 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089550 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089551 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089552 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089553 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089554 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089555 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089556 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089557 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089558 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089559 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089560 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089561 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089562 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089563 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089564 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089565 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089566 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089567 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089568 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089569 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089570 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089571 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089572 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089573 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089574 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089575 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089576 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089577 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089578 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089579 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089580 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089581 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089582 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089583 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089584 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089585 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089586 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089587 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089588 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089589 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089590 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089591 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089592 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089593 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089594 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089595 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089596 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089597 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089598 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089599 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089600 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089601 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089602 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089603 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089604 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089605 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089606 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089607 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089608 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089609 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089610 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089611 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089612 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089613 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089614 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089615 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089616 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089617 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089618 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089619 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089620 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089621 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089622 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089623 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089624 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089625 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089626 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089627 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089628 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089629 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089630 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089631 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089632 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089633 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089634 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089635 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089636 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089637 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089638 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089639 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089640 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089641 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089642 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089643 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089644 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089645 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089646 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089647 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089648 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089649 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089650 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089651 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089652 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089653 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089654 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089655 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089656 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089657 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089658 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089659 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089660 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089661 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089662 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089663 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089664 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089665 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089666 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089667 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089668 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089669 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089670 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089671 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089672 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089673 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089674 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089675 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089676 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089677 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089678 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089679 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089680 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089681 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089682 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089683 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089684 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089685 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089686 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089687 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089688 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089689 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089690 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089691 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089692 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089693 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089694 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089695 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089696 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089697 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089698 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089699 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089700 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089701 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089702 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089703 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089704 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089705 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089706 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089707 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089708 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089709 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089710 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089711 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089712 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089713 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089714 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089715 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089716 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089717 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089718 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089719 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089720 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089721 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089722 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089723 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089724 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089725 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089726 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089727 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089728 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089729 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089730 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089731 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089732 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089733 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089734 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089735 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089736 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089737 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089738 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089739 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089740 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089741 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089742 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089743 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089744 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089745 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089746 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089747 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089748 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089749 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089750 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089751 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089752 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089753 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089754 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089755 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089756 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089757 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089758 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089759 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089760 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089761 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089762 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089763 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089764 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089765 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089766 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089767 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089768 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089769 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089770 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089771 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089772 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089773 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089774 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089775 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089776 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089777 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089778 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089779 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089780 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089781 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089782 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089783 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089784 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089785 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089786 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089787 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089788 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089789 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089790 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089791 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089792 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089793 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089794 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089795 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089796 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089797 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089798 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089799 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089800 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089801 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089802 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089803 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089804 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089805 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089806 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089807 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089808 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089809 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089810 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089811 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089812 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089813 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089814 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089815 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089816 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089817 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089818 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089819 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089820 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089821 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089822 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089823 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089824 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089825 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089826 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089827 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089828 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089829 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089830 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089831 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089832 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089833 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089834 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089835 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089836 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089837 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089838 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089839 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089840 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089841 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089842 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089843 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089844 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089845 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089846 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089847 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089848 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089849 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089850 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089851 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089852 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089853 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089854 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089855 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089856 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089857 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089858 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089859 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089860 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089861 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089862 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089863 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089864 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089865 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089866 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089867 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089868 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089869 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089870 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089871 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089872 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089873 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089874 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089875 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089876 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089877 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089878 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089879 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089880 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089881 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089882 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089883 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089884 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089885 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089886 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089887 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089888 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089889 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089890 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089891 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089892 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089893 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089894 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089895 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089896 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089897 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089898 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089899 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089900 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089901 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089902 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089903 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089904 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089905 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089906 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089907 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089908 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089909 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089910 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089911 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089912 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089913 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089914 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089915 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089916 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089917 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089918 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089919 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089920 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089921 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089922 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089923 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089924 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089925 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089926 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089927 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089928 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089929 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089930 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089931 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089932 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089933 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089934 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089935 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089936 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089937 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089938 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089939 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089940 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089941 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089942 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089943 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089944 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089945 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089946 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089947 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089948 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089949 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089950 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089951 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089952 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089953 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089954 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089955 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089956 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089957 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089958 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089959 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089960 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089961 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089962 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089963 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089964 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089965 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089966 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089967 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089968 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089969 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089970 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089971 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089972 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089973 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089974 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089975 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089976 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089977 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089978 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089979 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089980 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089981 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089982 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089983 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089984 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089985 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089986 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089987 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089988 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089989 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089990 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089991 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089992 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089993 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089994 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089995 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089996 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089997 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089998 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1089999 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090000 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090001 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090002 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090003 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090004 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090005 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090006 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090007 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090008 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090009 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090010 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090011 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090012 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090013 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090014 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090015 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090016 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090017 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090018 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090019 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090020 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090021 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090022 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090023 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090024 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090025 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090026 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090027 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090028 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090029 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090030 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090031 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090032 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090033 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090034 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090035 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090036 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090037 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090038 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090039 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090040 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090041 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090042 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090043 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090044 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090045 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090046 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090047 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090048 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090049 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090050 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090051 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090052 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090053 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090054 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090055 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090056 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090057 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090058 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090059 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090060 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090061 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090062 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090063 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090064 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090065 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090066 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090067 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090068 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090069 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090070 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090071 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090072 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090073 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090074 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090075 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090076 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090077 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090078 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090079 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090080 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090081 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090082 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090083 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090084 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090085 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090086 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090087 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090088 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090089 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090090 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090091 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090092 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090093 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090094 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090095 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090096 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090097 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090098 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090099 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090100 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090101 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090102 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090103 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090104 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090105 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090106 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090107 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090108 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090109 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090110 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090111 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090112 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090113 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090114 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090115 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090116 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090117 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090118 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090119 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090120 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090121 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090122 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090123 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090124 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090125 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090126 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090127 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090128 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090129 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090130 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090131 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090132 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090133 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090134 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090135 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090136 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090137 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090138 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090139 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090140 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090141 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090142 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090143 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090144 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090145 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090146 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090147 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090148 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090149 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090150 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090151 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090152 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090153 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090154 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090155 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090156 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090157 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090158 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090159 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090160 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090161 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090162 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090163 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090164 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090165 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090166 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090167 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090168 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090169 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090170 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090171 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090172 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090173 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090174 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090175 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090176 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090177 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090178 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090179 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090180 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090181 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090182 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090183 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090184 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090185 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090186 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090187 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090188 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090189 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090190 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090191 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090192 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090193 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090194 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090195 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090196 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090197 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090198 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090199 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090200 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090201 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090202 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090203 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090204 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090205 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090206 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090207 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090208 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090209 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090210 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090211 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090212 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090213 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090214 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090215 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090216 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090217 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090218 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090219 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090220 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090221 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090222 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090223 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090224 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090225 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090226 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090227 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090228 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090229 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090230 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090231 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090232 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090233 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090234 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090235 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090236 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090237 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090238 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090239 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090240 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090241 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090242 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090243 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090244 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090245 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090246 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090247 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090248 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090249 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090250 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090251 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090252 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090253 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090254 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090255 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090256 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090257 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090258 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090259 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090260 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090261 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090262 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090263 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090264 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090265 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090266 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090267 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090268 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090269 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090270 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090271 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090272 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090273 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090274 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090275 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090276 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090277 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090278 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090279 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090280 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090281 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090282 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090283 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090284 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090285 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090286 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090287 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090288 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090289 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090290 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090291 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090292 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090293 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090294 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090295 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090296 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090297 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090298 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090299 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090300 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090301 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090302 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090303 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090304 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090305 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090306 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090307 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090308 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090309 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090310 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090311 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090312 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090313 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090314 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090315 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090316 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090317 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090318 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090319 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090320 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090321 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090322 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090323 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090324 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090325 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090326 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090327 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090328 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090329 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090330 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090331 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090332 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090333 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090334 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090335 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090336 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090337 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090338 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090339 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090340 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090341 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090342 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090343 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090344 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090345 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090346 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090347 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090348 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090349 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090350 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090351 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090352 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090353 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090354 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090355 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090356 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090357 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090358 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090359 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090360 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090361 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090362 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090363 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090364 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090365 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090366 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090367 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090368 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090369 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090370 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090371 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090372 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090373 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090374 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090375 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090376 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090377 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090378 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090379 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090380 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090381 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090382 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090383 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090384 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090385 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090386 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090387 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090388 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090389 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090390 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090391 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090392 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090393 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090394 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090395 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090396 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090397 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090398 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090399 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090400 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090401 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090402 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090403 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090404 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090405 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090406 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090407 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090408 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090409 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090410 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090411 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090412 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090413 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090414 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090415 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090416 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090417 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090418 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090419 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090420 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090421 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090422 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090423 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090424 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090425 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090426 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090427 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090428 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090429 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090430 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090431 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090432 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090433 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090434 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090435 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090436 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090437 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090438 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090439 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090440 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090441 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090442 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090443 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090444 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090445 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090446 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090447 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090448 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090449 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090450 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090451 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090452 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090453 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090454 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090455 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090456 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090457 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090458 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090459 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090460 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090461 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090462 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090463 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090464 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090465 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090466 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090467 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090468 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090469 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090470 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090471 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090472 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090473 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090474 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090475 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090476 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090477 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090478 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090479 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090480 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090481 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090482 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090483 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090484 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090485 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090486 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090487 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090488 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090489 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090490 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090491 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090492 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090493 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090494 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090495 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090496 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090497 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090498 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090499 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090500 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090501 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090502 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090503 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090504 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090505 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090506 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090507 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090508 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090509 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090510 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090511 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090512 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090513 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090514 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090515 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090516 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090517 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090518 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090519 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090520 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090521 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090522 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090523 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090524 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090525 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090526 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090527 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090528 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090529 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090530 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090531 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090532 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090533 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090534 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090535 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090536 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090537 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090538 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090539 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090540 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090541 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090542 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090543 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090544 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090545 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090546 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090547 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090548 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090549 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090550 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090551 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090552 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090553 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090554 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090555 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090556 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090557 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090558 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090559 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090560 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090561 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090562 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090563 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090564 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090565 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090566 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090567 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090568 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090569 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090570 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090571 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090572 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090573 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090574 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090575 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090576 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090577 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090578 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090579 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090580 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090581 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090582 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090583 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090584 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090585 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090586 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090587 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090588 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090589 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090590 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090591 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090592 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090593 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090594 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090595 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090596 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090597 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090598 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090599 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090600 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090601 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090602 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090603 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090604 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090605 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090606 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090607 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090608 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090609 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090610 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090611 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090612 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090613 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090614 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090615 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090616 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090617 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090618 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090619 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090620 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090621 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090622 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090623 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090624 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090625 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090626 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090627 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090628 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090629 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090630 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090631 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090632 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090633 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090634 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090635 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090636 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090637 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090638 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090639 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090640 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090641 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090642 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090643 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090644 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090645 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090646 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090647 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090648 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090649 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090650 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090651 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090652 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090653 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090654 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090655 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090656 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090657 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090658 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090659 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090660 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090661 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090662 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090663 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090664 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090665 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090666 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090667 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090668 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090669 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090670 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090671 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090672 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090673 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090674 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090675 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090676 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090677 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090678 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090679 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090680 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090681 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090682 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090683 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090684 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090685 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090686 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090687 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090688 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090689 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090690 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090691 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090692 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090693 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090694 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090695 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090696 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090697 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090698 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090699 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090700 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090701 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090702 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090703 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090704 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090705 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090706 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090707 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090708 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090709 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090710 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090711 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090712 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090713 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090714 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090715 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090716 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090717 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090718 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090719 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090720 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090721 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090722 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090723 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090724 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090725 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090726 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090727 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090728 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090729 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090730 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090731 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090732 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090733 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090734 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090735 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090736 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090737 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090738 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090739 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090740 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090741 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090742 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090743 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090744 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090745 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090746 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090747 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090748 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090749 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090750 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090751 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090752 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090753 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090754 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090755 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090756 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090757 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090758 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090759 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090760 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090761 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090762 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090763 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090764 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090765 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090766 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090767 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090768 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090769 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090770 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090771 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090772 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090773 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090774 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090775 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090776 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090777 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090778 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090779 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090780 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090781 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090782 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090783 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090784 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090785 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090786 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090787 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090788 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090789 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090790 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090791 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090792 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090793 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090794 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090795 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090796 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090797 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090798 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090799 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090800 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090801 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090802 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090803 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090804 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090805 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090806 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090807 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090808 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090809 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090810 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090811 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090812 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090813 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090814 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090815 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090816 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090817 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090818 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090819 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090820 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090821 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090822 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090823 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090824 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090825 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090826 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090827 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090828 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090829 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090830 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090831 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090832 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090833 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090834 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090835 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090836 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090837 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090838 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090839 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090840 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090841 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090842 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090843 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090844 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090845 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090846 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090847 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090848 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090849 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090850 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090851 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090852 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090853 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090854 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090855 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090856 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090857 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090858 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090859 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090860 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090861 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090862 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090863 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090864 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090865 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090866 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090867 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090868 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090869 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090870 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090871 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090872 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090873 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090874 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090875 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090876 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090877 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090878 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090879 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090880 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090881 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090882 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090883 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090884 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090885 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090886 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090887 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090888 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090889 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090890 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090891 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090892 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090893 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090894 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090895 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090896 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090897 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090898 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090899 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090900 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090901 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090902 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090903 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090904 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090905 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090906 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090907 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090908 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090909 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090910 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090911 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090912 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090913 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090914 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090915 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090916 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090917 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090918 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090919 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090920 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090921 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090922 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090923 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090924 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090925 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090926 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090927 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090928 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090929 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090930 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090931 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090932 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090933 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090934 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090935 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090936 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090937 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090938 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090939 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090940 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090941 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090942 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090943 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090944 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090945 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090946 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090947 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090948 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090949 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090950 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090951 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090952 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090953 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090954 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090955 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090956 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090957 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090958 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090959 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090960 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090961 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090962 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090963 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090964 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090965 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090966 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090967 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090968 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090969 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090970 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090971 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090972 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090973 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090974 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090975 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090976 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090977 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090978 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090979 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090980 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090981 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090982 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090983 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090984 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090985 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090986 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090987 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090988 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090989 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090990 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090991 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090992 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090993 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090994 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090995 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090996 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090997 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090998 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1090999 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091000 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091001 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091002 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091003 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091004 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091005 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091006 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091007 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091008 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091009 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091010 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091011 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091012 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091013 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091014 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091015 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091016 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091017 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091018 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091019 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091020 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091021 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091022 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091023 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091024 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091025 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091026 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091027 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091028 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091029 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091030 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091031 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091032 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091033 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091034 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091035 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091036 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091037 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091038 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091039 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091040 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091041 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091042 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091043 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091044 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091045 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091046 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091047 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091048 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091049 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091050 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091051 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091052 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091053 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091054 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091055 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091056 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091057 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091058 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091059 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091060 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091061 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091062 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091063 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091064 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091065 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091066 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091067 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091068 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091069 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091070 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091071 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091072 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091073 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091074 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091075 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091076 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091077 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091078 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091079 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091080 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091081 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091082 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091083 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091084 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091085 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091086 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091087 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091088 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091089 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091090 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091091 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091092 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091093 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091094 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091095 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091096 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091097 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091098 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091099 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091100 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091101 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091102 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091103 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091104 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091105 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091106 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091107 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091108 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091109 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091110 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091111 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091112 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091113 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091114 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091115 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091116 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091117 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091118 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091119 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091120 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091121 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091122 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091123 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091124 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091125 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091126 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091127 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091128 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091129 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091130 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091131 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091132 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091133 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091134 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091135 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091136 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091137 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091138 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091139 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091140 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091141 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091142 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091143 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091144 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091145 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091146 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091147 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091148 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091149 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091150 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091151 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091152 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091153 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091154 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091155 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091156 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091157 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091158 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091159 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091160 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091161 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091162 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091163 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091164 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091165 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091166 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091167 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091168 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091169 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091170 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091171 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091172 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091173 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091174 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091175 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091176 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091177 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091178 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091179 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091180 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091181 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091182 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091183 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091184 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091185 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091186 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091187 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091188 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091189 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091190 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091191 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091192 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091193 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091194 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091195 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091196 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091197 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091198 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091199 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091200 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091201 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091202 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091203 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091204 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091205 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091206 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091207 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091208 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091209 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091210 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091211 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091212 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091213 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091214 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091215 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091216 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091217 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091218 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091219 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091220 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091221 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091222 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091223 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091224 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091225 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091226 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091227 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091228 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091229 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091230 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091231 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091232 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091233 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091234 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091235 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091236 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091237 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091238 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091239 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091240 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091241 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091242 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091243 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091244 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091245 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091246 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091247 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091248 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091249 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091250 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091251 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091252 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091253 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091254 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091255 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091256 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091257 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091258 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091259 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091260 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091261 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091262 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091263 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091264 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091265 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091266 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091267 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091268 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091269 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091270 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091271 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091272 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091273 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091274 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091275 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091276 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091277 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091278 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091279 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091280 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091281 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091282 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091283 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091284 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091285 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091286 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091287 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091288 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091289 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091290 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091291 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091292 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091293 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091294 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091295 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091296 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091297 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091298 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091299 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091300 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091301 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091302 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091303 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091304 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091305 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091306 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091307 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091308 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091309 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091310 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091311 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091312 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091313 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091314 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091315 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091316 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091317 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091318 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091319 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091320 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091321 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091322 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091323 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091324 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091325 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091326 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091327 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091328 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091329 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091330 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091331 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091332 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091333 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091334 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091335 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091336 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091337 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091338 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091339 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091340 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091341 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091342 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091343 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091344 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091345 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091346 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091347 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091348 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091349 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091350 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091351 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091352 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091353 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091354 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091355 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091356 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091357 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091358 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091359 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091360 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091361 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091362 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091363 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091364 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091365 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091366 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091367 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091368 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091369 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091370 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091371 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091372 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091373 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091374 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091375 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091376 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091377 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091378 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091379 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091380 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091381 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091382 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091383 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091384 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091385 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091386 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091387 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091388 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091389 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091390 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091391 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091392 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091393 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091394 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091395 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091396 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091397 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091398 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091399 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091400 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091401 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091402 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091403 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091404 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091405 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091406 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091407 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091408 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091409 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091410 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091411 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091412 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091413 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091414 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091415 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091416 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091417 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091418 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091419 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091420 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091421 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091422 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091423 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091424 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091425 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091426 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091427 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091428 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091429 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091430 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091431 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091432 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091433 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091434 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091435 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091436 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091437 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091438 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091439 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091440 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091441 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091442 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091443 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091444 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091445 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091446 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091447 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091448 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091449 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091450 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091451 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091452 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091453 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091454 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091455 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091456 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091457 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091458 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091459 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091460 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091461 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091462 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091463 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091464 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091465 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091466 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091467 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091468 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091469 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091470 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091471 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091472 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091473 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091474 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091475 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091476 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091477 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091478 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091479 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091480 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091481 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091482 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091483 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091484 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091485 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091486 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091487 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091488 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091489 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091490 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091491 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091492 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091493 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091494 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091495 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091496 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091497 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091498 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091499 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091500 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091501 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091502 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091503 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091504 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091505 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091506 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091507 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091508 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091509 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091510 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091511 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091512 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091513 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091514 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091515 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091516 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091517 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091518 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091519 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091520 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091521 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091522 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091523 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091524 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091525 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091526 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091527 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091528 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091529 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091530 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091531 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091532 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091533 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091534 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091535 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091536 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091537 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091538 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091539 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091540 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091541 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091542 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091543 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091544 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091545 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091546 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091547 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091548 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091549 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091550 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091551 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091552 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091553 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091554 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091555 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091556 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091557 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091558 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091559 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091560 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091561 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091562 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091563 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091564 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091565 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091566 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091567 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091568 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091569 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091570 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091571 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091572 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091573 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091574 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091575 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091576 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091577 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091578 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091579 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091580 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091581 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091582 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091583 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091584 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091585 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091586 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091587 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091588 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091589 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091590 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091591 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091592 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091593 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091594 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091595 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091596 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091597 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091598 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091599 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091600 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091601 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091602 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091603 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091604 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091605 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091606 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091607 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091608 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091609 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091610 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091611 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091612 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091613 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091614 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091615 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091616 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091617 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091618 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091619 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091620 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091621 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091622 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091623 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091624 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091625 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091626 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091627 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091628 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091629 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091630 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091631 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091632 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091633 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091634 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091635 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091636 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091637 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091638 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091639 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091640 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091641 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091642 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091643 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091644 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091645 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091646 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091647 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091648 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091649 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091650 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091651 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091652 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091653 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091654 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091655 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091656 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091657 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091658 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091659 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091660 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091661 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091662 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091663 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091664 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091665 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091666 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091667 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091668 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091669 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091670 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091671 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091672 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091673 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091674 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091675 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091676 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091677 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091678 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091679 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091680 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091681 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091682 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091683 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091684 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091685 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091686 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091687 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091688 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091689 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091690 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091691 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091692 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091693 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091694 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091695 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091696 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091697 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091698 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091699 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091700 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091701 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091702 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091703 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091704 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091705 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091706 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091707 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091708 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091709 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091710 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091711 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091712 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091713 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091714 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091715 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091716 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091717 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091718 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091719 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091720 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091721 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091722 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091723 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091724 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091725 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091726 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091727 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091728 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091729 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091730 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091731 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091732 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091733 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091734 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091735 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091736 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091737 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091738 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091739 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091740 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091741 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091742 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091743 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091744 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091745 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091746 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091747 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091748 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091749 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091750 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091751 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091752 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091753 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091754 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091755 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091756 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091757 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091758 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091759 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091760 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091761 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091762 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091763 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091764 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091765 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091766 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091767 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091768 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091769 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091770 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091771 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091772 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091773 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091774 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091775 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091776 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091777 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091778 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091779 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091780 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091781 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091782 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091783 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091784 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091785 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091786 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091787 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091788 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091789 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091790 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091791 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091792 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091793 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091794 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091795 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091796 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091797 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091798 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091799 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091800 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091801 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091802 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091803 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091804 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091805 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091806 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091807 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091808 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091809 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091810 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091811 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091812 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091813 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091814 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091815 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091816 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091817 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091818 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091819 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091820 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091821 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091822 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091823 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091824 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091825 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091826 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091827 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091828 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091829 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091830 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091831 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091832 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091833 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091834 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091835 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091836 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091837 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091838 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091839 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091840 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091841 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091842 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091843 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091844 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091845 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091846 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091847 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091848 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091849 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091850 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091851 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091852 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091853 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091854 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091855 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091856 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091857 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091858 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091859 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091860 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091861 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091862 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091863 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091864 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091865 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091866 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091867 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091868 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091869 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091870 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091871 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091872 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091873 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091874 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091875 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091876 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091877 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091878 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091879 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091880 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091881 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091882 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091883 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091884 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091885 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091886 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091887 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091888 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091889 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091890 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091891 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091892 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091893 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091894 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091895 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091896 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091897 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091898 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091899 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091900 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091901 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091902 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091903 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091904 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091905 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091906 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091907 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091908 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091909 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091910 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091911 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091912 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091913 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091914 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091915 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091916 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091917 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091918 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091919 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091920 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091921 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091922 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091923 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091924 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091925 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091926 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091927 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091928 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091929 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091930 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091931 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091932 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091933 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091934 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091935 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091936 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091937 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091938 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091939 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091940 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091941 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091942 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091943 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091944 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091945 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091946 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091947 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091948 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091949 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091950 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091951 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091952 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091953 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091954 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091955 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091956 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091957 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091958 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091959 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091960 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091961 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091962 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091963 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091964 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091965 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091966 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091967 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091968 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091969 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091970 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091971 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091972 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091973 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091974 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091975 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091976 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091977 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091978 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091979 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091980 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091981 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091982 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091983 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091984 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091985 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091986 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091987 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091988 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091989 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091990 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091991 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091992 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091993 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091994 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091995 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091996 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091997 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091998 (in epoch 155)\n",
      "WARNING:absl:loss is nan for step 1091999 (in epoch 155)\n",
      "INFO:absl:[155] train_loss=nan, train_x1_loss=nan, train_x2_loss=nan\n",
      "INFO:absl:[155] val_loss=nan\n",
      "INFO:absl:[155] test_loss=nan\n",
      "WARNING:absl:loss is nan for step 1092000 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092001 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092002 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092003 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092004 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092005 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092006 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092007 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092008 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092009 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092010 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092011 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092012 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092013 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092014 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092015 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092016 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092017 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092018 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092019 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092020 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092021 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092022 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092023 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092024 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092025 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092026 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092027 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092028 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092029 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092030 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092031 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092032 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092033 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092034 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092035 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092036 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092037 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092038 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092039 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092040 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092041 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092042 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092043 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092044 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092045 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092046 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092047 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092048 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092049 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092050 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092051 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092052 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092053 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092054 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092055 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092056 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092057 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092058 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092059 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092060 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092061 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092062 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092063 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092064 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092065 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092066 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092067 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092068 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092069 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092070 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092071 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092072 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092073 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092074 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092075 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092076 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092077 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092078 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092079 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092080 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092081 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092082 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092083 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092084 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092085 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092086 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092087 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092088 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092089 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092090 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092091 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092092 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092093 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092094 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092095 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092096 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092097 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092098 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092099 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092100 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092101 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092102 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092103 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092104 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092105 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092106 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092107 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092108 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092109 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092110 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092111 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092112 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092113 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092114 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092115 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092116 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092117 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092118 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092119 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092120 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092121 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092122 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092123 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092124 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092125 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092126 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092127 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092128 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092129 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092130 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092131 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092132 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092133 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092134 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092135 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092136 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092137 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092138 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092139 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092140 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092141 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092142 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092143 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092144 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092145 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092146 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092147 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092148 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092149 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092150 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092151 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092152 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092153 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092154 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092155 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092156 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092157 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092158 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092159 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092160 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092161 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092162 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092163 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092164 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092165 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092166 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092167 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092168 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092169 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092170 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092171 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092172 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092173 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092174 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092175 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092176 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092177 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092178 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092179 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092180 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092181 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092182 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092183 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092184 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092185 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092186 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092187 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092188 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092189 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092190 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092191 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092192 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092193 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092194 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092195 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092196 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092197 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092198 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092199 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092200 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092201 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092202 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092203 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092204 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092205 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092206 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092207 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092208 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092209 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092210 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092211 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092212 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092213 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092214 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092215 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092216 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092217 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092218 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092219 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092220 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092221 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092222 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092223 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092224 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092225 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092226 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092227 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092228 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092229 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092230 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092231 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092232 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092233 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092234 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092235 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092236 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092237 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092238 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092239 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092240 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092241 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092242 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092243 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092244 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092245 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092246 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092247 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092248 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092249 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092250 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092251 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092252 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092253 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092254 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092255 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092256 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092257 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092258 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092259 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092260 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092261 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092262 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092263 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092264 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092265 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092266 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092267 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092268 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092269 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092270 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092271 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092272 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092273 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092274 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092275 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092276 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092277 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092278 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092279 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092280 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092281 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092282 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092283 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092284 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092285 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092286 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092287 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092288 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092289 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092290 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092291 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092292 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092293 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092294 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092295 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092296 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092297 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092298 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092299 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092300 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092301 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092302 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092303 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092304 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092305 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092306 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092307 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092308 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092309 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092310 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092311 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092312 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092313 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092314 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092315 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092316 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092317 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092318 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092319 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092320 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092321 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092322 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092323 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092324 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092325 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092326 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092327 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092328 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092329 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092330 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092331 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092332 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092333 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092334 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092335 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092336 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092337 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092338 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092339 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092340 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092341 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092342 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092343 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092344 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092345 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092346 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092347 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092348 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092349 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092350 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092351 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092352 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092353 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092354 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092355 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092356 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092357 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092358 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092359 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092360 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092361 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092362 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092363 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092364 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092365 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092366 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092367 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092368 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092369 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092370 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092371 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092372 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092373 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092374 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092375 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092376 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092377 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092378 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092379 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092380 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092381 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092382 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092383 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092384 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092385 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092386 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092387 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092388 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092389 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092390 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092391 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092392 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092393 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092394 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092395 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092396 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092397 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092398 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092399 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092400 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092401 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092402 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092403 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092404 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092405 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092406 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092407 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092408 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092409 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092410 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092411 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092412 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092413 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092414 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092415 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092416 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092417 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092418 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092419 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092420 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092421 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092422 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092423 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092424 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092425 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092426 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092427 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092428 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092429 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092430 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092431 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092432 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092433 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092434 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092435 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092436 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092437 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092438 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092439 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092440 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092441 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092442 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092443 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092444 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092445 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092446 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092447 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092448 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092449 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092450 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092451 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092452 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092453 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092454 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092455 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092456 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092457 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092458 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092459 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092460 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092461 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092462 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092463 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092464 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092465 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092466 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092467 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092468 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092469 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092470 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092471 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092472 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092473 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092474 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092475 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092476 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092477 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092478 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092479 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092480 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092481 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092482 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092483 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092484 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092485 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092486 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092487 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092488 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092489 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092490 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092491 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092492 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092493 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092494 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092495 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092496 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092497 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092498 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092499 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092500 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092501 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092502 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092503 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092504 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092505 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092506 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092507 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092508 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092509 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092510 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092511 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092512 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092513 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092514 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092515 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092516 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092517 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092518 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092519 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092520 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092521 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092522 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092523 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092524 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092525 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092526 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092527 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092528 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092529 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092530 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092531 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092532 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092533 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092534 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092535 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092536 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092537 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092538 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092539 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092540 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092541 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092542 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092543 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092544 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092545 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092546 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092547 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092548 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092549 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092550 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092551 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092552 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092553 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092554 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092555 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092556 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092557 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092558 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092559 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092560 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092561 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092562 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092563 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092564 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092565 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092566 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092567 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092568 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092569 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092570 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092571 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092572 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092573 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092574 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092575 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092576 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092577 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092578 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092579 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092580 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092581 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092582 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092583 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092584 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092585 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092586 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092587 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092588 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092589 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092590 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092591 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092592 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092593 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092594 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092595 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092596 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092597 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092598 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092599 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092600 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092601 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092602 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092603 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092604 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092605 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092606 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092607 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092608 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092609 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092610 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092611 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092612 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092613 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092614 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092615 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092616 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092617 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092618 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092619 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092620 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092621 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092622 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092623 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092624 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092625 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092626 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092627 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092628 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092629 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092630 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092631 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092632 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092633 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092634 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092635 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092636 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092637 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092638 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092639 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092640 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092641 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092642 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092643 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092644 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092645 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092646 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092647 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092648 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092649 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092650 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092651 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092652 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092653 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092654 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092655 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092656 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092657 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092658 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092659 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092660 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092661 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092662 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092663 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092664 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092665 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092666 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092667 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092668 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092669 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092670 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092671 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092672 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092673 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092674 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092675 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092676 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092677 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092678 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092679 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092680 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092681 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092682 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092683 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092684 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092685 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092686 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092687 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092688 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092689 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092690 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092691 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092692 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092693 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092694 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092695 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092696 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092697 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092698 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092699 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092700 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092701 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092702 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092703 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092704 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092705 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092706 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092707 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092708 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092709 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092710 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092711 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092712 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092713 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092714 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092715 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092716 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092717 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092718 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092719 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092720 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092721 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092722 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092723 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092724 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092725 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092726 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092727 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092728 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092729 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092730 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092731 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092732 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092733 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092734 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092735 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092736 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092737 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092738 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092739 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092740 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092741 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092742 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092743 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092744 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092745 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092746 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092747 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092748 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092749 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092750 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092751 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092752 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092753 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092754 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092755 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092756 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092757 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092758 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092759 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092760 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092761 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092762 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092763 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092764 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092765 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092766 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092767 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092768 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092769 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092770 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092771 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092772 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092773 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092774 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092775 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092776 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092777 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092778 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092779 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092780 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092781 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092782 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092783 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092784 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092785 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092786 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092787 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092788 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092789 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092790 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092791 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092792 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092793 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092794 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092795 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092796 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092797 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092798 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092799 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092800 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092801 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092802 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092803 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092804 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092805 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092806 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092807 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092808 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092809 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092810 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092811 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092812 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092813 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092814 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092815 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092816 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092817 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092818 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092819 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092820 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092821 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092822 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092823 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092824 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092825 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092826 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092827 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092828 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092829 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092830 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092831 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092832 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092833 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092834 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092835 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092836 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092837 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092838 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092839 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092840 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092841 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092842 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092843 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092844 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092845 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092846 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092847 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092848 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092849 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092850 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092851 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092852 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092853 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092854 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092855 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092856 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092857 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092858 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092859 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092860 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092861 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092862 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092863 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092864 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092865 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092866 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092867 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092868 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092869 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092870 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092871 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092872 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092873 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092874 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092875 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092876 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092877 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092878 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092879 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092880 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092881 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092882 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092883 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092884 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092885 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092886 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092887 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092888 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092889 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092890 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092891 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092892 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092893 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092894 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092895 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092896 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092897 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092898 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092899 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092900 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092901 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092902 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092903 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092904 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092905 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092906 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092907 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092908 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092909 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092910 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092911 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092912 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092913 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092914 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092915 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092916 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092917 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092918 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092919 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092920 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092921 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092922 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092923 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092924 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092925 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092926 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092927 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092928 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092929 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092930 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092931 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092932 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092933 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092934 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092935 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092936 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092937 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092938 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092939 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092940 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092941 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092942 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092943 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092944 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092945 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092946 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092947 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092948 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092949 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092950 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092951 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092952 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092953 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092954 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092955 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092956 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092957 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092958 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092959 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092960 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092961 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092962 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092963 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092964 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092965 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092966 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092967 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092968 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092969 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092970 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092971 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092972 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092973 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092974 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092975 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092976 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092977 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092978 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092979 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092980 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092981 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092982 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092983 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092984 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092985 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092986 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092987 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092988 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092989 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092990 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092991 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092992 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092993 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092994 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092995 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092996 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092997 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092998 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1092999 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093000 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093001 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093002 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093003 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093004 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093005 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093006 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093007 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093008 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093009 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093010 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093011 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093012 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093013 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093014 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093015 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093016 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093017 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093018 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093019 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093020 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093021 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093022 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093023 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093024 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093025 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093026 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093027 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093028 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093029 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093030 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093031 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093032 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093033 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093034 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093035 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093036 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093037 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093038 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093039 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093040 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093041 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093042 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093043 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093044 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093045 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093046 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093047 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093048 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093049 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093050 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093051 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093052 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093053 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093054 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093055 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093056 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093057 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093058 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093059 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093060 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093061 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093062 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093063 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093064 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093065 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093066 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093067 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093068 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093069 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093070 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093071 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093072 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093073 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093074 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093075 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093076 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093077 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093078 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093079 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093080 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093081 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093082 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093083 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093084 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093085 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093086 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093087 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093088 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093089 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093090 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093091 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093092 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093093 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093094 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093095 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093096 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093097 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093098 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093099 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093100 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093101 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093102 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093103 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093104 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093105 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093106 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093107 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093108 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093109 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093110 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093111 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093112 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093113 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093114 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093115 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093116 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093117 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093118 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093119 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093120 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093121 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093122 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093123 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093124 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093125 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093126 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093127 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093128 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093129 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093130 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093131 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093132 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093133 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093134 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093135 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093136 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093137 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093138 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093139 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093140 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093141 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093142 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093143 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093144 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093145 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093146 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093147 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093148 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093149 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093150 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093151 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093152 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093153 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093154 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093155 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093156 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093157 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093158 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093159 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093160 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093161 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093162 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093163 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093164 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093165 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093166 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093167 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093168 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093169 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093170 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093171 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093172 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093173 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093174 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093175 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093176 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093177 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093178 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093179 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093180 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093181 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093182 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093183 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093184 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093185 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093186 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093187 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093188 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093189 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093190 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093191 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093192 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093193 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093194 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093195 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093196 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093197 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093198 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093199 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093200 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093201 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093202 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093203 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093204 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093205 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093206 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093207 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093208 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093209 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093210 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093211 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093212 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093213 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093214 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093215 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093216 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093217 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093218 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093219 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093220 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093221 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093222 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093223 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093224 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093225 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093226 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093227 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093228 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093229 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093230 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093231 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093232 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093233 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093234 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093235 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093236 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093237 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093238 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093239 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093240 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093241 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093242 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093243 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093244 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093245 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093246 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093247 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093248 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093249 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093250 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093251 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093252 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093253 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093254 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093255 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093256 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093257 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093258 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093259 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093260 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093261 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093262 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093263 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093264 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093265 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093266 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093267 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093268 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093269 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093270 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093271 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093272 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093273 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093274 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093275 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093276 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093277 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093278 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093279 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093280 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093281 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093282 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093283 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093284 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093285 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093286 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093287 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093288 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093289 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093290 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093291 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093292 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093293 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093294 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093295 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093296 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093297 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093298 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093299 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093300 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093301 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093302 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093303 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093304 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093305 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093306 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093307 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093308 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093309 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093310 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093311 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093312 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093313 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093314 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093315 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093316 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093317 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093318 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093319 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093320 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093321 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093322 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093323 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093324 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093325 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093326 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093327 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093328 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093329 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093330 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093331 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093332 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093333 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093334 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093335 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093336 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093337 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093338 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093339 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093340 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093341 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093342 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093343 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093344 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093345 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093346 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093347 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093348 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093349 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093350 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093351 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093352 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093353 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093354 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093355 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093356 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093357 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093358 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093359 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093360 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093361 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093362 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093363 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093364 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093365 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093366 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093367 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093368 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093369 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093370 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093371 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093372 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093373 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093374 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093375 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093376 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093377 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093378 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093379 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093380 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093381 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093382 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093383 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093384 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093385 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093386 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093387 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093388 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093389 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093390 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093391 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093392 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093393 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093394 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093395 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093396 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093397 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093398 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093399 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093400 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093401 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093402 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093403 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093404 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093405 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093406 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093407 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093408 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093409 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093410 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093411 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093412 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093413 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093414 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093415 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093416 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093417 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093418 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093419 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093420 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093421 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093422 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093423 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093424 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093425 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093426 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093427 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093428 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093429 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093430 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093431 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093432 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093433 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093434 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093435 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093436 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093437 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093438 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093439 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093440 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093441 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093442 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093443 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093444 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093445 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093446 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093447 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093448 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093449 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093450 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093451 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093452 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093453 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093454 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093455 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093456 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093457 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093458 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093459 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093460 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093461 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093462 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093463 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093464 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093465 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093466 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093467 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093468 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093469 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093470 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093471 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093472 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093473 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093474 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093475 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093476 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093477 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093478 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093479 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093480 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093481 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093482 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093483 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093484 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093485 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093486 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093487 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093488 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093489 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093490 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093491 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093492 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093493 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093494 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093495 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093496 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093497 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093498 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093499 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093500 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093501 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093502 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093503 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093504 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093505 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093506 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093507 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093508 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093509 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093510 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093511 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093512 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093513 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093514 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093515 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093516 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093517 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093518 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093519 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093520 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093521 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093522 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093523 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093524 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093525 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093526 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093527 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093528 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093529 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093530 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093531 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093532 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093533 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093534 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093535 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093536 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093537 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093538 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093539 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093540 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093541 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093542 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093543 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093544 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093545 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093546 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093547 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093548 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093549 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093550 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093551 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093552 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093553 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093554 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093555 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093556 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093557 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093558 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093559 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093560 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093561 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093562 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093563 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093564 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093565 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093566 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093567 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093568 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093569 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093570 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093571 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093572 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093573 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093574 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093575 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093576 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093577 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093578 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093579 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093580 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093581 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093582 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093583 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093584 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093585 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093586 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093587 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093588 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093589 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093590 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093591 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093592 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093593 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093594 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093595 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093596 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093597 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093598 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093599 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093600 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093601 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093602 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093603 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093604 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093605 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093606 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093607 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093608 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093609 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093610 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093611 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093612 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093613 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093614 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093615 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093616 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093617 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093618 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093619 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093620 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093621 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093622 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093623 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093624 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093625 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093626 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093627 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093628 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093629 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093630 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093631 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093632 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093633 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093634 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093635 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093636 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093637 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093638 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093639 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093640 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093641 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093642 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093643 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093644 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093645 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093646 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093647 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093648 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093649 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093650 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093651 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093652 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093653 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093654 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093655 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093656 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093657 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093658 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093659 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093660 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093661 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093662 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093663 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093664 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093665 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093666 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093667 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093668 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093669 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093670 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093671 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093672 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093673 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093674 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093675 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093676 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093677 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093678 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093679 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093680 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093681 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093682 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093683 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093684 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093685 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093686 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093687 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093688 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093689 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093690 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093691 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093692 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093693 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093694 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093695 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093696 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093697 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093698 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093699 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093700 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093701 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093702 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093703 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093704 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093705 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093706 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093707 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093708 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093709 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093710 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093711 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093712 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093713 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093714 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093715 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093716 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093717 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093718 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093719 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093720 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093721 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093722 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093723 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093724 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093725 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093726 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093727 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093728 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093729 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093730 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093731 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093732 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093733 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093734 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093735 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093736 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093737 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093738 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093739 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093740 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093741 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093742 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093743 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093744 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093745 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093746 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093747 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093748 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093749 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093750 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093751 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093752 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093753 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093754 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093755 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093756 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093757 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093758 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093759 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093760 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093761 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093762 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093763 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093764 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093765 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093766 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093767 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093768 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093769 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093770 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093771 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093772 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093773 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093774 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093775 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093776 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093777 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093778 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093779 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093780 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093781 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093782 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093783 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093784 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093785 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093786 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093787 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093788 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093789 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093790 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093791 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093792 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093793 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093794 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093795 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093796 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093797 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093798 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093799 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093800 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093801 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093802 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093803 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093804 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093805 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093806 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093807 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093808 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093809 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093810 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093811 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093812 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093813 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093814 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093815 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093816 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093817 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093818 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093819 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093820 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093821 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093822 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093823 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093824 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093825 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093826 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093827 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093828 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093829 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093830 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093831 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093832 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093833 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093834 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093835 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093836 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093837 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093838 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093839 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093840 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093841 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093842 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093843 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093844 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093845 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093846 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093847 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093848 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093849 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093850 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093851 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093852 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093853 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093854 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093855 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093856 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093857 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093858 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093859 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093860 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093861 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093862 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093863 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093864 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093865 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093866 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093867 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093868 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093869 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093870 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093871 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093872 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093873 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093874 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093875 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093876 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093877 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093878 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093879 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093880 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093881 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093882 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093883 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093884 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093885 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093886 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093887 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093888 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093889 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093890 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093891 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093892 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093893 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093894 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093895 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093896 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093897 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093898 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093899 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093900 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093901 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093902 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093903 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093904 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093905 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093906 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093907 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093908 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093909 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093910 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093911 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093912 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093913 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093914 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093915 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093916 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093917 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093918 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093919 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093920 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093921 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093922 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093923 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093924 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093925 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093926 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093927 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093928 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093929 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093930 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093931 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093932 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093933 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093934 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093935 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093936 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093937 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093938 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093939 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093940 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093941 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093942 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093943 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093944 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093945 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093946 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093947 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093948 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093949 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093950 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093951 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093952 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093953 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093954 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093955 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093956 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093957 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093958 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093959 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093960 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093961 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093962 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093963 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093964 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093965 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093966 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093967 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093968 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093969 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093970 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093971 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093972 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093973 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093974 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093975 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093976 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093977 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093978 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093979 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093980 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093981 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093982 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093983 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093984 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093985 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093986 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093987 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093988 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093989 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093990 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093991 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093992 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093993 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093994 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093995 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093996 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093997 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093998 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1093999 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094000 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094001 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094002 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094003 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094004 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094005 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094006 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094007 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094008 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094009 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094010 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094011 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094012 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094013 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094014 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094015 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094016 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094017 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094018 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094019 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094020 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094021 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094022 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094023 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094024 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094025 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094026 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094027 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094028 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094029 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094030 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094031 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094032 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094033 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094034 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094035 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094036 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094037 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094038 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094039 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094040 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094041 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094042 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094043 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094044 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094045 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094046 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094047 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094048 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094049 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094050 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094051 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094052 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094053 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094054 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094055 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094056 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094057 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094058 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094059 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094060 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094061 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094062 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094063 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094064 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094065 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094066 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094067 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094068 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094069 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094070 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094071 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094072 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094073 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094074 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094075 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094076 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094077 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094078 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094079 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094080 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094081 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094082 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094083 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094084 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094085 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094086 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094087 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094088 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094089 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094090 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094091 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094092 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094093 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094094 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094095 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094096 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094097 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094098 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094099 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094100 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094101 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094102 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094103 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094104 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094105 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094106 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094107 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094108 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094109 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094110 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094111 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094112 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094113 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094114 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094115 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094116 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094117 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094118 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094119 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094120 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094121 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094122 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094123 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094124 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094125 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094126 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094127 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094128 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094129 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094130 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094131 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094132 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094133 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094134 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094135 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094136 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094137 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094138 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094139 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094140 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094141 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094142 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094143 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094144 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094145 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094146 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094147 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094148 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094149 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094150 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094151 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094152 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094153 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094154 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094155 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094156 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094157 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094158 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094159 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094160 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094161 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094162 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094163 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094164 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094165 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094166 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094167 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094168 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094169 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094170 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094171 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094172 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094173 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094174 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094175 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094176 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094177 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094178 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094179 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094180 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094181 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094182 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094183 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094184 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094185 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094186 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094187 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094188 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094189 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094190 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094191 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094192 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094193 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094194 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094195 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094196 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094197 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094198 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094199 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094200 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094201 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094202 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094203 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094204 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094205 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094206 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094207 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094208 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094209 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094210 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094211 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094212 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094213 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094214 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094215 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094216 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094217 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094218 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094219 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094220 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094221 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094222 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094223 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094224 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094225 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094226 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094227 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094228 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094229 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094230 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094231 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094232 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094233 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094234 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094235 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094236 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094237 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094238 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094239 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094240 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094241 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094242 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094243 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094244 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094245 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094246 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094247 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094248 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094249 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094250 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094251 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094252 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094253 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094254 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094255 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094256 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094257 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094258 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094259 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094260 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094261 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094262 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094263 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094264 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094265 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094266 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094267 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094268 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094269 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094270 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094271 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094272 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094273 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094274 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094275 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094276 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094277 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094278 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094279 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094280 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094281 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094282 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094283 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094284 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094285 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094286 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094287 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094288 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094289 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094290 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094291 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094292 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094293 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094294 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094295 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094296 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094297 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094298 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094299 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094300 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094301 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094302 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094303 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094304 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094305 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094306 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094307 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094308 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094309 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094310 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094311 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094312 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094313 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094314 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094315 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094316 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094317 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094318 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094319 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094320 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094321 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094322 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094323 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094324 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094325 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094326 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094327 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094328 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094329 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094330 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094331 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094332 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094333 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094334 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094335 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094336 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094337 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094338 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094339 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094340 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094341 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094342 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094343 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094344 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094345 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094346 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094347 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094348 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094349 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094350 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094351 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094352 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094353 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094354 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094355 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094356 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094357 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094358 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094359 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094360 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094361 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094362 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094363 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094364 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094365 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094366 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094367 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094368 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094369 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094370 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094371 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094372 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094373 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094374 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094375 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094376 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094377 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094378 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094379 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094380 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094381 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094382 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094383 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094384 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094385 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094386 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094387 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094388 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094389 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094390 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094391 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094392 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094393 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094394 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094395 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094396 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094397 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094398 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094399 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094400 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094401 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094402 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094403 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094404 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094405 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094406 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094407 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094408 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094409 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094410 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094411 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094412 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094413 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094414 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094415 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094416 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094417 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094418 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094419 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094420 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094421 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094422 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094423 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094424 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094425 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094426 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094427 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094428 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094429 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094430 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094431 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094432 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094433 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094434 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094435 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094436 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094437 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094438 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094439 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094440 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094441 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094442 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094443 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094444 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094445 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094446 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094447 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094448 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094449 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094450 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094451 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094452 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094453 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094454 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094455 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094456 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094457 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094458 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094459 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094460 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094461 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094462 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094463 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094464 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094465 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094466 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094467 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094468 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094469 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094470 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094471 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094472 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094473 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094474 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094475 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094476 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094477 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094478 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094479 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094480 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094481 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094482 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094483 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094484 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094485 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094486 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094487 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094488 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094489 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094490 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094491 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094492 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094493 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094494 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094495 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094496 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094497 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094498 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094499 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094500 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094501 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094502 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094503 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094504 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094505 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094506 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094507 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094508 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094509 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094510 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094511 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094512 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094513 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094514 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094515 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094516 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094517 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094518 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094519 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094520 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094521 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094522 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094523 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094524 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094525 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094526 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094527 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094528 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094529 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094530 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094531 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094532 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094533 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094534 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094535 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094536 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094537 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094538 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094539 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094540 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094541 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094542 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094543 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094544 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094545 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094546 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094547 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094548 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094549 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094550 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094551 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094552 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094553 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094554 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094555 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094556 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094557 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094558 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094559 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094560 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094561 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094562 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094563 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094564 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094565 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094566 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094567 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094568 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094569 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094570 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094571 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094572 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094573 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094574 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094575 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094576 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094577 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094578 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094579 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094580 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094581 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094582 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094583 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094584 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094585 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094586 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094587 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094588 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094589 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094590 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094591 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094592 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094593 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094594 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094595 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094596 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094597 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094598 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094599 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094600 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094601 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094602 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094603 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094604 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094605 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094606 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094607 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094608 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094609 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094610 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094611 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094612 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094613 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094614 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094615 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094616 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094617 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094618 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094619 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094620 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094621 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094622 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094623 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094624 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094625 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094626 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094627 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094628 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094629 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094630 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094631 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094632 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094633 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094634 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094635 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094636 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094637 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094638 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094639 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094640 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094641 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094642 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094643 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094644 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094645 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094646 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094647 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094648 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094649 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094650 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094651 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094652 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094653 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094654 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094655 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094656 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094657 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094658 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094659 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094660 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094661 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094662 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094663 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094664 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094665 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094666 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094667 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094668 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094669 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094670 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094671 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094672 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094673 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094674 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094675 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094676 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094677 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094678 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094679 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094680 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094681 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094682 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094683 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094684 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094685 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094686 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094687 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094688 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094689 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094690 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094691 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094692 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094693 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094694 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094695 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094696 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094697 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094698 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094699 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094700 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094701 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094702 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094703 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094704 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094705 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094706 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094707 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094708 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094709 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094710 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094711 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094712 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094713 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094714 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094715 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094716 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094717 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094718 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094719 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094720 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094721 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094722 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094723 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094724 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094725 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094726 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094727 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094728 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094729 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094730 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094731 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094732 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094733 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094734 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094735 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094736 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094737 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094738 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094739 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094740 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094741 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094742 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094743 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094744 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094745 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094746 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094747 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094748 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094749 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094750 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094751 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094752 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094753 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094754 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094755 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094756 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094757 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094758 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094759 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094760 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094761 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094762 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094763 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094764 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094765 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094766 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094767 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094768 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094769 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094770 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094771 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094772 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094773 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094774 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094775 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094776 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094777 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094778 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094779 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094780 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094781 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094782 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094783 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094784 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094785 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094786 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094787 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094788 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094789 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094790 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094791 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094792 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094793 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094794 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094795 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094796 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094797 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094798 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094799 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094800 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094801 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094802 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094803 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094804 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094805 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094806 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094807 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094808 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094809 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094810 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094811 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094812 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094813 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094814 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094815 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094816 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094817 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094818 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094819 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094820 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094821 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094822 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094823 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094824 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094825 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094826 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094827 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094828 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094829 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094830 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094831 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094832 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094833 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094834 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094835 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094836 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094837 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094838 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094839 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094840 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094841 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094842 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094843 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094844 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094845 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094846 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094847 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094848 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094849 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094850 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094851 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094852 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094853 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094854 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094855 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094856 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094857 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094858 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094859 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094860 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094861 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094862 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094863 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094864 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094865 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094866 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094867 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094868 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094869 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094870 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094871 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094872 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094873 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094874 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094875 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094876 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094877 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094878 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094879 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094880 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094881 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094882 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094883 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094884 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094885 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094886 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094887 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094888 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094889 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094890 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094891 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094892 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094893 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094894 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094895 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094896 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094897 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094898 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094899 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094900 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094901 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094902 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094903 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094904 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094905 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094906 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094907 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094908 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094909 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094910 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094911 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094912 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094913 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094914 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094915 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094916 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094917 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094918 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094919 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094920 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094921 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094922 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094923 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094924 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094925 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094926 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094927 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094928 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094929 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094930 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094931 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094932 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094933 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094934 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094935 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094936 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094937 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094938 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094939 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094940 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094941 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094942 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094943 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094944 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094945 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094946 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094947 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094948 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094949 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094950 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094951 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094952 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094953 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094954 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094955 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094956 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094957 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094958 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094959 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094960 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094961 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094962 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094963 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094964 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094965 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094966 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094967 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094968 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094969 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094970 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094971 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094972 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094973 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094974 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094975 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094976 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094977 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094978 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094979 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094980 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094981 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094982 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094983 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094984 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094985 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094986 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094987 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094988 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094989 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094990 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094991 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094992 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094993 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094994 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094995 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094996 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1094997 (in epoch 156)\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "WARNING:absl:loss is nan for step 1096306 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096307 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096308 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096309 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096310 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096311 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096312 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096313 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096314 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096315 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096316 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096317 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096318 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096319 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096320 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096321 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096322 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096323 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096324 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096325 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096326 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096327 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096328 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096329 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096330 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096331 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096332 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096333 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096334 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096335 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096336 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096337 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096338 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096339 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096340 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096341 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096342 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096343 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096344 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096345 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096346 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096347 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096348 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096349 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096350 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096351 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096352 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096353 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096354 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096355 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096356 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096357 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096358 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096359 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096360 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096361 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096362 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096363 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096364 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096365 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096366 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096367 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096368 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096369 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096370 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096371 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096372 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096373 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096374 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096375 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096376 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096377 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096378 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096379 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096380 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096381 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096382 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096383 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096384 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096385 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096386 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096387 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096388 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096389 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096390 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096391 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096392 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096393 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096394 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096395 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096396 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096397 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096398 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096399 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096400 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096401 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096402 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096403 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096404 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096405 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096406 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096407 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096408 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096409 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096410 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096411 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096412 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096413 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096414 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096415 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096416 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096417 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096418 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096419 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096420 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096421 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096422 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096423 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096424 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096425 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096426 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096427 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096428 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096429 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096430 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096431 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096432 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096433 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096434 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096435 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096436 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096437 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096438 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096439 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096440 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096441 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096442 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096443 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096444 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096445 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096446 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096447 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096448 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096449 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096450 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096451 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096452 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096453 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096454 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096455 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096456 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096457 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096458 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096459 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096460 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096461 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096462 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096463 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096464 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096465 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096466 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096467 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096468 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096469 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096470 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096471 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096472 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096473 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096474 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096475 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096476 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096477 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096478 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096479 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096480 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096481 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096482 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096483 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096484 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096485 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096486 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096487 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096488 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096489 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096490 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096491 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096492 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096493 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096494 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096495 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096496 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096497 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096498 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096499 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096500 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096501 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096502 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096503 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096504 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096505 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096506 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096507 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096508 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096509 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096510 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096511 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096512 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096513 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096514 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096515 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096516 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096517 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096518 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096519 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096520 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096521 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096522 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096523 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096524 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096525 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096526 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096527 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096528 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096529 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096530 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096531 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096532 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096533 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096534 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096535 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096536 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096537 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096538 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096539 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096540 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096541 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096542 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096543 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096544 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096545 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096546 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096547 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096548 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096549 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096550 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096551 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096552 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096553 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096554 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096555 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096556 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096557 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096558 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096559 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096560 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096561 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096562 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096563 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096564 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096565 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096566 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096567 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096568 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096569 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096570 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096571 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096572 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096573 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096574 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096575 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096576 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096577 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096578 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096579 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096580 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096581 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096582 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096583 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096584 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096585 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096586 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096587 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096588 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096589 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096590 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096591 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096592 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096593 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096594 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096595 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096596 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096597 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096598 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096599 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096600 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096601 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096602 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096603 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096604 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096605 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096606 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096607 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096608 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096609 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096610 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096611 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096612 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096613 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096614 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096615 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096616 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096617 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096618 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096619 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096620 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096621 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096622 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096623 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096624 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096625 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096626 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096627 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096628 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096629 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096630 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096631 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096632 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096633 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096634 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096635 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096636 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096637 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096638 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096639 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096640 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096641 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096642 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096643 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096644 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096645 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096646 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096647 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096648 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096649 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096650 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096651 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096652 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096653 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096654 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096655 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096656 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096657 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096658 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096659 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096660 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096661 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096662 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096663 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096664 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096665 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096666 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096667 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096668 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096669 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096670 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096671 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096672 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096673 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096674 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096675 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096676 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096677 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096678 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096679 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096680 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096681 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096682 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096683 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096684 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096685 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096686 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096687 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096688 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096689 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096690 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096691 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096692 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096693 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096694 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096695 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096696 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096697 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096698 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096699 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096700 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096701 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096702 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096703 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096704 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096705 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096706 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096707 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096708 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096709 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096710 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096711 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096712 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096713 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096714 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096715 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096716 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096717 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096718 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096719 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096720 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096721 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096722 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096723 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096724 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096725 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096726 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096727 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096728 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096729 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096730 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096731 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096732 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096733 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096734 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096735 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096736 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096737 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096738 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096739 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096740 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096741 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096742 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096743 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096744 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096745 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096746 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096747 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096748 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096749 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096750 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096751 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096752 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096753 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096754 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096755 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096756 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096757 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096758 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096759 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096760 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096761 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096762 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096763 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096764 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096765 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096766 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096767 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096768 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096769 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096770 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096771 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096772 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096773 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096774 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096775 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096776 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096777 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096778 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096779 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096780 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096781 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096782 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096783 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096784 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096785 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096786 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096787 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096788 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096789 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096790 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096791 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096792 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096793 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096794 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096795 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096796 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096797 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096798 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096799 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096800 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096801 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096802 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096803 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096804 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096805 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096806 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096807 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096808 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096809 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096810 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096811 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096812 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096813 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096814 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096815 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096816 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096817 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096818 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096819 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096820 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096821 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096822 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096823 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096824 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096825 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096826 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096827 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096828 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096829 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096830 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096831 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096832 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096833 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096834 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096835 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096836 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096837 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096838 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096839 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096840 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096841 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096842 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096843 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096844 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096845 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096846 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096847 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096848 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096849 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096850 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096851 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096852 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096853 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096854 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096855 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096856 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096857 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096858 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096859 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096860 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096861 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096862 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096863 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096864 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096865 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096866 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096867 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096868 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096869 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096870 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096871 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096872 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096873 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096874 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096875 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096876 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096877 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096878 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096879 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096880 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096881 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096882 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096883 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096884 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096885 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096886 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096887 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096888 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096889 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096890 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096891 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096892 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096893 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096894 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096895 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096896 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096897 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096898 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096899 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096900 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096901 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096902 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096903 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096904 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096905 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096906 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096907 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096908 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096909 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096910 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096911 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096912 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096913 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096914 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096915 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096916 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096917 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096918 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096919 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096920 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096921 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096922 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096923 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096924 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096925 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096926 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096927 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096928 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096929 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096930 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096931 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096932 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096933 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096934 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096935 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096936 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096937 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096938 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096939 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096940 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096941 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096942 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096943 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096944 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096945 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096946 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096947 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096948 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096949 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096950 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096951 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096952 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096953 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096954 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096955 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096956 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096957 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096958 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096959 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096960 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096961 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096962 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096963 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096964 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096965 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096966 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096967 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096968 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096969 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096970 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096971 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096972 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096973 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096974 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096975 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096976 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096977 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096978 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096979 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096980 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096981 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096982 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096983 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096984 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096985 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096986 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096987 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096988 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096989 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096990 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096991 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096992 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096993 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096994 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096995 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096996 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096997 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096998 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1096999 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097000 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097001 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097002 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097003 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097004 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097005 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097006 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097007 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097008 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097009 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097010 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097011 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097012 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097013 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097014 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097015 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097016 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097017 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097018 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097019 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097020 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097021 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097022 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097023 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097024 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097025 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097026 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097027 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097028 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097029 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097030 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097031 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097032 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097033 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097034 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097035 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097036 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097037 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097038 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097039 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097040 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097041 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097042 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097043 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097044 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097045 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097046 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097047 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097048 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097049 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097050 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097051 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097052 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097053 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097054 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097055 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097056 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097057 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097058 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097059 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097060 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097061 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097062 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097063 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097064 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097065 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097066 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097067 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097068 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097069 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097070 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097071 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097072 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097073 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097074 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097075 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097076 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097077 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097078 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097079 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097080 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097081 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097082 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097083 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097084 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097085 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097086 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097087 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097088 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097089 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097090 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097091 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097092 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097093 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097094 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097095 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097096 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097097 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097098 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097099 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097100 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097101 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097102 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097103 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097104 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097105 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097106 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097107 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097108 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097109 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097110 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097111 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097112 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097113 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097114 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097115 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097116 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097117 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097118 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097119 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097120 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097121 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097122 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097123 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097124 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097125 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097126 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097127 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097128 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097129 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097130 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097131 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097132 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097133 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097134 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097135 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097136 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097137 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097138 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097139 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097140 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097141 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097142 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097143 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097144 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097145 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097146 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097147 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097148 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097149 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097150 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097151 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097152 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097153 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097154 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097155 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097156 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097157 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097158 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097159 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097160 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097161 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097162 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097163 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097164 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097165 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097166 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097167 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097168 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097169 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097170 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097171 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097172 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097173 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097174 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097175 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097176 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097177 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097178 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097179 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097180 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097181 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097182 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097183 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097184 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097185 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097186 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097187 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097188 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097189 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097190 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097191 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097192 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097193 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097194 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097195 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097196 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097197 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097198 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097199 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097200 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097201 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097202 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097203 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097204 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097205 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097206 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097207 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097208 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097209 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097210 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097211 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097212 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097213 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097214 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097215 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097216 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097217 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097218 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097219 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097220 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097221 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097222 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097223 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097224 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097225 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097226 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097227 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097228 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097229 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097230 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097231 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097232 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097233 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097234 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097235 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097236 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097237 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097238 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097239 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097240 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097241 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097242 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097243 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097244 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097245 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097246 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097247 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097248 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097249 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097250 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097251 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097252 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097253 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097254 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097255 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097256 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097257 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097258 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097259 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097260 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097261 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097262 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097263 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097264 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097265 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097266 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097267 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097268 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097269 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097270 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097271 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097272 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097273 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097274 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097275 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097276 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097277 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097278 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097279 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097280 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097281 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097282 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097283 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097284 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097285 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097286 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097287 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097288 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097289 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097290 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097291 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097292 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097293 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097294 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097295 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097296 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097297 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097298 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097299 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097300 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097301 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097302 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097303 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097304 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097305 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097306 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097307 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097308 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097309 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097310 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097311 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097312 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097313 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097314 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097315 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097316 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097317 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097318 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097319 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097320 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097321 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097322 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097323 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097324 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097325 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097326 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097327 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097328 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097329 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097330 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097331 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097332 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097333 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097334 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097335 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097336 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097337 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097338 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097339 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097340 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097341 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097342 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097343 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097344 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097345 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097346 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097347 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097348 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097349 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097350 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097351 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097352 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097353 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097354 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097355 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097356 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097357 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097358 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097359 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097360 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097361 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097362 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097363 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097364 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097365 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097366 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097367 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097368 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097369 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097370 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097371 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097372 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097373 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097374 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097375 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097376 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097377 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097378 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097379 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097380 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097381 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097382 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097383 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097384 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097385 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097386 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097387 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097388 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097389 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097390 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097391 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097392 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097393 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097394 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097395 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097396 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097397 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097398 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097399 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097400 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097401 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097402 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097403 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097404 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097405 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097406 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097407 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097408 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097409 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097410 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097411 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097412 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097413 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097414 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097415 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097416 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097417 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097418 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097419 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097420 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097421 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097422 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097423 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097424 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097425 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097426 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097427 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097428 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097429 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097430 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097431 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097432 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097433 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097434 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097435 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097436 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097437 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097438 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097439 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097440 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097441 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097442 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097443 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097444 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097445 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097446 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097447 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097448 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097449 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097450 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097451 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097452 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097453 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097454 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097455 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097456 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097457 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097458 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097459 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097460 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097461 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097462 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097463 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097464 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097465 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097466 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097467 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097468 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097469 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097470 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097471 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097472 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097473 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097474 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097475 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097476 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097477 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097478 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097479 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097480 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097481 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097482 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097483 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097484 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097485 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097486 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097487 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097488 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097489 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097490 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097491 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097492 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097493 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097494 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097495 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097496 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097497 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097498 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097499 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097500 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097501 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097502 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097503 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097504 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097505 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097506 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097507 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097508 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097509 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097510 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097511 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097512 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097513 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097514 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097515 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097516 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097517 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097518 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097519 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097520 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097521 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097522 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097523 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097524 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097525 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097526 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097527 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097528 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097529 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097530 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097531 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097532 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097533 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097534 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097535 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097536 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097537 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097538 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097539 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097540 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097541 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097542 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097543 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097544 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097545 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097546 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097547 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097548 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097549 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097550 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097551 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097552 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097553 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097554 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097555 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097556 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097557 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097558 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097559 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097560 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097561 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097562 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097563 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097564 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097565 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097566 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097567 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097568 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097569 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097570 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097571 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097572 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097573 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097574 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097575 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097576 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097577 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097578 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097579 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097580 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097581 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097582 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097583 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097584 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097585 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097586 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097587 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097588 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097589 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097590 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097591 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097592 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097593 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097594 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097595 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097596 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097597 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097598 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097599 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097600 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097601 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097602 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097603 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097604 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097605 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097606 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097607 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097608 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097609 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097610 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097611 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097612 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097613 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097614 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097615 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097616 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097617 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097618 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097619 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097620 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097621 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097622 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097623 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097624 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097625 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097626 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097627 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097628 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097629 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097630 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097631 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097632 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097633 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097634 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097635 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097636 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097637 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097638 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097639 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097640 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097641 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097642 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097643 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097644 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097645 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097646 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097647 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097648 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097649 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097650 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097651 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097652 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097653 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097654 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097655 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097656 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097657 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097658 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097659 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097660 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097661 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097662 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097663 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097664 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097665 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097666 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097667 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097668 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097669 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097670 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097671 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097672 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097673 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097674 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097675 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097676 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097677 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097678 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097679 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097680 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097681 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097682 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097683 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097684 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097685 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097686 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097687 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097688 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097689 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097690 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097691 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097692 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097693 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097694 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097695 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097696 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097697 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097698 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097699 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097700 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097701 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097702 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097703 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097704 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097705 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097706 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097707 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097708 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097709 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097710 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097711 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097712 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097713 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097714 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097715 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097716 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097717 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097718 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097719 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097720 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097721 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097722 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097723 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097724 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097725 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097726 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097727 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097728 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097729 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097730 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097731 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097732 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097733 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097734 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097735 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097736 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097737 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097738 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097739 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097740 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097741 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097742 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097743 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097744 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097745 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097746 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097747 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097748 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097749 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097750 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097751 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097752 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097753 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097754 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097755 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097756 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097757 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097758 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097759 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097760 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097761 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097762 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097763 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097764 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097765 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097766 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097767 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097768 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097769 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097770 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097771 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097772 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097773 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097774 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097775 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097776 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097777 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097778 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097779 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097780 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097781 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097782 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097783 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097784 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097785 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097786 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097787 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097788 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097789 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097790 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097791 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097792 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097793 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097794 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097795 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097796 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097797 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097798 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097799 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097800 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097801 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097802 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097803 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097804 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097805 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097806 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097807 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097808 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097809 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097810 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097811 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097812 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097813 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097814 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097815 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097816 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097817 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097818 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097819 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097820 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097821 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097822 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097823 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097824 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097825 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097826 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097827 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097828 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097829 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097830 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097831 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097832 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097833 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097834 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097835 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097836 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097837 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097838 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097839 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097840 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097841 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097842 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097843 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097844 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097845 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097846 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097847 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097848 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097849 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097850 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097851 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097852 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097853 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097854 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097855 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097856 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097857 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097858 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097859 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097860 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097861 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097862 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097863 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097864 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097865 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097866 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097867 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097868 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097869 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097870 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097871 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097872 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097873 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097874 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097875 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097876 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097877 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097878 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097879 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097880 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097881 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097882 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097883 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097884 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097885 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097886 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097887 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097888 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097889 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097890 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097891 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097892 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097893 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097894 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097895 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097896 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097897 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097898 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097899 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097900 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097901 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097902 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097903 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097904 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097905 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097906 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097907 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097908 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097909 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097910 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097911 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097912 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097913 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097914 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097915 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097916 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097917 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097918 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097919 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097920 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097921 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097922 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097923 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097924 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097925 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097926 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097927 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097928 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097929 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097930 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097931 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097932 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097933 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097934 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097935 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097936 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097937 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097938 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097939 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097940 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097941 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097942 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097943 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097944 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097945 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097946 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097947 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097948 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097949 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097950 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097951 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097952 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097953 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097954 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097955 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097956 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097957 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097958 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097959 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097960 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097961 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097962 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097963 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097964 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097965 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097966 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097967 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097968 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097969 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097970 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097971 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097972 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097973 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097974 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097975 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097976 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097977 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097978 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097979 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097980 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097981 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097982 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097983 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097984 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097985 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097986 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097987 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097988 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097989 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097990 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097991 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097992 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097993 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097994 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097995 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097996 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097997 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097998 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1097999 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098000 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098001 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098002 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098003 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098004 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098005 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098006 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098007 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098008 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098009 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098010 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098011 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098012 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098013 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098014 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098015 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098016 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098017 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098018 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098019 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098020 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098021 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098022 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098023 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098024 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098025 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098026 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098027 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098028 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098029 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098030 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098031 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098032 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098033 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098034 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098035 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098036 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098037 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098038 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098039 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098040 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098041 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098042 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098043 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098044 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098045 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098046 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098047 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098048 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098049 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098050 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098051 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098052 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098053 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098054 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098055 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098056 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098057 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098058 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098059 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098060 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098061 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098062 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098063 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098064 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098065 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098066 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098067 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098068 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098069 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098070 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098071 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098072 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098073 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098074 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098075 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098076 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098077 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098078 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098079 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098080 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098081 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098082 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098083 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098084 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098085 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098086 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098087 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098088 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098089 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098090 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098091 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098092 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098093 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098094 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098095 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098096 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098097 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098098 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098099 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098100 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098101 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098102 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098103 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098104 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098105 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098106 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098107 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098108 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098109 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098110 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098111 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098112 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098113 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098114 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098115 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098116 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098117 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098118 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098119 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098120 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098121 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098122 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098123 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098124 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098125 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098126 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098127 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098128 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098129 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098130 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098131 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098132 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098133 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098134 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098135 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098136 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098137 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098138 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098139 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098140 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098141 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098142 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098143 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098144 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098145 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098146 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098147 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098148 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098149 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098150 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098151 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098152 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098153 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098154 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098155 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098156 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098157 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098158 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098159 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098160 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098161 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098162 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098163 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098164 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098165 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098166 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098167 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098168 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098169 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098170 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098171 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098172 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098173 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098174 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098175 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098176 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098177 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098178 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098179 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098180 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098181 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098182 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098183 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098184 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098185 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098186 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098187 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098188 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098189 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098190 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098191 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098192 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098193 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098194 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098195 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098196 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098197 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098198 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098199 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098200 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098201 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098202 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098203 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098204 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098205 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098206 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098207 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098208 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098209 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098210 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098211 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098212 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098213 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098214 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098215 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098216 (in epoch 156)\n",
      "WARNING:absl:loss is nan for step 1098217 (in epoch 156)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/miamirabelli/Desktop/GNN Research/lorenzGNN/best_hparam_edges.ipynb Cell 26\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/best_hparam_edges.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m trial \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_trials\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/best_hparam_edges.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     _, _, connected_7_eval_metric, connected_7_epoch_losses \u001b[39m=\u001b[39m train_and_evaluate_with_data(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/best_hparam_edges.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         config\u001b[39m=\u001b[39;49mconnected_configs_7[trial\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m], workdir\u001b[39m=\u001b[39;49mconnected_workdirs_7[trial\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m], datasets\u001b[39m=\u001b[39;49mall_connected_7_datasets[trial\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/best_hparam_edges.ipynb#X34sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     connected_7_eval_metrics\u001b[39m.\u001b[39mappend(connected_7_eval_metric)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/best_hparam_edges.ipynb#X34sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     all_connected_7_epoch_losses\u001b[39m.\u001b[39mappend(connected_7_epoch_losses)\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/utils/jraph_training.py:607\u001b[0m, in \u001b[0;36mtrain_and_evaluate_with_data\u001b[0;34m(config, workdir, datasets, trial)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[39m# Perform one step of training.\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \u001b[39mwith\u001b[39;00m jax\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mStepTraceAnnotation(\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, step_num\u001b[39m=\u001b[39mstep):\n\u001b[1;32m    606\u001b[0m     \u001b[39m# graphs = jax.tree_util.tree_map(np.asarray, next(train_iter))\u001b[39;00m\n\u001b[0;32m--> 607\u001b[0m     state, metrics_update, _ \u001b[39m=\u001b[39m train_step(\n\u001b[1;32m    608\u001b[0m         state\u001b[39m=\u001b[39;49mstate, \n\u001b[1;32m    609\u001b[0m         n_rollout_steps\u001b[39m=\u001b[39;49mn_rollout_steps, \n\u001b[1;32m    610\u001b[0m         input_window_graphs\u001b[39m=\u001b[39;49minput_window_graphs, \n\u001b[1;32m    611\u001b[0m         target_window_graphs\u001b[39m=\u001b[39;49mtarget_window_graphs, \n\u001b[1;32m    612\u001b[0m         rngs\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mdropout\u001b[39;49m\u001b[39m'\u001b[39;49m: dropout_rng},\n\u001b[1;32m    613\u001b[0m     )\n\u001b[1;32m    614\u001b[0m     \u001b[39mif\u001b[39;00m jnp\u001b[39m.\u001b[39misnan(metrics_update\u001b[39m.\u001b[39mloss\u001b[39m.\u001b[39mtotal): \n\u001b[1;32m    615\u001b[0m         logging\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mloss is nan for step \u001b[39m\u001b[39m{\u001b[39;00mstep\u001b[39m}\u001b[39;00m\u001b[39m (in epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m<string>:1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(_cls, trace)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for trial in range(num_trials-1):\n",
    "    _, _, connected_7_eval_metric, connected_7_epoch_losses = train_and_evaluate_with_data(\n",
    "        config=connected_configs_7[trial+1], workdir=connected_workdirs_7[trial+1], datasets=all_connected_7_datasets[trial+1])\n",
    "    \n",
    "    connected_7_eval_metrics.append(connected_7_eval_metric)\n",
    "    \n",
    "    all_connected_7_epoch_losses.append(connected_7_epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'relu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 10, 'dropout_rate': 0.013287043114620523, 'edge_features': (8, 8), 'epochs': 100, 'eval_every_epochs': 1, 'fully_connected_edges': 5, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 1, 'layer_norm': False, 'learning_rate': 0.00045346796177033903, 'log_every_epochs': 1, 'max_checkpts_to_keep': None, 'model': 'MLPGraphNetwork', 'momentum': 0.8712873602503628, 'n_blocks': 1, 'n_samples': 5000, 'node_features': (32, 2), 'normalize': True, 'optimizer': 'sgd', 'output_delay': 0, 'output_steps': 4, 'sample_buffer': -4, 'seed': 65, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 120, 'timestep_duration': 3, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| Name                                   | Shape    | Size | Mean     | Std   |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 8)   | 48   | -0.103   | 0.37  |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (8, 8)   | 64   | -0.0189  | 0.344 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (32,)    | 32   | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (19, 32) | 608  | -0.00843 | 0.219 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)     | 2    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (32, 2)  | 64   | -0.00627 | 0.195 |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "Total: 834\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Checkpoint None does not exist.\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.00s.\n",
      "INFO:absl:Starting training.\n",
      "INFO:absl:[0] train_loss=0.7901866436004639, train_x1_loss=0.1585734337568283, train_x2_loss=0.6316131949424744\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[0] val_loss=1.2194744348526\n",
      "INFO:absl:[0] test_loss=1.2956864833831787\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Created artifact [10] Profile of type ArtifactType.URL and value None.\n",
      "INFO:absl:[1] train_loss=0.6629409790039062, train_x1_loss=0.10178180783987045, train_x2_loss=0.5611597299575806\n",
      "INFO:absl:[1] val_loss=1.144710898399353\n",
      "INFO:absl:[1] test_loss=1.2207245826721191\n",
      "INFO:absl:[2] train_loss=0.6372787356376648, train_x1_loss=0.09594681113958359, train_x2_loss=0.5413309931755066\n",
      "INFO:absl:[2] val_loss=1.099320888519287\n",
      "INFO:absl:[2] test_loss=1.1726365089416504\n",
      "INFO:absl:[3] train_loss=0.6204469203948975, train_x1_loss=0.091860331594944, train_x2_loss=0.5285862684249878\n",
      "INFO:absl:[3] val_loss=1.063155174255371\n",
      "INFO:absl:[3] test_loss=1.1328970193862915\n",
      "INFO:absl:[4] train_loss=0.6082788705825806, train_x1_loss=0.08763936161994934, train_x2_loss=0.5206397771835327\n",
      "INFO:absl:[4] val_loss=1.0424295663833618\n",
      "INFO:absl:[4] test_loss=1.1081267595291138\n",
      "INFO:absl:[5] train_loss=0.5977176427841187, train_x1_loss=0.08466432243585587, train_x2_loss=0.5130528211593628\n",
      "INFO:absl:[5] val_loss=1.0249488353729248\n",
      "INFO:absl:[5] test_loss=1.0901836156845093\n",
      "INFO:absl:[6] train_loss=0.5895062685012817, train_x1_loss=0.08258405327796936, train_x2_loss=0.5069221258163452\n",
      "INFO:absl:[6] val_loss=1.0127030611038208\n",
      "INFO:absl:[6] test_loss=1.0774723291397095\n",
      "INFO:absl:[7] train_loss=0.584779679775238, train_x1_loss=0.07898782193660736, train_x2_loss=0.5057917833328247\n",
      "INFO:absl:[7] val_loss=1.0085381269454956\n",
      "INFO:absl:[7] test_loss=1.0765286684036255\n",
      "INFO:absl:[8] train_loss=0.5800418257713318, train_x1_loss=0.07898791134357452, train_x2_loss=0.5010534524917603\n",
      "INFO:absl:[8] val_loss=1.0004150867462158\n",
      "INFO:absl:[8] test_loss=1.0705151557922363\n",
      "INFO:absl:[9] train_loss=0.5773285031318665, train_x1_loss=0.07824355363845825, train_x2_loss=0.4990873634815216\n",
      "INFO:absl:[9] val_loss=0.9934045076370239\n",
      "INFO:absl:[9] test_loss=1.0612202882766724\n",
      "INFO:absl:[10] train_loss=0.5721719264984131, train_x1_loss=0.07677146047353745, train_x2_loss=0.4954005181789398\n",
      "INFO:absl:[10] val_loss=0.9825491309165955\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[10] test_loss=1.0429308414459229\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[11] train_loss=0.5683778524398804, train_x1_loss=0.07630425691604614, train_x2_loss=0.4920717775821686\n",
      "INFO:absl:[11] val_loss=0.9826384782791138\n",
      "INFO:absl:[11] test_loss=1.0507975816726685\n",
      "INFO:absl:[12] train_loss=0.5678345561027527, train_x1_loss=0.07537828385829926, train_x2_loss=0.49245598912239075\n",
      "INFO:absl:[12] val_loss=0.9704165458679199\n",
      "INFO:absl:[12] test_loss=1.0338116884231567\n",
      "INFO:absl:[13] train_loss=0.5618629455566406, train_x1_loss=0.07362395524978638, train_x2_loss=0.4882383644580841\n",
      "INFO:absl:[13] val_loss=0.9661991596221924\n",
      "INFO:absl:[13] test_loss=1.0310025215148926\n",
      "INFO:absl:[14] train_loss=0.5609395503997803, train_x1_loss=0.07467693835496902, train_x2_loss=0.48626238107681274\n",
      "INFO:absl:[14] val_loss=0.9552502632141113\n",
      "INFO:absl:[14] test_loss=1.0105922222137451\n",
      "INFO:absl:[15] train_loss=0.5571185350418091, train_x1_loss=0.07251863926649094, train_x2_loss=0.4845998287200928\n",
      "INFO:absl:[15] val_loss=0.9560437798500061\n",
      "INFO:absl:[15] test_loss=1.0213922262191772\n",
      "INFO:absl:[16] train_loss=0.5511798858642578, train_x1_loss=0.07223307341337204, train_x2_loss=0.4789470136165619\n",
      "INFO:absl:[16] val_loss=0.9451069831848145\n",
      "INFO:absl:[16] test_loss=1.0057755708694458\n",
      "INFO:absl:[17] train_loss=0.5497212409973145, train_x1_loss=0.0714448019862175, train_x2_loss=0.47827666997909546\n",
      "INFO:absl:[17] val_loss=0.942369282245636\n",
      "INFO:absl:[17] test_loss=0.9969581365585327\n",
      "INFO:absl:[18] train_loss=0.5474123358726501, train_x1_loss=0.07219023257493973, train_x2_loss=0.47522178292274475\n",
      "INFO:absl:[18] val_loss=0.9336391091346741\n",
      "INFO:absl:[18] test_loss=0.9898890256881714\n",
      "INFO:absl:[19] train_loss=0.5432238578796387, train_x1_loss=0.07038319110870361, train_x2_loss=0.4728417694568634\n",
      "INFO:absl:[19] val_loss=0.9411317706108093\n",
      "INFO:absl:[19] test_loss=0.9935665726661682\n",
      "INFO:absl:[20] train_loss=0.5420078635215759, train_x1_loss=0.0727994367480278, train_x2_loss=0.4692089259624481\n",
      "INFO:absl:[20] val_loss=0.9086729288101196\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[20] test_loss=0.9589564800262451\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[21] train_loss=0.5374998450279236, train_x1_loss=0.0714976117014885, train_x2_loss=0.4660024046897888\n",
      "INFO:absl:[21] val_loss=0.9254136085510254\n",
      "INFO:absl:[21] test_loss=0.9738554358482361\n",
      "INFO:absl:[22] train_loss=0.5314609408378601, train_x1_loss=0.07194755226373672, train_x2_loss=0.45951247215270996\n",
      "INFO:absl:[22] val_loss=0.8653115034103394\n",
      "INFO:absl:[22] test_loss=0.906038761138916\n",
      "INFO:absl:[23] train_loss=0.5275201797485352, train_x1_loss=0.07247664779424667, train_x2_loss=0.4550442099571228\n",
      "INFO:absl:[23] val_loss=0.895014226436615\n",
      "INFO:absl:[23] test_loss=0.9473403692245483\n",
      "INFO:absl:[24] train_loss=0.5217710137367249, train_x1_loss=0.0721602514386177, train_x2_loss=0.44961076974868774\n",
      "INFO:absl:[24] val_loss=0.8677153587341309\n",
      "INFO:absl:[24] test_loss=0.9119654893875122\n",
      "INFO:absl:[25] train_loss=0.5200266242027283, train_x1_loss=0.07177378237247467, train_x2_loss=0.4482518136501312\n",
      "INFO:absl:[25] val_loss=0.8521203398704529\n",
      "INFO:absl:[25] test_loss=0.895224928855896\n",
      "INFO:absl:[26] train_loss=0.5145982503890991, train_x1_loss=0.07080115377902985, train_x2_loss=0.4437970519065857\n",
      "INFO:absl:[26] val_loss=0.8311108946800232\n",
      "INFO:absl:[26] test_loss=0.872778058052063\n",
      "INFO:absl:[27] train_loss=0.5129227042198181, train_x1_loss=0.07010900974273682, train_x2_loss=0.4428139626979828\n",
      "INFO:absl:[27] val_loss=0.8569409251213074\n",
      "INFO:absl:[27] test_loss=0.9088287949562073\n",
      "INFO:absl:[28] train_loss=0.5078465342521667, train_x1_loss=0.07031816989183426, train_x2_loss=0.4375275671482086\n",
      "INFO:absl:[28] val_loss=0.8453719019889832\n",
      "INFO:absl:[28] test_loss=0.8953019976615906\n",
      "INFO:absl:[29] train_loss=0.5040680766105652, train_x1_loss=0.06929145008325577, train_x2_loss=0.4347762167453766\n",
      "INFO:absl:[29] val_loss=0.799968421459198\n",
      "INFO:absl:[29] test_loss=0.8406751155853271\n",
      "INFO:absl:[30] train_loss=0.5001348257064819, train_x1_loss=0.06880614161491394, train_x2_loss=0.43132874369621277\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[30] val_loss=0.8149405121803284\n",
      "INFO:absl:[30] test_loss=0.8606597781181335\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[31] train_loss=0.49599236249923706, train_x1_loss=0.0687185600399971, train_x2_loss=0.4272732138633728\n",
      "INFO:absl:[31] val_loss=0.7948119640350342\n",
      "INFO:absl:[31] test_loss=0.8376080393791199\n",
      "INFO:absl:[32] train_loss=0.49636948108673096, train_x1_loss=0.06954086571931839, train_x2_loss=0.426828533411026\n",
      "INFO:absl:[32] val_loss=0.8217695951461792\n",
      "INFO:absl:[32] test_loss=0.8699589967727661\n",
      "INFO:absl:[33] train_loss=0.4950600266456604, train_x1_loss=0.06868329644203186, train_x2_loss=0.4263768196105957\n",
      "INFO:absl:[33] val_loss=0.802128255367279\n",
      "INFO:absl:[33] test_loss=0.8452193737030029\n",
      "INFO:absl:[34] train_loss=0.492946594953537, train_x1_loss=0.06826846301555634, train_x2_loss=0.42467811703681946\n",
      "INFO:absl:[34] val_loss=0.8226182460784912\n",
      "INFO:absl:[34] test_loss=0.8734818696975708\n",
      "INFO:absl:[35] train_loss=0.4925994575023651, train_x1_loss=0.06898412853479385, train_x2_loss=0.4236161410808563\n",
      "INFO:absl:[35] val_loss=0.7880558371543884\n",
      "INFO:absl:[35] test_loss=0.8293069005012512\n",
      "INFO:absl:[36] train_loss=0.48933812975883484, train_x1_loss=0.06851758807897568, train_x2_loss=0.42082011699676514\n",
      "INFO:absl:[36] val_loss=0.8000385761260986\n",
      "INFO:absl:[36] test_loss=0.8417364358901978\n",
      "INFO:absl:[37] train_loss=0.48906847834587097, train_x1_loss=0.06756992638111115, train_x2_loss=0.42149820923805237\n",
      "INFO:absl:[37] val_loss=0.7685283422470093\n",
      "INFO:absl:[37] test_loss=0.8100574612617493\n",
      "INFO:absl:[38] train_loss=0.48690730333328247, train_x1_loss=0.06724559515714645, train_x2_loss=0.4196622967720032\n",
      "INFO:absl:[38] val_loss=0.7996989488601685\n",
      "INFO:absl:[38] test_loss=0.8498391509056091\n",
      "INFO:absl:[39] train_loss=0.48461025953292847, train_x1_loss=0.06731084734201431, train_x2_loss=0.4172983765602112\n",
      "INFO:absl:[39] val_loss=0.773487389087677\n",
      "INFO:absl:[39] test_loss=0.8132710456848145\n",
      "INFO:absl:[40] train_loss=0.48301300406455994, train_x1_loss=0.06649324297904968, train_x2_loss=0.41652077436447144\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[40] val_loss=0.7890863418579102\n",
      "INFO:absl:[40] test_loss=0.8335069417953491\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[41] train_loss=0.4856765568256378, train_x1_loss=0.06836041808128357, train_x2_loss=0.4173160493373871\n",
      "INFO:absl:[41] val_loss=0.8024818897247314\n",
      "INFO:absl:[41] test_loss=0.8471765518188477\n",
      "INFO:absl:[42] train_loss=0.48373642563819885, train_x1_loss=0.06783957779407501, train_x2_loss=0.415897399187088\n",
      "INFO:absl:[42] val_loss=0.795718252658844\n",
      "INFO:absl:[42] test_loss=0.8380637764930725\n",
      "INFO:absl:[43] train_loss=0.4819602370262146, train_x1_loss=0.06677912175655365, train_x2_loss=0.4151812195777893\n",
      "INFO:absl:[43] val_loss=0.7754235863685608\n",
      "INFO:absl:[43] test_loss=0.8158007264137268\n",
      "INFO:absl:[44] train_loss=0.479754775762558, train_x1_loss=0.06773728132247925, train_x2_loss=0.412017822265625\n",
      "INFO:absl:[44] val_loss=0.811678946018219\n",
      "INFO:absl:[44] test_loss=0.8593194484710693\n",
      "INFO:absl:[45] train_loss=0.4800705909729004, train_x1_loss=0.0672077089548111, train_x2_loss=0.41286367177963257\n",
      "INFO:absl:[45] val_loss=0.7794284820556641\n",
      "INFO:absl:[45] test_loss=0.8229374885559082\n",
      "INFO:absl:[46] train_loss=0.4818534553050995, train_x1_loss=0.06744270771741867, train_x2_loss=0.41441085934638977\n",
      "INFO:absl:[46] val_loss=0.8051310181617737\n",
      "INFO:absl:[46] test_loss=0.846396803855896\n",
      "INFO:absl:[47] train_loss=0.47749724984169006, train_x1_loss=0.06780899316072464, train_x2_loss=0.40968871116638184\n",
      "INFO:absl:[47] val_loss=0.7851731181144714\n",
      "INFO:absl:[47] test_loss=0.8327956199645996\n",
      "INFO:absl:[48] train_loss=0.4779658317565918, train_x1_loss=0.06633567065000534, train_x2_loss=0.4116305112838745\n",
      "INFO:absl:[48] val_loss=0.7930145859718323\n",
      "INFO:absl:[48] test_loss=0.8419283628463745\n",
      "INFO:absl:[49] train_loss=0.4787333607673645, train_x1_loss=0.06646637618541718, train_x2_loss=0.4122669994831085\n",
      "INFO:absl:[49] val_loss=0.7747449278831482\n",
      "INFO:absl:[49] test_loss=0.8242934942245483\n",
      "INFO:absl:[50] train_loss=0.4790378510951996, train_x1_loss=0.06677084416151047, train_x2_loss=0.4122687578201294\n",
      "INFO:absl:[50] val_loss=0.7669336795806885\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[50] test_loss=0.8114830255508423\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[51] train_loss=0.4751814305782318, train_x1_loss=0.06569099426269531, train_x2_loss=0.4094899594783783\n",
      "INFO:absl:[51] val_loss=0.73598712682724\n",
      "INFO:absl:[51] test_loss=0.7775819897651672\n",
      "INFO:absl:[52] train_loss=0.4757069945335388, train_x1_loss=0.06602901220321655, train_x2_loss=0.40967798233032227\n",
      "INFO:absl:[52] val_loss=0.7516288161277771\n",
      "INFO:absl:[52] test_loss=0.7919691205024719\n",
      "INFO:absl:[53] train_loss=0.4748460650444031, train_x1_loss=0.065849669277668, train_x2_loss=0.4089965224266052\n",
      "INFO:absl:[53] val_loss=0.7763730883598328\n",
      "INFO:absl:[53] test_loss=0.8169928789138794\n",
      "INFO:absl:[54] train_loss=0.4740307629108429, train_x1_loss=0.066226527094841, train_x2_loss=0.40780380368232727\n",
      "INFO:absl:[54] val_loss=0.7795082926750183\n",
      "INFO:absl:[54] test_loss=0.8182224035263062\n",
      "INFO:absl:[55] train_loss=0.472546249628067, train_x1_loss=0.06683176010847092, train_x2_loss=0.4057142734527588\n",
      "INFO:absl:[55] val_loss=0.774638295173645\n",
      "INFO:absl:[55] test_loss=0.8194279670715332\n",
      "INFO:absl:Setting work unit notes: 3263.6 steps/s, 56.0% (196000/350000), ETA: 0m (1m : 0.0% checkpoint, 12.5% eval)\n",
      "INFO:absl:[196000] steps_per_sec=3263.569328\n",
      "INFO:absl:[56] train_loss=0.47181013226509094, train_x1_loss=0.06636922061443329, train_x2_loss=0.405441552400589\n",
      "INFO:absl:[56] val_loss=0.7387972474098206\n",
      "INFO:absl:[56] test_loss=0.776056170463562\n",
      "INFO:absl:[57] train_loss=0.47413304448127747, train_x1_loss=0.06710825115442276, train_x2_loss=0.40702417492866516\n",
      "INFO:absl:[57] val_loss=0.7917519807815552\n",
      "INFO:absl:[57] test_loss=0.8367727398872375\n",
      "INFO:absl:[58] train_loss=0.47297394275665283, train_x1_loss=0.06698502600193024, train_x2_loss=0.4059883952140808\n",
      "INFO:absl:[58] val_loss=0.7408725619316101\n",
      "INFO:absl:[58] test_loss=0.7796081304550171\n",
      "INFO:absl:[59] train_loss=0.4730202257633209, train_x1_loss=0.06596088409423828, train_x2_loss=0.40705981850624084\n",
      "INFO:absl:[59] val_loss=0.7509284615516663\n",
      "INFO:absl:[59] test_loss=0.7945204973220825\n",
      "INFO:absl:[60] train_loss=0.4718962013721466, train_x1_loss=0.06533732265233994, train_x2_loss=0.4065583348274231\n",
      "INFO:absl:[60] val_loss=0.720477819442749\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[60] test_loss=0.7600329518318176\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[61] train_loss=0.4700036942958832, train_x1_loss=0.06706773489713669, train_x2_loss=0.4029375910758972\n",
      "INFO:absl:[61] val_loss=0.7376583218574524\n",
      "INFO:absl:[61] test_loss=0.7781648635864258\n",
      "INFO:absl:[62] train_loss=0.46812161803245544, train_x1_loss=0.06674662977457047, train_x2_loss=0.401375949382782\n",
      "INFO:absl:[62] val_loss=0.7267100811004639\n",
      "INFO:absl:[62] test_loss=0.7645977735519409\n",
      "INFO:absl:[63] train_loss=0.4701579511165619, train_x1_loss=0.06647980958223343, train_x2_loss=0.40367820858955383\n",
      "INFO:absl:[63] val_loss=0.7409027218818665\n",
      "INFO:absl:[63] test_loss=0.7852334976196289\n",
      "INFO:absl:[64] train_loss=0.4697433114051819, train_x1_loss=0.06633235514163971, train_x2_loss=0.40341058373451233\n",
      "INFO:absl:[64] val_loss=0.7709537744522095\n",
      "INFO:absl:[64] test_loss=0.816594123840332\n",
      "INFO:absl:[65] train_loss=0.4658612608909607, train_x1_loss=0.06553950160741806, train_x2_loss=0.40032148361206055\n",
      "INFO:absl:[65] val_loss=0.7269483804702759\n",
      "INFO:absl:[65] test_loss=0.7621033191680908\n",
      "INFO:absl:[66] train_loss=0.4683687686920166, train_x1_loss=0.06620946526527405, train_x2_loss=0.40215811133384705\n",
      "INFO:absl:[66] val_loss=0.7311605215072632\n",
      "INFO:absl:[66] test_loss=0.7726365923881531\n",
      "INFO:absl:[67] train_loss=0.46760591864585876, train_x1_loss=0.06547306478023529, train_x2_loss=0.40213388204574585\n",
      "INFO:absl:[67] val_loss=0.7439254522323608\n",
      "INFO:absl:[67] test_loss=0.7820451259613037\n",
      "INFO:absl:[68] train_loss=0.46478816866874695, train_x1_loss=0.06613972038030624, train_x2_loss=0.3986482322216034\n",
      "INFO:absl:[68] val_loss=0.7257852554321289\n",
      "INFO:absl:[68] test_loss=0.766369640827179\n",
      "INFO:absl:[69] train_loss=0.4658985733985901, train_x1_loss=0.06610814481973648, train_x2_loss=0.39978986978530884\n",
      "INFO:absl:[69] val_loss=0.7553092837333679\n",
      "INFO:absl:[69] test_loss=0.7972218990325928\n",
      "INFO:absl:[70] train_loss=0.46511971950531006, train_x1_loss=0.06601747125387192, train_x2_loss=0.3991018831729889\n",
      "INFO:absl:[70] val_loss=0.7046178579330444\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[70] test_loss=0.7441868185997009\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[71] train_loss=0.46569550037384033, train_x1_loss=0.06520228087902069, train_x2_loss=0.4004936218261719\n",
      "INFO:absl:[71] val_loss=0.7581853866577148\n",
      "INFO:absl:[71] test_loss=0.8029059767723083\n",
      "INFO:absl:[72] train_loss=0.4623472988605499, train_x1_loss=0.06518152356147766, train_x2_loss=0.3971655070781708\n",
      "INFO:absl:[72] val_loss=0.7438655495643616\n",
      "INFO:absl:[72] test_loss=0.7911596298217773\n",
      "INFO:absl:[73] train_loss=0.4628562927246094, train_x1_loss=0.06462281197309494, train_x2_loss=0.3982328772544861\n",
      "INFO:absl:[73] val_loss=0.7280253767967224\n",
      "INFO:absl:[73] test_loss=0.764934241771698\n",
      "INFO:absl:[74] train_loss=0.4644680917263031, train_x1_loss=0.0656648725271225, train_x2_loss=0.39880284667015076\n",
      "INFO:absl:[74] val_loss=0.7374653220176697\n",
      "INFO:absl:[74] test_loss=0.7749910950660706\n",
      "INFO:absl:[75] train_loss=0.4622581899166107, train_x1_loss=0.06538865715265274, train_x2_loss=0.39686861634254456\n",
      "INFO:absl:[75] val_loss=0.7146550416946411\n",
      "INFO:absl:[75] test_loss=0.7531693577766418\n",
      "INFO:absl:[76] train_loss=0.461948037147522, train_x1_loss=0.06503791362047195, train_x2_loss=0.3969101905822754\n",
      "INFO:absl:[76] val_loss=0.73360675573349\n",
      "INFO:absl:[76] test_loss=0.7745081782341003\n",
      "INFO:absl:[77] train_loss=0.46055319905281067, train_x1_loss=0.06461775302886963, train_x2_loss=0.3959355056285858\n",
      "INFO:absl:[77] val_loss=0.7454181909561157\n",
      "INFO:absl:[77] test_loss=0.7906658053398132\n",
      "INFO:absl:[78] train_loss=0.4633258283138275, train_x1_loss=0.06559139490127563, train_x2_loss=0.39773473143577576\n",
      "INFO:absl:[78] val_loss=0.7436907291412354\n",
      "INFO:absl:[78] test_loss=0.7861268520355225\n",
      "INFO:absl:[79] train_loss=0.45892274379730225, train_x1_loss=0.06534154713153839, train_x2_loss=0.3935815989971161\n",
      "INFO:absl:[79] val_loss=0.7052630186080933\n",
      "INFO:absl:[79] test_loss=0.7384455800056458\n",
      "INFO:absl:[80] train_loss=0.45976561307907104, train_x1_loss=0.0661349967122078, train_x2_loss=0.3936305344104767\n",
      "INFO:absl:[80] val_loss=0.7193650603294373\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[80] test_loss=0.7590020895004272\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[81] train_loss=0.4596996605396271, train_x1_loss=0.06557296216487885, train_x2_loss=0.3941255211830139\n",
      "INFO:absl:[81] val_loss=0.7072268724441528\n",
      "INFO:absl:[81] test_loss=0.7484189867973328\n",
      "INFO:absl:[82] train_loss=0.4596680998802185, train_x1_loss=0.06510446965694427, train_x2_loss=0.39456284046173096\n",
      "INFO:absl:[82] val_loss=0.7143014073371887\n",
      "INFO:absl:[82] test_loss=0.75342857837677\n",
      "INFO:absl:[83] train_loss=0.45978450775146484, train_x1_loss=0.06506292521953583, train_x2_loss=0.3947211503982544\n",
      "INFO:absl:[83] val_loss=0.7224457263946533\n",
      "INFO:absl:[83] test_loss=0.762824296951294\n",
      "INFO:absl:[84] train_loss=0.4597419798374176, train_x1_loss=0.06660211831331253, train_x2_loss=0.39313939213752747\n",
      "INFO:absl:[84] val_loss=0.7329254150390625\n",
      "INFO:absl:[84] test_loss=0.7739925980567932\n",
      "INFO:absl:[85] train_loss=0.45629003643989563, train_x1_loss=0.0656546950340271, train_x2_loss=0.39063531160354614\n",
      "INFO:absl:[85] val_loss=0.7170215845108032\n",
      "INFO:absl:[85] test_loss=0.7565176486968994\n",
      "INFO:absl:[86] train_loss=0.45943668484687805, train_x1_loss=0.06587056815624237, train_x2_loss=0.39356642961502075\n",
      "INFO:absl:[86] val_loss=0.6994057893753052\n",
      "INFO:absl:[86] test_loss=0.7368318438529968\n",
      "INFO:absl:[87] train_loss=0.457641065120697, train_x1_loss=0.06586708128452301, train_x2_loss=0.391774445772171\n",
      "INFO:absl:[87] val_loss=0.7343006730079651\n",
      "INFO:absl:[87] test_loss=0.7743300199508667\n",
      "INFO:absl:[88] train_loss=0.45627710223197937, train_x1_loss=0.06446056067943573, train_x2_loss=0.39181622862815857\n",
      "INFO:absl:[88] val_loss=0.7262853384017944\n",
      "INFO:absl:[88] test_loss=0.7666596174240112\n",
      "INFO:absl:[89] train_loss=0.4561968743801117, train_x1_loss=0.0661572515964508, train_x2_loss=0.39003968238830566\n",
      "INFO:absl:[89] val_loss=0.6985229253768921\n",
      "INFO:absl:[89] test_loss=0.737586498260498\n",
      "INFO:absl:[90] train_loss=0.45657432079315186, train_x1_loss=0.06407918781042099, train_x2_loss=0.39249467849731445\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[90] val_loss=0.6912495493888855\n",
      "INFO:absl:[90] test_loss=0.7286301851272583\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[91] train_loss=0.45613229274749756, train_x1_loss=0.06488452851772308, train_x2_loss=0.39124807715415955\n",
      "INFO:absl:[91] val_loss=0.689553439617157\n",
      "INFO:absl:[91] test_loss=0.7237263321876526\n",
      "INFO:absl:[92] train_loss=0.4568770229816437, train_x1_loss=0.06489241123199463, train_x2_loss=0.3919847011566162\n",
      "INFO:absl:[92] val_loss=0.7034397125244141\n",
      "INFO:absl:[92] test_loss=0.7412327527999878\n",
      "INFO:absl:[93] train_loss=0.45621562004089355, train_x1_loss=0.06521853804588318, train_x2_loss=0.3909963071346283\n",
      "INFO:absl:[93] val_loss=0.73174649477005\n",
      "INFO:absl:[93] test_loss=0.7689435482025146\n",
      "INFO:absl:[94] train_loss=0.4544939696788788, train_x1_loss=0.06487613171339035, train_x2_loss=0.38961756229400635\n",
      "INFO:absl:[94] val_loss=0.7138156294822693\n",
      "INFO:absl:[94] test_loss=0.7514006495475769\n",
      "INFO:absl:[95] train_loss=0.45700132846832275, train_x1_loss=0.06577911227941513, train_x2_loss=0.3912220597267151\n",
      "INFO:absl:[95] val_loss=0.7209101319313049\n",
      "INFO:absl:[95] test_loss=0.7619703412055969\n",
      "INFO:absl:[96] train_loss=0.4547450840473175, train_x1_loss=0.06561249494552612, train_x2_loss=0.3891323208808899\n",
      "INFO:absl:[96] val_loss=0.6989386081695557\n",
      "INFO:absl:[96] test_loss=0.7340735793113708\n",
      "INFO:absl:[97] train_loss=0.45688462257385254, train_x1_loss=0.06552083045244217, train_x2_loss=0.39136388897895813\n",
      "INFO:absl:[97] val_loss=0.6929792165756226\n",
      "INFO:absl:[97] test_loss=0.7314202189445496\n",
      "INFO:absl:[98] train_loss=0.4579191505908966, train_x1_loss=0.06559369713068008, train_x2_loss=0.3923257291316986\n",
      "INFO:absl:[98] val_loss=0.6926406621932983\n",
      "INFO:absl:[98] test_loss=0.7315784692764282\n",
      "INFO:absl:[99] train_loss=0.455068439245224, train_x1_loss=0.06484802812337875, train_x2_loss=0.3902203142642975\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[99] val_loss=0.7159271836280823\n",
      "INFO:absl:[99] test_loss=0.7548367977142334\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'relu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 10, 'dropout_rate': 0.013287043114620523, 'edge_features': (8, 8), 'epochs': 100, 'eval_every_epochs': 1, 'fully_connected_edges': 5, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 1, 'layer_norm': False, 'learning_rate': 0.00045346796177033903, 'log_every_epochs': 1, 'max_checkpts_to_keep': None, 'model': 'MLPGraphNetwork', 'momentum': 0.8712873602503628, 'n_blocks': 1, 'n_samples': 5000, 'node_features': (32, 2), 'normalize': True, 'optimizer': 'sgd', 'output_delay': 0, 'output_steps': 4, 'sample_buffer': -4, 'seed': 66, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 120, 'timestep_duration': 3, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| Name                                   | Shape    | Size | Mean     | Std   |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 8)   | 48   | -0.103   | 0.37  |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (8, 8)   | 64   | -0.0189  | 0.344 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (32,)    | 32   | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (19, 32) | 608  | -0.00843 | 0.219 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)     | 2    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (32, 2)  | 64   | -0.00627 | 0.195 |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "Total: 834\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Checkpoint None does not exist.\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.00s.\n",
      "INFO:absl:Starting training.\n",
      "INFO:absl:[0] train_loss=0.8303170204162598, train_x1_loss=0.16994810104370117, train_x2_loss=0.6603677272796631\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[0] val_loss=1.247101902961731\n",
      "INFO:absl:[0] test_loss=1.2652227878570557\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Created artifact [10] Profile of type ArtifactType.URL and value None.\n",
      "INFO:absl:[1] train_loss=0.673610270023346, train_x1_loss=0.09932292997837067, train_x2_loss=0.5742874145507812\n",
      "INFO:absl:[1] val_loss=1.1529186964035034\n",
      "INFO:absl:[1] test_loss=1.1790038347244263\n",
      "INFO:absl:[2] train_loss=0.6367100477218628, train_x1_loss=0.09305921941995621, train_x2_loss=0.5436522960662842\n",
      "INFO:absl:[2] val_loss=1.1031192541122437\n",
      "INFO:absl:[2] test_loss=1.1330255270004272\n",
      "INFO:absl:[3] train_loss=0.6223058104515076, train_x1_loss=0.08840538561344147, train_x2_loss=0.5339015126228333\n",
      "INFO:absl:[3] val_loss=1.0706360340118408\n",
      "INFO:absl:[3] test_loss=1.1035583019256592\n",
      "INFO:absl:[4] train_loss=0.6087893843650818, train_x1_loss=0.08572675287723541, train_x2_loss=0.523063600063324\n",
      "INFO:absl:[4] val_loss=1.0420796871185303\n",
      "INFO:absl:[4] test_loss=1.077216386795044\n",
      "INFO:absl:[5] train_loss=0.5990243554115295, train_x1_loss=0.08314705640077591, train_x2_loss=0.5158772468566895\n",
      "INFO:absl:[5] val_loss=1.0298666954040527\n",
      "INFO:absl:[5] test_loss=1.0646294355392456\n",
      "INFO:absl:[6] train_loss=0.5905377268791199, train_x1_loss=0.0807647630572319, train_x2_loss=0.5097736716270447\n",
      "INFO:absl:[6] val_loss=1.0055842399597168\n",
      "INFO:absl:[6] test_loss=1.0436859130859375\n",
      "INFO:absl:[7] train_loss=0.582196056842804, train_x1_loss=0.07889255881309509, train_x2_loss=0.503303587436676\n",
      "INFO:absl:[7] val_loss=0.981732964515686\n",
      "INFO:absl:[7] test_loss=1.0232512950897217\n",
      "INFO:absl:[8] train_loss=0.571785569190979, train_x1_loss=0.07756561785936356, train_x2_loss=0.49422016739845276\n",
      "INFO:absl:[8] val_loss=0.9650301933288574\n",
      "INFO:absl:[8] test_loss=1.0038089752197266\n",
      "INFO:absl:[9] train_loss=0.5646369457244873, train_x1_loss=0.07641509175300598, train_x2_loss=0.4882209002971649\n",
      "INFO:absl:[9] val_loss=0.9429171085357666\n",
      "INFO:absl:[9] test_loss=0.984909176826477\n",
      "INFO:absl:[10] train_loss=0.5583677291870117, train_x1_loss=0.07512998580932617, train_x2_loss=0.4832383096218109\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[10] val_loss=0.9237473607063293\n",
      "INFO:absl:[10] test_loss=0.9661357402801514\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[11] train_loss=0.5504762530326843, train_x1_loss=0.074368417263031, train_x2_loss=0.47610723972320557\n",
      "INFO:absl:[11] val_loss=0.9112959504127502\n",
      "INFO:absl:[11] test_loss=0.952619731426239\n",
      "INFO:absl:[12] train_loss=0.5463585257530212, train_x1_loss=0.0755695104598999, train_x2_loss=0.4707892835140228\n",
      "INFO:absl:[12] val_loss=0.8830128908157349\n",
      "INFO:absl:[12] test_loss=0.9258089065551758\n",
      "INFO:absl:[13] train_loss=0.5408507585525513, train_x1_loss=0.07568012177944183, train_x2_loss=0.4651714265346527\n",
      "INFO:absl:[13] val_loss=0.8678789138793945\n",
      "INFO:absl:[13] test_loss=0.9093486070632935\n",
      "INFO:absl:[14] train_loss=0.5330527424812317, train_x1_loss=0.07541557401418686, train_x2_loss=0.45763838291168213\n",
      "INFO:absl:[14] val_loss=0.8381699919700623\n",
      "INFO:absl:[14] test_loss=0.8793089389801025\n",
      "INFO:absl:[15] train_loss=0.5316661596298218, train_x1_loss=0.07609641551971436, train_x2_loss=0.45556822419166565\n",
      "INFO:absl:[15] val_loss=0.82615065574646\n",
      "INFO:absl:[15] test_loss=0.8679202795028687\n",
      "INFO:absl:[16] train_loss=0.5268864035606384, train_x1_loss=0.0755685493350029, train_x2_loss=0.45131880044937134\n",
      "INFO:absl:[16] val_loss=0.823910653591156\n",
      "INFO:absl:[16] test_loss=0.8654628992080688\n",
      "INFO:absl:[17] train_loss=0.5222831964492798, train_x1_loss=0.07541396468877792, train_x2_loss=0.446869432926178\n",
      "INFO:absl:[17] val_loss=0.8123265504837036\n",
      "INFO:absl:[17] test_loss=0.8567047119140625\n",
      "INFO:absl:[18] train_loss=0.5193528532981873, train_x1_loss=0.07648064196109772, train_x2_loss=0.4428732991218567\n",
      "INFO:absl:[18] val_loss=0.8110648989677429\n",
      "INFO:absl:[18] test_loss=0.8505851030349731\n",
      "INFO:absl:[19] train_loss=0.519946277141571, train_x1_loss=0.07578203827142715, train_x2_loss=0.4441637396812439\n",
      "INFO:absl:[19] val_loss=0.7946741580963135\n",
      "INFO:absl:[19] test_loss=0.8387848138809204\n",
      "INFO:absl:[20] train_loss=0.5196996331214905, train_x1_loss=0.07756922394037247, train_x2_loss=0.44212958216667175\n",
      "INFO:absl:[20] val_loss=0.7985283136367798\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[20] test_loss=0.8411986827850342\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[21] train_loss=0.5145787000656128, train_x1_loss=0.07589474320411682, train_x2_loss=0.43868377804756165\n",
      "INFO:absl:[21] val_loss=0.7820215225219727\n",
      "INFO:absl:[21] test_loss=0.8271157741546631\n",
      "INFO:absl:[22] train_loss=0.5157501697540283, train_x1_loss=0.07613281160593033, train_x2_loss=0.43961766362190247\n",
      "INFO:absl:[22] val_loss=0.7847979664802551\n",
      "INFO:absl:[22] test_loss=0.8309199213981628\n",
      "INFO:absl:[23] train_loss=0.5125300884246826, train_x1_loss=0.07690351456403732, train_x2_loss=0.43562576174736023\n",
      "INFO:absl:[23] val_loss=0.7948851585388184\n",
      "INFO:absl:[23] test_loss=0.8388292789459229\n",
      "INFO:absl:[24] train_loss=0.510418176651001, train_x1_loss=0.07673794031143188, train_x2_loss=0.43368080258369446\n",
      "INFO:absl:[24] val_loss=0.763107180595398\n",
      "INFO:absl:[24] test_loss=0.8135125637054443\n",
      "INFO:absl:[25] train_loss=0.507727861404419, train_x1_loss=0.07553525269031525, train_x2_loss=0.4321918189525604\n",
      "INFO:absl:[25] val_loss=0.7694262266159058\n",
      "INFO:absl:[25] test_loss=0.817514955997467\n",
      "INFO:absl:[26] train_loss=0.50801682472229, train_x1_loss=0.0746757760643959, train_x2_loss=0.43334081768989563\n",
      "INFO:absl:[26] val_loss=0.7690321207046509\n",
      "INFO:absl:[26] test_loss=0.8120291233062744\n",
      "INFO:absl:[27] train_loss=0.5051948428153992, train_x1_loss=0.07539305835962296, train_x2_loss=0.4298015534877777\n",
      "INFO:absl:[27] val_loss=0.7637811303138733\n",
      "INFO:absl:[27] test_loss=0.8097225427627563\n",
      "INFO:absl:[28] train_loss=0.5053719282150269, train_x1_loss=0.07680968195199966, train_x2_loss=0.42856165766716003\n",
      "INFO:absl:[28] val_loss=0.7524524927139282\n",
      "INFO:absl:[28] test_loss=0.7987744212150574\n",
      "INFO:absl:[29] train_loss=0.503627359867096, train_x1_loss=0.07655662298202515, train_x2_loss=0.4270712435245514\n",
      "INFO:absl:[29] val_loss=0.7707234621047974\n",
      "INFO:absl:[29] test_loss=0.8208888173103333\n",
      "INFO:absl:[30] train_loss=0.5014172196388245, train_x1_loss=0.0754791796207428, train_x2_loss=0.4259384870529175\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[30] val_loss=0.7377619743347168\n",
      "INFO:absl:[30] test_loss=0.7876042723655701\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[31] train_loss=0.4955725371837616, train_x1_loss=0.0748954638838768, train_x2_loss=0.4206775724887848\n",
      "INFO:absl:[31] val_loss=0.7464393973350525\n",
      "INFO:absl:[31] test_loss=0.7956789135932922\n",
      "INFO:absl:[32] train_loss=0.4956587851047516, train_x1_loss=0.07545475661754608, train_x2_loss=0.4202038645744324\n",
      "INFO:absl:[32] val_loss=0.7331523299217224\n",
      "INFO:absl:[32] test_loss=0.7846241593360901\n",
      "INFO:absl:[33] train_loss=0.49727028608322144, train_x1_loss=0.07515597343444824, train_x2_loss=0.4221145510673523\n",
      "INFO:absl:[33] val_loss=0.7444789409637451\n",
      "INFO:absl:[33] test_loss=0.796108067035675\n",
      "INFO:absl:[34] train_loss=0.49770456552505493, train_x1_loss=0.07552573829889297, train_x2_loss=0.4221801459789276\n",
      "INFO:absl:[34] val_loss=0.7418871521949768\n",
      "INFO:absl:[34] test_loss=0.7905289530754089\n",
      "INFO:absl:[35] train_loss=0.495234876871109, train_x1_loss=0.07506829500198364, train_x2_loss=0.4201671779155731\n",
      "INFO:absl:[35] val_loss=0.7281036376953125\n",
      "INFO:absl:[35] test_loss=0.7804086804389954\n",
      "INFO:absl:[36] train_loss=0.4926745593547821, train_x1_loss=0.0747000202536583, train_x2_loss=0.4179743528366089\n",
      "INFO:absl:[36] val_loss=0.7371120452880859\n",
      "INFO:absl:[36] test_loss=0.7893886566162109\n",
      "INFO:absl:[37] train_loss=0.4946029782295227, train_x1_loss=0.0747709795832634, train_x2_loss=0.4198322892189026\n",
      "INFO:absl:[37] val_loss=0.7510892748832703\n",
      "INFO:absl:[37] test_loss=0.8005320429801941\n",
      "INFO:absl:[38] train_loss=0.4939022362232208, train_x1_loss=0.07447174936532974, train_x2_loss=0.41942963004112244\n",
      "INFO:absl:[38] val_loss=0.7368839979171753\n",
      "INFO:absl:[38] test_loss=0.784010648727417\n",
      "INFO:absl:[39] train_loss=0.49218693375587463, train_x1_loss=0.07372906804084778, train_x2_loss=0.41845810413360596\n",
      "INFO:absl:[39] val_loss=0.7542445063591003\n",
      "INFO:absl:[39] test_loss=0.8042404651641846\n",
      "INFO:absl:[40] train_loss=0.4882722496986389, train_x1_loss=0.07334817200899124, train_x2_loss=0.41492441296577454\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[40] val_loss=0.7267783284187317\n",
      "INFO:absl:[40] test_loss=0.7791339755058289\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[41] train_loss=0.49209314584732056, train_x1_loss=0.07414720952510834, train_x2_loss=0.4179452359676361\n",
      "INFO:absl:[41] val_loss=0.7414771318435669\n",
      "INFO:absl:[41] test_loss=0.7893239259719849\n",
      "INFO:absl:[42] train_loss=0.4887535870075226, train_x1_loss=0.07328545302152634, train_x2_loss=0.4154675304889679\n",
      "INFO:absl:[42] val_loss=0.7173518538475037\n",
      "INFO:absl:[42] test_loss=0.7691746950149536\n",
      "INFO:absl:[43] train_loss=0.48689040541648865, train_x1_loss=0.07356453686952591, train_x2_loss=0.41332584619522095\n",
      "INFO:absl:[43] val_loss=0.7350123524665833\n",
      "INFO:absl:[43] test_loss=0.7856064438819885\n",
      "INFO:absl:[44] train_loss=0.49219202995300293, train_x1_loss=0.07381662726402283, train_x2_loss=0.4183747470378876\n",
      "INFO:absl:[44] val_loss=0.7262088060379028\n",
      "INFO:absl:[44] test_loss=0.7746819853782654\n",
      "INFO:absl:[45] train_loss=0.48630642890930176, train_x1_loss=0.07351068407297134, train_x2_loss=0.4127958416938782\n",
      "INFO:absl:[45] val_loss=0.7287032008171082\n",
      "INFO:absl:[45] test_loss=0.7756337523460388\n",
      "INFO:absl:[46] train_loss=0.48728957772254944, train_x1_loss=0.0728600025177002, train_x2_loss=0.4144301414489746\n",
      "INFO:absl:[46] val_loss=0.7339140772819519\n",
      "INFO:absl:[46] test_loss=0.7827561497688293\n",
      "INFO:absl:[47] train_loss=0.4857132136821747, train_x1_loss=0.07395833730697632, train_x2_loss=0.4117549955844879\n",
      "INFO:absl:[47] val_loss=0.7153905630111694\n",
      "INFO:absl:[47] test_loss=0.7670186161994934\n",
      "INFO:absl:[48] train_loss=0.48322558403015137, train_x1_loss=0.07147657871246338, train_x2_loss=0.4117486774921417\n",
      "INFO:absl:[48] val_loss=0.722635805606842\n",
      "INFO:absl:[48] test_loss=0.7734436988830566\n",
      "INFO:absl:[49] train_loss=0.4815449118614197, train_x1_loss=0.0724179819226265, train_x2_loss=0.40912625193595886\n",
      "INFO:absl:[49] val_loss=0.7272785902023315\n",
      "INFO:absl:[49] test_loss=0.7763822674751282\n",
      "INFO:absl:[50] train_loss=0.4831356108188629, train_x1_loss=0.07248207926750183, train_x2_loss=0.41065356135368347\n",
      "INFO:absl:[50] val_loss=0.7198446989059448\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[50] test_loss=0.7699535489082336\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[51] train_loss=0.4860905706882477, train_x1_loss=0.07331997156143188, train_x2_loss=0.4127715229988098\n",
      "INFO:absl:[51] val_loss=0.7176386117935181\n",
      "INFO:absl:[51] test_loss=0.7651559710502625\n",
      "INFO:absl:[52] train_loss=0.4828335642814636, train_x1_loss=0.07262622565031052, train_x2_loss=0.41020771861076355\n",
      "INFO:absl:[52] val_loss=0.6965745687484741\n",
      "INFO:absl:[52] test_loss=0.7472536563873291\n",
      "INFO:absl:[53] train_loss=0.48303455114364624, train_x1_loss=0.07226421684026718, train_x2_loss=0.4107704162597656\n",
      "INFO:absl:[53] val_loss=0.7242337465286255\n",
      "INFO:absl:[53] test_loss=0.7686838507652283\n",
      "INFO:absl:[54] train_loss=0.4814976751804352, train_x1_loss=0.07272514700889587, train_x2_loss=0.40877217054367065\n",
      "INFO:absl:[54] val_loss=0.7326748371124268\n",
      "INFO:absl:[54] test_loss=0.7780740261077881\n",
      "INFO:absl:Setting work unit notes: 3261.9 steps/s, 55.9% (195715/350000), ETA: 0m (1m : 0.1% checkpoint, 12.3% eval)\n",
      "INFO:absl:[195715] steps_per_sec=3261.908838\n",
      "INFO:absl:[55] train_loss=0.48024794459342957, train_x1_loss=0.07246695458889008, train_x2_loss=0.40777936577796936\n",
      "INFO:absl:[55] val_loss=0.7205158472061157\n",
      "INFO:absl:[55] test_loss=0.766594409942627\n",
      "INFO:absl:[56] train_loss=0.47965365648269653, train_x1_loss=0.07211652398109436, train_x2_loss=0.40753868222236633\n",
      "INFO:absl:[56] val_loss=0.7215332984924316\n",
      "INFO:absl:[56] test_loss=0.770314633846283\n",
      "INFO:absl:[57] train_loss=0.47777658700942993, train_x1_loss=0.07194597274065018, train_x2_loss=0.405830979347229\n",
      "INFO:absl:[57] val_loss=0.717561662197113\n",
      "INFO:absl:[57] test_loss=0.7634769678115845\n",
      "INFO:absl:[58] train_loss=0.4795439541339874, train_x1_loss=0.07119931280612946, train_x2_loss=0.4083446264266968\n",
      "INFO:absl:[58] val_loss=0.7009568214416504\n",
      "INFO:absl:[58] test_loss=0.749180018901825\n",
      "INFO:absl:[59] train_loss=0.4797622561454773, train_x1_loss=0.07159435003995895, train_x2_loss=0.40816730260849\n",
      "INFO:absl:[59] val_loss=0.7339856028556824\n",
      "INFO:absl:[59] test_loss=0.7794193029403687\n",
      "INFO:absl:[60] train_loss=0.4764809012413025, train_x1_loss=0.07003017514944077, train_x2_loss=0.4064503312110901\n",
      "INFO:absl:[60] val_loss=0.691990077495575\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[60] test_loss=0.7424573302268982\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[61] train_loss=0.47696003317832947, train_x1_loss=0.0709945410490036, train_x2_loss=0.40596556663513184\n",
      "INFO:absl:[61] val_loss=0.7189085483551025\n",
      "INFO:absl:[61] test_loss=0.7654697299003601\n",
      "INFO:absl:[62] train_loss=0.4762217402458191, train_x1_loss=0.07160403579473495, train_x2_loss=0.4046184718608856\n",
      "INFO:absl:[62] val_loss=0.715911328792572\n",
      "INFO:absl:[62] test_loss=0.7630903720855713\n",
      "INFO:absl:[63] train_loss=0.4775044918060303, train_x1_loss=0.07033604383468628, train_x2_loss=0.40716856718063354\n",
      "INFO:absl:[63] val_loss=0.7113497257232666\n",
      "INFO:absl:[63] test_loss=0.7591232061386108\n",
      "INFO:absl:[64] train_loss=0.47692784667015076, train_x1_loss=0.07210856676101685, train_x2_loss=0.40481916069984436\n",
      "INFO:absl:[64] val_loss=0.6974180340766907\n",
      "INFO:absl:[64] test_loss=0.749275803565979\n",
      "INFO:absl:[65] train_loss=0.4779758155345917, train_x1_loss=0.07104267925024033, train_x2_loss=0.4069333076477051\n",
      "INFO:absl:[65] val_loss=0.7169139385223389\n",
      "INFO:absl:[65] test_loss=0.7619028687477112\n",
      "INFO:absl:[66] train_loss=0.47568023204803467, train_x1_loss=0.07062815874814987, train_x2_loss=0.40505197644233704\n",
      "INFO:absl:[66] val_loss=0.717897355556488\n",
      "INFO:absl:[66] test_loss=0.76437908411026\n",
      "INFO:absl:[67] train_loss=0.47472506761550903, train_x1_loss=0.07079070061445236, train_x2_loss=0.4039345681667328\n",
      "INFO:absl:[67] val_loss=0.7254607677459717\n",
      "INFO:absl:[67] test_loss=0.7722747921943665\n",
      "INFO:absl:[68] train_loss=0.4710063934326172, train_x1_loss=0.06976133584976196, train_x2_loss=0.40124568343162537\n",
      "INFO:absl:[68] val_loss=0.6916563510894775\n",
      "INFO:absl:[68] test_loss=0.7398304343223572\n",
      "INFO:absl:[69] train_loss=0.4717020094394684, train_x1_loss=0.07112159579992294, train_x2_loss=0.40057945251464844\n",
      "INFO:absl:[69] val_loss=0.6885545253753662\n",
      "INFO:absl:[69] test_loss=0.7372106313705444\n",
      "INFO:absl:[70] train_loss=0.4730616509914398, train_x1_loss=0.07099170237779617, train_x2_loss=0.4020695686340332\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[70] val_loss=0.7103185057640076\n",
      "INFO:absl:[70] test_loss=0.7563096284866333\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[71] train_loss=0.4719821512699127, train_x1_loss=0.06985672563314438, train_x2_loss=0.40212517976760864\n",
      "INFO:absl:[71] val_loss=0.6773869395256042\n",
      "INFO:absl:[71] test_loss=0.7266564965248108\n",
      "INFO:absl:[72] train_loss=0.4707197844982147, train_x1_loss=0.06879889219999313, train_x2_loss=0.40192118287086487\n",
      "INFO:absl:[72] val_loss=0.6864627003669739\n",
      "INFO:absl:[72] test_loss=0.7315921187400818\n",
      "INFO:absl:[73] train_loss=0.47266387939453125, train_x1_loss=0.06973651796579361, train_x2_loss=0.4029266834259033\n",
      "INFO:absl:[73] val_loss=0.6918746829032898\n",
      "INFO:absl:[73] test_loss=0.7428479194641113\n",
      "INFO:absl:[74] train_loss=0.4709674119949341, train_x1_loss=0.06943771988153458, train_x2_loss=0.4015295207500458\n",
      "INFO:absl:[74] val_loss=0.6894689202308655\n",
      "INFO:absl:[74] test_loss=0.7366130352020264\n",
      "INFO:absl:[75] train_loss=0.4712691903114319, train_x1_loss=0.06936963647603989, train_x2_loss=0.4018993675708771\n",
      "INFO:absl:[75] val_loss=0.7251814603805542\n",
      "INFO:absl:[75] test_loss=0.7667917609214783\n",
      "INFO:absl:[76] train_loss=0.46950170397758484, train_x1_loss=0.06991027295589447, train_x2_loss=0.39959144592285156\n",
      "INFO:absl:[76] val_loss=0.6943214535713196\n",
      "INFO:absl:[76] test_loss=0.73869389295578\n",
      "INFO:absl:[77] train_loss=0.47293877601623535, train_x1_loss=0.07011990994215012, train_x2_loss=0.4028189480304718\n",
      "INFO:absl:[77] val_loss=0.6999344229698181\n",
      "INFO:absl:[77] test_loss=0.7456503510475159\n",
      "INFO:absl:[78] train_loss=0.47090187668800354, train_x1_loss=0.06981851905584335, train_x2_loss=0.4010835289955139\n",
      "INFO:absl:[78] val_loss=0.6979078054428101\n",
      "INFO:absl:[78] test_loss=0.7409144043922424\n",
      "INFO:absl:[79] train_loss=0.4675924479961395, train_x1_loss=0.06779338419437408, train_x2_loss=0.39979806542396545\n",
      "INFO:absl:[79] val_loss=0.7187610864639282\n",
      "INFO:absl:[79] test_loss=0.7602788805961609\n",
      "INFO:absl:[80] train_loss=0.46919068694114685, train_x1_loss=0.07042361795902252, train_x2_loss=0.3987674415111542\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[80] val_loss=0.6936192512512207\n",
      "INFO:absl:[80] test_loss=0.7365926504135132\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[81] train_loss=0.47167202830314636, train_x1_loss=0.06990252435207367, train_x2_loss=0.4017688035964966\n",
      "INFO:absl:[81] val_loss=0.6781433820724487\n",
      "INFO:absl:[81] test_loss=0.7248945236206055\n",
      "INFO:absl:[82] train_loss=0.46955540776252747, train_x1_loss=0.06905709952116013, train_x2_loss=0.4004978835582733\n",
      "INFO:absl:[82] val_loss=0.7077550292015076\n",
      "INFO:absl:[82] test_loss=0.7514616250991821\n",
      "INFO:absl:[83] train_loss=0.46607348322868347, train_x1_loss=0.0694928765296936, train_x2_loss=0.3965812623500824\n",
      "INFO:absl:[83] val_loss=0.6706990599632263\n",
      "INFO:absl:[83] test_loss=0.7164672017097473\n",
      "INFO:absl:[84] train_loss=0.46980828046798706, train_x1_loss=0.06977605074644089, train_x2_loss=0.40003305673599243\n",
      "INFO:absl:[84] val_loss=0.696245014667511\n",
      "INFO:absl:[84] test_loss=0.7417306900024414\n",
      "INFO:absl:[85] train_loss=0.4674942195415497, train_x1_loss=0.06987302750349045, train_x2_loss=0.39762163162231445\n",
      "INFO:absl:[85] val_loss=0.6898174285888672\n",
      "INFO:absl:[85] test_loss=0.7352240681648254\n",
      "INFO:absl:[86] train_loss=0.46780407428741455, train_x1_loss=0.06897003948688507, train_x2_loss=0.39883387088775635\n",
      "INFO:absl:[86] val_loss=0.6826740503311157\n",
      "INFO:absl:[86] test_loss=0.728818416595459\n",
      "INFO:absl:[87] train_loss=0.46472397446632385, train_x1_loss=0.0679454505443573, train_x2_loss=0.39677831530570984\n",
      "INFO:absl:[87] val_loss=0.69651198387146\n",
      "INFO:absl:[87] test_loss=0.7420551776885986\n",
      "INFO:absl:[88] train_loss=0.4656229317188263, train_x1_loss=0.06886284798383713, train_x2_loss=0.3967595100402832\n",
      "INFO:absl:[88] val_loss=0.687636137008667\n",
      "INFO:absl:[88] test_loss=0.7335299849510193\n",
      "INFO:absl:[89] train_loss=0.46603572368621826, train_x1_loss=0.06858450174331665, train_x2_loss=0.39745011925697327\n",
      "INFO:absl:[89] val_loss=0.6924172043800354\n",
      "INFO:absl:[89] test_loss=0.7359638810157776\n",
      "INFO:absl:[90] train_loss=0.46392345428466797, train_x1_loss=0.06759676337242126, train_x2_loss=0.39632686972618103\n",
      "INFO:absl:[90] val_loss=0.670307993888855\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[90] test_loss=0.7171809673309326\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[91] train_loss=0.4642687141895294, train_x1_loss=0.06741674244403839, train_x2_loss=0.3968517780303955\n",
      "INFO:absl:[91] val_loss=0.6888883709907532\n",
      "INFO:absl:[91] test_loss=0.732162594795227\n",
      "INFO:absl:[92] train_loss=0.46112143993377686, train_x1_loss=0.06667836010456085, train_x2_loss=0.3944425880908966\n",
      "INFO:absl:[92] val_loss=0.6910422444343567\n",
      "INFO:absl:[92] test_loss=0.7333319783210754\n",
      "INFO:absl:[93] train_loss=0.46535617113113403, train_x1_loss=0.06861671060323715, train_x2_loss=0.39673924446105957\n",
      "INFO:absl:[93] val_loss=0.6770845651626587\n",
      "INFO:absl:[93] test_loss=0.7266693711280823\n",
      "INFO:absl:[94] train_loss=0.46352723240852356, train_x1_loss=0.06818646192550659, train_x2_loss=0.3953406512737274\n",
      "INFO:absl:[94] val_loss=0.6898493766784668\n",
      "INFO:absl:[94] test_loss=0.7327947020530701\n",
      "INFO:absl:[95] train_loss=0.46220239996910095, train_x1_loss=0.06803763657808304, train_x2_loss=0.3941654860973358\n",
      "INFO:absl:[95] val_loss=0.6738773584365845\n",
      "INFO:absl:[95] test_loss=0.7185584902763367\n",
      "INFO:absl:[96] train_loss=0.46314212679862976, train_x1_loss=0.06762120127677917, train_x2_loss=0.39552050828933716\n",
      "INFO:absl:[96] val_loss=0.682340681552887\n",
      "INFO:absl:[96] test_loss=0.7250388264656067\n",
      "INFO:absl:[97] train_loss=0.4633968770503998, train_x1_loss=0.06720361858606339, train_x2_loss=0.39619338512420654\n",
      "INFO:absl:[97] val_loss=0.6835466623306274\n",
      "INFO:absl:[97] test_loss=0.7283706068992615\n",
      "INFO:absl:[98] train_loss=0.4643207788467407, train_x1_loss=0.0682910606265068, train_x2_loss=0.39602869749069214\n",
      "INFO:absl:[98] val_loss=0.6733106374740601\n",
      "INFO:absl:[98] test_loss=0.719063937664032\n",
      "INFO:absl:[99] train_loss=0.4611092209815979, train_x1_loss=0.06810973584651947, train_x2_loss=0.39299920201301575\n",
      "INFO:absl:[99] val_loss=0.6783508062362671\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[99] test_loss=0.7225168347358704\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'relu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 10, 'dropout_rate': 0.013287043114620523, 'edge_features': (8, 8), 'epochs': 100, 'eval_every_epochs': 1, 'fully_connected_edges': 5, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 1, 'layer_norm': False, 'learning_rate': 0.00045346796177033903, 'log_every_epochs': 1, 'max_checkpts_to_keep': None, 'model': 'MLPGraphNetwork', 'momentum': 0.8712873602503628, 'n_blocks': 1, 'n_samples': 5000, 'node_features': (32, 2), 'normalize': True, 'optimizer': 'sgd', 'output_delay': 0, 'output_steps': 4, 'sample_buffer': -4, 'seed': 67, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 120, 'timestep_duration': 3, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| Name                                   | Shape    | Size | Mean     | Std   |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 8)   | 48   | -0.103   | 0.37  |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (8, 8)   | 64   | -0.0189  | 0.344 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (32,)    | 32   | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (19, 32) | 608  | -0.00843 | 0.219 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)     | 2    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (32, 2)  | 64   | -0.00627 | 0.195 |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "Total: 834\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Checkpoint None does not exist.\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.01s.\n",
      "INFO:absl:Starting training.\n",
      "INFO:absl:[0] train_loss=0.8218179941177368, train_x1_loss=0.16027292609214783, train_x2_loss=0.6615450382232666\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[0] val_loss=1.1945942640304565\n",
      "INFO:absl:[0] test_loss=1.282991886138916\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Created artifact [10] Profile of type ArtifactType.URL and value None.\n",
      "INFO:absl:[1] train_loss=0.6867971420288086, train_x1_loss=0.10029982030391693, train_x2_loss=0.5864962935447693\n",
      "INFO:absl:[1] val_loss=1.1370394229888916\n",
      "INFO:absl:[1] test_loss=1.2209843397140503\n",
      "INFO:absl:[2] train_loss=0.6579275727272034, train_x1_loss=0.09374460577964783, train_x2_loss=0.5641829371452332\n",
      "INFO:absl:[2] val_loss=1.084105134010315\n",
      "INFO:absl:[2] test_loss=1.1625899076461792\n",
      "INFO:absl:[3] train_loss=0.6383960843086243, train_x1_loss=0.09034765511751175, train_x2_loss=0.5480482578277588\n",
      "INFO:absl:[3] val_loss=1.0474853515625\n",
      "INFO:absl:[3] test_loss=1.1230634450912476\n",
      "INFO:absl:[4] train_loss=0.6267334818840027, train_x1_loss=0.08813510835170746, train_x2_loss=0.5385988354682922\n",
      "INFO:absl:[4] val_loss=1.0353304147720337\n",
      "INFO:absl:[4] test_loss=1.1073681116104126\n",
      "INFO:absl:[5] train_loss=0.6173664927482605, train_x1_loss=0.08417192846536636, train_x2_loss=0.5331941246986389\n",
      "INFO:absl:[5] val_loss=1.0176149606704712\n",
      "INFO:absl:[5] test_loss=1.0870347023010254\n",
      "INFO:absl:[6] train_loss=0.6118341088294983, train_x1_loss=0.0827639102935791, train_x2_loss=0.5290696024894714\n",
      "INFO:absl:[6] val_loss=1.0181000232696533\n",
      "INFO:absl:[6] test_loss=1.0870699882507324\n",
      "INFO:absl:[7] train_loss=0.6080641150474548, train_x1_loss=0.08184189349412918, train_x2_loss=0.5262222290039062\n",
      "INFO:absl:[7] val_loss=1.0102438926696777\n",
      "INFO:absl:[7] test_loss=1.0809564590454102\n",
      "INFO:absl:[8] train_loss=0.6001569032669067, train_x1_loss=0.07961901277303696, train_x2_loss=0.5205397605895996\n",
      "INFO:absl:[8] val_loss=1.0093339681625366\n",
      "INFO:absl:[8] test_loss=1.0784063339233398\n",
      "INFO:absl:[9] train_loss=0.5965596437454224, train_x1_loss=0.07860571146011353, train_x2_loss=0.5179533958435059\n",
      "INFO:absl:[9] val_loss=0.9899937510490417\n",
      "INFO:absl:[9] test_loss=1.058864951133728\n",
      "INFO:absl:[10] train_loss=0.5905969142913818, train_x1_loss=0.07623346894979477, train_x2_loss=0.5143635869026184\n",
      "INFO:absl:[10] val_loss=0.9738677740097046\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[10] test_loss=1.0450862646102905\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[11] train_loss=0.585340678691864, train_x1_loss=0.07473922520875931, train_x2_loss=0.5106017589569092\n",
      "INFO:absl:[11] val_loss=0.9622473120689392\n",
      "INFO:absl:[11] test_loss=1.0360556840896606\n",
      "INFO:absl:[12] train_loss=0.5818778872489929, train_x1_loss=0.07509150356054306, train_x2_loss=0.5067867040634155\n",
      "INFO:absl:[12] val_loss=0.9559056758880615\n",
      "INFO:absl:[12] test_loss=1.027857780456543\n",
      "INFO:absl:[13] train_loss=0.5764986276626587, train_x1_loss=0.07436647266149521, train_x2_loss=0.5021321773529053\n",
      "INFO:absl:[13] val_loss=0.9423210620880127\n",
      "INFO:absl:[13] test_loss=1.0166215896606445\n",
      "INFO:absl:[14] train_loss=0.5708751678466797, train_x1_loss=0.07381546497344971, train_x2_loss=0.49705958366394043\n",
      "INFO:absl:[14] val_loss=0.917881190776825\n",
      "INFO:absl:[14] test_loss=0.9920861721038818\n",
      "INFO:absl:[15] train_loss=0.563460648059845, train_x1_loss=0.07413656264543533, train_x2_loss=0.48932430148124695\n",
      "INFO:absl:[15] val_loss=0.8974969983100891\n",
      "INFO:absl:[15] test_loss=0.9704611897468567\n",
      "INFO:absl:[16] train_loss=0.555738627910614, train_x1_loss=0.07297701388597488, train_x2_loss=0.4827633798122406\n",
      "INFO:absl:[16] val_loss=0.9044439196586609\n",
      "INFO:absl:[16] test_loss=0.9767043590545654\n",
      "INFO:absl:[17] train_loss=0.551983654499054, train_x1_loss=0.07317570596933365, train_x2_loss=0.47880783677101135\n",
      "INFO:absl:[17] val_loss=0.8759137988090515\n",
      "INFO:absl:[17] test_loss=0.9474865198135376\n",
      "INFO:absl:[18] train_loss=0.5463651418685913, train_x1_loss=0.07232823967933655, train_x2_loss=0.4740367531776428\n",
      "INFO:absl:[18] val_loss=0.8425000905990601\n",
      "INFO:absl:[18] test_loss=0.9115108847618103\n",
      "INFO:absl:[19] train_loss=0.5417866110801697, train_x1_loss=0.07341833412647247, train_x2_loss=0.4683688282966614\n",
      "INFO:absl:[19] val_loss=0.8369110822677612\n",
      "INFO:absl:[19] test_loss=0.9055061936378479\n",
      "INFO:absl:[20] train_loss=0.5417301654815674, train_x1_loss=0.07367142289876938, train_x2_loss=0.468058317899704\n",
      "INFO:absl:[20] val_loss=0.840562105178833\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[20] test_loss=0.9111599922180176\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[21] train_loss=0.5348089933395386, train_x1_loss=0.07214748114347458, train_x2_loss=0.4626609683036804\n",
      "INFO:absl:[21] val_loss=0.8183258771896362\n",
      "INFO:absl:[21] test_loss=0.8886238932609558\n",
      "INFO:absl:[22] train_loss=0.532885730266571, train_x1_loss=0.07323794066905975, train_x2_loss=0.4596487581729889\n",
      "INFO:absl:[22] val_loss=0.8223896026611328\n",
      "INFO:absl:[22] test_loss=0.8908365964889526\n",
      "INFO:absl:[23] train_loss=0.5320069193840027, train_x1_loss=0.07393281906843185, train_x2_loss=0.4580726623535156\n",
      "INFO:absl:[23] val_loss=0.8115332126617432\n",
      "INFO:absl:[23] test_loss=0.8834068775177002\n",
      "INFO:absl:[24] train_loss=0.5249554514884949, train_x1_loss=0.07364511489868164, train_x2_loss=0.4513101875782013\n",
      "INFO:absl:[24] val_loss=0.8018186092376709\n",
      "INFO:absl:[24] test_loss=0.8675557971000671\n",
      "INFO:absl:[25] train_loss=0.527489185333252, train_x1_loss=0.0743662565946579, train_x2_loss=0.45312371850013733\n",
      "INFO:absl:[25] val_loss=0.8079811334609985\n",
      "INFO:absl:[25] test_loss=0.8779923915863037\n",
      "INFO:absl:[26] train_loss=0.5264906287193298, train_x1_loss=0.07492130249738693, train_x2_loss=0.4515698552131653\n",
      "INFO:absl:[26] val_loss=0.7990165948867798\n",
      "INFO:absl:[26] test_loss=0.8672992587089539\n",
      "INFO:absl:[27] train_loss=0.5236661434173584, train_x1_loss=0.07428392767906189, train_x2_loss=0.4493822157382965\n",
      "INFO:absl:[27] val_loss=0.794154167175293\n",
      "INFO:absl:[27] test_loss=0.8641819357872009\n",
      "INFO:absl:[28] train_loss=0.5235427021980286, train_x1_loss=0.0739816427230835, train_x2_loss=0.44956132769584656\n",
      "INFO:absl:[28] val_loss=0.769872784614563\n",
      "INFO:absl:[28] test_loss=0.8388092517852783\n",
      "INFO:absl:Setting work unit notes: 911.8 steps/s, 29.5% (103223/350000), ETA: 4m (1m : 0.0% checkpoint, 4.0% eval)\n",
      "INFO:absl:[103223] steps_per_sec=911.819215\n",
      "INFO:absl:[29] train_loss=0.5204281806945801, train_x1_loss=0.07339656352996826, train_x2_loss=0.44703301787376404\n",
      "INFO:absl:[29] val_loss=0.781177282333374\n",
      "INFO:absl:[29] test_loss=0.8487071394920349\n",
      "INFO:absl:[30] train_loss=0.5241413116455078, train_x1_loss=0.07340974360704422, train_x2_loss=0.45073166489601135\n",
      "INFO:absl:[30] val_loss=0.7794721126556396\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[30] test_loss=0.8467786312103271\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[31] train_loss=0.5166348814964294, train_x1_loss=0.07320066541433334, train_x2_loss=0.44343578815460205\n",
      "INFO:absl:[31] val_loss=0.7983815670013428\n",
      "INFO:absl:[31] test_loss=0.8657270669937134\n",
      "INFO:absl:[32] train_loss=0.5200968980789185, train_x1_loss=0.07382292300462723, train_x2_loss=0.4462735056877136\n",
      "INFO:absl:[32] val_loss=0.7663228511810303\n",
      "INFO:absl:[32] test_loss=0.8324391841888428\n",
      "INFO:absl:[33] train_loss=0.5194317102432251, train_x1_loss=0.0744180977344513, train_x2_loss=0.44501447677612305\n",
      "INFO:absl:[33] val_loss=0.7692714333534241\n",
      "INFO:absl:[33] test_loss=0.8368731141090393\n",
      "INFO:absl:[34] train_loss=0.5168080925941467, train_x1_loss=0.07373902201652527, train_x2_loss=0.44306954741477966\n",
      "INFO:absl:[34] val_loss=0.766786515712738\n",
      "INFO:absl:[34] test_loss=0.834994375705719\n",
      "INFO:absl:[35] train_loss=0.5139773488044739, train_x1_loss=0.07425534725189209, train_x2_loss=0.43972158432006836\n",
      "INFO:absl:[35] val_loss=0.7801299691200256\n",
      "INFO:absl:[35] test_loss=0.8474037647247314\n",
      "INFO:absl:[36] train_loss=0.5100047588348389, train_x1_loss=0.07409396022558212, train_x2_loss=0.4359111189842224\n",
      "INFO:absl:[36] val_loss=0.7682288289070129\n",
      "INFO:absl:[36] test_loss=0.8364208936691284\n",
      "INFO:absl:[37] train_loss=0.5159468054771423, train_x1_loss=0.07422567158937454, train_x2_loss=0.4417208731174469\n",
      "INFO:absl:[37] val_loss=0.7440634965896606\n",
      "INFO:absl:[37] test_loss=0.8094082474708557\n",
      "INFO:absl:[38] train_loss=0.5090866684913635, train_x1_loss=0.07224739342927933, train_x2_loss=0.4368392527103424\n",
      "INFO:absl:[38] val_loss=0.7843251824378967\n",
      "INFO:absl:[38] test_loss=0.8507775068283081\n",
      "INFO:absl:[39] train_loss=0.5110748410224915, train_x1_loss=0.07343962788581848, train_x2_loss=0.4376351833343506\n",
      "INFO:absl:[39] val_loss=0.7699565291404724\n",
      "INFO:absl:[39] test_loss=0.8351309299468994\n",
      "INFO:absl:[40] train_loss=0.5072669982910156, train_x1_loss=0.07277998328208923, train_x2_loss=0.4344866871833801\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[40] val_loss=0.743448793888092\n",
      "INFO:absl:[40] test_loss=0.8082712292671204\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[41] train_loss=0.5080639719963074, train_x1_loss=0.07409937679767609, train_x2_loss=0.4339659810066223\n",
      "INFO:absl:[41] val_loss=0.755433201789856\n",
      "INFO:absl:[41] test_loss=0.8191978931427002\n",
      "INFO:absl:[42] train_loss=0.5086882710456848, train_x1_loss=0.07325150817632675, train_x2_loss=0.43543732166290283\n",
      "INFO:absl:[42] val_loss=0.755763590335846\n",
      "INFO:absl:[42] test_loss=0.8244401812553406\n",
      "INFO:absl:[43] train_loss=0.5103296637535095, train_x1_loss=0.07549908757209778, train_x2_loss=0.4348312318325043\n",
      "INFO:absl:[43] val_loss=0.745184063911438\n",
      "INFO:absl:[43] test_loss=0.8095335364341736\n",
      "INFO:absl:[44] train_loss=0.510191798210144, train_x1_loss=0.074445441365242, train_x2_loss=0.4357455372810364\n",
      "INFO:absl:[44] val_loss=0.7699544429779053\n",
      "INFO:absl:[44] test_loss=0.8302329182624817\n",
      "INFO:absl:[45] train_loss=0.5049092769622803, train_x1_loss=0.07217605412006378, train_x2_loss=0.4327327609062195\n",
      "INFO:absl:[45] val_loss=0.7418233752250671\n",
      "INFO:absl:[45] test_loss=0.8074503540992737\n",
      "INFO:absl:[46] train_loss=0.5025253295898438, train_x1_loss=0.07208427786827087, train_x2_loss=0.430440753698349\n",
      "INFO:absl:[46] val_loss=0.7258487343788147\n",
      "INFO:absl:[46] test_loss=0.7914724349975586\n",
      "INFO:absl:[47] train_loss=0.5037862658500671, train_x1_loss=0.07315005362033844, train_x2_loss=0.4306360185146332\n",
      "INFO:absl:[47] val_loss=0.7563655972480774\n",
      "INFO:absl:[47] test_loss=0.8222693204879761\n",
      "INFO:absl:[48] train_loss=0.5022507309913635, train_x1_loss=0.07277361303567886, train_x2_loss=0.42947709560394287\n",
      "INFO:absl:[48] val_loss=0.7705618143081665\n",
      "INFO:absl:[48] test_loss=0.8357958197593689\n",
      "INFO:absl:[49] train_loss=0.5055117607116699, train_x1_loss=0.0740194097161293, train_x2_loss=0.4314914643764496\n",
      "INFO:absl:[49] val_loss=0.753261923789978\n",
      "INFO:absl:[49] test_loss=0.8173950910568237\n",
      "INFO:absl:[50] train_loss=0.5007052421569824, train_x1_loss=0.07171270996332169, train_x2_loss=0.4289914071559906\n",
      "INFO:absl:[50] val_loss=0.7270715236663818\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[50] test_loss=0.7878721952438354\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[51] train_loss=0.5029550790786743, train_x1_loss=0.07304123789072037, train_x2_loss=0.4299134314060211\n",
      "INFO:absl:[51] val_loss=0.764456570148468\n",
      "INFO:absl:[51] test_loss=0.8271065354347229\n",
      "INFO:absl:[52] train_loss=0.5026177167892456, train_x1_loss=0.07310372591018677, train_x2_loss=0.42951345443725586\n",
      "INFO:absl:[52] val_loss=0.7410920262336731\n",
      "INFO:absl:[52] test_loss=0.8039581775665283\n",
      "INFO:absl:[53] train_loss=0.4989944398403168, train_x1_loss=0.07141920179128647, train_x2_loss=0.4275745749473572\n",
      "INFO:absl:[53] val_loss=0.7311559915542603\n",
      "INFO:absl:[53] test_loss=0.7953616976737976\n",
      "INFO:absl:[54] train_loss=0.49882376194000244, train_x1_loss=0.07259942591190338, train_x2_loss=0.42622387409210205\n",
      "INFO:absl:[54] val_loss=0.7379661202430725\n",
      "INFO:absl:[54] test_loss=0.7992081642150879\n",
      "INFO:absl:[55] train_loss=0.49746671319007874, train_x1_loss=0.07212848961353302, train_x2_loss=0.4253374934196472\n",
      "INFO:absl:[55] val_loss=0.745607316493988\n",
      "INFO:absl:[55] test_loss=0.807930052280426\n",
      "INFO:absl:[56] train_loss=0.4950445294380188, train_x1_loss=0.07202794402837753, train_x2_loss=0.4230163097381592\n",
      "INFO:absl:[56] val_loss=0.7510275840759277\n",
      "INFO:absl:[56] test_loss=0.812692403793335\n",
      "INFO:absl:[57] train_loss=0.4947691559791565, train_x1_loss=0.07081787288188934, train_x2_loss=0.42395129799842834\n",
      "INFO:absl:[57] val_loss=0.7322699427604675\n",
      "INFO:absl:[57] test_loss=0.7935248613357544\n",
      "INFO:absl:[58] train_loss=0.4981271028518677, train_x1_loss=0.07189927995204926, train_x2_loss=0.4262271821498871\n",
      "INFO:absl:[58] val_loss=0.7291306257247925\n",
      "INFO:absl:[58] test_loss=0.7900665402412415\n",
      "INFO:absl:[59] train_loss=0.4955269396305084, train_x1_loss=0.07199625670909882, train_x2_loss=0.4235302805900574\n",
      "INFO:absl:[59] val_loss=0.7129461765289307\n",
      "INFO:absl:[59] test_loss=0.7741479873657227\n",
      "INFO:absl:[60] train_loss=0.4928832948207855, train_x1_loss=0.07040740549564362, train_x2_loss=0.42247557640075684\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[60] val_loss=0.7261751294136047\n",
      "INFO:absl:[60] test_loss=0.7923552393913269\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[61] train_loss=0.491619735956192, train_x1_loss=0.07039374858140945, train_x2_loss=0.4212268888950348\n",
      "INFO:absl:[61] val_loss=0.7277624607086182\n",
      "INFO:absl:[61] test_loss=0.7874652147293091\n",
      "INFO:absl:[62] train_loss=0.49519577622413635, train_x1_loss=0.072798952460289, train_x2_loss=0.4223964512348175\n",
      "INFO:absl:[62] val_loss=0.7374069690704346\n",
      "INFO:absl:[62] test_loss=0.7965185642242432\n",
      "INFO:absl:[63] train_loss=0.49041712284088135, train_x1_loss=0.07024741172790527, train_x2_loss=0.42016932368278503\n",
      "INFO:absl:[63] val_loss=0.7049991488456726\n",
      "INFO:absl:[63] test_loss=0.7615725994110107\n",
      "INFO:absl:[64] train_loss=0.4927084147930145, train_x1_loss=0.07242763042449951, train_x2_loss=0.42027997970581055\n",
      "INFO:absl:[64] val_loss=0.7133971452713013\n",
      "INFO:absl:[64] test_loss=0.7712282538414001\n",
      "INFO:absl:[65] train_loss=0.495137482881546, train_x1_loss=0.07185982167720795, train_x2_loss=0.4232783317565918\n",
      "INFO:absl:[65] val_loss=0.7103198170661926\n",
      "INFO:absl:[65] test_loss=0.771269679069519\n",
      "INFO:absl:[66] train_loss=0.4948806166648865, train_x1_loss=0.07296831160783768, train_x2_loss=0.4219123125076294\n",
      "INFO:absl:[66] val_loss=0.7295981645584106\n",
      "INFO:absl:[66] test_loss=0.7925188541412354\n",
      "INFO:absl:[67] train_loss=0.4928736686706543, train_x1_loss=0.07124437391757965, train_x2_loss=0.421628475189209\n",
      "INFO:absl:[67] val_loss=0.7158786058425903\n",
      "INFO:absl:[67] test_loss=0.7755559682846069\n",
      "INFO:absl:[68] train_loss=0.4912218749523163, train_x1_loss=0.07108438014984131, train_x2_loss=0.4201371371746063\n",
      "INFO:absl:[68] val_loss=0.730319082736969\n",
      "INFO:absl:[68] test_loss=0.7894753217697144\n",
      "INFO:absl:[69] train_loss=0.49234479665756226, train_x1_loss=0.07201165705919266, train_x2_loss=0.42033401131629944\n",
      "INFO:absl:[69] val_loss=0.7162981033325195\n",
      "INFO:absl:[69] test_loss=0.7778281569480896\n",
      "INFO:absl:[70] train_loss=0.49115413427352905, train_x1_loss=0.07158450782299042, train_x2_loss=0.4195701479911804\n",
      "INFO:absl:[70] val_loss=0.7227571606636047\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[70] test_loss=0.7796652317047119\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[71] train_loss=0.4943651556968689, train_x1_loss=0.07156279683113098, train_x2_loss=0.42280226945877075\n",
      "INFO:absl:[71] val_loss=0.7193061113357544\n",
      "INFO:absl:[71] test_loss=0.7794923186302185\n",
      "INFO:absl:[72] train_loss=0.49050354957580566, train_x1_loss=0.07082554697990417, train_x2_loss=0.4196782112121582\n",
      "INFO:absl:[72] val_loss=0.7433083653450012\n",
      "INFO:absl:[72] test_loss=0.8075323700904846\n",
      "INFO:absl:[73] train_loss=0.4872009754180908, train_x1_loss=0.07018134742975235, train_x2_loss=0.41702085733413696\n",
      "INFO:absl:[73] val_loss=0.7178366780281067\n",
      "INFO:absl:[73] test_loss=0.7774315476417542\n",
      "INFO:absl:[74] train_loss=0.490474671125412, train_x1_loss=0.07027894258499146, train_x2_loss=0.42019546031951904\n",
      "INFO:absl:[74] val_loss=0.7145217061042786\n",
      "INFO:absl:[74] test_loss=0.7741097211837769\n",
      "INFO:absl:[75] train_loss=0.48866739869117737, train_x1_loss=0.07161134481430054, train_x2_loss=0.417055606842041\n",
      "INFO:absl:[75] val_loss=0.7352988719940186\n",
      "INFO:absl:[75] test_loss=0.7915248274803162\n",
      "INFO:absl:[76] train_loss=0.4881308376789093, train_x1_loss=0.07003674656152725, train_x2_loss=0.4180937707424164\n",
      "INFO:absl:[76] val_loss=0.6908913850784302\n",
      "INFO:absl:[76] test_loss=0.7465466260910034\n",
      "INFO:absl:Setting work unit notes: 240.3 steps/s, 77.9% (272730/350000), ETA: 5m (13m : 0.0% checkpoint, 1.3% eval)\n",
      "INFO:absl:[272730] steps_per_sec=240.339914\n",
      "INFO:absl:[77] train_loss=0.4897797405719757, train_x1_loss=0.07024206221103668, train_x2_loss=0.4195372760295868\n",
      "INFO:absl:[77] val_loss=0.7103230357170105\n",
      "INFO:absl:[77] test_loss=0.7679618000984192\n",
      "INFO:absl:[78] train_loss=0.4888608455657959, train_x1_loss=0.07142943888902664, train_x2_loss=0.417431503534317\n",
      "INFO:absl:[78] val_loss=0.6996291875839233\n",
      "INFO:absl:[78] test_loss=0.7585107684135437\n",
      "INFO:absl:[79] train_loss=0.48429128527641296, train_x1_loss=0.07060431689023972, train_x2_loss=0.41368722915649414\n",
      "INFO:absl:[79] val_loss=0.7280994653701782\n",
      "INFO:absl:[79] test_loss=0.791529655456543\n",
      "INFO:absl:[80] train_loss=0.4875607192516327, train_x1_loss=0.07089384645223618, train_x2_loss=0.41666698455810547\n",
      "INFO:absl:[80] val_loss=0.7195451259613037\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[80] test_loss=0.7775456309318542\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[81] train_loss=0.4855852723121643, train_x1_loss=0.0706033781170845, train_x2_loss=0.41498178243637085\n",
      "INFO:absl:[81] val_loss=0.7032248973846436\n",
      "INFO:absl:[81] test_loss=0.7598861455917358\n",
      "INFO:absl:[82] train_loss=0.485316663980484, train_x1_loss=0.07006318122148514, train_x2_loss=0.4152536392211914\n",
      "INFO:absl:[82] val_loss=0.6982921957969666\n",
      "INFO:absl:[82] test_loss=0.7563278079032898\n",
      "INFO:absl:[83] train_loss=0.4845890700817108, train_x1_loss=0.0692482441663742, train_x2_loss=0.41534101963043213\n",
      "INFO:absl:[83] val_loss=0.7179299592971802\n",
      "INFO:absl:[83] test_loss=0.7747879028320312\n",
      "INFO:absl:[84] train_loss=0.48542216420173645, train_x1_loss=0.07045035064220428, train_x2_loss=0.41497185826301575\n",
      "INFO:absl:[84] val_loss=0.7231659293174744\n",
      "INFO:absl:[84] test_loss=0.7830451130867004\n",
      "INFO:absl:[85] train_loss=0.48674917221069336, train_x1_loss=0.07074166089296341, train_x2_loss=0.4160071611404419\n",
      "INFO:absl:[85] val_loss=0.7220696806907654\n",
      "INFO:absl:[85] test_loss=0.7805590629577637\n",
      "INFO:absl:[86] train_loss=0.4824397563934326, train_x1_loss=0.06961783766746521, train_x2_loss=0.41282275319099426\n",
      "INFO:absl:[86] val_loss=0.7342610955238342\n",
      "INFO:absl:[86] test_loss=0.7890352606773376\n",
      "INFO:absl:[87] train_loss=0.4857192039489746, train_x1_loss=0.07050830125808716, train_x2_loss=0.4152100384235382\n",
      "INFO:absl:[87] val_loss=0.6907767057418823\n",
      "INFO:absl:[87] test_loss=0.7496201992034912\n",
      "INFO:absl:[88] train_loss=0.4847760498523712, train_x1_loss=0.06915182620286942, train_x2_loss=0.41562479734420776\n",
      "INFO:absl:[88] val_loss=0.7117103338241577\n",
      "INFO:absl:[88] test_loss=0.7703369855880737\n",
      "INFO:absl:[89] train_loss=0.4834052622318268, train_x1_loss=0.07124698162078857, train_x2_loss=0.4121573269367218\n",
      "INFO:absl:[89] val_loss=0.6976573467254639\n",
      "INFO:absl:[89] test_loss=0.7525659203529358\n",
      "INFO:absl:[90] train_loss=0.48234719038009644, train_x1_loss=0.07033265382051468, train_x2_loss=0.4120142161846161\n",
      "INFO:absl:[90] val_loss=0.7078664302825928\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[90] test_loss=0.7642408609390259\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[91] train_loss=0.4873470366001129, train_x1_loss=0.0715053379535675, train_x2_loss=0.41584235429763794\n",
      "INFO:absl:[91] val_loss=0.7172636985778809\n",
      "INFO:absl:[91] test_loss=0.7708742618560791\n",
      "INFO:absl:[92] train_loss=0.48321083188056946, train_x1_loss=0.07063881307840347, train_x2_loss=0.4125714600086212\n",
      "INFO:absl:[92] val_loss=0.7054975628852844\n",
      "INFO:absl:[92] test_loss=0.7608386874198914\n",
      "INFO:absl:[93] train_loss=0.4823506772518158, train_x1_loss=0.0707801803946495, train_x2_loss=0.41157054901123047\n",
      "INFO:absl:[93] val_loss=0.722722589969635\n",
      "INFO:absl:[93] test_loss=0.7827395796775818\n",
      "INFO:absl:[94] train_loss=0.48010456562042236, train_x1_loss=0.06942301988601685, train_x2_loss=0.4106806516647339\n",
      "INFO:absl:[94] val_loss=0.7069773077964783\n",
      "INFO:absl:[94] test_loss=0.7610641121864319\n",
      "INFO:absl:[95] train_loss=0.4875395894050598, train_x1_loss=0.0715416669845581, train_x2_loss=0.4159972071647644\n",
      "INFO:absl:[95] val_loss=0.7064481377601624\n",
      "INFO:absl:[95] test_loss=0.7598512172698975\n",
      "INFO:absl:[96] train_loss=0.4826568365097046, train_x1_loss=0.06920728832483292, train_x2_loss=0.4134499132633209\n",
      "INFO:absl:[96] val_loss=0.6944431662559509\n",
      "INFO:absl:[96] test_loss=0.7490707039833069\n",
      "INFO:absl:[97] train_loss=0.4805509150028229, train_x1_loss=0.06924372911453247, train_x2_loss=0.411307156085968\n",
      "INFO:absl:[97] val_loss=0.6998741030693054\n",
      "INFO:absl:[97] test_loss=0.75518798828125\n",
      "INFO:absl:[98] train_loss=0.48060426115989685, train_x1_loss=0.06968463212251663, train_x2_loss=0.4109203815460205\n",
      "INFO:absl:[98] val_loss=0.6891687512397766\n",
      "INFO:absl:[98] test_loss=0.7434436082839966\n",
      "INFO:absl:[99] train_loss=0.48299354314804077, train_x1_loss=0.07009240984916687, train_x2_loss=0.4129013121128082\n",
      "INFO:absl:[99] val_loss=0.6934632658958435\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[99] test_loss=0.749264657497406\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'relu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 10, 'dropout_rate': 0.013287043114620523, 'edge_features': (8, 8), 'epochs': 100, 'eval_every_epochs': 1, 'fully_connected_edges': 5, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 1, 'layer_norm': False, 'learning_rate': 0.00045346796177033903, 'log_every_epochs': 1, 'max_checkpts_to_keep': None, 'model': 'MLPGraphNetwork', 'momentum': 0.8712873602503628, 'n_blocks': 1, 'n_samples': 5000, 'node_features': (32, 2), 'normalize': True, 'optimizer': 'sgd', 'output_delay': 0, 'output_steps': 4, 'sample_buffer': -4, 'seed': 68, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 120, 'timestep_duration': 3, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| Name                                   | Shape    | Size | Mean     | Std   |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 8)   | 48   | -0.103   | 0.37  |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (8, 8)   | 64   | -0.0189  | 0.344 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (32,)    | 32   | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (19, 32) | 608  | -0.00843 | 0.219 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)     | 2    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (32, 2)  | 64   | -0.00627 | 0.195 |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "Total: 834\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Checkpoint None does not exist.\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.00s.\n",
      "INFO:absl:Starting training.\n",
      "INFO:absl:[0] train_loss=0.8075045943260193, train_x1_loss=0.1658221334218979, train_x2_loss=0.6416821479797363\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[0] val_loss=1.1913552284240723\n",
      "INFO:absl:[0] test_loss=1.2239106893539429\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Created artifact [10] Profile of type ArtifactType.URL and value None.\n",
      "INFO:absl:[1] train_loss=0.6727510690689087, train_x1_loss=0.10383129864931107, train_x2_loss=0.5689200162887573\n",
      "INFO:absl:[1] val_loss=1.1235010623931885\n",
      "INFO:absl:[1] test_loss=1.1599148511886597\n",
      "INFO:absl:[2] train_loss=0.639736533164978, train_x1_loss=0.09443478286266327, train_x2_loss=0.545301079750061\n",
      "INFO:absl:[2] val_loss=1.0846929550170898\n",
      "INFO:absl:[2] test_loss=1.1187634468078613\n",
      "INFO:absl:[3] train_loss=0.6224325895309448, train_x1_loss=0.08810322731733322, train_x2_loss=0.5343295335769653\n",
      "INFO:absl:[3] val_loss=1.0599957704544067\n",
      "INFO:absl:[3] test_loss=1.0931183099746704\n",
      "INFO:absl:[4] train_loss=0.6085404753684998, train_x1_loss=0.08436159789562225, train_x2_loss=0.5241773724555969\n",
      "INFO:absl:[4] val_loss=1.0295546054840088\n",
      "INFO:absl:[4] test_loss=1.0618726015090942\n",
      "INFO:absl:[5] train_loss=0.5993611216545105, train_x1_loss=0.08169309794902802, train_x2_loss=0.5176675915718079\n",
      "INFO:absl:[5] val_loss=1.0234891176223755\n",
      "INFO:absl:[5] test_loss=1.0534100532531738\n",
      "INFO:absl:[6] train_loss=0.591883659362793, train_x1_loss=0.07940614223480225, train_x2_loss=0.5124781131744385\n",
      "INFO:absl:[6] val_loss=1.0150192975997925\n",
      "INFO:absl:[6] test_loss=1.0489585399627686\n",
      "INFO:absl:[7] train_loss=0.5887175798416138, train_x1_loss=0.07770758867263794, train_x2_loss=0.5110087990760803\n",
      "INFO:absl:[7] val_loss=1.0048738718032837\n",
      "INFO:absl:[7] test_loss=1.0397975444793701\n",
      "INFO:absl:[8] train_loss=0.5831795334815979, train_x1_loss=0.07641497999429703, train_x2_loss=0.5067653656005859\n",
      "INFO:absl:[8] val_loss=0.9897802472114563\n",
      "INFO:absl:[8] test_loss=1.020198106765747\n",
      "INFO:absl:[9] train_loss=0.5780863165855408, train_x1_loss=0.07544145733118057, train_x2_loss=0.502644956111908\n",
      "INFO:absl:[9] val_loss=0.9804920554161072\n",
      "INFO:absl:[9] test_loss=1.009161353111267\n",
      "INFO:absl:[10] train_loss=0.5755563378334045, train_x1_loss=0.07389531284570694, train_x2_loss=0.5016606450080872\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[10] val_loss=0.9655826687812805\n",
      "INFO:absl:[10] test_loss=0.9920580387115479\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[11] train_loss=0.5684041380882263, train_x1_loss=0.07279522716999054, train_x2_loss=0.49560895562171936\n",
      "INFO:absl:[11] val_loss=0.9558361172676086\n",
      "INFO:absl:[11] test_loss=0.9819598197937012\n",
      "INFO:absl:[12] train_loss=0.5656378269195557, train_x1_loss=0.07251882553100586, train_x2_loss=0.49311938881874084\n",
      "INFO:absl:[12] val_loss=0.9474484920501709\n",
      "INFO:absl:[12] test_loss=0.9700461626052856\n",
      "INFO:absl:[13] train_loss=0.5616884231567383, train_x1_loss=0.07114414125680923, train_x2_loss=0.49054431915283203\n",
      "INFO:absl:[13] val_loss=0.9373763203620911\n",
      "INFO:absl:[13] test_loss=0.956613302230835\n",
      "INFO:absl:[14] train_loss=0.5591050982475281, train_x1_loss=0.0713357999920845, train_x2_loss=0.48776912689208984\n",
      "INFO:absl:[14] val_loss=0.9290775656700134\n",
      "INFO:absl:[14] test_loss=0.9503112435340881\n",
      "INFO:absl:[15] train_loss=0.557367205619812, train_x1_loss=0.07033824175596237, train_x2_loss=0.48702943325042725\n",
      "INFO:absl:[15] val_loss=0.9267098903656006\n",
      "INFO:absl:[15] test_loss=0.9435855150222778\n",
      "INFO:absl:[16] train_loss=0.5518388748168945, train_x1_loss=0.06845102459192276, train_x2_loss=0.48338839411735535\n",
      "INFO:absl:[16] val_loss=0.917355477809906\n",
      "INFO:absl:[16] test_loss=0.9329949021339417\n",
      "INFO:absl:[17] train_loss=0.5482069253921509, train_x1_loss=0.06893850117921829, train_x2_loss=0.4792692959308624\n",
      "INFO:absl:[17] val_loss=0.9073774814605713\n",
      "INFO:absl:[17] test_loss=0.9233118295669556\n",
      "INFO:absl:[18] train_loss=0.5431873202323914, train_x1_loss=0.06962509453296661, train_x2_loss=0.4735625684261322\n",
      "INFO:absl:[18] val_loss=0.8936566114425659\n",
      "INFO:absl:[18] test_loss=0.9086137413978577\n",
      "INFO:absl:[19] train_loss=0.5389412045478821, train_x1_loss=0.06907303631305695, train_x2_loss=0.46986815333366394\n",
      "INFO:absl:[19] val_loss=0.8832711577415466\n",
      "INFO:absl:[19] test_loss=0.9006156325340271\n",
      "INFO:absl:[20] train_loss=0.5339705944061279, train_x1_loss=0.0689556673169136, train_x2_loss=0.46501418948173523\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[20] val_loss=0.8540598154067993\n",
      "INFO:absl:[20] test_loss=0.8691648244857788\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[21] train_loss=0.5331363677978516, train_x1_loss=0.06933124363422394, train_x2_loss=0.46380478143692017\n",
      "INFO:absl:[21] val_loss=0.8530283570289612\n",
      "INFO:absl:[21] test_loss=0.8656089305877686\n",
      "INFO:absl:[22] train_loss=0.5239790081977844, train_x1_loss=0.06803727149963379, train_x2_loss=0.4559417963027954\n",
      "INFO:absl:[22] val_loss=0.8479079008102417\n",
      "INFO:absl:[22] test_loss=0.8615802526473999\n",
      "INFO:absl:[23] train_loss=0.5193874835968018, train_x1_loss=0.06803882867097855, train_x2_loss=0.4513484835624695\n",
      "INFO:absl:[23] val_loss=0.8285443186759949\n",
      "INFO:absl:[23] test_loss=0.8432178497314453\n",
      "INFO:absl:[24] train_loss=0.5173920392990112, train_x1_loss=0.06913895159959793, train_x2_loss=0.4482518434524536\n",
      "INFO:absl:[24] val_loss=0.8045094609260559\n",
      "INFO:absl:[24] test_loss=0.8148603439331055\n",
      "INFO:absl:[25] train_loss=0.5124908685684204, train_x1_loss=0.06777694821357727, train_x2_loss=0.4447139501571655\n",
      "INFO:absl:[25] val_loss=0.7971066236495972\n",
      "INFO:absl:[25] test_loss=0.8100927472114563\n",
      "INFO:absl:[26] train_loss=0.5143407583236694, train_x1_loss=0.06890107691287994, train_x2_loss=0.44544026255607605\n",
      "INFO:absl:[26] val_loss=0.7907384634017944\n",
      "INFO:absl:[26] test_loss=0.8006964921951294\n",
      "INFO:absl:[27] train_loss=0.506771981716156, train_x1_loss=0.0675673708319664, train_x2_loss=0.43920350074768066\n",
      "INFO:absl:[27] val_loss=0.7894842624664307\n",
      "INFO:absl:[27] test_loss=0.7992616295814514\n",
      "INFO:absl:[28] train_loss=0.5050877928733826, train_x1_loss=0.06707099080085754, train_x2_loss=0.4380170404911041\n",
      "INFO:absl:[28] val_loss=0.7952606678009033\n",
      "INFO:absl:[28] test_loss=0.8085019588470459\n",
      "INFO:absl:[29] train_loss=0.5054080486297607, train_x1_loss=0.06834661215543747, train_x2_loss=0.4370613098144531\n",
      "INFO:absl:[29] val_loss=0.7815523147583008\n",
      "INFO:absl:[29] test_loss=0.7929768562316895\n",
      "INFO:absl:[30] train_loss=0.5002527832984924, train_x1_loss=0.06664157658815384, train_x2_loss=0.43361032009124756\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[30] val_loss=0.7785887718200684\n",
      "INFO:absl:[30] test_loss=0.7915135025978088\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[31] train_loss=0.499673992395401, train_x1_loss=0.0668894425034523, train_x2_loss=0.4327845573425293\n",
      "INFO:absl:[31] val_loss=0.7581958174705505\n",
      "INFO:absl:[31] test_loss=0.770112931728363\n",
      "INFO:absl:[32] train_loss=0.49685344099998474, train_x1_loss=0.06728848814964294, train_x2_loss=0.4295644760131836\n",
      "INFO:absl:[32] val_loss=0.759661853313446\n",
      "INFO:absl:[32] test_loss=0.7720897793769836\n",
      "INFO:absl:[33] train_loss=0.4979422688484192, train_x1_loss=0.06590498983860016, train_x2_loss=0.43203747272491455\n",
      "INFO:absl:[33] val_loss=0.7703927755355835\n",
      "INFO:absl:[33] test_loss=0.7857511043548584\n",
      "INFO:absl:[34] train_loss=0.4948599934577942, train_x1_loss=0.06704708188772202, train_x2_loss=0.42781275510787964\n",
      "INFO:absl:[34] val_loss=0.7465433478355408\n",
      "INFO:absl:[34] test_loss=0.7597551941871643\n",
      "INFO:absl:[35] train_loss=0.4909720718860626, train_x1_loss=0.06599517166614532, train_x2_loss=0.4249768853187561\n",
      "INFO:absl:[35] val_loss=0.7473440766334534\n",
      "INFO:absl:[35] test_loss=0.7600308656692505\n",
      "INFO:absl:[36] train_loss=0.4906041920185089, train_x1_loss=0.06633025407791138, train_x2_loss=0.4242735803127289\n",
      "INFO:absl:[36] val_loss=0.7403736114501953\n",
      "INFO:absl:[36] test_loss=0.7488260865211487\n",
      "INFO:absl:[37] train_loss=0.49134787917137146, train_x1_loss=0.06654731184244156, train_x2_loss=0.42480042576789856\n",
      "INFO:absl:[37] val_loss=0.7270858883857727\n",
      "INFO:absl:[37] test_loss=0.7387526035308838\n",
      "INFO:absl:[38] train_loss=0.4888109266757965, train_x1_loss=0.06672771275043488, train_x2_loss=0.42208266258239746\n",
      "INFO:absl:[38] val_loss=0.733885645866394\n",
      "INFO:absl:[38] test_loss=0.7452083826065063\n",
      "INFO:absl:[39] train_loss=0.4882519543170929, train_x1_loss=0.06677885353565216, train_x2_loss=0.42147260904312134\n",
      "INFO:absl:[39] val_loss=0.7333554625511169\n",
      "INFO:absl:[39] test_loss=0.7462269067764282\n",
      "INFO:absl:[40] train_loss=0.48629748821258545, train_x1_loss=0.06567376106977463, train_x2_loss=0.42062345147132874\n",
      "INFO:absl:[40] val_loss=0.7389482259750366\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[40] test_loss=0.7483884692192078\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[41] train_loss=0.48643365502357483, train_x1_loss=0.06662400811910629, train_x2_loss=0.4198101758956909\n",
      "INFO:absl:[41] val_loss=0.7371790409088135\n",
      "INFO:absl:[41] test_loss=0.747567355632782\n",
      "INFO:absl:[42] train_loss=0.4857339560985565, train_x1_loss=0.06492988765239716, train_x2_loss=0.4208050072193146\n",
      "INFO:absl:[42] val_loss=0.7312912940979004\n",
      "INFO:absl:[42] test_loss=0.7406175136566162\n",
      "INFO:absl:[43] train_loss=0.4841747581958771, train_x1_loss=0.06436486542224884, train_x2_loss=0.41980981826782227\n",
      "INFO:absl:[43] val_loss=0.7245479822158813\n",
      "INFO:absl:[43] test_loss=0.735826313495636\n",
      "INFO:absl:[44] train_loss=0.4840072989463806, train_x1_loss=0.06561598926782608, train_x2_loss=0.4183913469314575\n",
      "INFO:absl:[44] val_loss=0.7298539280891418\n",
      "INFO:absl:[44] test_loss=0.7362627983093262\n",
      "INFO:absl:[45] train_loss=0.4833296239376068, train_x1_loss=0.06494192779064178, train_x2_loss=0.41838741302490234\n",
      "INFO:absl:[45] val_loss=0.7131599187850952\n",
      "INFO:absl:[45] test_loss=0.7257843613624573\n",
      "INFO:absl:[46] train_loss=0.48165571689605713, train_x1_loss=0.06557954847812653, train_x2_loss=0.4160754382610321\n",
      "INFO:absl:[46] val_loss=0.7358521223068237\n",
      "INFO:absl:[46] test_loss=0.7450089454650879\n",
      "INFO:absl:[47] train_loss=0.47950679063796997, train_x1_loss=0.06479580700397491, train_x2_loss=0.4147120416164398\n",
      "INFO:absl:[47] val_loss=0.7178815603256226\n",
      "INFO:absl:[47] test_loss=0.7269337773323059\n",
      "INFO:absl:[48] train_loss=0.48125070333480835, train_x1_loss=0.0646001473069191, train_x2_loss=0.41665011644363403\n",
      "INFO:absl:[48] val_loss=0.7174187898635864\n",
      "INFO:absl:[48] test_loss=0.7257243990898132\n",
      "INFO:absl:[49] train_loss=0.47969865798950195, train_x1_loss=0.06510337442159653, train_x2_loss=0.4145955741405487\n",
      "INFO:absl:[49] val_loss=0.7140980362892151\n",
      "INFO:absl:[49] test_loss=0.7226445078849792\n",
      "INFO:absl:[50] train_loss=0.4788658618927002, train_x1_loss=0.06448551267385483, train_x2_loss=0.4143814146518707\n",
      "INFO:absl:[50] val_loss=0.7055106163024902\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[50] test_loss=0.7140728235244751\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[51] train_loss=0.48187056183815, train_x1_loss=0.06657296419143677, train_x2_loss=0.41529732942581177\n",
      "INFO:absl:[51] val_loss=0.714129626750946\n",
      "INFO:absl:[51] test_loss=0.7197667956352234\n",
      "INFO:absl:[52] train_loss=0.47827470302581787, train_x1_loss=0.06448088586330414, train_x2_loss=0.41379329562187195\n",
      "INFO:absl:[52] val_loss=0.724279522895813\n",
      "INFO:absl:[52] test_loss=0.7285448312759399\n",
      "INFO:absl:[53] train_loss=0.4755205512046814, train_x1_loss=0.06563103199005127, train_x2_loss=0.40988999605178833\n",
      "INFO:absl:[53] val_loss=0.7146066427230835\n",
      "INFO:absl:[53] test_loss=0.720162570476532\n",
      "INFO:absl:[54] train_loss=0.47651153802871704, train_x1_loss=0.06616488099098206, train_x2_loss=0.4103471338748932\n",
      "INFO:absl:[54] val_loss=0.7107647061347961\n",
      "INFO:absl:[54] test_loss=0.7149858474731445\n",
      "INFO:absl:Setting work unit notes: 3230.9 steps/s, 55.4% (193853/350000), ETA: 0m (1m : 0.0% checkpoint, 12.5% eval)\n",
      "INFO:absl:[193853] steps_per_sec=3230.879726\n",
      "INFO:absl:[55] train_loss=0.4756251275539398, train_x1_loss=0.06665211915969849, train_x2_loss=0.40897268056869507\n",
      "INFO:absl:[55] val_loss=0.7156380414962769\n",
      "INFO:absl:[55] test_loss=0.7195639610290527\n",
      "INFO:absl:[56] train_loss=0.47438108921051025, train_x1_loss=0.06606273353099823, train_x2_loss=0.40831807255744934\n",
      "INFO:absl:[56] val_loss=0.7048365473747253\n",
      "INFO:absl:[56] test_loss=0.7104406356811523\n",
      "INFO:absl:[57] train_loss=0.4751560091972351, train_x1_loss=0.06484538316726685, train_x2_loss=0.41030994057655334\n",
      "INFO:absl:[57] val_loss=0.7057734727859497\n",
      "INFO:absl:[57] test_loss=0.7071890830993652\n",
      "INFO:absl:[58] train_loss=0.4737415313720703, train_x1_loss=0.06548506021499634, train_x2_loss=0.4082571566104889\n",
      "INFO:absl:[58] val_loss=0.7001212239265442\n",
      "INFO:absl:[58] test_loss=0.7070690393447876\n",
      "INFO:absl:[59] train_loss=0.46980908513069153, train_x1_loss=0.06542756408452988, train_x2_loss=0.40438202023506165\n",
      "INFO:absl:[59] val_loss=0.7040007710456848\n",
      "INFO:absl:[59] test_loss=0.7087751030921936\n",
      "INFO:absl:[60] train_loss=0.47209763526916504, train_x1_loss=0.06591831147670746, train_x2_loss=0.40617936849594116\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[60] val_loss=0.7007449269294739\n",
      "INFO:absl:[60] test_loss=0.704316258430481\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[61] train_loss=0.47149306535720825, train_x1_loss=0.06526118516921997, train_x2_loss=0.4062316119670868\n",
      "INFO:absl:[61] val_loss=0.6999387145042419\n",
      "INFO:absl:[61] test_loss=0.7054087519645691\n",
      "INFO:absl:[62] train_loss=0.4718116223812103, train_x1_loss=0.06591913849115372, train_x2_loss=0.40589162707328796\n",
      "INFO:absl:[62] val_loss=0.6879568099975586\n",
      "INFO:absl:[62] test_loss=0.6905075907707214\n",
      "INFO:absl:[63] train_loss=0.4702049493789673, train_x1_loss=0.06639928370714188, train_x2_loss=0.4038051664829254\n",
      "INFO:absl:[63] val_loss=0.6871994137763977\n",
      "INFO:absl:[63] test_loss=0.6914864182472229\n",
      "INFO:absl:[64] train_loss=0.46898341178894043, train_x1_loss=0.06615229696035385, train_x2_loss=0.4028308689594269\n",
      "INFO:absl:[64] val_loss=0.7023821473121643\n",
      "INFO:absl:[64] test_loss=0.7032281756401062\n",
      "INFO:absl:[65] train_loss=0.4706258773803711, train_x1_loss=0.06599029898643494, train_x2_loss=0.40463557839393616\n",
      "INFO:absl:[65] val_loss=0.7065601348876953\n",
      "INFO:absl:[65] test_loss=0.7113130688667297\n",
      "INFO:absl:[66] train_loss=0.46990448236465454, train_x1_loss=0.06710520386695862, train_x2_loss=0.4027990400791168\n",
      "INFO:absl:[66] val_loss=0.6873672008514404\n",
      "INFO:absl:[66] test_loss=0.6927754878997803\n",
      "INFO:absl:[67] train_loss=0.46739569306373596, train_x1_loss=0.0653461292386055, train_x2_loss=0.40205007791519165\n",
      "INFO:absl:[67] val_loss=0.7161382436752319\n",
      "INFO:absl:[67] test_loss=0.723435640335083\n",
      "INFO:absl:[68] train_loss=0.4652489423751831, train_x1_loss=0.0655432939529419, train_x2_loss=0.3997058570384979\n",
      "INFO:absl:[68] val_loss=0.6932297945022583\n",
      "INFO:absl:[68] test_loss=0.6974543929100037\n",
      "INFO:absl:[69] train_loss=0.4681716859340668, train_x1_loss=0.06623505800962448, train_x2_loss=0.40193697810173035\n",
      "INFO:absl:[69] val_loss=0.6838454008102417\n",
      "INFO:absl:[69] test_loss=0.6853224635124207\n",
      "INFO:absl:[70] train_loss=0.4670993387699127, train_x1_loss=0.06612607091665268, train_x2_loss=0.40097248554229736\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[70] val_loss=0.6896423697471619\n",
      "INFO:absl:[70] test_loss=0.6965646743774414\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[71] train_loss=0.466431200504303, train_x1_loss=0.06547776609659195, train_x2_loss=0.4009537100791931\n",
      "INFO:absl:[71] val_loss=0.6821340918540955\n",
      "INFO:absl:[71] test_loss=0.6871578693389893\n",
      "INFO:absl:[72] train_loss=0.46763908863067627, train_x1_loss=0.066683828830719, train_x2_loss=0.40095505118370056\n",
      "INFO:absl:[72] val_loss=0.6950891017913818\n",
      "INFO:absl:[72] test_loss=0.6982603669166565\n",
      "INFO:absl:[73] train_loss=0.46685633063316345, train_x1_loss=0.06519246101379395, train_x2_loss=0.40166381001472473\n",
      "INFO:absl:[73] val_loss=0.686008095741272\n",
      "INFO:absl:[73] test_loss=0.6902123689651489\n",
      "INFO:absl:[74] train_loss=0.46415892243385315, train_x1_loss=0.06623058766126633, train_x2_loss=0.3979283571243286\n",
      "INFO:absl:[74] val_loss=0.6807928085327148\n",
      "INFO:absl:[74] test_loss=0.6813972592353821\n",
      "INFO:absl:[75] train_loss=0.46403589844703674, train_x1_loss=0.0654652938246727, train_x2_loss=0.3985704779624939\n",
      "INFO:absl:[75] val_loss=0.6815703511238098\n",
      "INFO:absl:[75] test_loss=0.6822129487991333\n",
      "INFO:absl:[76] train_loss=0.465626060962677, train_x1_loss=0.06723041087388992, train_x2_loss=0.3983953893184662\n",
      "INFO:absl:[76] val_loss=0.684047281742096\n",
      "INFO:absl:[76] test_loss=0.687546968460083\n",
      "INFO:absl:[77] train_loss=0.46590375900268555, train_x1_loss=0.06578348577022552, train_x2_loss=0.40011969208717346\n",
      "INFO:absl:[77] val_loss=0.6858544945716858\n",
      "INFO:absl:[77] test_loss=0.6904918551445007\n",
      "INFO:absl:[78] train_loss=0.4659537672996521, train_x1_loss=0.06635554134845734, train_x2_loss=0.39959895610809326\n",
      "INFO:absl:[78] val_loss=0.6834266781806946\n",
      "INFO:absl:[78] test_loss=0.6863242983818054\n",
      "INFO:absl:[79] train_loss=0.4620256721973419, train_x1_loss=0.06541365385055542, train_x2_loss=0.39661216735839844\n",
      "INFO:absl:[79] val_loss=0.6837430596351624\n",
      "INFO:absl:[79] test_loss=0.6883814930915833\n",
      "INFO:absl:[80] train_loss=0.4643997251987457, train_x1_loss=0.06703971326351166, train_x2_loss=0.39735934138298035\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[80] val_loss=0.686832845211029\n",
      "INFO:absl:[80] test_loss=0.6893556714057922\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[81] train_loss=0.4634203314781189, train_x1_loss=0.06604510545730591, train_x2_loss=0.3973759710788727\n",
      "INFO:absl:[81] val_loss=0.6823009252548218\n",
      "INFO:absl:[81] test_loss=0.6889572739601135\n",
      "INFO:absl:[82] train_loss=0.46362200379371643, train_x1_loss=0.06568282097578049, train_x2_loss=0.39793944358825684\n",
      "INFO:absl:[82] val_loss=0.6840192079544067\n",
      "INFO:absl:[82] test_loss=0.6873656511306763\n",
      "INFO:absl:[83] train_loss=0.4607139229774475, train_x1_loss=0.06543358415365219, train_x2_loss=0.3952803909778595\n",
      "INFO:absl:[83] val_loss=0.6794459223747253\n",
      "INFO:absl:[83] test_loss=0.6802651286125183\n",
      "INFO:absl:[84] train_loss=0.4609927535057068, train_x1_loss=0.06699972599744797, train_x2_loss=0.3939928412437439\n",
      "INFO:absl:[84] val_loss=0.6839941143989563\n",
      "INFO:absl:[84] test_loss=0.682812511920929\n",
      "INFO:absl:[85] train_loss=0.46180638670921326, train_x1_loss=0.06594541668891907, train_x2_loss=0.39586198329925537\n",
      "INFO:absl:[85] val_loss=0.6835112571716309\n",
      "INFO:absl:[85] test_loss=0.684654951095581\n",
      "INFO:absl:[86] train_loss=0.4636683166027069, train_x1_loss=0.06660324335098267, train_x2_loss=0.39706477522850037\n",
      "INFO:absl:[86] val_loss=0.6756807565689087\n",
      "INFO:absl:[86] test_loss=0.6794472932815552\n",
      "INFO:absl:[87] train_loss=0.459796279668808, train_x1_loss=0.06574609130620956, train_x2_loss=0.3940504193305969\n",
      "INFO:absl:[87] val_loss=0.6773396730422974\n",
      "INFO:absl:[87] test_loss=0.6828122735023499\n",
      "INFO:absl:[88] train_loss=0.4605298638343811, train_x1_loss=0.06496982276439667, train_x2_loss=0.39556023478507996\n",
      "INFO:absl:[88] val_loss=0.6891883611679077\n",
      "INFO:absl:[88] test_loss=0.6933606863021851\n",
      "INFO:absl:[89] train_loss=0.45860937237739563, train_x1_loss=0.06506389379501343, train_x2_loss=0.39354532957077026\n",
      "INFO:absl:[89] val_loss=0.6755706071853638\n",
      "INFO:absl:[89] test_loss=0.6763439178466797\n",
      "INFO:absl:[90] train_loss=0.45932716131210327, train_x1_loss=0.06514044851064682, train_x2_loss=0.39418578147888184\n",
      "INFO:absl:[90] val_loss=0.6706970930099487\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[90] test_loss=0.6716417074203491\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[91] train_loss=0.45933791995048523, train_x1_loss=0.06484705954790115, train_x2_loss=0.3944915235042572\n",
      "INFO:absl:[91] val_loss=0.6900906562805176\n",
      "INFO:absl:[91] test_loss=0.6906354427337646\n",
      "INFO:absl:[92] train_loss=0.459928423166275, train_x1_loss=0.06486359983682632, train_x2_loss=0.39506494998931885\n",
      "INFO:absl:[92] val_loss=0.6788943409919739\n",
      "INFO:absl:[92] test_loss=0.6829122304916382\n",
      "INFO:absl:[93] train_loss=0.4607178568840027, train_x1_loss=0.0661386027932167, train_x2_loss=0.3945786952972412\n",
      "INFO:absl:[93] val_loss=0.6666038632392883\n",
      "INFO:absl:[93] test_loss=0.6681463122367859\n",
      "INFO:absl:[94] train_loss=0.4599994122982025, train_x1_loss=0.0644059106707573, train_x2_loss=0.39559370279312134\n",
      "INFO:absl:[94] val_loss=0.6818704605102539\n",
      "INFO:absl:[94] test_loss=0.6880995631217957\n",
      "INFO:absl:[95] train_loss=0.4594990015029907, train_x1_loss=0.06650742143392563, train_x2_loss=0.39299193024635315\n",
      "INFO:absl:[95] val_loss=0.6942279934883118\n",
      "INFO:absl:[95] test_loss=0.6939442157745361\n",
      "INFO:absl:[96] train_loss=0.460521399974823, train_x1_loss=0.06493402272462845, train_x2_loss=0.3955872058868408\n",
      "INFO:absl:[96] val_loss=0.6847950220108032\n",
      "INFO:absl:[96] test_loss=0.6910622119903564\n",
      "INFO:absl:[97] train_loss=0.460781067609787, train_x1_loss=0.06486327946186066, train_x2_loss=0.3959180414676666\n",
      "INFO:absl:[97] val_loss=0.6800108551979065\n",
      "INFO:absl:[97] test_loss=0.6834160685539246\n",
      "INFO:absl:[98] train_loss=0.45905551314353943, train_x1_loss=0.06544750183820724, train_x2_loss=0.3936081528663635\n",
      "INFO:absl:[98] val_loss=0.6670529842376709\n",
      "INFO:absl:[98] test_loss=0.6702621579170227\n",
      "INFO:absl:[99] train_loss=0.4569212794303894, train_x1_loss=0.06494025141000748, train_x2_loss=0.39198148250579834\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[99] val_loss=0.682555615901947\n",
      "INFO:absl:[99] test_loss=0.686165988445282\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'relu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 10, 'dropout_rate': 0.013287043114620523, 'edge_features': (8, 8), 'epochs': 100, 'eval_every_epochs': 1, 'fully_connected_edges': 5, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 1, 'layer_norm': False, 'learning_rate': 0.00045346796177033903, 'log_every_epochs': 1, 'max_checkpts_to_keep': None, 'model': 'MLPGraphNetwork', 'momentum': 0.8712873602503628, 'n_blocks': 1, 'n_samples': 5000, 'node_features': (32, 2), 'normalize': True, 'optimizer': 'sgd', 'output_delay': 0, 'output_steps': 4, 'sample_buffer': -4, 'seed': 69, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 120, 'timestep_duration': 3, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| Name                                   | Shape    | Size | Mean     | Std   |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 8)   | 48   | -0.103   | 0.37  |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (8, 8)   | 64   | -0.0189  | 0.344 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (32,)    | 32   | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (19, 32) | 608  | -0.00843 | 0.219 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)     | 2    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (32, 2)  | 64   | -0.00627 | 0.195 |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "Total: 834\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Checkpoint None does not exist.\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.00s.\n",
      "INFO:absl:Starting training.\n",
      "INFO:absl:[0] train_loss=0.8533350229263306, train_x1_loss=0.1809595227241516, train_x2_loss=0.6723767518997192\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[0] val_loss=1.2797646522521973\n",
      "INFO:absl:[0] test_loss=1.1646925210952759\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Created artifact [10] Profile of type ArtifactType.URL and value None.\n",
      "INFO:absl:[1] train_loss=0.7081605195999146, train_x1_loss=0.11345967650413513, train_x2_loss=0.5947007536888123\n",
      "INFO:absl:[1] val_loss=1.1936765909194946\n",
      "INFO:absl:[1] test_loss=1.0821717977523804\n",
      "INFO:absl:[2] train_loss=0.6707211136817932, train_x1_loss=0.10207510739564896, train_x2_loss=0.5686455368995667\n",
      "INFO:absl:[2] val_loss=1.1201939582824707\n",
      "INFO:absl:[2] test_loss=1.0152863264083862\n",
      "INFO:absl:[3] train_loss=0.648795485496521, train_x1_loss=0.0961730033159256, train_x2_loss=0.55262291431427\n",
      "INFO:absl:[3] val_loss=1.0752428770065308\n",
      "INFO:absl:[3] test_loss=0.9761742353439331\n",
      "INFO:absl:[4] train_loss=0.6319743990898132, train_x1_loss=0.09195840358734131, train_x2_loss=0.5400165319442749\n",
      "INFO:absl:[4] val_loss=1.0575573444366455\n",
      "INFO:absl:[4] test_loss=0.9635177850723267\n",
      "INFO:absl:[5] train_loss=0.6204585433006287, train_x1_loss=0.08836822211742401, train_x2_loss=0.5320896506309509\n",
      "INFO:absl:[5] val_loss=1.0460810661315918\n",
      "INFO:absl:[5] test_loss=0.9545206427574158\n",
      "INFO:absl:[6] train_loss=0.6135518550872803, train_x1_loss=0.08779226988554001, train_x2_loss=0.5257603526115417\n",
      "INFO:absl:[6] val_loss=1.0378130674362183\n",
      "INFO:absl:[6] test_loss=0.9500309824943542\n",
      "INFO:absl:[7] train_loss=0.6077802777290344, train_x1_loss=0.08444307744503021, train_x2_loss=0.5233389735221863\n",
      "INFO:absl:[7] val_loss=1.0254443883895874\n",
      "INFO:absl:[7] test_loss=0.9379922747612\n",
      "INFO:absl:[8] train_loss=0.6027925610542297, train_x1_loss=0.08473052084445953, train_x2_loss=0.5180623531341553\n",
      "INFO:absl:[8] val_loss=1.0120607614517212\n",
      "INFO:absl:[8] test_loss=0.9248141646385193\n",
      "INFO:absl:[9] train_loss=0.5998485088348389, train_x1_loss=0.08428735285997391, train_x2_loss=0.5155608057975769\n",
      "INFO:absl:[9] val_loss=1.0087981224060059\n",
      "INFO:absl:[9] test_loss=0.921356201171875\n",
      "INFO:absl:[10] train_loss=0.595729410648346, train_x1_loss=0.0816757082939148, train_x2_loss=0.5140539407730103\n",
      "INFO:absl:[10] val_loss=0.9959028959274292\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[10] test_loss=0.9098436832427979\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[11] train_loss=0.5920483469963074, train_x1_loss=0.081366166472435, train_x2_loss=0.5106837153434753\n",
      "INFO:absl:[11] val_loss=0.9976577758789062\n",
      "INFO:absl:[11] test_loss=0.9103461503982544\n",
      "INFO:absl:[12] train_loss=0.5913327932357788, train_x1_loss=0.08124281466007233, train_x2_loss=0.5100896954536438\n",
      "INFO:absl:[12] val_loss=1.000726580619812\n",
      "INFO:absl:[12] test_loss=0.9142756462097168\n",
      "INFO:absl:[13] train_loss=0.5855085849761963, train_x1_loss=0.07941554486751556, train_x2_loss=0.5060936212539673\n",
      "INFO:absl:[13] val_loss=0.9786750674247742\n",
      "INFO:absl:[13] test_loss=0.893174946308136\n",
      "INFO:absl:[14] train_loss=0.5843375325202942, train_x1_loss=0.07959410548210144, train_x2_loss=0.5047428011894226\n",
      "INFO:absl:[14] val_loss=0.9759501814842224\n",
      "INFO:absl:[14] test_loss=0.8904539942741394\n",
      "INFO:absl:[15] train_loss=0.580658495426178, train_x1_loss=0.07792836427688599, train_x2_loss=0.5027298331260681\n",
      "INFO:absl:[15] val_loss=0.9831241965293884\n",
      "INFO:absl:[15] test_loss=0.8980069756507874\n",
      "INFO:absl:[16] train_loss=0.5784333348274231, train_x1_loss=0.07954052090644836, train_x2_loss=0.49889227747917175\n",
      "INFO:absl:[16] val_loss=0.9625707864761353\n",
      "INFO:absl:[16] test_loss=0.879124104976654\n",
      "INFO:absl:[17] train_loss=0.5749687552452087, train_x1_loss=0.07829060405492783, train_x2_loss=0.49667805433273315\n",
      "INFO:absl:[17] val_loss=0.9616028070449829\n",
      "INFO:absl:[17] test_loss=0.8762699961662292\n",
      "INFO:absl:[18] train_loss=0.5748471021652222, train_x1_loss=0.0797298401594162, train_x2_loss=0.49511778354644775\n",
      "INFO:absl:[18] val_loss=0.9489201307296753\n",
      "INFO:absl:[18] test_loss=0.8616085052490234\n",
      "INFO:absl:[19] train_loss=0.5703540444374084, train_x1_loss=0.07821603864431381, train_x2_loss=0.4921379089355469\n",
      "INFO:absl:[19] val_loss=0.9391720294952393\n",
      "INFO:absl:[19] test_loss=0.8553452491760254\n",
      "INFO:absl:[20] train_loss=0.5675790905952454, train_x1_loss=0.07901224493980408, train_x2_loss=0.48856568336486816\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[20] val_loss=0.9286522269248962\n",
      "INFO:absl:[20] test_loss=0.8447635173797607\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[21] train_loss=0.5638471245765686, train_x1_loss=0.07875512540340424, train_x2_loss=0.4850910007953644\n",
      "INFO:absl:[21] val_loss=0.9277104735374451\n",
      "INFO:absl:[21] test_loss=0.8411787152290344\n",
      "INFO:absl:[22] train_loss=0.5609638094902039, train_x1_loss=0.07784849405288696, train_x2_loss=0.48311513662338257\n",
      "INFO:absl:[22] val_loss=0.9242963790893555\n",
      "INFO:absl:[22] test_loss=0.8392863869667053\n",
      "INFO:absl:[23] train_loss=0.5559393167495728, train_x1_loss=0.07782470434904099, train_x2_loss=0.47811359167099\n",
      "INFO:absl:[23] val_loss=0.9122611880302429\n",
      "INFO:absl:[23] test_loss=0.8263207077980042\n",
      "INFO:absl:[24] train_loss=0.55455482006073, train_x1_loss=0.0793626680970192, train_x2_loss=0.47519221901893616\n",
      "INFO:absl:[24] val_loss=0.8896986842155457\n",
      "INFO:absl:[24] test_loss=0.8021988272666931\n",
      "INFO:absl:[25] train_loss=0.548559844493866, train_x1_loss=0.0779314935207367, train_x2_loss=0.470628559589386\n",
      "INFO:absl:[25] val_loss=0.8750830292701721\n",
      "INFO:absl:[25] test_loss=0.7894030809402466\n",
      "INFO:absl:[26] train_loss=0.5439316034317017, train_x1_loss=0.07722539454698563, train_x2_loss=0.46670618653297424\n",
      "INFO:absl:[26] val_loss=0.8537256717681885\n",
      "INFO:absl:[26] test_loss=0.7677608132362366\n",
      "INFO:absl:[27] train_loss=0.5391139984130859, train_x1_loss=0.07809774577617645, train_x2_loss=0.46101775765419006\n",
      "INFO:absl:[27] val_loss=0.8472565412521362\n",
      "INFO:absl:[27] test_loss=0.7616809606552124\n",
      "INFO:absl:[28] train_loss=0.5348342657089233, train_x1_loss=0.07714373618364334, train_x2_loss=0.45768970251083374\n",
      "INFO:absl:[28] val_loss=0.8548160791397095\n",
      "INFO:absl:[28] test_loss=0.7724295854568481\n",
      "INFO:absl:[29] train_loss=0.5295000672340393, train_x1_loss=0.07668227702379227, train_x2_loss=0.45281773805618286\n",
      "INFO:absl:[29] val_loss=0.837106466293335\n",
      "INFO:absl:[29] test_loss=0.7553646564483643\n",
      "INFO:absl:[30] train_loss=0.525137186050415, train_x1_loss=0.07605943083763123, train_x2_loss=0.4490778148174286\n",
      "INFO:absl:[30] val_loss=0.8249272704124451\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[30] test_loss=0.7416142821311951\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[31] train_loss=0.5231413841247559, train_x1_loss=0.07671879976987839, train_x2_loss=0.44642317295074463\n",
      "INFO:absl:[31] val_loss=0.7917812466621399\n",
      "INFO:absl:[31] test_loss=0.7099747061729431\n",
      "INFO:absl:[32] train_loss=0.5240606665611267, train_x1_loss=0.07768671214580536, train_x2_loss=0.4463745951652527\n",
      "INFO:absl:[32] val_loss=0.8044953346252441\n",
      "INFO:absl:[32] test_loss=0.7219772338867188\n",
      "INFO:absl:[33] train_loss=0.5195791721343994, train_x1_loss=0.07662446051836014, train_x2_loss=0.4429546892642975\n",
      "INFO:absl:[33] val_loss=0.81706303358078\n",
      "INFO:absl:[33] test_loss=0.7345551252365112\n",
      "INFO:absl:[34] train_loss=0.5195074677467346, train_x1_loss=0.07739146053791046, train_x2_loss=0.4421156942844391\n",
      "INFO:absl:[34] val_loss=0.8035256266593933\n",
      "INFO:absl:[34] test_loss=0.7233444452285767\n",
      "INFO:absl:[35] train_loss=0.5158311128616333, train_x1_loss=0.07574822008609772, train_x2_loss=0.4400830566883087\n",
      "INFO:absl:[35] val_loss=0.7819516658782959\n",
      "INFO:absl:[35] test_loss=0.6988727450370789\n",
      "INFO:absl:[36] train_loss=0.5163986682891846, train_x1_loss=0.07697780430316925, train_x2_loss=0.439421147108078\n",
      "INFO:absl:[36] val_loss=0.7974342107772827\n",
      "INFO:absl:[36] test_loss=0.717558741569519\n",
      "INFO:absl:[37] train_loss=0.511108934879303, train_x1_loss=0.07647358626127243, train_x2_loss=0.43463435769081116\n",
      "INFO:absl:[37] val_loss=0.8008250594139099\n",
      "INFO:absl:[37] test_loss=0.720383882522583\n",
      "INFO:absl:[38] train_loss=0.513177216053009, train_x1_loss=0.07657947391271591, train_x2_loss=0.43659839034080505\n",
      "INFO:absl:[38] val_loss=0.7579087018966675\n",
      "INFO:absl:[38] test_loss=0.678092896938324\n",
      "INFO:absl:[39] train_loss=0.5092169642448425, train_x1_loss=0.07588662207126617, train_x2_loss=0.4333299994468689\n",
      "INFO:absl:[39] val_loss=0.7854275703430176\n",
      "INFO:absl:[39] test_loss=0.7047619223594666\n",
      "INFO:absl:[40] train_loss=0.5077805519104004, train_x1_loss=0.07525186240673065, train_x2_loss=0.4325283467769623\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[40] val_loss=0.7835925221443176\n",
      "INFO:absl:[40] test_loss=0.7007912993431091\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[41] train_loss=0.5076232552528381, train_x1_loss=0.07721642404794693, train_x2_loss=0.4304071366786957\n",
      "INFO:absl:[41] val_loss=0.7559718489646912\n",
      "INFO:absl:[41] test_loss=0.6752862930297852\n",
      "INFO:absl:[42] train_loss=0.5062346458435059, train_x1_loss=0.07542331516742706, train_x2_loss=0.4308113157749176\n",
      "INFO:absl:[42] val_loss=0.7737733125686646\n",
      "INFO:absl:[42] test_loss=0.692399263381958\n",
      "INFO:absl:[43] train_loss=0.5010231733322144, train_x1_loss=0.07508252561092377, train_x2_loss=0.42594006657600403\n",
      "INFO:absl:[43] val_loss=0.7589594125747681\n",
      "INFO:absl:[43] test_loss=0.6770704388618469\n",
      "INFO:absl:[44] train_loss=0.5043474435806274, train_x1_loss=0.07550632208585739, train_x2_loss=0.4288410246372223\n",
      "INFO:absl:[44] val_loss=0.7586542367935181\n",
      "INFO:absl:[44] test_loss=0.6765591502189636\n",
      "INFO:absl:[45] train_loss=0.49960243701934814, train_x1_loss=0.07454464584589005, train_x2_loss=0.4250580966472626\n",
      "INFO:absl:[45] val_loss=0.7687847018241882\n",
      "INFO:absl:[45] test_loss=0.6861382126808167\n",
      "INFO:absl:[46] train_loss=0.49968746304512024, train_x1_loss=0.07457222044467926, train_x2_loss=0.42511531710624695\n",
      "INFO:absl:[46] val_loss=0.7873865365982056\n",
      "INFO:absl:[46] test_loss=0.707275927066803\n",
      "INFO:absl:[47] train_loss=0.49528878927230835, train_x1_loss=0.07328671962022781, train_x2_loss=0.42200201749801636\n",
      "INFO:absl:[47] val_loss=0.7467917203903198\n",
      "INFO:absl:[47] test_loss=0.6654040813446045\n",
      "INFO:absl:[48] train_loss=0.49532586336135864, train_x1_loss=0.07401438802480698, train_x2_loss=0.4213109612464905\n",
      "INFO:absl:[48] val_loss=0.7479836344718933\n",
      "INFO:absl:[48] test_loss=0.6643649935722351\n",
      "INFO:absl:[49] train_loss=0.4929334819316864, train_x1_loss=0.07372203469276428, train_x2_loss=0.419210821390152\n",
      "INFO:absl:[49] val_loss=0.7330488562583923\n",
      "INFO:absl:[49] test_loss=0.6500284671783447\n",
      "INFO:absl:[50] train_loss=0.4941359758377075, train_x1_loss=0.073697529733181, train_x2_loss=0.42043814063072205\n",
      "INFO:absl:[50] val_loss=0.7310150265693665\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[50] test_loss=0.6466073989868164\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[51] train_loss=0.4948519468307495, train_x1_loss=0.07497870922088623, train_x2_loss=0.4198741316795349\n",
      "INFO:absl:[51] val_loss=0.7124009728431702\n",
      "INFO:absl:[51] test_loss=0.6313263773918152\n",
      "INFO:absl:[52] train_loss=0.48913171887397766, train_x1_loss=0.07166655361652374, train_x2_loss=0.4174656867980957\n",
      "INFO:absl:[52] val_loss=0.7256646156311035\n",
      "INFO:absl:[52] test_loss=0.6446191072463989\n",
      "INFO:absl:[53] train_loss=0.48852965235710144, train_x1_loss=0.07232699543237686, train_x2_loss=0.41620227694511414\n",
      "INFO:absl:[53] val_loss=0.7190761566162109\n",
      "INFO:absl:[53] test_loss=0.6368375420570374\n",
      "INFO:absl:[54] train_loss=0.4888976812362671, train_x1_loss=0.07216168940067291, train_x2_loss=0.4167359471321106\n",
      "INFO:absl:[54] val_loss=0.72263103723526\n",
      "INFO:absl:[54] test_loss=0.638953447341919\n",
      "INFO:absl:Setting work unit notes: 3231.5 steps/s, 55.4% (193889/350000), ETA: 0m (1m : 0.1% checkpoint, 12.4% eval)\n",
      "INFO:absl:[193889] steps_per_sec=3231.479340\n",
      "INFO:absl:[55] train_loss=0.4885759949684143, train_x1_loss=0.07269573211669922, train_x2_loss=0.41587984561920166\n",
      "INFO:absl:[55] val_loss=0.7166818976402283\n",
      "INFO:absl:[55] test_loss=0.63364177942276\n",
      "INFO:absl:[56] train_loss=0.4862268269062042, train_x1_loss=0.07091788202524185, train_x2_loss=0.4153086245059967\n",
      "INFO:absl:[56] val_loss=0.7313528656959534\n",
      "INFO:absl:[56] test_loss=0.6532272696495056\n",
      "INFO:absl:[57] train_loss=0.4858495891094208, train_x1_loss=0.07238630205392838, train_x2_loss=0.4134632349014282\n",
      "INFO:absl:[57] val_loss=0.7218174934387207\n",
      "INFO:absl:[57] test_loss=0.641791820526123\n",
      "INFO:absl:[58] train_loss=0.48456570506095886, train_x1_loss=0.07141071557998657, train_x2_loss=0.41315585374832153\n",
      "INFO:absl:[58] val_loss=0.72482830286026\n",
      "INFO:absl:[58] test_loss=0.6420301795005798\n",
      "INFO:absl:[59] train_loss=0.479173868894577, train_x1_loss=0.06979583948850632, train_x2_loss=0.4093777537345886\n",
      "INFO:absl:[59] val_loss=0.7150448560714722\n",
      "INFO:absl:[59] test_loss=0.6318195462226868\n",
      "INFO:absl:[60] train_loss=0.48219287395477295, train_x1_loss=0.07056169211864471, train_x2_loss=0.41163113713264465\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[60] val_loss=0.6900586485862732\n",
      "INFO:absl:[60] test_loss=0.6096192002296448\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[61] train_loss=0.4805041551589966, train_x1_loss=0.06965066492557526, train_x2_loss=0.4108541011810303\n",
      "INFO:absl:[61] val_loss=0.7069475054740906\n",
      "INFO:absl:[61] test_loss=0.6249732375144958\n",
      "INFO:absl:[62] train_loss=0.47792813181877136, train_x1_loss=0.07020982354879379, train_x2_loss=0.4077187776565552\n",
      "INFO:absl:[62] val_loss=0.7263760566711426\n",
      "INFO:absl:[62] test_loss=0.6484522223472595\n",
      "INFO:absl:[63] train_loss=0.47843676805496216, train_x1_loss=0.069117970764637, train_x2_loss=0.4093189537525177\n",
      "INFO:absl:[63] val_loss=0.7078876495361328\n",
      "INFO:absl:[63] test_loss=0.6291762590408325\n",
      "INFO:absl:[64] train_loss=0.48062214255332947, train_x1_loss=0.07064826786518097, train_x2_loss=0.40997380018234253\n",
      "INFO:absl:[64] val_loss=0.7221267223358154\n",
      "INFO:absl:[64] test_loss=0.6460840702056885\n",
      "INFO:absl:[65] train_loss=0.4763321578502655, train_x1_loss=0.0691816434264183, train_x2_loss=0.40715086460113525\n",
      "INFO:absl:[65] val_loss=0.6818166375160217\n",
      "INFO:absl:[65] test_loss=0.6004911661148071\n",
      "INFO:absl:[66] train_loss=0.47602587938308716, train_x1_loss=0.0703296810388565, train_x2_loss=0.40569666028022766\n",
      "INFO:absl:[66] val_loss=0.6965107321739197\n",
      "INFO:absl:[66] test_loss=0.6174677610397339\n",
      "INFO:absl:[67] train_loss=0.47465068101882935, train_x1_loss=0.06932231038808823, train_x2_loss=0.40532854199409485\n",
      "INFO:absl:[67] val_loss=0.6987969875335693\n",
      "INFO:absl:[67] test_loss=0.6197959780693054\n",
      "INFO:absl:[68] train_loss=0.47085875272750854, train_x1_loss=0.06913945078849792, train_x2_loss=0.4017198085784912\n",
      "INFO:absl:[68] val_loss=0.7195074558258057\n",
      "INFO:absl:[68] test_loss=0.6354277729988098\n",
      "INFO:absl:[69] train_loss=0.47178396582603455, train_x1_loss=0.07024766504764557, train_x2_loss=0.4015362858772278\n",
      "INFO:absl:[69] val_loss=0.6969445943832397\n",
      "INFO:absl:[69] test_loss=0.611726701259613\n",
      "INFO:absl:[70] train_loss=0.47433361411094666, train_x1_loss=0.06970994174480438, train_x2_loss=0.4046226143836975\n",
      "INFO:absl:[70] val_loss=0.6868104338645935\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[70] test_loss=0.6073102355003357\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[71] train_loss=0.47312670946121216, train_x1_loss=0.06962687522172928, train_x2_loss=0.4034996032714844\n",
      "INFO:absl:[71] val_loss=0.6764252781867981\n",
      "INFO:absl:[71] test_loss=0.5977689027786255\n",
      "INFO:absl:[72] train_loss=0.4736689329147339, train_x1_loss=0.06927261501550674, train_x2_loss=0.40439629554748535\n",
      "INFO:absl:[72] val_loss=0.6845958828926086\n",
      "INFO:absl:[72] test_loss=0.6077898144721985\n",
      "INFO:absl:[73] train_loss=0.47040054202079773, train_x1_loss=0.06893007457256317, train_x2_loss=0.4014701545238495\n",
      "INFO:absl:[73] val_loss=0.703018069267273\n",
      "INFO:absl:[73] test_loss=0.6269131302833557\n",
      "INFO:absl:[74] train_loss=0.46992936730384827, train_x1_loss=0.06848498433828354, train_x2_loss=0.40144407749176025\n",
      "INFO:absl:[74] val_loss=0.6762409806251526\n",
      "INFO:absl:[74] test_loss=0.5935169458389282\n",
      "INFO:absl:[75] train_loss=0.471557080745697, train_x1_loss=0.07057765126228333, train_x2_loss=0.4009794294834137\n",
      "INFO:absl:[75] val_loss=0.6980516910552979\n",
      "INFO:absl:[75] test_loss=0.6154164671897888\n",
      "INFO:absl:[76] train_loss=0.46789342164993286, train_x1_loss=0.06899608671665192, train_x2_loss=0.39889657497406006\n",
      "INFO:absl:[76] val_loss=0.6802281737327576\n",
      "INFO:absl:[76] test_loss=0.5999812483787537\n",
      "INFO:absl:[77] train_loss=0.4700498878955841, train_x1_loss=0.06812850385904312, train_x2_loss=0.401920884847641\n",
      "INFO:absl:[77] val_loss=0.6782149076461792\n",
      "INFO:absl:[77] test_loss=0.5962986350059509\n",
      "INFO:absl:[78] train_loss=0.47122082114219666, train_x1_loss=0.06897144764661789, train_x2_loss=0.40224865078926086\n",
      "INFO:absl:[78] val_loss=0.6663734912872314\n",
      "INFO:absl:[78] test_loss=0.5873092412948608\n",
      "INFO:absl:[79] train_loss=0.4669385254383087, train_x1_loss=0.06732307374477386, train_x2_loss=0.39961665868759155\n",
      "INFO:absl:[79] val_loss=0.6757363677024841\n",
      "INFO:absl:[79] test_loss=0.5970597863197327\n",
      "INFO:absl:[80] train_loss=0.46726566553115845, train_x1_loss=0.06975952535867691, train_x2_loss=0.397506445646286\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[80] val_loss=0.6858261823654175\n",
      "INFO:absl:[80] test_loss=0.6049140691757202\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[81] train_loss=0.46631038188934326, train_x1_loss=0.0679573342204094, train_x2_loss=0.3983525037765503\n",
      "INFO:absl:[81] val_loss=0.7071312069892883\n",
      "INFO:absl:[81] test_loss=0.6310973763465881\n",
      "INFO:absl:[82] train_loss=0.46523064374923706, train_x1_loss=0.06790079921483994, train_x2_loss=0.3973293602466583\n",
      "INFO:absl:[82] val_loss=0.6762847304344177\n",
      "INFO:absl:[82] test_loss=0.5940802097320557\n",
      "INFO:absl:[83] train_loss=0.465526282787323, train_x1_loss=0.06709670275449753, train_x2_loss=0.39842867851257324\n",
      "INFO:absl:[83] val_loss=0.6718981266021729\n",
      "INFO:absl:[83] test_loss=0.590265691280365\n",
      "INFO:absl:[84] train_loss=0.4653684198856354, train_x1_loss=0.06801418960094452, train_x2_loss=0.39735403656959534\n",
      "INFO:absl:[84] val_loss=0.6662514805793762\n",
      "INFO:absl:[84] test_loss=0.5839515328407288\n",
      "INFO:absl:[85] train_loss=0.46633434295654297, train_x1_loss=0.06778500229120255, train_x2_loss=0.398549884557724\n",
      "INFO:absl:[85] val_loss=0.6626639366149902\n",
      "INFO:absl:[85] test_loss=0.581633985042572\n",
      "INFO:absl:[86] train_loss=0.46446025371551514, train_x1_loss=0.06737370789051056, train_x2_loss=0.39708685874938965\n",
      "INFO:absl:[86] val_loss=0.689631998538971\n",
      "INFO:absl:[86] test_loss=0.6092106103897095\n",
      "INFO:absl:[87] train_loss=0.4643011689186096, train_x1_loss=0.06671591848134995, train_x2_loss=0.39758458733558655\n",
      "INFO:absl:[87] val_loss=0.6584652066230774\n",
      "INFO:absl:[87] test_loss=0.5760707259178162\n",
      "INFO:absl:[88] train_loss=0.463644415140152, train_x1_loss=0.06578253209590912, train_x2_loss=0.397861123085022\n",
      "INFO:absl:[88] val_loss=0.6712960004806519\n",
      "INFO:absl:[88] test_loss=0.59000164270401\n",
      "INFO:absl:[89] train_loss=0.46228718757629395, train_x1_loss=0.06691204756498337, train_x2_loss=0.3953750431537628\n",
      "INFO:absl:[89] val_loss=0.6622121334075928\n",
      "INFO:absl:[89] test_loss=0.5822500586509705\n",
      "INFO:absl:[90] train_loss=0.46009209752082825, train_x1_loss=0.06556697189807892, train_x2_loss=0.39452505111694336\n",
      "INFO:absl:[90] val_loss=0.6695582866668701\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[90] test_loss=0.5861316919326782\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[91] train_loss=0.4634881913661957, train_x1_loss=0.06693090498447418, train_x2_loss=0.3965572714805603\n",
      "INFO:absl:[91] val_loss=0.6789509654045105\n",
      "INFO:absl:[91] test_loss=0.5981956720352173\n",
      "INFO:absl:[92] train_loss=0.45925313234329224, train_x1_loss=0.0648285374045372, train_x2_loss=0.39442452788352966\n",
      "INFO:absl:[92] val_loss=0.6708559393882751\n",
      "INFO:absl:[92] test_loss=0.5878867506980896\n",
      "INFO:absl:[93] train_loss=0.46385160088539124, train_x1_loss=0.0669112429022789, train_x2_loss=0.3969406187534332\n",
      "INFO:absl:[93] val_loss=0.6553574800491333\n",
      "INFO:absl:[93] test_loss=0.5746655464172363\n",
      "INFO:absl:[94] train_loss=0.461350679397583, train_x1_loss=0.06606156378984451, train_x2_loss=0.3952893316745758\n",
      "INFO:absl:[94] val_loss=0.6764235496520996\n",
      "INFO:absl:[94] test_loss=0.5951106548309326\n",
      "INFO:absl:[95] train_loss=0.45914462208747864, train_x1_loss=0.06641639769077301, train_x2_loss=0.3927282392978668\n",
      "INFO:absl:[95] val_loss=0.6626488566398621\n",
      "INFO:absl:[95] test_loss=0.5829665064811707\n",
      "INFO:absl:[96] train_loss=0.4610503315925598, train_x1_loss=0.06561846286058426, train_x2_loss=0.39543139934539795\n",
      "INFO:absl:[96] val_loss=0.6801279187202454\n",
      "INFO:absl:[96] test_loss=0.5990386009216309\n",
      "INFO:absl:[97] train_loss=0.46049413084983826, train_x1_loss=0.06629185378551483, train_x2_loss=0.39420202374458313\n",
      "INFO:absl:[97] val_loss=0.6675733923912048\n",
      "INFO:absl:[97] test_loss=0.5881821513175964\n",
      "INFO:absl:[98] train_loss=0.4575214087963104, train_x1_loss=0.06551502645015717, train_x2_loss=0.3920063078403473\n",
      "INFO:absl:[98] val_loss=0.6521708369255066\n",
      "INFO:absl:[98] test_loss=0.5725501775741577\n",
      "INFO:absl:[99] train_loss=0.45976370573043823, train_x1_loss=0.06518243998289108, train_x2_loss=0.39458057284355164\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[99] val_loss=0.6446909308433533\n",
      "INFO:absl:[99] test_loss=0.5660200119018555\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'relu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 10, 'dropout_rate': 0.013287043114620523, 'edge_features': (8, 8), 'epochs': 100, 'eval_every_epochs': 1, 'fully_connected_edges': 5, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 1, 'layer_norm': False, 'learning_rate': 0.00045346796177033903, 'log_every_epochs': 1, 'max_checkpts_to_keep': None, 'model': 'MLPGraphNetwork', 'momentum': 0.8712873602503628, 'n_blocks': 1, 'n_samples': 5000, 'node_features': (32, 2), 'normalize': True, 'optimizer': 'sgd', 'output_delay': 0, 'output_steps': 4, 'sample_buffer': -4, 'seed': 70, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 120, 'timestep_duration': 3, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| Name                                   | Shape    | Size | Mean     | Std   |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 8)   | 48   | -0.103   | 0.37  |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (8, 8)   | 64   | -0.0189  | 0.344 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (32,)    | 32   | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (19, 32) | 608  | -0.00843 | 0.219 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)     | 2    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (32, 2)  | 64   | -0.00627 | 0.195 |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "Total: 834\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Checkpoint None does not exist.\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.00s.\n",
      "INFO:absl:Starting training.\n",
      "INFO:absl:[0] train_loss=0.8102825880050659, train_x1_loss=0.1603042185306549, train_x2_loss=0.6499772667884827\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[0] val_loss=1.2319321632385254\n",
      "INFO:absl:[0] test_loss=1.2628377676010132\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Created artifact [10] Profile of type ArtifactType.URL and value None.\n",
      "INFO:absl:[1] train_loss=0.6818458437919617, train_x1_loss=0.10518702119588852, train_x2_loss=0.5766589045524597\n",
      "INFO:absl:[1] val_loss=1.1635040044784546\n",
      "INFO:absl:[1] test_loss=1.1963478326797485\n",
      "INFO:absl:[2] train_loss=0.651437520980835, train_x1_loss=0.09693046659231186, train_x2_loss=0.5545088052749634\n",
      "INFO:absl:[2] val_loss=1.121910572052002\n",
      "INFO:absl:[2] test_loss=1.1563599109649658\n",
      "INFO:absl:[3] train_loss=0.6308623552322388, train_x1_loss=0.09098461270332336, train_x2_loss=0.5398764610290527\n",
      "INFO:absl:[3] val_loss=1.0959749221801758\n",
      "INFO:absl:[3] test_loss=1.131935477256775\n",
      "INFO:absl:[4] train_loss=0.6188732385635376, train_x1_loss=0.08858480304479599, train_x2_loss=0.530288815498352\n",
      "INFO:absl:[4] val_loss=1.080173134803772\n",
      "INFO:absl:[4] test_loss=1.1168451309204102\n",
      "INFO:absl:[5] train_loss=0.6105300784111023, train_x1_loss=0.08523046970367432, train_x2_loss=0.5253005623817444\n",
      "INFO:absl:[5] val_loss=1.0780811309814453\n",
      "INFO:absl:[5] test_loss=1.117455244064331\n",
      "INFO:absl:[6] train_loss=0.602841854095459, train_x1_loss=0.08396037667989731, train_x2_loss=0.5188816785812378\n",
      "INFO:absl:[6] val_loss=1.0518122911453247\n",
      "INFO:absl:[6] test_loss=1.0916920900344849\n",
      "INFO:absl:[7] train_loss=0.5976764559745789, train_x1_loss=0.08198182284832001, train_x2_loss=0.5156934857368469\n",
      "INFO:absl:[7] val_loss=1.055629849433899\n",
      "INFO:absl:[7] test_loss=1.094886302947998\n",
      "INFO:absl:[8] train_loss=0.591777503490448, train_x1_loss=0.08087025582790375, train_x2_loss=0.510907769203186\n",
      "INFO:absl:[8] val_loss=1.0601723194122314\n",
      "INFO:absl:[8] test_loss=1.1028891801834106\n",
      "INFO:absl:[9] train_loss=0.5887225866317749, train_x1_loss=0.0806911513209343, train_x2_loss=0.508030891418457\n",
      "INFO:absl:[9] val_loss=1.0183511972427368\n",
      "INFO:absl:[9] test_loss=1.0578696727752686\n",
      "INFO:absl:[10] train_loss=0.5826954245567322, train_x1_loss=0.07899882644414902, train_x2_loss=0.5036972165107727\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[10] val_loss=1.0301415920257568\n",
      "INFO:absl:[10] test_loss=1.0735596418380737\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[11] train_loss=0.5763956904411316, train_x1_loss=0.07746714353561401, train_x2_loss=0.4989289343357086\n",
      "INFO:absl:[11] val_loss=1.011458396911621\n",
      "INFO:absl:[11] test_loss=1.053571343421936\n",
      "INFO:absl:[12] train_loss=0.5722187757492065, train_x1_loss=0.07787728309631348, train_x2_loss=0.4943412244319916\n",
      "INFO:absl:[12] val_loss=1.0248782634735107\n",
      "INFO:absl:[12] test_loss=1.0681051015853882\n",
      "INFO:absl:[13] train_loss=0.567766010761261, train_x1_loss=0.07614710927009583, train_x2_loss=0.4916178584098816\n",
      "INFO:absl:[13] val_loss=1.0006835460662842\n",
      "INFO:absl:[13] test_loss=1.0433584451675415\n",
      "INFO:absl:[14] train_loss=0.562253475189209, train_x1_loss=0.07602310925722122, train_x2_loss=0.48623034358024597\n",
      "INFO:absl:[14] val_loss=0.959926962852478\n",
      "INFO:absl:[14] test_loss=0.9990812540054321\n",
      "INFO:absl:[15] train_loss=0.5526028871536255, train_x1_loss=0.07476437836885452, train_x2_loss=0.4778383672237396\n",
      "INFO:absl:[15] val_loss=0.9566419720649719\n",
      "INFO:absl:[15] test_loss=0.9958279132843018\n",
      "INFO:absl:[16] train_loss=0.5492090582847595, train_x1_loss=0.07368992269039154, train_x2_loss=0.47551774978637695\n",
      "INFO:absl:[16] val_loss=0.9385002255439758\n",
      "INFO:absl:[16] test_loss=0.9763951301574707\n",
      "INFO:absl:[17] train_loss=0.5417308807373047, train_x1_loss=0.07419029623270035, train_x2_loss=0.4675396978855133\n",
      "INFO:absl:[17] val_loss=0.9160966873168945\n",
      "INFO:absl:[17] test_loss=0.9523857235908508\n",
      "INFO:absl:[18] train_loss=0.5395731329917908, train_x1_loss=0.07320369780063629, train_x2_loss=0.4663695693016052\n",
      "INFO:absl:[18] val_loss=0.9202115535736084\n",
      "INFO:absl:[18] test_loss=0.9569275379180908\n",
      "INFO:absl:[19] train_loss=0.5323780179023743, train_x1_loss=0.07230380922555923, train_x2_loss=0.460073858499527\n",
      "INFO:absl:[19] val_loss=0.8863531947135925\n",
      "INFO:absl:[19] test_loss=0.927989661693573\n",
      "INFO:absl:[20] train_loss=0.5300466418266296, train_x1_loss=0.0729740560054779, train_x2_loss=0.4570731818675995\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[20] val_loss=0.9015659093856812\n",
      "INFO:absl:[20] test_loss=0.9444299340248108\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[21] train_loss=0.5250697135925293, train_x1_loss=0.07220731675624847, train_x2_loss=0.4528629779815674\n",
      "INFO:absl:[21] val_loss=0.8886393308639526\n",
      "INFO:absl:[21] test_loss=0.9317560791969299\n",
      "INFO:absl:[22] train_loss=0.5206232070922852, train_x1_loss=0.07082230597734451, train_x2_loss=0.449800580739975\n",
      "INFO:absl:[22] val_loss=0.8751807808876038\n",
      "INFO:absl:[22] test_loss=0.9145599007606506\n",
      "INFO:absl:[23] train_loss=0.5192071795463562, train_x1_loss=0.07023613899946213, train_x2_loss=0.4489712119102478\n",
      "INFO:absl:[23] val_loss=0.8798419237136841\n",
      "INFO:absl:[23] test_loss=0.9210307002067566\n",
      "INFO:absl:[24] train_loss=0.5191568732261658, train_x1_loss=0.07120220363140106, train_x2_loss=0.4479551911354065\n",
      "INFO:absl:[24] val_loss=0.8321791887283325\n",
      "INFO:absl:[24] test_loss=0.8781671524047852\n",
      "INFO:absl:[25] train_loss=0.5157127976417542, train_x1_loss=0.07010654360055923, train_x2_loss=0.445605605840683\n",
      "INFO:absl:[25] val_loss=0.8493684530258179\n",
      "INFO:absl:[25] test_loss=0.8899251818656921\n",
      "INFO:absl:[26] train_loss=0.5085641145706177, train_x1_loss=0.06950099766254425, train_x2_loss=0.4390622675418854\n",
      "INFO:absl:[26] val_loss=0.8544164896011353\n",
      "INFO:absl:[26] test_loss=0.8993867039680481\n",
      "INFO:absl:[27] train_loss=0.5138771533966064, train_x1_loss=0.06910824030637741, train_x2_loss=0.4447686970233917\n",
      "INFO:absl:[27] val_loss=0.8417524099349976\n",
      "INFO:absl:[27] test_loss=0.8859723806381226\n",
      "INFO:absl:[28] train_loss=0.5101487040519714, train_x1_loss=0.06943178176879883, train_x2_loss=0.4407173991203308\n",
      "INFO:absl:[28] val_loss=0.8091577291488647\n",
      "INFO:absl:[28] test_loss=0.8535996675491333\n",
      "INFO:absl:[29] train_loss=0.507846474647522, train_x1_loss=0.06853103637695312, train_x2_loss=0.43931546807289124\n",
      "INFO:absl:[29] val_loss=0.8106399178504944\n",
      "INFO:absl:[29] test_loss=0.8550431728363037\n",
      "INFO:absl:[30] train_loss=0.5074539184570312, train_x1_loss=0.06803670525550842, train_x2_loss=0.4394175708293915\n",
      "INFO:absl:[30] val_loss=0.8238457441329956\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[30] test_loss=0.866189181804657\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[31] train_loss=0.5047865509986877, train_x1_loss=0.06797697395086288, train_x2_loss=0.43680936098098755\n",
      "INFO:absl:[31] val_loss=0.8012241721153259\n",
      "INFO:absl:[31] test_loss=0.8461652398109436\n",
      "INFO:absl:[32] train_loss=0.5044091939926147, train_x1_loss=0.06922385096549988, train_x2_loss=0.43518438935279846\n",
      "INFO:absl:[32] val_loss=0.7860158085823059\n",
      "INFO:absl:[32] test_loss=0.83130943775177\n",
      "INFO:absl:[33] train_loss=0.5037006735801697, train_x1_loss=0.06792907416820526, train_x2_loss=0.4357720911502838\n",
      "INFO:absl:[33] val_loss=0.7950617074966431\n",
      "INFO:absl:[33] test_loss=0.8404369950294495\n",
      "INFO:absl:[34] train_loss=0.5006561279296875, train_x1_loss=0.06761127710342407, train_x2_loss=0.4330459237098694\n",
      "INFO:absl:[34] val_loss=0.7956418395042419\n",
      "INFO:absl:[34] test_loss=0.8418683409690857\n",
      "INFO:absl:[35] train_loss=0.502510130405426, train_x1_loss=0.06716208904981613, train_x2_loss=0.43534767627716064\n",
      "INFO:absl:[35] val_loss=0.7858461141586304\n",
      "INFO:absl:[35] test_loss=0.828248143196106\n",
      "INFO:absl:[36] train_loss=0.49774906039237976, train_x1_loss=0.0676233246922493, train_x2_loss=0.43012535572052\n",
      "INFO:absl:[36] val_loss=0.7775061726570129\n",
      "INFO:absl:[36] test_loss=0.8254215121269226\n",
      "INFO:absl:[37] train_loss=0.4994710087776184, train_x1_loss=0.06745895743370056, train_x2_loss=0.432012140750885\n",
      "INFO:absl:[37] val_loss=0.808882474899292\n",
      "INFO:absl:[37] test_loss=0.8552383184432983\n",
      "INFO:absl:[38] train_loss=0.49823248386383057, train_x1_loss=0.06786609441041946, train_x2_loss=0.4303671419620514\n",
      "INFO:absl:[38] val_loss=0.797696590423584\n",
      "INFO:absl:[38] test_loss=0.8488191366195679\n",
      "INFO:absl:[39] train_loss=0.4970467984676361, train_x1_loss=0.066338911652565, train_x2_loss=0.43070822954177856\n",
      "INFO:absl:[39] val_loss=0.797885537147522\n",
      "INFO:absl:[39] test_loss=0.8426182270050049\n",
      "INFO:absl:[40] train_loss=0.4950248599052429, train_x1_loss=0.06705819815397263, train_x2_loss=0.42796725034713745\n",
      "INFO:absl:[40] val_loss=0.7524049878120422\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[40] test_loss=0.8018351197242737\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[41] train_loss=0.4952959716320038, train_x1_loss=0.06782622635364532, train_x2_loss=0.4274698793888092\n",
      "INFO:absl:[41] val_loss=0.774766743183136\n",
      "INFO:absl:[41] test_loss=0.8197014927864075\n",
      "INFO:absl:[42] train_loss=0.49360090494155884, train_x1_loss=0.06626614928245544, train_x2_loss=0.4273347556591034\n",
      "INFO:absl:[42] val_loss=0.7958030104637146\n",
      "INFO:absl:[42] test_loss=0.8430973291397095\n",
      "INFO:absl:[43] train_loss=0.49611422419548035, train_x1_loss=0.06729681044816971, train_x2_loss=0.42881670594215393\n",
      "INFO:absl:[43] val_loss=0.769289493560791\n",
      "INFO:absl:[43] test_loss=0.8161302804946899\n",
      "INFO:absl:[44] train_loss=0.49474188685417175, train_x1_loss=0.06767538189888, train_x2_loss=0.42706629633903503\n",
      "INFO:absl:[44] val_loss=0.7726645469665527\n",
      "INFO:absl:[44] test_loss=0.8202909231185913\n",
      "INFO:absl:[45] train_loss=0.4919449985027313, train_x1_loss=0.06678654998540878, train_x2_loss=0.4251587390899658\n",
      "INFO:absl:[45] val_loss=0.78244948387146\n",
      "INFO:absl:[45] test_loss=0.8272554874420166\n",
      "INFO:absl:[46] train_loss=0.49172502756118774, train_x1_loss=0.06615602970123291, train_x2_loss=0.4255690276622772\n",
      "INFO:absl:[46] val_loss=0.7600900530815125\n",
      "INFO:absl:[46] test_loss=0.8057474493980408\n",
      "INFO:absl:[47] train_loss=0.4914068281650543, train_x1_loss=0.06759467720985413, train_x2_loss=0.4238128066062927\n",
      "INFO:absl:[47] val_loss=0.7589176893234253\n",
      "INFO:absl:[47] test_loss=0.8070580959320068\n",
      "INFO:absl:[48] train_loss=0.49038270115852356, train_x1_loss=0.0668165385723114, train_x2_loss=0.42356541752815247\n",
      "INFO:absl:[48] val_loss=0.7549099326133728\n",
      "INFO:absl:[48] test_loss=0.8012953400611877\n",
      "INFO:absl:[49] train_loss=0.48785296082496643, train_x1_loss=0.06545699387788773, train_x2_loss=0.42239511013031006\n",
      "INFO:absl:[49] val_loss=0.7471266388893127\n",
      "INFO:absl:[49] test_loss=0.793490469455719\n",
      "INFO:absl:[50] train_loss=0.4854462444782257, train_x1_loss=0.06563380360603333, train_x2_loss=0.4198119044303894\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[50] val_loss=0.7449270486831665\n",
      "INFO:absl:[50] test_loss=0.7937788367271423\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[51] train_loss=0.4875762164592743, train_x1_loss=0.06733846664428711, train_x2_loss=0.4202366769313812\n",
      "INFO:absl:[51] val_loss=0.7533309459686279\n",
      "INFO:absl:[51] test_loss=0.8026940226554871\n",
      "INFO:absl:[52] train_loss=0.48753395676612854, train_x1_loss=0.06570687890052795, train_x2_loss=0.42182794213294983\n",
      "INFO:absl:[52] val_loss=0.7479714751243591\n",
      "INFO:absl:[52] test_loss=0.8010745644569397\n",
      "INFO:absl:[53] train_loss=0.48646068572998047, train_x1_loss=0.06664204597473145, train_x2_loss=0.41981884837150574\n",
      "INFO:absl:[53] val_loss=0.7397542595863342\n",
      "INFO:absl:[53] test_loss=0.7881619930267334\n",
      "INFO:absl:[54] train_loss=0.4844970703125, train_x1_loss=0.06652692705392838, train_x2_loss=0.4179697334766388\n",
      "INFO:absl:[54] val_loss=0.7435300350189209\n",
      "INFO:absl:[54] test_loss=0.7949934005737305\n",
      "INFO:absl:Setting work unit notes: 3208.6 steps/s, 55.0% (192517/350000), ETA: 0m (1m : 0.0% checkpoint, 12.5% eval)\n",
      "INFO:absl:[192517] steps_per_sec=3208.610955\n",
      "INFO:absl:[55] train_loss=0.48527172207832336, train_x1_loss=0.0679108202457428, train_x2_loss=0.4173603057861328\n",
      "INFO:absl:[55] val_loss=0.7579694986343384\n",
      "INFO:absl:[55] test_loss=0.8048635125160217\n",
      "INFO:absl:[56] train_loss=0.48506367206573486, train_x1_loss=0.06719890236854553, train_x2_loss=0.4178648889064789\n",
      "INFO:absl:[56] val_loss=0.7530043125152588\n",
      "INFO:absl:[56] test_loss=0.805106520652771\n",
      "INFO:absl:[57] train_loss=0.4825771450996399, train_x1_loss=0.06678955256938934, train_x2_loss=0.4157874882221222\n",
      "INFO:absl:[57] val_loss=0.733325719833374\n",
      "INFO:absl:[57] test_loss=0.7836079597473145\n",
      "INFO:absl:[58] train_loss=0.48121345043182373, train_x1_loss=0.06609181314706802, train_x2_loss=0.41512301564216614\n",
      "INFO:absl:[58] val_loss=0.7328134775161743\n",
      "INFO:absl:[58] test_loss=0.7823610305786133\n",
      "INFO:absl:[59] train_loss=0.48262467980384827, train_x1_loss=0.06666921079158783, train_x2_loss=0.4159560203552246\n",
      "INFO:absl:[59] val_loss=0.7327829003334045\n",
      "INFO:absl:[59] test_loss=0.7817608714103699\n",
      "INFO:absl:[60] train_loss=0.4822331666946411, train_x1_loss=0.06621315330266953, train_x2_loss=0.41601914167404175\n",
      "INFO:absl:[60] val_loss=0.7215170860290527\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[60] test_loss=0.7758036851882935\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[61] train_loss=0.4833426773548126, train_x1_loss=0.06698814779520035, train_x2_loss=0.4163549244403839\n",
      "INFO:absl:[61] val_loss=0.7280625104904175\n",
      "INFO:absl:[61] test_loss=0.7801793217658997\n",
      "INFO:absl:[62] train_loss=0.48046979308128357, train_x1_loss=0.06692120432853699, train_x2_loss=0.4135497212409973\n",
      "INFO:absl:[62] val_loss=0.7533310651779175\n",
      "INFO:absl:[62] test_loss=0.8043893575668335\n",
      "INFO:absl:[63] train_loss=0.47819721698760986, train_x1_loss=0.06650703400373459, train_x2_loss=0.4116899073123932\n",
      "INFO:absl:[63] val_loss=0.7286046743392944\n",
      "INFO:absl:[63] test_loss=0.7816579937934875\n",
      "INFO:absl:[64] train_loss=0.4799152612686157, train_x1_loss=0.06782624125480652, train_x2_loss=0.4120884835720062\n",
      "INFO:absl:[64] val_loss=0.718278706073761\n",
      "INFO:absl:[64] test_loss=0.7705264687538147\n",
      "INFO:absl:[65] train_loss=0.4804590046405792, train_x1_loss=0.06663525849580765, train_x2_loss=0.41382303833961487\n",
      "INFO:absl:[65] val_loss=0.7198382019996643\n",
      "INFO:absl:[65] test_loss=0.772667407989502\n",
      "INFO:absl:[66] train_loss=0.47954443097114563, train_x1_loss=0.06843237578868866, train_x2_loss=0.4111112356185913\n",
      "INFO:absl:[66] val_loss=0.7264406681060791\n",
      "INFO:absl:[66] test_loss=0.7764448523521423\n",
      "INFO:absl:[67] train_loss=0.47812703251838684, train_x1_loss=0.06687676161527634, train_x2_loss=0.4112498462200165\n",
      "INFO:absl:[67] val_loss=0.7111191153526306\n",
      "INFO:absl:[67] test_loss=0.7642138004302979\n",
      "INFO:absl:[68] train_loss=0.47691553831100464, train_x1_loss=0.0666593387722969, train_x2_loss=0.41025635600090027\n",
      "INFO:absl:[68] val_loss=0.7186412811279297\n",
      "INFO:absl:[68] test_loss=0.7683151960372925\n",
      "INFO:absl:[69] train_loss=0.4791755974292755, train_x1_loss=0.06803970038890839, train_x2_loss=0.41113534569740295\n",
      "INFO:absl:[69] val_loss=0.7277069091796875\n",
      "INFO:absl:[69] test_loss=0.7832987904548645\n",
      "INFO:absl:[70] train_loss=0.47746986150741577, train_x1_loss=0.06769241392612457, train_x2_loss=0.4097771942615509\n",
      "INFO:absl:[70] val_loss=0.7231230139732361\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[70] test_loss=0.7707640528678894\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[71] train_loss=0.4755292236804962, train_x1_loss=0.06658580899238586, train_x2_loss=0.40894392132759094\n",
      "INFO:absl:[71] val_loss=0.723680317401886\n",
      "INFO:absl:[71] test_loss=0.7758594155311584\n",
      "INFO:absl:[72] train_loss=0.4761103391647339, train_x1_loss=0.06710164248943329, train_x2_loss=0.4090086817741394\n",
      "INFO:absl:[72] val_loss=0.7130621671676636\n",
      "INFO:absl:[72] test_loss=0.7663042545318604\n",
      "INFO:absl:[73] train_loss=0.4755553603172302, train_x1_loss=0.06690769642591476, train_x2_loss=0.4086483120918274\n",
      "INFO:absl:[73] val_loss=0.7281998991966248\n",
      "INFO:absl:[73] test_loss=0.7767019867897034\n",
      "INFO:absl:[74] train_loss=0.4774567782878876, train_x1_loss=0.06755857914686203, train_x2_loss=0.4098978340625763\n",
      "INFO:absl:[74] val_loss=0.7229970097541809\n",
      "INFO:absl:[74] test_loss=0.7756155729293823\n",
      "INFO:absl:[75] train_loss=0.4745755195617676, train_x1_loss=0.0668792724609375, train_x2_loss=0.4076966941356659\n",
      "INFO:absl:[75] val_loss=0.7133736610412598\n",
      "INFO:absl:[75] test_loss=0.7660266160964966\n",
      "INFO:absl:[76] train_loss=0.47492292523384094, train_x1_loss=0.06740507483482361, train_x2_loss=0.40751752257347107\n",
      "INFO:absl:[76] val_loss=0.7266062498092651\n",
      "INFO:absl:[76] test_loss=0.7746891379356384\n",
      "INFO:absl:[77] train_loss=0.47367966175079346, train_x1_loss=0.06796933710575104, train_x2_loss=0.40571099519729614\n",
      "INFO:absl:[77] val_loss=0.7262563109397888\n",
      "INFO:absl:[77] test_loss=0.7747112512588501\n",
      "INFO:absl:[78] train_loss=0.4731996953487396, train_x1_loss=0.06665501743555069, train_x2_loss=0.40654388070106506\n",
      "INFO:absl:[78] val_loss=0.7120789885520935\n",
      "INFO:absl:[78] test_loss=0.7644487023353577\n",
      "INFO:absl:[79] train_loss=0.47304514050483704, train_x1_loss=0.06706767529249191, train_x2_loss=0.4059777557849884\n",
      "INFO:absl:[79] val_loss=0.7148522734642029\n",
      "INFO:absl:[79] test_loss=0.7653353214263916\n",
      "INFO:absl:[80] train_loss=0.473567396402359, train_x1_loss=0.06731045246124268, train_x2_loss=0.4062570035457611\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[80] val_loss=0.7191519141197205\n",
      "INFO:absl:[80] test_loss=0.7686318755149841\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[81] train_loss=0.4724985659122467, train_x1_loss=0.06799948215484619, train_x2_loss=0.4044995605945587\n",
      "INFO:absl:[81] val_loss=0.7053648233413696\n",
      "INFO:absl:[81] test_loss=0.7581955194473267\n",
      "INFO:absl:[82] train_loss=0.4698437750339508, train_x1_loss=0.06544221192598343, train_x2_loss=0.40440189838409424\n",
      "INFO:absl:[82] val_loss=0.7079187035560608\n",
      "INFO:absl:[82] test_loss=0.7601712942123413\n",
      "INFO:absl:[83] train_loss=0.47217607498168945, train_x1_loss=0.06644377112388611, train_x2_loss=0.40573248267173767\n",
      "INFO:absl:[83] val_loss=0.7058546543121338\n",
      "INFO:absl:[83] test_loss=0.7578210234642029\n",
      "INFO:absl:[84] train_loss=0.47416630387306213, train_x1_loss=0.06827090680599213, train_x2_loss=0.4058944880962372\n",
      "INFO:absl:[84] val_loss=0.707440972328186\n",
      "INFO:absl:[84] test_loss=0.7557786107063293\n",
      "INFO:absl:[85] train_loss=0.473120778799057, train_x1_loss=0.06797386705875397, train_x2_loss=0.4051482677459717\n",
      "INFO:absl:[85] val_loss=0.7107523679733276\n",
      "INFO:absl:[85] test_loss=0.7614130973815918\n",
      "INFO:absl:[86] train_loss=0.473375141620636, train_x1_loss=0.06777054816484451, train_x2_loss=0.4056049585342407\n",
      "INFO:absl:[86] val_loss=0.6963798999786377\n",
      "INFO:absl:[86] test_loss=0.7474919557571411\n",
      "INFO:absl:[87] train_loss=0.47070837020874023, train_x1_loss=0.06705789268016815, train_x2_loss=0.4036499261856079\n",
      "INFO:absl:[87] val_loss=0.7093122005462646\n",
      "INFO:absl:[87] test_loss=0.7608774304389954\n",
      "INFO:absl:[88] train_loss=0.4731614887714386, train_x1_loss=0.06766774505376816, train_x2_loss=0.4054945707321167\n",
      "INFO:absl:[88] val_loss=0.7124596238136292\n",
      "INFO:absl:[88] test_loss=0.7641977667808533\n",
      "INFO:absl:[89] train_loss=0.4687749147415161, train_x1_loss=0.06779714673757553, train_x2_loss=0.4009774923324585\n",
      "INFO:absl:[89] val_loss=0.7047780156135559\n",
      "INFO:absl:[89] test_loss=0.7570343613624573\n",
      "INFO:absl:[90] train_loss=0.47080105543136597, train_x1_loss=0.06721842288970947, train_x2_loss=0.40358150005340576\n",
      "INFO:absl:[90] val_loss=0.6972395777702332\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[90] test_loss=0.7503079175949097\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[91] train_loss=0.47298213839530945, train_x1_loss=0.06814245134592056, train_x2_loss=0.4048398435115814\n",
      "INFO:absl:[91] val_loss=0.6980128884315491\n",
      "INFO:absl:[91] test_loss=0.7535489797592163\n",
      "INFO:absl:[92] train_loss=0.4704786539077759, train_x1_loss=0.0665329173207283, train_x2_loss=0.40394479036331177\n",
      "INFO:absl:[92] val_loss=0.7081789374351501\n",
      "INFO:absl:[92] test_loss=0.7586985230445862\n",
      "INFO:absl:[93] train_loss=0.47217226028442383, train_x1_loss=0.06898053735494614, train_x2_loss=0.4031922519207001\n",
      "INFO:absl:[93] val_loss=0.7116166949272156\n",
      "INFO:absl:[93] test_loss=0.7605224847793579\n",
      "INFO:absl:[94] train_loss=0.46782997250556946, train_x1_loss=0.068031907081604, train_x2_loss=0.39979755878448486\n",
      "INFO:absl:[94] val_loss=0.7046517729759216\n",
      "INFO:absl:[94] test_loss=0.7545120716094971\n",
      "INFO:absl:[95] train_loss=0.46682459115982056, train_x1_loss=0.06711972504854202, train_x2_loss=0.3997046947479248\n",
      "INFO:absl:[95] val_loss=0.7019496560096741\n",
      "INFO:absl:[95] test_loss=0.7545645833015442\n",
      "INFO:absl:[96] train_loss=0.47014281153678894, train_x1_loss=0.0680946633219719, train_x2_loss=0.4020494222640991\n",
      "INFO:absl:[96] val_loss=0.7002744674682617\n",
      "INFO:absl:[96] test_loss=0.7514761686325073\n",
      "INFO:absl:[97] train_loss=0.46888649463653564, train_x1_loss=0.06763765960931778, train_x2_loss=0.4012499153614044\n",
      "INFO:absl:[97] val_loss=0.7073265910148621\n",
      "INFO:absl:[97] test_loss=0.7602505683898926\n",
      "INFO:absl:[98] train_loss=0.47124212980270386, train_x1_loss=0.06742279976606369, train_x2_loss=0.40381985902786255\n",
      "INFO:absl:[98] val_loss=0.7012461423873901\n",
      "INFO:absl:[98] test_loss=0.7560427188873291\n",
      "INFO:absl:[99] train_loss=0.46891409158706665, train_x1_loss=0.06704208999872208, train_x2_loss=0.40187233686447144\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[99] val_loss=0.7154982686042786\n",
      "INFO:absl:[99] test_loss=0.7647876739501953\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'relu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 10, 'dropout_rate': 0.013287043114620523, 'edge_features': (8, 8), 'epochs': 100, 'eval_every_epochs': 1, 'fully_connected_edges': 5, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 1, 'layer_norm': False, 'learning_rate': 0.00045346796177033903, 'log_every_epochs': 1, 'max_checkpts_to_keep': None, 'model': 'MLPGraphNetwork', 'momentum': 0.8712873602503628, 'n_blocks': 1, 'n_samples': 5000, 'node_features': (32, 2), 'normalize': True, 'optimizer': 'sgd', 'output_delay': 0, 'output_steps': 4, 'sample_buffer': -4, 'seed': 71, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 120, 'timestep_duration': 3, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| Name                                   | Shape    | Size | Mean     | Std   |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 8)   | 48   | -0.103   | 0.37  |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (8, 8)   | 64   | -0.0189  | 0.344 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (32,)    | 32   | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (19, 32) | 608  | -0.00843 | 0.219 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)     | 2    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (32, 2)  | 64   | -0.00627 | 0.195 |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "Total: 834\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Checkpoint None does not exist.\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.01s.\n",
      "INFO:absl:Starting training.\n",
      "INFO:absl:[0] train_loss=0.8102825880050659, train_x1_loss=0.1603042185306549, train_x2_loss=0.6499772667884827\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[0] val_loss=1.2319321632385254\n",
      "INFO:absl:[0] test_loss=1.2628377676010132\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Created artifact [10] Profile of type ArtifactType.URL and value None.\n",
      "INFO:absl:[1] train_loss=0.6818458437919617, train_x1_loss=0.10518702119588852, train_x2_loss=0.5766589045524597\n",
      "INFO:absl:[1] val_loss=1.1635040044784546\n",
      "INFO:absl:[1] test_loss=1.1963478326797485\n",
      "INFO:absl:[2] train_loss=0.651437520980835, train_x1_loss=0.09693046659231186, train_x2_loss=0.5545088052749634\n",
      "INFO:absl:[2] val_loss=1.121910572052002\n",
      "INFO:absl:[2] test_loss=1.1563599109649658\n",
      "INFO:absl:[3] train_loss=0.6308623552322388, train_x1_loss=0.09098461270332336, train_x2_loss=0.5398764610290527\n",
      "INFO:absl:[3] val_loss=1.0959749221801758\n",
      "INFO:absl:[3] test_loss=1.131935477256775\n",
      "INFO:absl:[4] train_loss=0.6188732385635376, train_x1_loss=0.08858480304479599, train_x2_loss=0.530288815498352\n",
      "INFO:absl:[4] val_loss=1.080173134803772\n",
      "INFO:absl:[4] test_loss=1.1168451309204102\n",
      "INFO:absl:[5] train_loss=0.6105300784111023, train_x1_loss=0.08523046970367432, train_x2_loss=0.5253005623817444\n",
      "INFO:absl:[5] val_loss=1.0780811309814453\n",
      "INFO:absl:[5] test_loss=1.117455244064331\n",
      "INFO:absl:[6] train_loss=0.602841854095459, train_x1_loss=0.08396037667989731, train_x2_loss=0.5188816785812378\n",
      "INFO:absl:[6] val_loss=1.0518122911453247\n",
      "INFO:absl:[6] test_loss=1.0916920900344849\n",
      "INFO:absl:[7] train_loss=0.5976764559745789, train_x1_loss=0.08198182284832001, train_x2_loss=0.5156934857368469\n",
      "INFO:absl:[7] val_loss=1.055629849433899\n",
      "INFO:absl:[7] test_loss=1.094886302947998\n",
      "INFO:absl:[8] train_loss=0.591777503490448, train_x1_loss=0.08087025582790375, train_x2_loss=0.510907769203186\n",
      "INFO:absl:[8] val_loss=1.0601723194122314\n",
      "INFO:absl:[8] test_loss=1.1028891801834106\n",
      "INFO:absl:[9] train_loss=0.5887225866317749, train_x1_loss=0.0806911513209343, train_x2_loss=0.508030891418457\n",
      "INFO:absl:[9] val_loss=1.0183511972427368\n",
      "INFO:absl:[9] test_loss=1.0578696727752686\n",
      "INFO:absl:[10] train_loss=0.5826954245567322, train_x1_loss=0.07899882644414902, train_x2_loss=0.5036972165107727\n",
      "INFO:absl:[10] val_loss=1.0301415920257568\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[10] test_loss=1.0735596418380737\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[11] train_loss=0.5763956904411316, train_x1_loss=0.07746714353561401, train_x2_loss=0.4989289343357086\n",
      "INFO:absl:[11] val_loss=1.011458396911621\n",
      "INFO:absl:[11] test_loss=1.053571343421936\n",
      "INFO:absl:[12] train_loss=0.5722187757492065, train_x1_loss=0.07787728309631348, train_x2_loss=0.4943412244319916\n",
      "INFO:absl:[12] val_loss=1.0248782634735107\n",
      "INFO:absl:[12] test_loss=1.0681051015853882\n",
      "INFO:absl:[13] train_loss=0.567766010761261, train_x1_loss=0.07614710927009583, train_x2_loss=0.4916178584098816\n",
      "INFO:absl:[13] val_loss=1.0006835460662842\n",
      "INFO:absl:[13] test_loss=1.0433584451675415\n",
      "INFO:absl:[14] train_loss=0.562253475189209, train_x1_loss=0.07602310925722122, train_x2_loss=0.48623034358024597\n",
      "INFO:absl:[14] val_loss=0.959926962852478\n",
      "INFO:absl:[14] test_loss=0.9990812540054321\n",
      "INFO:absl:[15] train_loss=0.5526028871536255, train_x1_loss=0.07476437836885452, train_x2_loss=0.4778383672237396\n",
      "INFO:absl:[15] val_loss=0.9566419720649719\n",
      "INFO:absl:[15] test_loss=0.9958279132843018\n",
      "INFO:absl:[16] train_loss=0.5492090582847595, train_x1_loss=0.07368992269039154, train_x2_loss=0.47551774978637695\n",
      "INFO:absl:[16] val_loss=0.9385002255439758\n",
      "INFO:absl:[16] test_loss=0.9763951301574707\n",
      "INFO:absl:[17] train_loss=0.5417308807373047, train_x1_loss=0.07419029623270035, train_x2_loss=0.4675396978855133\n",
      "INFO:absl:[17] val_loss=0.9160966873168945\n",
      "INFO:absl:[17] test_loss=0.9523857235908508\n",
      "INFO:absl:[18] train_loss=0.5395731329917908, train_x1_loss=0.07320369780063629, train_x2_loss=0.4663695693016052\n",
      "INFO:absl:[18] val_loss=0.9202115535736084\n",
      "INFO:absl:[18] test_loss=0.9569275379180908\n",
      "INFO:absl:[19] train_loss=0.5323780179023743, train_x1_loss=0.07230380922555923, train_x2_loss=0.460073858499527\n",
      "INFO:absl:[19] val_loss=0.8863531947135925\n",
      "INFO:absl:[19] test_loss=0.927989661693573\n",
      "INFO:absl:[20] train_loss=0.5300466418266296, train_x1_loss=0.0729740560054779, train_x2_loss=0.4570731818675995\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[20] val_loss=0.9015659093856812\n",
      "INFO:absl:[20] test_loss=0.9444299340248108\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[21] train_loss=0.5250697135925293, train_x1_loss=0.07220731675624847, train_x2_loss=0.4528629779815674\n",
      "INFO:absl:[21] val_loss=0.8886393308639526\n",
      "INFO:absl:[21] test_loss=0.9317560791969299\n",
      "INFO:absl:[22] train_loss=0.5206232070922852, train_x1_loss=0.07082230597734451, train_x2_loss=0.449800580739975\n",
      "INFO:absl:[22] val_loss=0.8751807808876038\n",
      "INFO:absl:[22] test_loss=0.9145599007606506\n",
      "INFO:absl:[23] train_loss=0.5192071795463562, train_x1_loss=0.07023613899946213, train_x2_loss=0.4489712119102478\n",
      "INFO:absl:[23] val_loss=0.8798419237136841\n",
      "INFO:absl:[23] test_loss=0.9210307002067566\n",
      "INFO:absl:[24] train_loss=0.5191568732261658, train_x1_loss=0.07120220363140106, train_x2_loss=0.4479551911354065\n",
      "INFO:absl:[24] val_loss=0.8321791887283325\n",
      "INFO:absl:[24] test_loss=0.8781671524047852\n",
      "INFO:absl:[25] train_loss=0.5157127976417542, train_x1_loss=0.07010654360055923, train_x2_loss=0.445605605840683\n",
      "INFO:absl:[25] val_loss=0.8493684530258179\n",
      "INFO:absl:[25] test_loss=0.8899251818656921\n",
      "INFO:absl:[26] train_loss=0.5085641145706177, train_x1_loss=0.06950099766254425, train_x2_loss=0.4390622675418854\n",
      "INFO:absl:[26] val_loss=0.8544164896011353\n",
      "INFO:absl:[26] test_loss=0.8993867039680481\n",
      "INFO:absl:[27] train_loss=0.5138771533966064, train_x1_loss=0.06910824030637741, train_x2_loss=0.4447686970233917\n",
      "INFO:absl:[27] val_loss=0.8417524099349976\n",
      "INFO:absl:[27] test_loss=0.8859723806381226\n",
      "INFO:absl:[28] train_loss=0.5101487040519714, train_x1_loss=0.06943178176879883, train_x2_loss=0.4407173991203308\n",
      "INFO:absl:[28] val_loss=0.8091577291488647\n",
      "INFO:absl:[28] test_loss=0.8535996675491333\n",
      "INFO:absl:[29] train_loss=0.507846474647522, train_x1_loss=0.06853103637695312, train_x2_loss=0.43931546807289124\n",
      "INFO:absl:[29] val_loss=0.8106399178504944\n",
      "INFO:absl:[29] test_loss=0.8550431728363037\n",
      "INFO:absl:[30] train_loss=0.5074539184570312, train_x1_loss=0.06803670525550842, train_x2_loss=0.4394175708293915\n",
      "INFO:absl:[30] val_loss=0.8238457441329956\n",
      "INFO:absl:[30] test_loss=0.866189181804657\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[31] train_loss=0.5047865509986877, train_x1_loss=0.06797697395086288, train_x2_loss=0.43680936098098755\n",
      "INFO:absl:[31] val_loss=0.8012241721153259\n",
      "INFO:absl:[31] test_loss=0.8461652398109436\n",
      "INFO:absl:[32] train_loss=0.5044091939926147, train_x1_loss=0.06922385096549988, train_x2_loss=0.43518438935279846\n",
      "INFO:absl:[32] val_loss=0.7860158085823059\n",
      "INFO:absl:[32] test_loss=0.83130943775177\n",
      "INFO:absl:[33] train_loss=0.5037006735801697, train_x1_loss=0.06792907416820526, train_x2_loss=0.4357720911502838\n",
      "INFO:absl:[33] val_loss=0.7950617074966431\n",
      "INFO:absl:[33] test_loss=0.8404369950294495\n",
      "INFO:absl:[34] train_loss=0.5006561279296875, train_x1_loss=0.06761127710342407, train_x2_loss=0.4330459237098694\n",
      "INFO:absl:[34] val_loss=0.7956418395042419\n",
      "INFO:absl:[34] test_loss=0.8418683409690857\n",
      "INFO:absl:[35] train_loss=0.502510130405426, train_x1_loss=0.06716208904981613, train_x2_loss=0.43534767627716064\n",
      "INFO:absl:[35] val_loss=0.7858461141586304\n",
      "INFO:absl:[35] test_loss=0.828248143196106\n",
      "INFO:absl:[36] train_loss=0.49774906039237976, train_x1_loss=0.0676233246922493, train_x2_loss=0.43012535572052\n",
      "INFO:absl:[36] val_loss=0.7775061726570129\n",
      "INFO:absl:[36] test_loss=0.8254215121269226\n",
      "INFO:absl:[37] train_loss=0.4994710087776184, train_x1_loss=0.06745895743370056, train_x2_loss=0.432012140750885\n",
      "INFO:absl:[37] val_loss=0.808882474899292\n",
      "INFO:absl:[37] test_loss=0.8552383184432983\n",
      "INFO:absl:[38] train_loss=0.49823248386383057, train_x1_loss=0.06786609441041946, train_x2_loss=0.4303671419620514\n",
      "INFO:absl:[38] val_loss=0.797696590423584\n",
      "INFO:absl:[38] test_loss=0.8488191366195679\n",
      "INFO:absl:[39] train_loss=0.4970467984676361, train_x1_loss=0.066338911652565, train_x2_loss=0.43070822954177856\n",
      "INFO:absl:[39] val_loss=0.797885537147522\n",
      "INFO:absl:[39] test_loss=0.8426182270050049\n",
      "INFO:absl:[40] train_loss=0.4950248599052429, train_x1_loss=0.06705819815397263, train_x2_loss=0.42796725034713745\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[40] val_loss=0.7524049878120422\n",
      "INFO:absl:[40] test_loss=0.8018351197242737\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[41] train_loss=0.4952959716320038, train_x1_loss=0.06782622635364532, train_x2_loss=0.4274698793888092\n",
      "INFO:absl:[41] val_loss=0.774766743183136\n",
      "INFO:absl:[41] test_loss=0.8197014927864075\n",
      "INFO:absl:[42] train_loss=0.49360090494155884, train_x1_loss=0.06626614928245544, train_x2_loss=0.4273347556591034\n",
      "INFO:absl:[42] val_loss=0.7958030104637146\n",
      "INFO:absl:[42] test_loss=0.8430973291397095\n",
      "INFO:absl:[43] train_loss=0.49611422419548035, train_x1_loss=0.06729681044816971, train_x2_loss=0.42881670594215393\n",
      "INFO:absl:[43] val_loss=0.769289493560791\n",
      "INFO:absl:[43] test_loss=0.8161302804946899\n",
      "INFO:absl:[44] train_loss=0.49474188685417175, train_x1_loss=0.06767538189888, train_x2_loss=0.42706629633903503\n",
      "INFO:absl:[44] val_loss=0.7726645469665527\n",
      "INFO:absl:[44] test_loss=0.8202909231185913\n",
      "INFO:absl:[45] train_loss=0.4919449985027313, train_x1_loss=0.06678654998540878, train_x2_loss=0.4251587390899658\n",
      "INFO:absl:[45] val_loss=0.78244948387146\n",
      "INFO:absl:[45] test_loss=0.8272554874420166\n",
      "INFO:absl:[46] train_loss=0.49172502756118774, train_x1_loss=0.06615602970123291, train_x2_loss=0.4255690276622772\n",
      "INFO:absl:[46] val_loss=0.7600900530815125\n",
      "INFO:absl:[46] test_loss=0.8057474493980408\n",
      "INFO:absl:[47] train_loss=0.4914068281650543, train_x1_loss=0.06759467720985413, train_x2_loss=0.4238128066062927\n",
      "INFO:absl:[47] val_loss=0.7589176893234253\n",
      "INFO:absl:[47] test_loss=0.8070580959320068\n",
      "INFO:absl:[48] train_loss=0.49038270115852356, train_x1_loss=0.0668165385723114, train_x2_loss=0.42356541752815247\n",
      "INFO:absl:[48] val_loss=0.7549099326133728\n",
      "INFO:absl:[48] test_loss=0.8012953400611877\n",
      "INFO:absl:[49] train_loss=0.48785296082496643, train_x1_loss=0.06545699387788773, train_x2_loss=0.42239511013031006\n",
      "INFO:absl:[49] val_loss=0.7471266388893127\n",
      "INFO:absl:[49] test_loss=0.793490469455719\n",
      "INFO:absl:[50] train_loss=0.4854462444782257, train_x1_loss=0.06563380360603333, train_x2_loss=0.4198119044303894\n",
      "INFO:absl:[50] val_loss=0.7449270486831665\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[50] test_loss=0.7937788367271423\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[51] train_loss=0.4875762164592743, train_x1_loss=0.06733846664428711, train_x2_loss=0.4202366769313812\n",
      "INFO:absl:[51] val_loss=0.7533309459686279\n",
      "INFO:absl:[51] test_loss=0.8026940226554871\n",
      "INFO:absl:[52] train_loss=0.48753395676612854, train_x1_loss=0.06570687890052795, train_x2_loss=0.42182794213294983\n",
      "INFO:absl:[52] val_loss=0.7479714751243591\n",
      "INFO:absl:[52] test_loss=0.8010745644569397\n",
      "INFO:absl:[53] train_loss=0.48646068572998047, train_x1_loss=0.06664204597473145, train_x2_loss=0.41981884837150574\n",
      "INFO:absl:[53] val_loss=0.7397542595863342\n",
      "INFO:absl:[53] test_loss=0.7881619930267334\n",
      "INFO:absl:Setting work unit notes: 3155.1 steps/s, 54.1% (189305/350000), ETA: 0m (1m : 0.1% checkpoint, 12.5% eval)\n",
      "INFO:absl:[189305] steps_per_sec=3155.079384\n",
      "INFO:absl:[54] train_loss=0.4844970703125, train_x1_loss=0.06652692705392838, train_x2_loss=0.4179697334766388\n",
      "INFO:absl:[54] val_loss=0.7435300350189209\n",
      "INFO:absl:[54] test_loss=0.7949934005737305\n",
      "INFO:absl:[55] train_loss=0.48527172207832336, train_x1_loss=0.0679108202457428, train_x2_loss=0.4173603057861328\n",
      "INFO:absl:[55] val_loss=0.7579694986343384\n",
      "INFO:absl:[55] test_loss=0.8048635125160217\n",
      "INFO:absl:[56] train_loss=0.48506367206573486, train_x1_loss=0.06719890236854553, train_x2_loss=0.4178648889064789\n",
      "INFO:absl:[56] val_loss=0.7530043125152588\n",
      "INFO:absl:[56] test_loss=0.805106520652771\n",
      "INFO:absl:[57] train_loss=0.4825771450996399, train_x1_loss=0.06678955256938934, train_x2_loss=0.4157874882221222\n",
      "INFO:absl:[57] val_loss=0.733325719833374\n",
      "INFO:absl:[57] test_loss=0.7836079597473145\n",
      "INFO:absl:[58] train_loss=0.48121345043182373, train_x1_loss=0.06609181314706802, train_x2_loss=0.41512301564216614\n",
      "INFO:absl:[58] val_loss=0.7328134775161743\n",
      "INFO:absl:[58] test_loss=0.7823610305786133\n",
      "INFO:absl:[59] train_loss=0.48262467980384827, train_x1_loss=0.06666921079158783, train_x2_loss=0.4159560203552246\n",
      "INFO:absl:[59] val_loss=0.7327829003334045\n",
      "INFO:absl:[59] test_loss=0.7817608714103699\n",
      "INFO:absl:[60] train_loss=0.4822331666946411, train_x1_loss=0.06621315330266953, train_x2_loss=0.41601914167404175\n",
      "INFO:absl:[60] val_loss=0.7215170860290527\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[60] test_loss=0.7758036851882935\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[61] train_loss=0.4833426773548126, train_x1_loss=0.06698814779520035, train_x2_loss=0.4163549244403839\n",
      "INFO:absl:[61] val_loss=0.7280625104904175\n",
      "INFO:absl:[61] test_loss=0.7801793217658997\n",
      "INFO:absl:[62] train_loss=0.48046979308128357, train_x1_loss=0.06692120432853699, train_x2_loss=0.4135497212409973\n",
      "INFO:absl:[62] val_loss=0.7533310651779175\n",
      "INFO:absl:[62] test_loss=0.8043893575668335\n",
      "INFO:absl:[63] train_loss=0.47819721698760986, train_x1_loss=0.06650703400373459, train_x2_loss=0.4116899073123932\n",
      "INFO:absl:[63] val_loss=0.7286046743392944\n",
      "INFO:absl:[63] test_loss=0.7816579937934875\n",
      "INFO:absl:[64] train_loss=0.4799152612686157, train_x1_loss=0.06782624125480652, train_x2_loss=0.4120884835720062\n",
      "INFO:absl:[64] val_loss=0.718278706073761\n",
      "INFO:absl:[64] test_loss=0.7705264687538147\n",
      "INFO:absl:[65] train_loss=0.4804590046405792, train_x1_loss=0.06663525849580765, train_x2_loss=0.41382303833961487\n",
      "INFO:absl:[65] val_loss=0.7198382019996643\n",
      "INFO:absl:[65] test_loss=0.772667407989502\n",
      "INFO:absl:[66] train_loss=0.47954443097114563, train_x1_loss=0.06843237578868866, train_x2_loss=0.4111112356185913\n",
      "INFO:absl:[66] val_loss=0.7264406681060791\n",
      "INFO:absl:[66] test_loss=0.7764448523521423\n",
      "INFO:absl:[67] train_loss=0.47812703251838684, train_x1_loss=0.06687676161527634, train_x2_loss=0.4112498462200165\n",
      "INFO:absl:[67] val_loss=0.7111191153526306\n",
      "INFO:absl:[67] test_loss=0.7642138004302979\n",
      "INFO:absl:[68] train_loss=0.47691553831100464, train_x1_loss=0.0666593387722969, train_x2_loss=0.41025635600090027\n",
      "INFO:absl:[68] val_loss=0.7186412811279297\n",
      "INFO:absl:[68] test_loss=0.7683151960372925\n",
      "INFO:absl:[69] train_loss=0.4791755974292755, train_x1_loss=0.06803970038890839, train_x2_loss=0.41113534569740295\n",
      "INFO:absl:[69] val_loss=0.7277069091796875\n",
      "INFO:absl:[69] test_loss=0.7832987904548645\n",
      "INFO:absl:[70] train_loss=0.47746986150741577, train_x1_loss=0.06769241392612457, train_x2_loss=0.4097771942615509\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[70] val_loss=0.7231230139732361\n",
      "INFO:absl:[70] test_loss=0.7707640528678894\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[71] train_loss=0.4755292236804962, train_x1_loss=0.06658580899238586, train_x2_loss=0.40894392132759094\n",
      "INFO:absl:[71] val_loss=0.723680317401886\n",
      "INFO:absl:[71] test_loss=0.7758594155311584\n",
      "INFO:absl:[72] train_loss=0.4761103391647339, train_x1_loss=0.06710164248943329, train_x2_loss=0.4090086817741394\n",
      "INFO:absl:[72] val_loss=0.7130621671676636\n",
      "INFO:absl:[72] test_loss=0.7663042545318604\n",
      "INFO:absl:[73] train_loss=0.4755553603172302, train_x1_loss=0.06690769642591476, train_x2_loss=0.4086483120918274\n",
      "INFO:absl:[73] val_loss=0.7281998991966248\n",
      "INFO:absl:[73] test_loss=0.7767019867897034\n",
      "INFO:absl:[74] train_loss=0.4774567782878876, train_x1_loss=0.06755857914686203, train_x2_loss=0.4098978340625763\n",
      "INFO:absl:[74] val_loss=0.7229970097541809\n",
      "INFO:absl:[74] test_loss=0.7756155729293823\n",
      "INFO:absl:[75] train_loss=0.4745755195617676, train_x1_loss=0.0668792724609375, train_x2_loss=0.4076966941356659\n",
      "INFO:absl:[75] val_loss=0.7133736610412598\n",
      "INFO:absl:[75] test_loss=0.7660266160964966\n",
      "INFO:absl:[76] train_loss=0.47492292523384094, train_x1_loss=0.06740507483482361, train_x2_loss=0.40751752257347107\n",
      "INFO:absl:[76] val_loss=0.7266062498092651\n",
      "INFO:absl:[76] test_loss=0.7746891379356384\n",
      "INFO:absl:[77] train_loss=0.47367966175079346, train_x1_loss=0.06796933710575104, train_x2_loss=0.40571099519729614\n",
      "INFO:absl:[77] val_loss=0.7262563109397888\n",
      "INFO:absl:[77] test_loss=0.7747112512588501\n",
      "INFO:absl:[78] train_loss=0.4731996953487396, train_x1_loss=0.06665501743555069, train_x2_loss=0.40654388070106506\n",
      "INFO:absl:[78] val_loss=0.7120789885520935\n",
      "INFO:absl:[78] test_loss=0.7644487023353577\n",
      "INFO:absl:[79] train_loss=0.47304514050483704, train_x1_loss=0.06706767529249191, train_x2_loss=0.4059777557849884\n",
      "INFO:absl:[79] val_loss=0.7148522734642029\n",
      "INFO:absl:[79] test_loss=0.7653353214263916\n",
      "INFO:absl:[80] train_loss=0.473567396402359, train_x1_loss=0.06731045246124268, train_x2_loss=0.4062570035457611\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[80] val_loss=0.7191519141197205\n",
      "INFO:absl:[80] test_loss=0.7686318755149841\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[81] train_loss=0.4724985659122467, train_x1_loss=0.06799948215484619, train_x2_loss=0.4044995605945587\n",
      "INFO:absl:[81] val_loss=0.7053648233413696\n",
      "INFO:absl:[81] test_loss=0.7581955194473267\n",
      "INFO:absl:[82] train_loss=0.4698437750339508, train_x1_loss=0.06544221192598343, train_x2_loss=0.40440189838409424\n",
      "INFO:absl:[82] val_loss=0.7079187035560608\n",
      "INFO:absl:[82] test_loss=0.7601712942123413\n",
      "INFO:absl:[83] train_loss=0.47217607498168945, train_x1_loss=0.06644377112388611, train_x2_loss=0.40573248267173767\n",
      "INFO:absl:[83] val_loss=0.7058546543121338\n",
      "INFO:absl:[83] test_loss=0.7578210234642029\n",
      "INFO:absl:[84] train_loss=0.47416630387306213, train_x1_loss=0.06827090680599213, train_x2_loss=0.4058944880962372\n",
      "INFO:absl:[84] val_loss=0.707440972328186\n",
      "INFO:absl:[84] test_loss=0.7557786107063293\n",
      "INFO:absl:[85] train_loss=0.473120778799057, train_x1_loss=0.06797386705875397, train_x2_loss=0.4051482677459717\n",
      "INFO:absl:[85] val_loss=0.7107523679733276\n",
      "INFO:absl:[85] test_loss=0.7614130973815918\n",
      "INFO:absl:[86] train_loss=0.473375141620636, train_x1_loss=0.06777054816484451, train_x2_loss=0.4056049585342407\n",
      "INFO:absl:[86] val_loss=0.6963798999786377\n",
      "INFO:absl:[86] test_loss=0.7474919557571411\n",
      "INFO:absl:[87] train_loss=0.47070837020874023, train_x1_loss=0.06705789268016815, train_x2_loss=0.4036499261856079\n",
      "INFO:absl:[87] val_loss=0.7093122005462646\n",
      "INFO:absl:[87] test_loss=0.7608774304389954\n",
      "INFO:absl:[88] train_loss=0.4731614887714386, train_x1_loss=0.06766774505376816, train_x2_loss=0.4054945707321167\n",
      "INFO:absl:[88] val_loss=0.7124596238136292\n",
      "INFO:absl:[88] test_loss=0.7641977667808533\n",
      "INFO:absl:[89] train_loss=0.4687749147415161, train_x1_loss=0.06779714673757553, train_x2_loss=0.4009774923324585\n",
      "INFO:absl:[89] val_loss=0.7047780156135559\n",
      "INFO:absl:[89] test_loss=0.7570343613624573\n",
      "INFO:absl:[90] train_loss=0.47080105543136597, train_x1_loss=0.06721842288970947, train_x2_loss=0.40358150005340576\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[90] val_loss=0.6972395777702332\n",
      "INFO:absl:[90] test_loss=0.7503079175949097\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[91] train_loss=0.47298213839530945, train_x1_loss=0.06814245134592056, train_x2_loss=0.4048398435115814\n",
      "INFO:absl:[91] val_loss=0.6980128884315491\n",
      "INFO:absl:[91] test_loss=0.7535489797592163\n",
      "INFO:absl:[92] train_loss=0.4704786539077759, train_x1_loss=0.0665329173207283, train_x2_loss=0.40394479036331177\n",
      "INFO:absl:[92] val_loss=0.7081789374351501\n",
      "INFO:absl:[92] test_loss=0.7586985230445862\n",
      "INFO:absl:[93] train_loss=0.47217226028442383, train_x1_loss=0.06898053735494614, train_x2_loss=0.4031922519207001\n",
      "INFO:absl:[93] val_loss=0.7116166949272156\n",
      "INFO:absl:[93] test_loss=0.7605224847793579\n",
      "INFO:absl:[94] train_loss=0.46782997250556946, train_x1_loss=0.068031907081604, train_x2_loss=0.39979755878448486\n",
      "INFO:absl:[94] val_loss=0.7046517729759216\n",
      "INFO:absl:[94] test_loss=0.7545120716094971\n",
      "INFO:absl:[95] train_loss=0.46682459115982056, train_x1_loss=0.06711972504854202, train_x2_loss=0.3997046947479248\n",
      "INFO:absl:[95] val_loss=0.7019496560096741\n",
      "INFO:absl:[95] test_loss=0.7545645833015442\n",
      "INFO:absl:[96] train_loss=0.47014281153678894, train_x1_loss=0.0680946633219719, train_x2_loss=0.4020494222640991\n",
      "INFO:absl:[96] val_loss=0.7002744674682617\n",
      "INFO:absl:[96] test_loss=0.7514761686325073\n",
      "INFO:absl:[97] train_loss=0.46888649463653564, train_x1_loss=0.06763765960931778, train_x2_loss=0.4012499153614044\n",
      "INFO:absl:[97] val_loss=0.7073265910148621\n",
      "INFO:absl:[97] test_loss=0.7602505683898926\n",
      "INFO:absl:[98] train_loss=0.47124212980270386, train_x1_loss=0.06742279976606369, train_x2_loss=0.40381985902786255\n",
      "INFO:absl:[98] val_loss=0.7012461423873901\n",
      "INFO:absl:[98] test_loss=0.7560427188873291\n",
      "INFO:absl:[99] train_loss=0.46891409158706665, train_x1_loss=0.06704208999872208, train_x2_loss=0.40187233686447144\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[99] val_loss=0.7154982686042786\n",
      "INFO:absl:[99] test_loss=0.7647876739501953\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'relu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 10, 'dropout_rate': 0.013287043114620523, 'edge_features': (8, 8), 'epochs': 100, 'eval_every_epochs': 1, 'fully_connected_edges': 5, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 1, 'layer_norm': False, 'learning_rate': 0.00045346796177033903, 'log_every_epochs': 1, 'max_checkpts_to_keep': None, 'model': 'MLPGraphNetwork', 'momentum': 0.8712873602503628, 'n_blocks': 1, 'n_samples': 5000, 'node_features': (32, 2), 'normalize': True, 'optimizer': 'sgd', 'output_delay': 0, 'output_steps': 4, 'sample_buffer': -4, 'seed': 72, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 120, 'timestep_duration': 3, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| Name                                   | Shape    | Size | Mean     | Std   |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 8)   | 48   | -0.103   | 0.37  |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (8, 8)   | 64   | -0.0189  | 0.344 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (32,)    | 32   | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (19, 32) | 608  | -0.00843 | 0.219 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)     | 2    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (32, 2)  | 64   | -0.00627 | 0.195 |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "Total: 834\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Checkpoint None does not exist.\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.00s.\n",
      "INFO:absl:Starting training.\n",
      "INFO:absl:[0] train_loss=0.7879564762115479, train_x1_loss=0.1572842299938202, train_x2_loss=0.6306717395782471\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[0] val_loss=1.300944209098816\n",
      "INFO:absl:[0] test_loss=1.2409213781356812\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Created artifact [10] Profile of type ArtifactType.URL and value None.\n",
      "INFO:absl:[1] train_loss=0.6687402129173279, train_x1_loss=0.10289494693279266, train_x2_loss=0.565844714641571\n",
      "INFO:absl:[1] val_loss=1.2325239181518555\n",
      "INFO:absl:[1] test_loss=1.1628456115722656\n",
      "INFO:absl:[2] train_loss=0.6381946802139282, train_x1_loss=0.09360960870981216, train_x2_loss=0.5445855259895325\n",
      "INFO:absl:[2] val_loss=1.1838046312332153\n",
      "INFO:absl:[2] test_loss=1.1166059970855713\n",
      "INFO:absl:[3] train_loss=0.6159093379974365, train_x1_loss=0.08712738007307053, train_x2_loss=0.5287812948226929\n",
      "INFO:absl:[3] val_loss=1.149743676185608\n",
      "INFO:absl:[3] test_loss=1.076612949371338\n",
      "INFO:absl:[4] train_loss=0.5995180606842041, train_x1_loss=0.08282773941755295, train_x2_loss=0.5166905522346497\n",
      "INFO:absl:[4] val_loss=1.1378287076950073\n",
      "INFO:absl:[4] test_loss=1.0660464763641357\n",
      "INFO:absl:[5] train_loss=0.5929545164108276, train_x1_loss=0.07902926206588745, train_x2_loss=0.5139265656471252\n",
      "INFO:absl:[5] val_loss=1.1259982585906982\n",
      "INFO:absl:[5] test_loss=1.052616000175476\n",
      "INFO:absl:[6] train_loss=0.5874188542366028, train_x1_loss=0.07827332615852356, train_x2_loss=0.50914466381073\n",
      "INFO:absl:[6] val_loss=1.108422040939331\n",
      "INFO:absl:[6] test_loss=1.0326508283615112\n",
      "INFO:absl:[7] train_loss=0.5796712040901184, train_x1_loss=0.07525777816772461, train_x2_loss=0.5044140219688416\n",
      "INFO:absl:[7] val_loss=1.1137632131576538\n",
      "INFO:absl:[7] test_loss=1.040555715560913\n",
      "INFO:absl:[8] train_loss=0.5766838192939758, train_x1_loss=0.07389814406633377, train_x2_loss=0.5027858018875122\n",
      "INFO:absl:[8] val_loss=1.10455322265625\n",
      "INFO:absl:[8] test_loss=1.0356495380401611\n",
      "INFO:absl:[9] train_loss=0.5748523473739624, train_x1_loss=0.07423929125070572, train_x2_loss=0.500613272190094\n",
      "INFO:absl:[9] val_loss=1.0973360538482666\n",
      "INFO:absl:[9] test_loss=1.0168997049331665\n",
      "INFO:absl:[10] train_loss=0.5693510174751282, train_x1_loss=0.07196317613124847, train_x2_loss=0.4973873198032379\n",
      "INFO:absl:[10] val_loss=1.0979539155960083\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[10] test_loss=1.0298537015914917\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[11] train_loss=0.565520703792572, train_x1_loss=0.07124276459217072, train_x2_loss=0.49427831172943115\n",
      "INFO:absl:[11] val_loss=1.0754259824752808\n",
      "INFO:absl:[11] test_loss=1.0040364265441895\n",
      "INFO:absl:[12] train_loss=0.563113272190094, train_x1_loss=0.07199342548847198, train_x2_loss=0.4911206066608429\n",
      "INFO:absl:[12] val_loss=1.0847560167312622\n",
      "INFO:absl:[12] test_loss=1.01418936252594\n",
      "INFO:absl:[13] train_loss=0.5580884218215942, train_x1_loss=0.06946475058794022, train_x2_loss=0.48862388730049133\n",
      "INFO:absl:[13] val_loss=1.0703848600387573\n",
      "INFO:absl:[13] test_loss=1.0023750066757202\n",
      "INFO:absl:[14] train_loss=0.5548124313354492, train_x1_loss=0.06940311938524246, train_x2_loss=0.4854089617729187\n",
      "INFO:absl:[14] val_loss=1.056727647781372\n",
      "INFO:absl:[14] test_loss=0.9812108874320984\n",
      "INFO:absl:[15] train_loss=0.5490736365318298, train_x1_loss=0.06935133039951324, train_x2_loss=0.479722261428833\n",
      "INFO:absl:[15] val_loss=1.0399575233459473\n",
      "INFO:absl:[15] test_loss=0.9635155200958252\n",
      "INFO:absl:[16] train_loss=0.5445602536201477, train_x1_loss=0.06808660179376602, train_x2_loss=0.47647401690483093\n",
      "INFO:absl:[16] val_loss=1.0415782928466797\n",
      "INFO:absl:[16] test_loss=0.9639281630516052\n",
      "INFO:absl:[17] train_loss=0.5423423051834106, train_x1_loss=0.067997045814991, train_x2_loss=0.47434478998184204\n",
      "INFO:absl:[17] val_loss=1.0177836418151855\n",
      "INFO:absl:[17] test_loss=0.9478089213371277\n",
      "INFO:absl:[18] train_loss=0.5390299558639526, train_x1_loss=0.06768114119768143, train_x2_loss=0.47134873270988464\n",
      "INFO:absl:[18] val_loss=1.011348009109497\n",
      "INFO:absl:[18] test_loss=0.9372502565383911\n",
      "INFO:absl:[19] train_loss=0.5357971787452698, train_x1_loss=0.06791061162948608, train_x2_loss=0.4678873121738434\n",
      "INFO:absl:[19] val_loss=0.9997008442878723\n",
      "INFO:absl:[19] test_loss=0.9305703639984131\n",
      "INFO:absl:[20] train_loss=0.5342479944229126, train_x1_loss=0.06839711219072342, train_x2_loss=0.46585169434547424\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[20] val_loss=0.9906303286552429\n",
      "INFO:absl:[20] test_loss=0.9217677116394043\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[21] train_loss=0.5285425186157227, train_x1_loss=0.06706167012453079, train_x2_loss=0.4614812135696411\n",
      "INFO:absl:[21] val_loss=0.9936912655830383\n",
      "INFO:absl:[21] test_loss=0.9265419244766235\n",
      "INFO:absl:[22] train_loss=0.5273927450180054, train_x1_loss=0.06731313467025757, train_x2_loss=0.4600789248943329\n",
      "INFO:absl:[22] val_loss=0.9730045795440674\n",
      "INFO:absl:[22] test_loss=0.9065281748771667\n",
      "INFO:absl:[23] train_loss=0.5241344571113586, train_x1_loss=0.06727233529090881, train_x2_loss=0.45686307549476624\n",
      "INFO:absl:[23] val_loss=0.9716306328773499\n",
      "INFO:absl:[23] test_loss=0.9084006547927856\n",
      "INFO:absl:[24] train_loss=0.5210789442062378, train_x1_loss=0.06718714535236359, train_x2_loss=0.4538918435573578\n",
      "INFO:absl:[24] val_loss=0.9866753816604614\n",
      "INFO:absl:[24] test_loss=0.9261049032211304\n",
      "INFO:absl:[25] train_loss=0.5215332508087158, train_x1_loss=0.06814632564783096, train_x2_loss=0.4533865749835968\n",
      "INFO:absl:[25] val_loss=0.9560102820396423\n",
      "INFO:absl:[25] test_loss=0.8940679430961609\n",
      "INFO:absl:[26] train_loss=0.517745852470398, train_x1_loss=0.06600234657526016, train_x2_loss=0.45174288749694824\n",
      "INFO:absl:[26] val_loss=0.9632455706596375\n",
      "INFO:absl:[26] test_loss=0.9041929841041565\n",
      "INFO:absl:[27] train_loss=0.515235185623169, train_x1_loss=0.06724504381418228, train_x2_loss=0.44799044728279114\n",
      "INFO:absl:[27] val_loss=0.9354280829429626\n",
      "INFO:absl:[27] test_loss=0.8728707432746887\n",
      "INFO:absl:[28] train_loss=0.5114153027534485, train_x1_loss=0.0665341466665268, train_x2_loss=0.4448811709880829\n",
      "INFO:absl:[28] val_loss=0.93721604347229\n",
      "INFO:absl:[28] test_loss=0.8795276880264282\n",
      "INFO:absl:[29] train_loss=0.5097278356552124, train_x1_loss=0.06727097183465958, train_x2_loss=0.4424566328525543\n",
      "INFO:absl:[29] val_loss=0.9592321515083313\n",
      "INFO:absl:[29] test_loss=0.9055536985397339\n",
      "INFO:absl:[30] train_loss=0.5057649612426758, train_x1_loss=0.06479823589324951, train_x2_loss=0.44096800684928894\n",
      "INFO:absl:[30] val_loss=0.9543601870536804\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[30] test_loss=0.8990254998207092\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[31] train_loss=0.5049313902854919, train_x1_loss=0.06605445593595505, train_x2_loss=0.43887799978256226\n",
      "INFO:absl:[31] val_loss=0.922468364238739\n",
      "INFO:absl:[31] test_loss=0.8657490611076355\n",
      "INFO:absl:[32] train_loss=0.506066083908081, train_x1_loss=0.06707102805376053, train_x2_loss=0.4389936029911041\n",
      "INFO:absl:[32] val_loss=0.9110449552536011\n",
      "INFO:absl:[32] test_loss=0.856636106967926\n",
      "INFO:absl:[33] train_loss=0.4997099041938782, train_x1_loss=0.06597655266523361, train_x2_loss=0.43373340368270874\n",
      "INFO:absl:[33] val_loss=0.9075077176094055\n",
      "INFO:absl:[33] test_loss=0.8557698130607605\n",
      "INFO:absl:[34] train_loss=0.49923402070999146, train_x1_loss=0.06631889194250107, train_x2_loss=0.43291524052619934\n",
      "INFO:absl:[34] val_loss=0.9273950457572937\n",
      "INFO:absl:[34] test_loss=0.8814737200737\n",
      "INFO:absl:[35] train_loss=0.49560990929603577, train_x1_loss=0.06569736450910568, train_x2_loss=0.42991212010383606\n",
      "INFO:absl:[35] val_loss=0.9043401479721069\n",
      "INFO:absl:[35] test_loss=0.8602348566055298\n",
      "INFO:absl:[36] train_loss=0.4900038242340088, train_x1_loss=0.06564736366271973, train_x2_loss=0.4243571162223816\n",
      "INFO:absl:[36] val_loss=0.9261664152145386\n",
      "INFO:absl:[36] test_loss=0.8860592246055603\n",
      "INFO:absl:[37] train_loss=0.4930175840854645, train_x1_loss=0.0660780817270279, train_x2_loss=0.4269402325153351\n",
      "INFO:absl:[37] val_loss=0.893570065498352\n",
      "INFO:absl:[37] test_loss=0.8441584706306458\n",
      "INFO:absl:[38] train_loss=0.4909438192844391, train_x1_loss=0.06576995551586151, train_x2_loss=0.42517292499542236\n",
      "INFO:absl:[38] val_loss=0.919479489326477\n",
      "INFO:absl:[38] test_loss=0.8780232071876526\n",
      "INFO:absl:[39] train_loss=0.48824188113212585, train_x1_loss=0.06686361134052277, train_x2_loss=0.4213785231113434\n",
      "INFO:absl:[39] val_loss=0.8774479031562805\n",
      "INFO:absl:[39] test_loss=0.8272021412849426\n",
      "INFO:absl:[40] train_loss=0.48649585247039795, train_x1_loss=0.06522401422262192, train_x2_loss=0.42127183079719543\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[40] val_loss=0.8565982580184937\n",
      "INFO:absl:[40] test_loss=0.8117380142211914\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[41] train_loss=0.48417097330093384, train_x1_loss=0.06647665798664093, train_x2_loss=0.41769394278526306\n",
      "INFO:absl:[41] val_loss=0.8845135569572449\n",
      "INFO:absl:[41] test_loss=0.8387050628662109\n",
      "INFO:absl:[42] train_loss=0.4838241636753082, train_x1_loss=0.06540042906999588, train_x2_loss=0.4184240400791168\n",
      "INFO:absl:[42] val_loss=0.8903008103370667\n",
      "INFO:absl:[42] test_loss=0.8439828753471375\n",
      "INFO:absl:[43] train_loss=0.48387742042541504, train_x1_loss=0.06588106602430344, train_x2_loss=0.4179966151714325\n",
      "INFO:absl:[43] val_loss=0.8723998069763184\n",
      "INFO:absl:[43] test_loss=0.8321328163146973\n",
      "INFO:absl:[44] train_loss=0.4801000654697418, train_x1_loss=0.06572947651147842, train_x2_loss=0.41437041759490967\n",
      "INFO:absl:[44] val_loss=0.8748789429664612\n",
      "INFO:absl:[44] test_loss=0.829920768737793\n",
      "INFO:absl:[45] train_loss=0.47964105010032654, train_x1_loss=0.06511648744344711, train_x2_loss=0.4145243465900421\n",
      "INFO:absl:[45] val_loss=0.9053325057029724\n",
      "INFO:absl:[45] test_loss=0.8676667213439941\n",
      "INFO:absl:[46] train_loss=0.48065879940986633, train_x1_loss=0.06522347778081894, train_x2_loss=0.415435254573822\n",
      "INFO:absl:[46] val_loss=0.8562140464782715\n",
      "INFO:absl:[46] test_loss=0.812488853931427\n",
      "INFO:absl:[47] train_loss=0.4810572564601898, train_x1_loss=0.06506625562906265, train_x2_loss=0.4159909188747406\n",
      "INFO:absl:[47] val_loss=0.8708195686340332\n",
      "INFO:absl:[47] test_loss=0.8311086297035217\n",
      "INFO:absl:[48] train_loss=0.4776135981082916, train_x1_loss=0.06487223505973816, train_x2_loss=0.4127419888973236\n",
      "INFO:absl:[48] val_loss=0.8793875575065613\n",
      "INFO:absl:[48] test_loss=0.8473919630050659\n",
      "INFO:absl:[49] train_loss=0.47820061445236206, train_x1_loss=0.06556633859872818, train_x2_loss=0.41263267397880554\n",
      "INFO:absl:[49] val_loss=0.8943526148796082\n",
      "INFO:absl:[49] test_loss=0.8627099394798279\n",
      "INFO:absl:[50] train_loss=0.47741323709487915, train_x1_loss=0.06551047414541245, train_x2_loss=0.4119029641151428\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[50] val_loss=0.8659083247184753\n",
      "INFO:absl:[50] test_loss=0.8320186734199524\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[51] train_loss=0.47397899627685547, train_x1_loss=0.06466703116893768, train_x2_loss=0.4093131721019745\n",
      "INFO:absl:[51] val_loss=0.8550474047660828\n",
      "INFO:absl:[51] test_loss=0.8142732977867126\n",
      "INFO:absl:[52] train_loss=0.47459936141967773, train_x1_loss=0.06347008049488068, train_x2_loss=0.4111293852329254\n",
      "INFO:absl:[52] val_loss=0.8820192813873291\n",
      "INFO:absl:[52] test_loss=0.8580649495124817\n",
      "INFO:absl:Setting work unit notes: 3121.4 steps/s, 53.5% (187287/350000), ETA: 0m (1m : 0.1% checkpoint, 12.5% eval)\n",
      "INFO:absl:[187287] steps_per_sec=3121.443811\n",
      "INFO:absl:[53] train_loss=0.4751869738101959, train_x1_loss=0.06482533365488052, train_x2_loss=0.4103616178035736\n",
      "INFO:absl:[53] val_loss=0.8657416105270386\n",
      "INFO:absl:[53] test_loss=0.8350611329078674\n",
      "INFO:absl:[54] train_loss=0.473308265209198, train_x1_loss=0.06413863599300385, train_x2_loss=0.4091694951057434\n",
      "INFO:absl:[54] val_loss=0.8817865252494812\n",
      "INFO:absl:[54] test_loss=0.8529576063156128\n",
      "INFO:absl:[55] train_loss=0.47625502943992615, train_x1_loss=0.06422317028045654, train_x2_loss=0.41203224658966064\n",
      "INFO:absl:[55] val_loss=0.8855907320976257\n",
      "INFO:absl:[55] test_loss=0.846360981464386\n",
      "INFO:absl:[56] train_loss=0.4728350043296814, train_x1_loss=0.06253030896186829, train_x2_loss=0.41030460596084595\n",
      "INFO:absl:[56] val_loss=0.8727163076400757\n",
      "INFO:absl:[56] test_loss=0.8383086919784546\n",
      "INFO:absl:[57] train_loss=0.47146347165107727, train_x1_loss=0.06267138570547104, train_x2_loss=0.40879231691360474\n",
      "INFO:absl:[57] val_loss=0.841429591178894\n",
      "INFO:absl:[57] test_loss=0.8008100986480713\n",
      "INFO:absl:[58] train_loss=0.47346460819244385, train_x1_loss=0.06443670392036438, train_x2_loss=0.4090290665626526\n",
      "INFO:absl:[58] val_loss=0.8284125924110413\n",
      "INFO:absl:[58] test_loss=0.7885571122169495\n",
      "INFO:absl:[59] train_loss=0.4700368344783783, train_x1_loss=0.06421168893575668, train_x2_loss=0.40582507848739624\n",
      "INFO:absl:[59] val_loss=0.8601729869842529\n",
      "INFO:absl:[59] test_loss=0.829030454158783\n",
      "INFO:absl:[60] train_loss=0.4702930152416229, train_x1_loss=0.06278783082962036, train_x2_loss=0.4075050950050354\n",
      "INFO:absl:[60] val_loss=0.8439390063285828\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[60] test_loss=0.8129451274871826\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[61] train_loss=0.4690978229045868, train_x1_loss=0.062363360077142715, train_x2_loss=0.406733900308609\n",
      "INFO:absl:[61] val_loss=0.8329199552536011\n",
      "INFO:absl:[61] test_loss=0.7987118363380432\n",
      "INFO:absl:[62] train_loss=0.4700060784816742, train_x1_loss=0.06321331113576889, train_x2_loss=0.40679261088371277\n",
      "INFO:absl:[62] val_loss=0.8392329216003418\n",
      "INFO:absl:[62] test_loss=0.8009033203125\n",
      "INFO:absl:[63] train_loss=0.4694814383983612, train_x1_loss=0.06256768852472305, train_x2_loss=0.4069134294986725\n",
      "INFO:absl:[63] val_loss=0.844488263130188\n",
      "INFO:absl:[63] test_loss=0.8127964735031128\n",
      "INFO:absl:[64] train_loss=0.4697504937648773, train_x1_loss=0.06235799193382263, train_x2_loss=0.4073910713195801\n",
      "INFO:absl:[64] val_loss=0.8062398433685303\n",
      "INFO:absl:[64] test_loss=0.7726610898971558\n",
      "INFO:absl:[65] train_loss=0.46832242608070374, train_x1_loss=0.06300666928291321, train_x2_loss=0.40531647205352783\n",
      "INFO:absl:[65] val_loss=0.8295804262161255\n",
      "INFO:absl:[65] test_loss=0.8021694421768188\n",
      "INFO:absl:[66] train_loss=0.46932774782180786, train_x1_loss=0.06334992498159409, train_x2_loss=0.4059773087501526\n",
      "INFO:absl:[66] val_loss=0.8392397165298462\n",
      "INFO:absl:[66] test_loss=0.8060003519058228\n",
      "INFO:absl:[67] train_loss=0.4659437835216522, train_x1_loss=0.061896998435258865, train_x2_loss=0.4040466547012329\n",
      "INFO:absl:[67] val_loss=0.8180477619171143\n",
      "INFO:absl:[67] test_loss=0.7828643321990967\n",
      "INFO:absl:[68] train_loss=0.4646899104118347, train_x1_loss=0.06281547248363495, train_x2_loss=0.4018743634223938\n",
      "INFO:absl:[68] val_loss=0.8391146659851074\n",
      "INFO:absl:[68] test_loss=0.8050927519798279\n",
      "INFO:absl:[69] train_loss=0.4656301736831665, train_x1_loss=0.06295713782310486, train_x2_loss=0.4026727080345154\n",
      "INFO:absl:[69] val_loss=0.8135462403297424\n",
      "INFO:absl:[69] test_loss=0.776144802570343\n",
      "INFO:absl:[70] train_loss=0.4644263982772827, train_x1_loss=0.06281674653291702, train_x2_loss=0.4016094505786896\n",
      "INFO:absl:[70] val_loss=0.8397858142852783\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[70] test_loss=0.8097731471061707\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[71] train_loss=0.4648230969905853, train_x1_loss=0.06227400153875351, train_x2_loss=0.4025498032569885\n",
      "INFO:absl:[71] val_loss=0.8327246904373169\n",
      "INFO:absl:[71] test_loss=0.7926859259605408\n",
      "INFO:absl:[72] train_loss=0.46381551027297974, train_x1_loss=0.062299054116010666, train_x2_loss=0.40151646733283997\n",
      "INFO:absl:[72] val_loss=0.8313170671463013\n",
      "INFO:absl:[72] test_loss=0.8056422472000122\n",
      "INFO:absl:[73] train_loss=0.4621865153312683, train_x1_loss=0.06154468283057213, train_x2_loss=0.40064212679862976\n",
      "INFO:absl:[73] val_loss=0.817207396030426\n",
      "INFO:absl:[73] test_loss=0.7893200516700745\n",
      "INFO:absl:[74] train_loss=0.4611084461212158, train_x1_loss=0.0613362118601799, train_x2_loss=0.39977267384529114\n",
      "INFO:absl:[74] val_loss=0.8326953649520874\n",
      "INFO:absl:[74] test_loss=0.8015881776809692\n",
      "INFO:absl:[75] train_loss=0.4624650180339813, train_x1_loss=0.06119513511657715, train_x2_loss=0.4012702703475952\n",
      "INFO:absl:[75] val_loss=0.7964662909507751\n",
      "INFO:absl:[75] test_loss=0.7638508677482605\n",
      "INFO:absl:[76] train_loss=0.4628094434738159, train_x1_loss=0.0627526193857193, train_x2_loss=0.400056391954422\n",
      "INFO:absl:[76] val_loss=0.813569188117981\n",
      "INFO:absl:[76] test_loss=0.7799842953681946\n",
      "INFO:absl:[77] train_loss=0.46277570724487305, train_x1_loss=0.06202970817685127, train_x2_loss=0.4007459580898285\n",
      "INFO:absl:[77] val_loss=0.8023546934127808\n",
      "INFO:absl:[77] test_loss=0.7711608409881592\n",
      "INFO:absl:[78] train_loss=0.46141868829727173, train_x1_loss=0.06274034827947617, train_x2_loss=0.3986777365207672\n",
      "INFO:absl:[78] val_loss=0.832950234413147\n",
      "INFO:absl:[78] test_loss=0.8030665516853333\n",
      "INFO:absl:[79] train_loss=0.45771127939224243, train_x1_loss=0.06101815029978752, train_x2_loss=0.3966940939426422\n",
      "INFO:absl:[79] val_loss=0.83570396900177\n",
      "INFO:absl:[79] test_loss=0.805099368095398\n",
      "INFO:absl:[80] train_loss=0.4600442349910736, train_x1_loss=0.06273865699768066, train_x2_loss=0.3973052501678467\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[80] val_loss=0.8120430111885071\n",
      "INFO:absl:[80] test_loss=0.7830096483230591\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[81] train_loss=0.4560246467590332, train_x1_loss=0.06110571324825287, train_x2_loss=0.39491894841194153\n",
      "INFO:absl:[81] val_loss=0.8254386186599731\n",
      "INFO:absl:[81] test_loss=0.7970367670059204\n",
      "INFO:absl:[82] train_loss=0.45730286836624146, train_x1_loss=0.06139402091503143, train_x2_loss=0.39590850472450256\n",
      "INFO:absl:[82] val_loss=0.8199943900108337\n",
      "INFO:absl:[82] test_loss=0.7922863364219666\n",
      "INFO:absl:[83] train_loss=0.45782649517059326, train_x1_loss=0.06110098958015442, train_x2_loss=0.3967256247997284\n",
      "INFO:absl:[83] val_loss=0.8203454613685608\n",
      "INFO:absl:[83] test_loss=0.7893964052200317\n",
      "INFO:absl:[84] train_loss=0.45831045508384705, train_x1_loss=0.06247498095035553, train_x2_loss=0.39583486318588257\n",
      "INFO:absl:[84] val_loss=0.8010733127593994\n",
      "INFO:absl:[84] test_loss=0.7695823311805725\n",
      "INFO:absl:[85] train_loss=0.4564582109451294, train_x1_loss=0.06121765077114105, train_x2_loss=0.39524102210998535\n",
      "INFO:absl:[85] val_loss=0.8128759860992432\n",
      "INFO:absl:[85] test_loss=0.7862911224365234\n",
      "INFO:absl:[86] train_loss=0.45566651225090027, train_x1_loss=0.06201460212469101, train_x2_loss=0.3936511278152466\n",
      "INFO:absl:[86] val_loss=0.8100675940513611\n",
      "INFO:absl:[86] test_loss=0.7818021774291992\n",
      "INFO:absl:[87] train_loss=0.4559713900089264, train_x1_loss=0.06171667203307152, train_x2_loss=0.39425572752952576\n",
      "INFO:absl:[87] val_loss=0.8145063519477844\n",
      "INFO:absl:[87] test_loss=0.7849428653717041\n",
      "INFO:absl:[88] train_loss=0.45762431621551514, train_x1_loss=0.0625481829047203, train_x2_loss=0.39507561922073364\n",
      "INFO:absl:[88] val_loss=0.8084728717803955\n",
      "INFO:absl:[88] test_loss=0.7824118733406067\n",
      "INFO:absl:[89] train_loss=0.45644402503967285, train_x1_loss=0.061858322471380234, train_x2_loss=0.3945861756801605\n",
      "INFO:absl:[89] val_loss=0.8216485977172852\n",
      "INFO:absl:[89] test_loss=0.7903127670288086\n",
      "INFO:absl:[90] train_loss=0.4559621214866638, train_x1_loss=0.06095999479293823, train_x2_loss=0.3950013518333435\n",
      "INFO:absl:[90] val_loss=0.799281120300293\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[90] test_loss=0.7655512094497681\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[91] train_loss=0.4586821496486664, train_x1_loss=0.06286968290805817, train_x2_loss=0.39581209421157837\n",
      "INFO:absl:[91] val_loss=0.7921699285507202\n",
      "INFO:absl:[91] test_loss=0.7645964026451111\n",
      "INFO:absl:[92] train_loss=0.45519188046455383, train_x1_loss=0.061797771602869034, train_x2_loss=0.3933941721916199\n",
      "INFO:absl:[92] val_loss=0.7840651869773865\n",
      "INFO:absl:[92] test_loss=0.7467717528343201\n",
      "INFO:absl:[93] train_loss=0.45388364791870117, train_x1_loss=0.06204940751194954, train_x2_loss=0.3918341100215912\n",
      "INFO:absl:[93] val_loss=0.7988723516464233\n",
      "INFO:absl:[93] test_loss=0.774713397026062\n",
      "INFO:absl:[94] train_loss=0.45208901166915894, train_x1_loss=0.061624396592378616, train_x2_loss=0.3904643654823303\n",
      "INFO:absl:[94] val_loss=0.8107500672340393\n",
      "INFO:absl:[94] test_loss=0.7861767411231995\n",
      "INFO:absl:[95] train_loss=0.4566376209259033, train_x1_loss=0.06158231943845749, train_x2_loss=0.39505478739738464\n",
      "INFO:absl:[95] val_loss=0.800022304058075\n",
      "INFO:absl:[95] test_loss=0.7669621109962463\n",
      "INFO:absl:[96] train_loss=0.45418888330459595, train_x1_loss=0.06171388551592827, train_x2_loss=0.3924753665924072\n",
      "INFO:absl:[96] val_loss=0.8073353171348572\n",
      "INFO:absl:[96] test_loss=0.7800939083099365\n",
      "INFO:absl:[97] train_loss=0.45391273498535156, train_x1_loss=0.061669621616601944, train_x2_loss=0.39224323630332947\n",
      "INFO:absl:[97] val_loss=0.8065951466560364\n",
      "INFO:absl:[97] test_loss=0.7705156803131104\n",
      "INFO:absl:[98] train_loss=0.4522458612918854, train_x1_loss=0.061261627823114395, train_x2_loss=0.39098361134529114\n",
      "INFO:absl:[98] val_loss=0.8145666122436523\n",
      "INFO:absl:[98] test_loss=0.7782005071640015\n",
      "INFO:absl:[99] train_loss=0.45243868231773376, train_x1_loss=0.06100091710686684, train_x2_loss=0.391437292098999\n",
      "INFO:absl:[99] val_loss=0.8118037581443787\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[99] test_loss=0.780617356300354\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'relu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 10, 'dropout_rate': 0.013287043114620523, 'edge_features': (8, 8), 'epochs': 100, 'eval_every_epochs': 1, 'fully_connected_edges': 5, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 1, 'layer_norm': False, 'learning_rate': 0.00045346796177033903, 'log_every_epochs': 1, 'max_checkpts_to_keep': None, 'model': 'MLPGraphNetwork', 'momentum': 0.8712873602503628, 'n_blocks': 1, 'n_samples': 5000, 'node_features': (32, 2), 'normalize': True, 'optimizer': 'sgd', 'output_delay': 0, 'output_steps': 4, 'sample_buffer': -4, 'seed': 73, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 120, 'timestep_duration': 3, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| Name                                   | Shape    | Size | Mean     | Std   |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 8)   | 48   | -0.103   | 0.37  |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (8, 8)   | 64   | -0.0189  | 0.344 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (32,)    | 32   | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (19, 32) | 608  | -0.00843 | 0.219 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)     | 2    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (32, 2)  | 64   | -0.00627 | 0.195 |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "Total: 834\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Checkpoint None does not exist.\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.00s.\n",
      "INFO:absl:Starting training.\n",
      "INFO:absl:[0] train_loss=0.7979426383972168, train_x1_loss=0.16288600862026215, train_x2_loss=0.635056734085083\n",
      "INFO:absl:[0] val_loss=1.231756567955017\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[0] test_loss=1.3651554584503174\n",
      "INFO:absl:Checkpoint.save() finished after 0.02s.\n",
      "INFO:absl:Created artifact [10] Profile of type ArtifactType.URL and value None.\n",
      "INFO:absl:[1] train_loss=0.6704224944114685, train_x1_loss=0.102938212454319, train_x2_loss=0.5674837231636047\n",
      "INFO:absl:[1] val_loss=1.178701639175415\n",
      "INFO:absl:[1] test_loss=1.3134067058563232\n",
      "INFO:absl:[2] train_loss=0.6400572657585144, train_x1_loss=0.09551655501127243, train_x2_loss=0.5445403456687927\n",
      "INFO:absl:[2] val_loss=1.0632224082946777\n",
      "INFO:absl:[2] test_loss=1.2075597047805786\n",
      "INFO:absl:[3] train_loss=0.619077205657959, train_x1_loss=0.08939982205629349, train_x2_loss=0.5296775698661804\n",
      "INFO:absl:[3] val_loss=1.0383729934692383\n",
      "INFO:absl:[3] test_loss=1.1783922910690308\n",
      "INFO:absl:[4] train_loss=0.610991895198822, train_x1_loss=0.08691701292991638, train_x2_loss=0.5240756273269653\n",
      "INFO:absl:[4] val_loss=1.024780511856079\n",
      "INFO:absl:[4] test_loss=1.1668106317520142\n",
      "INFO:absl:[5] train_loss=0.5993970632553101, train_x1_loss=0.08345884829759598, train_x2_loss=0.5159381031990051\n",
      "INFO:absl:[5] val_loss=0.9675562977790833\n",
      "INFO:absl:[5] test_loss=1.1058897972106934\n",
      "INFO:absl:[6] train_loss=0.5933588147163391, train_x1_loss=0.08050597459077835, train_x2_loss=0.5128523111343384\n",
      "INFO:absl:[6] val_loss=1.0134851932525635\n",
      "INFO:absl:[6] test_loss=1.1545439958572388\n",
      "INFO:absl:[7] train_loss=0.5879226326942444, train_x1_loss=0.07944279909133911, train_x2_loss=0.50847989320755\n",
      "INFO:absl:[7] val_loss=0.9825241565704346\n",
      "INFO:absl:[7] test_loss=1.1213853359222412\n",
      "INFO:absl:[8] train_loss=0.5844925045967102, train_x1_loss=0.07806039601564407, train_x2_loss=0.5064331889152527\n",
      "INFO:absl:[8] val_loss=0.9493843913078308\n",
      "INFO:absl:[8] test_loss=1.085663080215454\n",
      "INFO:absl:[9] train_loss=0.5789408683776855, train_x1_loss=0.07764337956905365, train_x2_loss=0.5012967586517334\n",
      "INFO:absl:[9] val_loss=0.9811373353004456\n",
      "INFO:absl:[9] test_loss=1.1201244592666626\n",
      "INFO:absl:[10] train_loss=0.573922872543335, train_x1_loss=0.07485085725784302, train_x2_loss=0.4990719258785248\n",
      "INFO:absl:[10] val_loss=0.9578167796134949\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[10] test_loss=1.09540855884552\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[11] train_loss=0.5715100169181824, train_x1_loss=0.07463331520557404, train_x2_loss=0.49687764048576355\n",
      "INFO:absl:[11] val_loss=0.9641435742378235\n",
      "INFO:absl:[11] test_loss=1.1004536151885986\n",
      "INFO:absl:[12] train_loss=0.568096935749054, train_x1_loss=0.07427758723497391, train_x2_loss=0.4938192069530487\n",
      "INFO:absl:[12] val_loss=0.9543182849884033\n",
      "INFO:absl:[12] test_loss=1.0929484367370605\n",
      "INFO:absl:[13] train_loss=0.5659276843070984, train_x1_loss=0.0730365514755249, train_x2_loss=0.4928908050060272\n",
      "INFO:absl:[13] val_loss=0.9593312740325928\n",
      "INFO:absl:[13] test_loss=1.0944098234176636\n",
      "INFO:absl:[14] train_loss=0.5635252594947815, train_x1_loss=0.07262946665287018, train_x2_loss=0.490896075963974\n",
      "INFO:absl:[14] val_loss=0.9320977330207825\n",
      "INFO:absl:[14] test_loss=1.0637433528900146\n",
      "INFO:absl:[15] train_loss=0.5588537454605103, train_x1_loss=0.07130613178014755, train_x2_loss=0.4875473082065582\n",
      "INFO:absl:[15] val_loss=0.9450384378433228\n",
      "INFO:absl:[15] test_loss=1.078795313835144\n",
      "INFO:absl:[16] train_loss=0.5593127012252808, train_x1_loss=0.07161926478147507, train_x2_loss=0.4876936078071594\n",
      "INFO:absl:[16] val_loss=0.937883198261261\n",
      "INFO:absl:[16] test_loss=1.0664963722229004\n",
      "INFO:absl:[17] train_loss=0.5555953979492188, train_x1_loss=0.06965699791908264, train_x2_loss=0.4859381914138794\n",
      "INFO:absl:[17] val_loss=0.9232276082038879\n",
      "INFO:absl:[17] test_loss=1.0496505498886108\n",
      "INFO:absl:[18] train_loss=0.5529897212982178, train_x1_loss=0.07045689970254898, train_x2_loss=0.48253247141838074\n",
      "INFO:absl:[18] val_loss=0.9115495085716248\n",
      "INFO:absl:[18] test_loss=1.0392745733261108\n",
      "INFO:absl:[19] train_loss=0.5495753288269043, train_x1_loss=0.06882616877555847, train_x2_loss=0.4807502329349518\n",
      "INFO:absl:[19] val_loss=0.9005864858627319\n",
      "INFO:absl:[19] test_loss=1.0246857404708862\n",
      "INFO:absl:[20] train_loss=0.5473814606666565, train_x1_loss=0.06957820802927017, train_x2_loss=0.47780320048332214\n",
      "INFO:absl:[20] val_loss=0.8828375339508057\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[20] test_loss=1.0101299285888672\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[21] train_loss=0.5436634421348572, train_x1_loss=0.06790795922279358, train_x2_loss=0.4757561981678009\n",
      "INFO:absl:[21] val_loss=0.8876595497131348\n",
      "INFO:absl:[21] test_loss=1.0142353773117065\n",
      "INFO:absl:[22] train_loss=0.5437171459197998, train_x1_loss=0.06804270297288895, train_x2_loss=0.4756743907928467\n",
      "INFO:absl:[22] val_loss=0.8994903564453125\n",
      "INFO:absl:[22] test_loss=1.0254456996917725\n",
      "INFO:absl:[23] train_loss=0.5400286316871643, train_x1_loss=0.06825385242700577, train_x2_loss=0.4717743396759033\n",
      "INFO:absl:[23] val_loss=0.8614208698272705\n",
      "INFO:absl:[23] test_loss=0.986359179019928\n",
      "INFO:absl:[24] train_loss=0.5379675030708313, train_x1_loss=0.06952495872974396, train_x2_loss=0.4684416949748993\n",
      "INFO:absl:[24] val_loss=0.8796079754829407\n",
      "INFO:absl:[24] test_loss=1.0087283849716187\n",
      "INFO:absl:[25] train_loss=0.5341972708702087, train_x1_loss=0.06780727207660675, train_x2_loss=0.4663902521133423\n",
      "INFO:absl:[25] val_loss=0.8619225025177002\n",
      "INFO:absl:[25] test_loss=0.9862648844718933\n",
      "INFO:absl:[26] train_loss=0.532914400100708, train_x1_loss=0.06865190714597702, train_x2_loss=0.4642638564109802\n",
      "INFO:absl:[26] val_loss=0.875385046005249\n",
      "INFO:absl:[26] test_loss=0.9984899759292603\n",
      "INFO:absl:[27] train_loss=0.532762885093689, train_x1_loss=0.0687635987997055, train_x2_loss=0.46399909257888794\n",
      "INFO:absl:[27] val_loss=0.8390902280807495\n",
      "INFO:absl:[27] test_loss=0.9592240452766418\n",
      "INFO:absl:[28] train_loss=0.5286014080047607, train_x1_loss=0.06794615089893341, train_x2_loss=0.4606551229953766\n",
      "INFO:absl:[28] val_loss=0.8315201997756958\n",
      "INFO:absl:[28] test_loss=0.9560261368751526\n",
      "INFO:absl:[29] train_loss=0.5271824598312378, train_x1_loss=0.06882894039154053, train_x2_loss=0.45835354924201965\n",
      "INFO:absl:[29] val_loss=0.8265776038169861\n",
      "INFO:absl:[29] test_loss=0.9415158629417419\n",
      "INFO:absl:[30] train_loss=0.5238921046257019, train_x1_loss=0.06779541820287704, train_x2_loss=0.4560975730419159\n",
      "INFO:absl:[30] val_loss=0.8358770608901978\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[30] test_loss=0.9495911002159119\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[31] train_loss=0.5208187103271484, train_x1_loss=0.06789324432611465, train_x2_loss=0.45292481780052185\n",
      "INFO:absl:[31] val_loss=0.8107282519340515\n",
      "INFO:absl:[31] test_loss=0.9295111298561096\n",
      "INFO:absl:[32] train_loss=0.5201196074485779, train_x1_loss=0.06755733489990234, train_x2_loss=0.45256274938583374\n",
      "INFO:absl:[32] val_loss=0.8014357686042786\n",
      "INFO:absl:[32] test_loss=0.9120693802833557\n",
      "INFO:absl:Setting work unit notes: 1921.8 steps/s, 33.0% (115500/350000), ETA: 2m (1m : 0.1% checkpoint, 41.8% eval)\n",
      "INFO:absl:[115500] steps_per_sec=1921.787257\n",
      "INFO:absl:[33] train_loss=0.5174495577812195, train_x1_loss=0.06768538802862167, train_x2_loss=0.44976434111595154\n",
      "INFO:absl:[33] val_loss=0.8008719086647034\n",
      "INFO:absl:[33] test_loss=0.9166632294654846\n",
      "INFO:absl:[34] train_loss=0.5136579275131226, train_x1_loss=0.06717823445796967, train_x2_loss=0.4464794993400574\n",
      "INFO:absl:[34] val_loss=0.7873314023017883\n",
      "INFO:absl:[34] test_loss=0.9048418402671814\n",
      "INFO:absl:[35] train_loss=0.5137537121772766, train_x1_loss=0.06764104217290878, train_x2_loss=0.44611307978630066\n",
      "INFO:absl:[35] val_loss=0.7868645787239075\n",
      "INFO:absl:[35] test_loss=0.9039333462715149\n",
      "INFO:absl:[36] train_loss=0.5072181820869446, train_x1_loss=0.06668145954608917, train_x2_loss=0.44053682684898376\n",
      "INFO:absl:[36] val_loss=0.7885810732841492\n",
      "INFO:absl:[36] test_loss=0.9016706347465515\n",
      "INFO:absl:[37] train_loss=0.5050165057182312, train_x1_loss=0.06716429442167282, train_x2_loss=0.4378519058227539\n",
      "INFO:absl:[37] val_loss=0.7737654447555542\n",
      "INFO:absl:[37] test_loss=0.8876492977142334\n",
      "INFO:absl:[38] train_loss=0.5021785497665405, train_x1_loss=0.06736178696155548, train_x2_loss=0.4348159730434418\n",
      "INFO:absl:[38] val_loss=0.7488012909889221\n",
      "INFO:absl:[38] test_loss=0.8672403693199158\n",
      "INFO:absl:[39] train_loss=0.5006123185157776, train_x1_loss=0.06727363914251328, train_x2_loss=0.4333387613296509\n",
      "INFO:absl:[39] val_loss=0.748874843120575\n",
      "INFO:absl:[39] test_loss=0.8664455413818359\n",
      "INFO:absl:[40] train_loss=0.49544206261634827, train_x1_loss=0.06511201709508896, train_x2_loss=0.4303305745124817\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[40] val_loss=0.7396258115768433\n",
      "INFO:absl:[40] test_loss=0.8542935252189636\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[41] train_loss=0.4946368932723999, train_x1_loss=0.06816162914037704, train_x2_loss=0.42647475004196167\n",
      "INFO:absl:[41] val_loss=0.7774041295051575\n",
      "INFO:absl:[41] test_loss=0.8950086236000061\n",
      "INFO:absl:[42] train_loss=0.49205145239830017, train_x1_loss=0.06705798953771591, train_x2_loss=0.42499276995658875\n",
      "INFO:absl:[42] val_loss=0.7504116296768188\n",
      "INFO:absl:[42] test_loss=0.867743968963623\n",
      "INFO:absl:[43] train_loss=0.4924033284187317, train_x1_loss=0.06806620210409164, train_x2_loss=0.4243370592594147\n",
      "INFO:absl:[43] val_loss=0.7427986264228821\n",
      "INFO:absl:[43] test_loss=0.8519015908241272\n",
      "INFO:absl:[44] train_loss=0.49156028032302856, train_x1_loss=0.06790472567081451, train_x2_loss=0.42365601658821106\n",
      "INFO:absl:[44] val_loss=0.7444412112236023\n",
      "INFO:absl:[44] test_loss=0.8637062311172485\n",
      "INFO:absl:[45] train_loss=0.4875586926937103, train_x1_loss=0.06667599081993103, train_x2_loss=0.42088285088539124\n",
      "INFO:absl:[45] val_loss=0.7260465621948242\n",
      "INFO:absl:[45] test_loss=0.8411074876785278\n",
      "INFO:absl:[46] train_loss=0.4863436818122864, train_x1_loss=0.0671272873878479, train_x2_loss=0.41921648383140564\n",
      "INFO:absl:[46] val_loss=0.7427579164505005\n",
      "INFO:absl:[46] test_loss=0.8602619171142578\n",
      "INFO:absl:[47] train_loss=0.48627281188964844, train_x1_loss=0.0668436586856842, train_x2_loss=0.4194296896457672\n",
      "INFO:absl:[47] val_loss=0.7475258111953735\n",
      "INFO:absl:[47] test_loss=0.8594489097595215\n",
      "INFO:absl:[48] train_loss=0.48501527309417725, train_x1_loss=0.0661526471376419, train_x2_loss=0.4188629388809204\n",
      "INFO:absl:[48] val_loss=0.7195666432380676\n",
      "INFO:absl:[48] test_loss=0.8339902758598328\n",
      "INFO:absl:[49] train_loss=0.48140496015548706, train_x1_loss=0.0669722780585289, train_x2_loss=0.4144324064254761\n",
      "INFO:absl:[49] val_loss=0.728572428226471\n",
      "INFO:absl:[49] test_loss=0.8397670984268188\n",
      "INFO:absl:[50] train_loss=0.484466016292572, train_x1_loss=0.06810794770717621, train_x2_loss=0.41635802388191223\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[50] val_loss=0.7035330533981323\n",
      "INFO:absl:[50] test_loss=0.8130067586898804\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[51] train_loss=0.48129069805145264, train_x1_loss=0.06695954501628876, train_x2_loss=0.41433092951774597\n",
      "INFO:absl:[51] val_loss=0.7095548510551453\n",
      "INFO:absl:[51] test_loss=0.8121286034584045\n",
      "INFO:absl:[52] train_loss=0.48006609082221985, train_x1_loss=0.06664439290761948, train_x2_loss=0.4134216010570526\n",
      "INFO:absl:[52] val_loss=0.6958931088447571\n",
      "INFO:absl:[52] test_loss=0.7997780442237854\n",
      "INFO:absl:[53] train_loss=0.48002365231513977, train_x1_loss=0.066298708319664, train_x2_loss=0.4137255549430847\n",
      "INFO:absl:[53] val_loss=0.7156177759170532\n",
      "INFO:absl:[53] test_loss=0.8250309228897095\n",
      "INFO:absl:[54] train_loss=0.4763656556606293, train_x1_loss=0.06672525405883789, train_x2_loss=0.4096411466598511\n",
      "INFO:absl:[54] val_loss=0.6814144253730774\n",
      "INFO:absl:[54] test_loss=0.786642849445343\n",
      "INFO:absl:[55] train_loss=0.479129821062088, train_x1_loss=0.06818899512290955, train_x2_loss=0.4109400808811188\n",
      "INFO:absl:[55] val_loss=0.7044538855552673\n",
      "INFO:absl:[55] test_loss=0.8102824091911316\n",
      "INFO:absl:[56] train_loss=0.4772149920463562, train_x1_loss=0.06664934754371643, train_x2_loss=0.4105653464794159\n",
      "INFO:absl:[56] val_loss=0.6871257424354553\n",
      "INFO:absl:[56] test_loss=0.7876465916633606\n",
      "INFO:absl:[57] train_loss=0.4734725058078766, train_x1_loss=0.06584058701992035, train_x2_loss=0.40763193368911743\n",
      "INFO:absl:[57] val_loss=0.6765720844268799\n",
      "INFO:absl:[57] test_loss=0.7853562831878662\n",
      "INFO:absl:[58] train_loss=0.47548314929008484, train_x1_loss=0.06704268604516983, train_x2_loss=0.4084412455558777\n",
      "INFO:absl:[58] val_loss=0.689190685749054\n",
      "INFO:absl:[58] test_loss=0.7906725406646729\n",
      "INFO:absl:[59] train_loss=0.47582903504371643, train_x1_loss=0.06730159372091293, train_x2_loss=0.4085270166397095\n",
      "INFO:absl:[59] val_loss=0.6853238344192505\n",
      "INFO:absl:[59] test_loss=0.788987934589386\n",
      "INFO:absl:[60] train_loss=0.47590580582618713, train_x1_loss=0.06751228868961334, train_x2_loss=0.40839314460754395\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[60] val_loss=0.7225719094276428\n",
      "INFO:absl:[60] test_loss=0.82713383436203\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[61] train_loss=0.4729088246822357, train_x1_loss=0.06619715690612793, train_x2_loss=0.40671172738075256\n",
      "INFO:absl:[61] val_loss=0.6762985587120056\n",
      "INFO:absl:[61] test_loss=0.7767971158027649\n",
      "INFO:absl:[62] train_loss=0.47106316685676575, train_x1_loss=0.06757578998804092, train_x2_loss=0.40348824858665466\n",
      "INFO:absl:[62] val_loss=0.6657893061637878\n",
      "INFO:absl:[62] test_loss=0.7621713280677795\n",
      "INFO:absl:[63] train_loss=0.46893683075904846, train_x1_loss=0.06553752720355988, train_x2_loss=0.40339961647987366\n",
      "INFO:absl:[63] val_loss=0.6786211133003235\n",
      "INFO:absl:[63] test_loss=0.7728549242019653\n",
      "INFO:absl:[64] train_loss=0.47054046392440796, train_x1_loss=0.06665439903736115, train_x2_loss=0.40388554334640503\n",
      "INFO:absl:[64] val_loss=0.6737915277481079\n",
      "INFO:absl:[64] test_loss=0.7723200917243958\n",
      "INFO:absl:[65] train_loss=0.46919146180152893, train_x1_loss=0.06606999784708023, train_x2_loss=0.40312209725379944\n",
      "INFO:absl:[65] val_loss=0.6769025325775146\n",
      "INFO:absl:[65] test_loss=0.7756956815719604\n",
      "INFO:absl:[66] train_loss=0.4669908881187439, train_x1_loss=0.06703253835439682, train_x2_loss=0.39995819330215454\n",
      "INFO:absl:[66] val_loss=0.7062896490097046\n",
      "INFO:absl:[66] test_loss=0.8002076148986816\n",
      "INFO:absl:[67] train_loss=0.4683818817138672, train_x1_loss=0.06532557308673859, train_x2_loss=0.40305623412132263\n",
      "INFO:absl:[67] val_loss=0.6767370700836182\n",
      "INFO:absl:[67] test_loss=0.7771052122116089\n",
      "INFO:absl:[68] train_loss=0.4652886688709259, train_x1_loss=0.06633283942937851, train_x2_loss=0.39895597100257874\n",
      "INFO:absl:[68] val_loss=0.7074738144874573\n",
      "INFO:absl:[68] test_loss=0.8108927607536316\n",
      "INFO:absl:[69] train_loss=0.46745121479034424, train_x1_loss=0.06682772189378738, train_x2_loss=0.40062251687049866\n",
      "INFO:absl:[69] val_loss=0.6686569452285767\n",
      "INFO:absl:[69] test_loss=0.7747012972831726\n",
      "INFO:absl:[70] train_loss=0.4669868052005768, train_x1_loss=0.0668983906507492, train_x2_loss=0.4000893831253052\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[70] val_loss=0.6772585511207581\n",
      "INFO:absl:[70] test_loss=0.7767297029495239\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[71] train_loss=0.46851810812950134, train_x1_loss=0.06667091697454453, train_x2_loss=0.401846706867218\n",
      "INFO:absl:[71] val_loss=0.6816219091415405\n",
      "INFO:absl:[71] test_loss=0.7811487913131714\n",
      "INFO:absl:[72] train_loss=0.46797481179237366, train_x1_loss=0.06769537180662155, train_x2_loss=0.4002804458141327\n",
      "INFO:absl:[72] val_loss=0.7100045084953308\n",
      "INFO:absl:[72] test_loss=0.8095187544822693\n",
      "INFO:absl:[73] train_loss=0.4649108052253723, train_x1_loss=0.06633567065000534, train_x2_loss=0.3985752761363983\n",
      "INFO:absl:[73] val_loss=0.7137718796730042\n",
      "INFO:absl:[73] test_loss=0.8132016658782959\n",
      "INFO:absl:[74] train_loss=0.46292173862457275, train_x1_loss=0.06494615972042084, train_x2_loss=0.3979750871658325\n",
      "INFO:absl:[74] val_loss=0.6841633319854736\n",
      "INFO:absl:[74] test_loss=0.7811872363090515\n",
      "INFO:absl:[75] train_loss=0.46452364325523376, train_x1_loss=0.06653603911399841, train_x2_loss=0.3979874849319458\n",
      "INFO:absl:[75] val_loss=0.6562479734420776\n",
      "INFO:absl:[75] test_loss=0.7552592158317566\n",
      "INFO:absl:[76] train_loss=0.460199773311615, train_x1_loss=0.06633647531270981, train_x2_loss=0.3938623070716858\n",
      "INFO:absl:[76] val_loss=0.6596032977104187\n",
      "INFO:absl:[76] test_loss=0.7579532861709595\n",
      "INFO:absl:[77] train_loss=0.4676263928413391, train_x1_loss=0.06714321672916412, train_x2_loss=0.40048328042030334\n",
      "INFO:absl:[77] val_loss=0.6593857407569885\n",
      "INFO:absl:[77] test_loss=0.7559247612953186\n",
      "INFO:absl:[78] train_loss=0.46363434195518494, train_x1_loss=0.06861957907676697, train_x2_loss=0.39501476287841797\n",
      "INFO:absl:[78] val_loss=0.6679964065551758\n",
      "INFO:absl:[78] test_loss=0.7599531412124634\n",
      "INFO:absl:[79] train_loss=0.4591573178768158, train_x1_loss=0.0666225329041481, train_x2_loss=0.39253509044647217\n",
      "INFO:absl:[79] val_loss=0.6414428949356079\n",
      "INFO:absl:[79] test_loss=0.7380138039588928\n",
      "INFO:absl:[80] train_loss=0.4621990919113159, train_x1_loss=0.06713743507862091, train_x2_loss=0.3950609266757965\n",
      "INFO:absl:[80] val_loss=0.6665584444999695\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[80] test_loss=0.7589902877807617\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[81] train_loss=0.46213507652282715, train_x1_loss=0.06600362807512283, train_x2_loss=0.39613112807273865\n",
      "INFO:absl:[81] val_loss=0.6698435544967651\n",
      "INFO:absl:[81] test_loss=0.7659938335418701\n",
      "INFO:absl:[82] train_loss=0.461679607629776, train_x1_loss=0.06654569506645203, train_x2_loss=0.3951338827610016\n",
      "INFO:absl:[82] val_loss=0.6798915863037109\n",
      "INFO:absl:[82] test_loss=0.7761582136154175\n",
      "INFO:absl:[83] train_loss=0.4595845639705658, train_x1_loss=0.06672362983226776, train_x2_loss=0.3928603231906891\n",
      "INFO:absl:[83] val_loss=0.6563783884048462\n",
      "INFO:absl:[83] test_loss=0.7519771456718445\n",
      "INFO:absl:[84] train_loss=0.4618493914604187, train_x1_loss=0.06739726662635803, train_x2_loss=0.39445266127586365\n",
      "INFO:absl:[84] val_loss=0.6844390034675598\n",
      "INFO:absl:[84] test_loss=0.7812744379043579\n",
      "INFO:absl:[85] train_loss=0.4613726735115051, train_x1_loss=0.06707599759101868, train_x2_loss=0.3942965567111969\n",
      "INFO:absl:[85] val_loss=0.6799404621124268\n",
      "INFO:absl:[85] test_loss=0.7771260142326355\n",
      "INFO:absl:[86] train_loss=0.4613036513328552, train_x1_loss=0.06779302656650543, train_x2_loss=0.39351004362106323\n",
      "INFO:absl:[86] val_loss=0.6499181985855103\n",
      "INFO:absl:[86] test_loss=0.7474607825279236\n",
      "INFO:absl:[87] train_loss=0.46094995737075806, train_x1_loss=0.06718286871910095, train_x2_loss=0.3937675654888153\n",
      "INFO:absl:[87] val_loss=0.6806305050849915\n",
      "INFO:absl:[87] test_loss=0.7741592526435852\n",
      "INFO:absl:[88] train_loss=0.45730486512184143, train_x1_loss=0.06671024858951569, train_x2_loss=0.390593945980072\n",
      "INFO:absl:[88] val_loss=0.6671268343925476\n",
      "INFO:absl:[88] test_loss=0.7603473663330078\n",
      "INFO:absl:[89] train_loss=0.4561941623687744, train_x1_loss=0.06547930091619492, train_x2_loss=0.3907144367694855\n",
      "INFO:absl:[89] val_loss=0.642394483089447\n",
      "INFO:absl:[89] test_loss=0.738552987575531\n",
      "INFO:absl:[90] train_loss=0.4578438103199005, train_x1_loss=0.06542191654443741, train_x2_loss=0.39242151379585266\n",
      "INFO:absl:[90] val_loss=0.6710572838783264\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[90] test_loss=0.7660087943077087\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[91] train_loss=0.4600648581981659, train_x1_loss=0.06671726703643799, train_x2_loss=0.39334869384765625\n",
      "INFO:absl:[91] val_loss=0.6657806634902954\n",
      "INFO:absl:[91] test_loss=0.7571811079978943\n",
      "INFO:absl:[92] train_loss=0.45914226770401, train_x1_loss=0.06697634607553482, train_x2_loss=0.3921648859977722\n",
      "INFO:absl:[92] val_loss=0.6832842230796814\n",
      "INFO:absl:[92] test_loss=0.7725039720535278\n",
      "INFO:absl:[93] train_loss=0.46143534779548645, train_x1_loss=0.06744886189699173, train_x2_loss=0.3939860165119171\n",
      "INFO:absl:[93] val_loss=0.6569596529006958\n",
      "INFO:absl:[93] test_loss=0.7517125606536865\n",
      "INFO:absl:Setting work unit notes: 3559.0 steps/s, 94.0% (329042/350000), ETA: 0m (2m : 0.1% checkpoint, 27.6% eval)\n",
      "INFO:absl:[329042] steps_per_sec=3559.022288\n",
      "INFO:absl:[94] train_loss=0.45841285586357117, train_x1_loss=0.06748147308826447, train_x2_loss=0.3909308612346649\n",
      "INFO:absl:[94] val_loss=0.6931974291801453\n",
      "INFO:absl:[94] test_loss=0.7948092818260193\n",
      "INFO:absl:[95] train_loss=0.45639199018478394, train_x1_loss=0.06710250675678253, train_x2_loss=0.3892895579338074\n",
      "INFO:absl:[95] val_loss=0.6652160882949829\n",
      "INFO:absl:[95] test_loss=0.7656047344207764\n",
      "INFO:absl:[96] train_loss=0.45730289816856384, train_x1_loss=0.0672210082411766, train_x2_loss=0.3900820016860962\n",
      "INFO:absl:[96] val_loss=0.6433366537094116\n",
      "INFO:absl:[96] test_loss=0.7399871349334717\n",
      "INFO:absl:[97] train_loss=0.45537441968917847, train_x1_loss=0.06769834458827972, train_x2_loss=0.3876757025718689\n",
      "INFO:absl:[97] val_loss=0.6622523665428162\n",
      "INFO:absl:[97] test_loss=0.7584934234619141\n",
      "INFO:absl:[98] train_loss=0.4574330747127533, train_x1_loss=0.06669009476900101, train_x2_loss=0.39074233174324036\n",
      "INFO:absl:[98] val_loss=0.7090232968330383\n",
      "INFO:absl:[98] test_loss=0.7970960140228271\n",
      "INFO:absl:[99] train_loss=0.45472633838653564, train_x1_loss=0.06620042771100998, train_x2_loss=0.38852518796920776\n",
      "INFO:absl:[99] val_loss=0.6605514287948608\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[99] test_loss=0.7524670958518982\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'relu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 10, 'dropout_rate': 0.013287043114620523, 'edge_features': (8, 8), 'epochs': 100, 'eval_every_epochs': 1, 'fully_connected_edges': 5, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 1, 'layer_norm': False, 'learning_rate': 0.00045346796177033903, 'log_every_epochs': 1, 'max_checkpts_to_keep': None, 'model': 'MLPGraphNetwork', 'momentum': 0.8712873602503628, 'n_blocks': 1, 'n_samples': 5000, 'node_features': (32, 2), 'normalize': True, 'optimizer': 'sgd', 'output_delay': 0, 'output_steps': 4, 'sample_buffer': -4, 'seed': 74, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 120, 'timestep_duration': 3, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| Name                                   | Shape    | Size | Mean     | Std   |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 8)   | 48   | -0.103   | 0.37  |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (8, 8)   | 64   | -0.0189  | 0.344 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (32,)    | 32   | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (19, 32) | 608  | -0.00843 | 0.219 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)     | 2    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (32, 2)  | 64   | -0.00627 | 0.195 |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "Total: 834\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Checkpoint None does not exist.\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.00s.\n",
      "INFO:absl:Starting training.\n",
      "INFO:absl:[0] train_loss=0.8044497966766357, train_x1_loss=0.15861225128173828, train_x2_loss=0.6458396315574646\n",
      "INFO:absl:[0] val_loss=1.2091161012649536\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[0] test_loss=1.1784354448318481\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Created artifact [10] Profile of type ArtifactType.URL and value None.\n",
      "INFO:absl:[1] train_loss=0.6828699111938477, train_x1_loss=0.1030348688364029, train_x2_loss=0.5798354744911194\n",
      "INFO:absl:[1] val_loss=1.1615325212478638\n",
      "INFO:absl:[1] test_loss=1.134700894355774\n",
      "INFO:absl:[2] train_loss=0.6543262600898743, train_x1_loss=0.09625319391489029, train_x2_loss=0.5580726265907288\n",
      "INFO:absl:[2] val_loss=1.1232199668884277\n",
      "INFO:absl:[2] test_loss=1.0963008403778076\n",
      "INFO:absl:[3] train_loss=0.6353354454040527, train_x1_loss=0.09130236506462097, train_x2_loss=0.544032096862793\n",
      "INFO:absl:[3] val_loss=1.102504849433899\n",
      "INFO:absl:[3] test_loss=1.0730394124984741\n",
      "INFO:absl:[4] train_loss=0.6226019859313965, train_x1_loss=0.08769174665212631, train_x2_loss=0.5349099636077881\n",
      "INFO:absl:[4] val_loss=1.073511004447937\n",
      "INFO:absl:[4] test_loss=1.0452868938446045\n",
      "INFO:absl:[5] train_loss=0.6126427054405212, train_x1_loss=0.08495574444532394, train_x2_loss=0.5276871919631958\n",
      "INFO:absl:[5] val_loss=1.0740312337875366\n",
      "INFO:absl:[5] test_loss=1.0468195676803589\n",
      "INFO:absl:[6] train_loss=0.608244776725769, train_x1_loss=0.08270137012004852, train_x2_loss=0.5255439281463623\n",
      "INFO:absl:[6] val_loss=1.0787721872329712\n",
      "INFO:absl:[6] test_loss=1.0528440475463867\n",
      "INFO:absl:[7] train_loss=0.6044930815696716, train_x1_loss=0.08066266030073166, train_x2_loss=0.5238300561904907\n",
      "INFO:absl:[7] val_loss=1.0620065927505493\n",
      "INFO:absl:[7] test_loss=1.0361982583999634\n",
      "INFO:absl:[8] train_loss=0.5978471040725708, train_x1_loss=0.07977109402418137, train_x2_loss=0.5180754065513611\n",
      "INFO:absl:[8] val_loss=1.0574603080749512\n",
      "INFO:absl:[8] test_loss=1.0303845405578613\n",
      "INFO:absl:[9] train_loss=0.5954982042312622, train_x1_loss=0.07830510288476944, train_x2_loss=0.5171921253204346\n",
      "INFO:absl:[9] val_loss=1.0379245281219482\n",
      "INFO:absl:[9] test_loss=1.0091290473937988\n",
      "INFO:absl:[10] train_loss=0.5904416441917419, train_x1_loss=0.07636874169111252, train_x2_loss=0.5140727758407593\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[10] val_loss=1.0344551801681519\n",
      "INFO:absl:[10] test_loss=1.0070072412490845\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[11] train_loss=0.5843117237091064, train_x1_loss=0.07454978674650192, train_x2_loss=0.5097616314888\n",
      "INFO:absl:[11] val_loss=1.0506458282470703\n",
      "INFO:absl:[11] test_loss=1.0246264934539795\n",
      "INFO:absl:[12] train_loss=0.5835874676704407, train_x1_loss=0.07482263445854187, train_x2_loss=0.5087645649909973\n",
      "INFO:absl:[12] val_loss=1.0227949619293213\n",
      "INFO:absl:[12] test_loss=0.9930540323257446\n",
      "INFO:absl:[13] train_loss=0.5802256464958191, train_x1_loss=0.07407020777463913, train_x2_loss=0.5061556100845337\n",
      "INFO:absl:[13] val_loss=1.0178557634353638\n",
      "INFO:absl:[13] test_loss=0.9906204342842102\n",
      "INFO:absl:[14] train_loss=0.5763314366340637, train_x1_loss=0.07333232462406158, train_x2_loss=0.5029999017715454\n",
      "INFO:absl:[14] val_loss=1.0268781185150146\n",
      "INFO:absl:[14] test_loss=1.0022118091583252\n",
      "INFO:absl:[15] train_loss=0.5737958550453186, train_x1_loss=0.0732661560177803, train_x2_loss=0.5005289912223816\n",
      "INFO:absl:[15] val_loss=1.0089428424835205\n",
      "INFO:absl:[15] test_loss=0.9834529757499695\n",
      "INFO:absl:[16] train_loss=0.5720049738883972, train_x1_loss=0.07270355522632599, train_x2_loss=0.49930185079574585\n",
      "INFO:absl:[16] val_loss=0.9778380393981934\n",
      "INFO:absl:[16] test_loss=0.9503337740898132\n",
      "INFO:absl:[17] train_loss=0.5669407844543457, train_x1_loss=0.0711693987250328, train_x2_loss=0.4957713782787323\n",
      "INFO:absl:[17] val_loss=0.9610701203346252\n",
      "INFO:absl:[17] test_loss=0.9302898049354553\n",
      "INFO:absl:[18] train_loss=0.5629515647888184, train_x1_loss=0.0702449157834053, train_x2_loss=0.4927055537700653\n",
      "INFO:absl:[18] val_loss=1.0009050369262695\n",
      "INFO:absl:[18] test_loss=0.9750422239303589\n",
      "INFO:absl:[19] train_loss=0.559489905834198, train_x1_loss=0.06986886262893677, train_x2_loss=0.48962149024009705\n",
      "INFO:absl:[19] val_loss=0.9657423496246338\n",
      "INFO:absl:[19] test_loss=0.9365966320037842\n",
      "INFO:absl:[20] train_loss=0.5593996047973633, train_x1_loss=0.07151947170495987, train_x2_loss=0.487881064414978\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[20] val_loss=0.9452089667320251\n",
      "INFO:absl:[20] test_loss=0.9169889092445374\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[21] train_loss=0.5515191555023193, train_x1_loss=0.06846169382333755, train_x2_loss=0.4830571413040161\n",
      "INFO:absl:[21] val_loss=0.9262247085571289\n",
      "INFO:absl:[21] test_loss=0.8985007405281067\n",
      "INFO:absl:[22] train_loss=0.548818826675415, train_x1_loss=0.06844398379325867, train_x2_loss=0.48037511110305786\n",
      "INFO:absl:[22] val_loss=0.9196010231971741\n",
      "INFO:absl:[22] test_loss=0.8902641534805298\n",
      "INFO:absl:[23] train_loss=0.5496048331260681, train_x1_loss=0.06966617703437805, train_x2_loss=0.47993841767311096\n",
      "INFO:absl:[23] val_loss=0.9043426513671875\n",
      "INFO:absl:[23] test_loss=0.8749133944511414\n",
      "INFO:absl:[24] train_loss=0.5430022478103638, train_x1_loss=0.07002241909503937, train_x2_loss=0.47297927737236023\n",
      "INFO:absl:[24] val_loss=0.8948884606361389\n",
      "INFO:absl:[24] test_loss=0.8689053058624268\n",
      "INFO:absl:[25] train_loss=0.540274977684021, train_x1_loss=0.06968258321285248, train_x2_loss=0.47059330344200134\n",
      "INFO:absl:[25] val_loss=0.889247715473175\n",
      "INFO:absl:[25] test_loss=0.8622618317604065\n",
      "INFO:absl:[26] train_loss=0.5372981429100037, train_x1_loss=0.06992962956428528, train_x2_loss=0.46736815571784973\n",
      "INFO:absl:[26] val_loss=0.8619316816329956\n",
      "INFO:absl:[26] test_loss=0.8367615938186646\n",
      "INFO:absl:[27] train_loss=0.5343531966209412, train_x1_loss=0.07026830315589905, train_x2_loss=0.4640846848487854\n",
      "INFO:absl:[27] val_loss=0.8629368543624878\n",
      "INFO:absl:[27] test_loss=0.8410362005233765\n",
      "INFO:absl:[28] train_loss=0.5338047742843628, train_x1_loss=0.07051943987607956, train_x2_loss=0.46328529715538025\n",
      "INFO:absl:[28] val_loss=0.8401108384132385\n",
      "INFO:absl:[28] test_loss=0.8165608644485474\n",
      "INFO:absl:[29] train_loss=0.5285926461219788, train_x1_loss=0.06983280926942825, train_x2_loss=0.4587600827217102\n",
      "INFO:absl:[29] val_loss=0.8217645287513733\n",
      "INFO:absl:[29] test_loss=0.8009800910949707\n",
      "INFO:absl:[30] train_loss=0.5234791040420532, train_x1_loss=0.06893996894359589, train_x2_loss=0.45453858375549316\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[30] val_loss=0.811353862285614\n",
      "INFO:absl:[30] test_loss=0.7859293222427368\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[31] train_loss=0.5210383534431458, train_x1_loss=0.07000885158777237, train_x2_loss=0.45103082060813904\n",
      "INFO:absl:[31] val_loss=0.8252846002578735\n",
      "INFO:absl:[31] test_loss=0.8056856393814087\n",
      "INFO:absl:[32] train_loss=0.5218830704689026, train_x1_loss=0.07062085717916489, train_x2_loss=0.45126163959503174\n",
      "INFO:absl:[32] val_loss=0.7934789657592773\n",
      "INFO:absl:[32] test_loss=0.7738379240036011\n",
      "INFO:absl:[33] train_loss=0.523909330368042, train_x1_loss=0.07078471779823303, train_x2_loss=0.45312514901161194\n",
      "INFO:absl:[33] val_loss=0.7997021675109863\n",
      "INFO:absl:[33] test_loss=0.7818492650985718\n",
      "INFO:absl:[34] train_loss=0.5190496444702148, train_x1_loss=0.0699576884508133, train_x2_loss=0.4490926265716553\n",
      "INFO:absl:[34] val_loss=0.7959697842597961\n",
      "INFO:absl:[34] test_loss=0.7755278944969177\n",
      "INFO:absl:[35] train_loss=0.5144591927528381, train_x1_loss=0.06983670592308044, train_x2_loss=0.44462206959724426\n",
      "INFO:absl:[35] val_loss=0.8212698698043823\n",
      "INFO:absl:[35] test_loss=0.8095012307167053\n",
      "INFO:absl:[36] train_loss=0.5100473761558533, train_x1_loss=0.06893520057201385, train_x2_loss=0.4411115050315857\n",
      "INFO:absl:[36] val_loss=0.7799666523933411\n",
      "INFO:absl:[36] test_loss=0.7622647285461426\n",
      "INFO:absl:[37] train_loss=0.512101411819458, train_x1_loss=0.06839761137962341, train_x2_loss=0.44370412826538086\n",
      "INFO:absl:[37] val_loss=0.7999940514564514\n",
      "INFO:absl:[37] test_loss=0.7862430214881897\n",
      "INFO:absl:[38] train_loss=0.51011723279953, train_x1_loss=0.06833698600530624, train_x2_loss=0.4417814016342163\n",
      "INFO:absl:[38] val_loss=0.7662806510925293\n",
      "INFO:absl:[38] test_loss=0.7482060790061951\n",
      "INFO:absl:[39] train_loss=0.506641685962677, train_x1_loss=0.06827802956104279, train_x2_loss=0.43836456537246704\n",
      "INFO:absl:[39] val_loss=0.813811182975769\n",
      "INFO:absl:[39] test_loss=0.8010804653167725\n",
      "INFO:absl:[40] train_loss=0.5071500539779663, train_x1_loss=0.06812243908643723, train_x2_loss=0.43902790546417236\n",
      "INFO:absl:[40] val_loss=0.7770394682884216\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[40] test_loss=0.7681850790977478\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[41] train_loss=0.5048837661743164, train_x1_loss=0.06894896179437637, train_x2_loss=0.43593502044677734\n",
      "INFO:absl:[41] val_loss=0.7546035647392273\n",
      "INFO:absl:[41] test_loss=0.741390585899353\n",
      "INFO:absl:[42] train_loss=0.5046172142028809, train_x1_loss=0.06866504997015, train_x2_loss=0.435952752828598\n",
      "INFO:absl:[42] val_loss=0.7601865530014038\n",
      "INFO:absl:[42] test_loss=0.7475783824920654\n",
      "INFO:absl:[43] train_loss=0.5029966831207275, train_x1_loss=0.06828589737415314, train_x2_loss=0.43471094965934753\n",
      "INFO:absl:[43] val_loss=0.7541347742080688\n",
      "INFO:absl:[43] test_loss=0.7407800555229187\n",
      "INFO:absl:[44] train_loss=0.5017023682594299, train_x1_loss=0.06917058676481247, train_x2_loss=0.43253201246261597\n",
      "INFO:absl:[44] val_loss=0.7501643896102905\n",
      "INFO:absl:[44] test_loss=0.7355409860610962\n",
      "INFO:absl:[45] train_loss=0.5013189315795898, train_x1_loss=0.06785529106855392, train_x2_loss=0.43346428871154785\n",
      "INFO:absl:[45] val_loss=0.7690960168838501\n",
      "INFO:absl:[45] test_loss=0.7583599090576172\n",
      "INFO:absl:[46] train_loss=0.5015712380409241, train_x1_loss=0.06875661015510559, train_x2_loss=0.4328150153160095\n",
      "INFO:absl:[46] val_loss=0.794535756111145\n",
      "INFO:absl:[46] test_loss=0.7881004214286804\n",
      "INFO:absl:[47] train_loss=0.4981742203235626, train_x1_loss=0.068659707903862, train_x2_loss=0.42951449751853943\n",
      "INFO:absl:[47] val_loss=0.7822807431221008\n",
      "INFO:absl:[47] test_loss=0.7728039026260376\n",
      "INFO:absl:[48] train_loss=0.500007152557373, train_x1_loss=0.06732890009880066, train_x2_loss=0.4326794743537903\n",
      "INFO:absl:[48] val_loss=0.761604905128479\n",
      "INFO:absl:[48] test_loss=0.7493658065795898\n",
      "INFO:absl:[49] train_loss=0.4968288540840149, train_x1_loss=0.06833754479885101, train_x2_loss=0.42849060893058777\n",
      "INFO:absl:[49] val_loss=0.7530989646911621\n",
      "INFO:absl:[49] test_loss=0.7432796359062195\n",
      "INFO:absl:[50] train_loss=0.4972010552883148, train_x1_loss=0.06834108382463455, train_x2_loss=0.42885977029800415\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[50] val_loss=0.7541866302490234\n",
      "INFO:absl:[50] test_loss=0.7391342520713806\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[51] train_loss=0.49474865198135376, train_x1_loss=0.06836176663637161, train_x2_loss=0.42638686299324036\n",
      "INFO:absl:[51] val_loss=0.7212879657745361\n",
      "INFO:absl:[51] test_loss=0.7074105143547058\n",
      "INFO:absl:[52] train_loss=0.4930988550186157, train_x1_loss=0.06733668595552444, train_x2_loss=0.4257626533508301\n",
      "INFO:absl:[52] val_loss=0.7467556595802307\n",
      "INFO:absl:[52] test_loss=0.7355829477310181\n",
      "INFO:absl:[53] train_loss=0.49366503953933716, train_x1_loss=0.06834884732961655, train_x2_loss=0.4253159761428833\n",
      "INFO:absl:[53] val_loss=0.7332723140716553\n",
      "INFO:absl:[53] test_loss=0.7184489965438843\n",
      "INFO:absl:Setting work unit notes: 3148.6 steps/s, 54.0% (189000/350000), ETA: 0m (1m : 0.1% checkpoint, 12.8% eval)\n",
      "INFO:absl:[189000] steps_per_sec=3148.562581\n",
      "INFO:absl:[54] train_loss=0.4926183819770813, train_x1_loss=0.06820551306009293, train_x2_loss=0.4244132936000824\n",
      "INFO:absl:[54] val_loss=0.7297149300575256\n",
      "INFO:absl:[54] test_loss=0.7141653895378113\n",
      "INFO:absl:[55] train_loss=0.49679866433143616, train_x1_loss=0.06915350258350372, train_x2_loss=0.427644819021225\n",
      "INFO:absl:[55] val_loss=0.7571871876716614\n",
      "INFO:absl:[55] test_loss=0.7437885999679565\n",
      "INFO:absl:[56] train_loss=0.48970770835876465, train_x1_loss=0.0667247474193573, train_x2_loss=0.4229832589626312\n",
      "INFO:absl:[56] val_loss=0.7396463751792908\n",
      "INFO:absl:[56] test_loss=0.7305808067321777\n",
      "INFO:absl:[57] train_loss=0.49067848920822144, train_x1_loss=0.0677984431385994, train_x2_loss=0.4228796064853668\n",
      "INFO:absl:[57] val_loss=0.7185971736907959\n",
      "INFO:absl:[57] test_loss=0.7059407234191895\n",
      "INFO:absl:[58] train_loss=0.48915499448776245, train_x1_loss=0.0677434578537941, train_x2_loss=0.42141133546829224\n",
      "INFO:absl:[58] val_loss=0.7315914034843445\n",
      "INFO:absl:[58] test_loss=0.7189478874206543\n",
      "INFO:absl:[59] train_loss=0.49087488651275635, train_x1_loss=0.06845482438802719, train_x2_loss=0.42241954803466797\n",
      "INFO:absl:[59] val_loss=0.7229034304618835\n",
      "INFO:absl:[59] test_loss=0.7073334455490112\n",
      "INFO:absl:[60] train_loss=0.49149173498153687, train_x1_loss=0.06723105162382126, train_x2_loss=0.42426103353500366\n",
      "INFO:absl:[60] val_loss=0.7329993844032288\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[60] test_loss=0.722651481628418\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[61] train_loss=0.4869934320449829, train_x1_loss=0.0673184022307396, train_x2_loss=0.41967514157295227\n",
      "INFO:absl:[61] val_loss=0.7228121757507324\n",
      "INFO:absl:[61] test_loss=0.7112337350845337\n",
      "INFO:absl:[62] train_loss=0.48928049206733704, train_x1_loss=0.06733441352844238, train_x2_loss=0.42194682359695435\n",
      "INFO:absl:[62] val_loss=0.7230727672576904\n",
      "INFO:absl:[62] test_loss=0.710224449634552\n",
      "INFO:absl:[63] train_loss=0.4905415177345276, train_x1_loss=0.06729429215192795, train_x2_loss=0.4232478141784668\n",
      "INFO:absl:[63] val_loss=0.7268781661987305\n",
      "INFO:absl:[63] test_loss=0.714603841304779\n",
      "INFO:absl:[64] train_loss=0.4890907406806946, train_x1_loss=0.06890910118818283, train_x2_loss=0.4201814532279968\n",
      "INFO:absl:[64] val_loss=0.7411587238311768\n",
      "INFO:absl:[64] test_loss=0.7284495830535889\n",
      "INFO:absl:[65] train_loss=0.48846110701560974, train_x1_loss=0.06791555136442184, train_x2_loss=0.4205452799797058\n",
      "INFO:absl:[65] val_loss=0.7246547341346741\n",
      "INFO:absl:[65] test_loss=0.7119938731193542\n",
      "INFO:absl:[66] train_loss=0.48628485202789307, train_x1_loss=0.068427175283432, train_x2_loss=0.41785678267478943\n",
      "INFO:absl:[66] val_loss=0.7043007612228394\n",
      "INFO:absl:[66] test_loss=0.6877822875976562\n",
      "INFO:absl:[67] train_loss=0.4859776496887207, train_x1_loss=0.06653168052434921, train_x2_loss=0.4194451868534088\n",
      "INFO:absl:[67] val_loss=0.700340211391449\n",
      "INFO:absl:[67] test_loss=0.6835684776306152\n",
      "INFO:absl:[68] train_loss=0.48450595140457153, train_x1_loss=0.06716028600931168, train_x2_loss=0.41734474897384644\n",
      "INFO:absl:[68] val_loss=0.7198024392127991\n",
      "INFO:absl:[68] test_loss=0.7042922973632812\n",
      "INFO:absl:[69] train_loss=0.4876827597618103, train_x1_loss=0.06802948564291, train_x2_loss=0.41965389251708984\n",
      "INFO:absl:[69] val_loss=0.7280636429786682\n",
      "INFO:absl:[69] test_loss=0.7160361409187317\n",
      "INFO:absl:[70] train_loss=0.48501336574554443, train_x1_loss=0.06698200106620789, train_x2_loss=0.41803163290023804\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[70] val_loss=0.7525346279144287\n",
      "INFO:absl:[70] test_loss=0.738715410232544\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[71] train_loss=0.4858008921146393, train_x1_loss=0.06736498326063156, train_x2_loss=0.418435275554657\n",
      "INFO:absl:[71] val_loss=0.7258087396621704\n",
      "INFO:absl:[71] test_loss=0.7096877098083496\n",
      "INFO:absl:[72] train_loss=0.48614197969436646, train_x1_loss=0.06734414398670197, train_x2_loss=0.41879788041114807\n",
      "INFO:absl:[72] val_loss=0.7615625858306885\n",
      "INFO:absl:[72] test_loss=0.7503932118415833\n",
      "INFO:absl:[73] train_loss=0.4864473044872284, train_x1_loss=0.06775081157684326, train_x2_loss=0.41869664192199707\n",
      "INFO:absl:[73] val_loss=0.6991936564445496\n",
      "INFO:absl:[73] test_loss=0.680260419845581\n",
      "INFO:absl:[74] train_loss=0.48195120692253113, train_x1_loss=0.06620364636182785, train_x2_loss=0.41574764251708984\n",
      "INFO:absl:[74] val_loss=0.7642990350723267\n",
      "INFO:absl:[74] test_loss=0.7518572807312012\n",
      "INFO:absl:[75] train_loss=0.4847029745578766, train_x1_loss=0.06672064960002899, train_x2_loss=0.41798126697540283\n",
      "INFO:absl:[75] val_loss=0.7046797871589661\n",
      "INFO:absl:[75] test_loss=0.6879291534423828\n",
      "INFO:absl:[76] train_loss=0.4853474497795105, train_x1_loss=0.06709866970777512, train_x2_loss=0.41824859380722046\n",
      "INFO:absl:[76] val_loss=0.726391613483429\n",
      "INFO:absl:[76] test_loss=0.7120972275733948\n",
      "INFO:absl:[77] train_loss=0.4820862412452698, train_x1_loss=0.06661543995141983, train_x2_loss=0.415471613407135\n",
      "INFO:absl:[77] val_loss=0.6985872387886047\n",
      "INFO:absl:[77] test_loss=0.6843482851982117\n",
      "INFO:absl:[78] train_loss=0.48301583528518677, train_x1_loss=0.06731078773736954, train_x2_loss=0.4157046973705292\n",
      "INFO:absl:[78] val_loss=0.7132678627967834\n",
      "INFO:absl:[78] test_loss=0.6964883804321289\n",
      "INFO:absl:[79] train_loss=0.4813615083694458, train_x1_loss=0.06643444299697876, train_x2_loss=0.4149273633956909\n",
      "INFO:absl:[79] val_loss=0.6970580220222473\n",
      "INFO:absl:[79] test_loss=0.6814270615577698\n",
      "INFO:absl:[80] train_loss=0.4836886525154114, train_x1_loss=0.06841527670621872, train_x2_loss=0.4152737259864807\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[80] val_loss=0.7387682199478149\n",
      "INFO:absl:[80] test_loss=0.7212873697280884\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[81] train_loss=0.48362866044044495, train_x1_loss=0.06667979061603546, train_x2_loss=0.4169483482837677\n",
      "INFO:absl:[81] val_loss=0.7069096565246582\n",
      "INFO:absl:[81] test_loss=0.6893678903579712\n",
      "INFO:absl:[82] train_loss=0.4808674454689026, train_x1_loss=0.06752844899892807, train_x2_loss=0.4133392870426178\n",
      "INFO:absl:[82] val_loss=0.6980417966842651\n",
      "INFO:absl:[82] test_loss=0.6796087622642517\n",
      "INFO:absl:[83] train_loss=0.48039010167121887, train_x1_loss=0.0667194277048111, train_x2_loss=0.4136703610420227\n",
      "INFO:absl:[83] val_loss=0.7248990535736084\n",
      "INFO:absl:[83] test_loss=0.7066697478294373\n",
      "INFO:absl:[84] train_loss=0.48188871145248413, train_x1_loss=0.06743205338716507, train_x2_loss=0.41445738077163696\n",
      "INFO:absl:[84] val_loss=0.7156952619552612\n",
      "INFO:absl:[84] test_loss=0.6959257125854492\n",
      "INFO:absl:[85] train_loss=0.48149606585502625, train_x1_loss=0.06710348278284073, train_x2_loss=0.4143912196159363\n",
      "INFO:absl:[85] val_loss=0.7273015379905701\n",
      "INFO:absl:[85] test_loss=0.7140141129493713\n",
      "INFO:absl:[86] train_loss=0.48017942905426025, train_x1_loss=0.066359743475914, train_x2_loss=0.4138198494911194\n",
      "INFO:absl:[86] val_loss=0.7428562641143799\n",
      "INFO:absl:[86] test_loss=0.728413999080658\n",
      "INFO:absl:[87] train_loss=0.4785540997982025, train_x1_loss=0.0664491355419159, train_x2_loss=0.4121047854423523\n",
      "INFO:absl:[87] val_loss=0.6973885893821716\n",
      "INFO:absl:[87] test_loss=0.6773467063903809\n",
      "INFO:absl:[88] train_loss=0.4821932911872864, train_x1_loss=0.06686056405305862, train_x2_loss=0.4153325855731964\n",
      "INFO:absl:[88] val_loss=0.6924236416816711\n",
      "INFO:absl:[88] test_loss=0.6761544942855835\n",
      "INFO:absl:[89] train_loss=0.47805526852607727, train_x1_loss=0.06663686782121658, train_x2_loss=0.4114188551902771\n",
      "INFO:absl:[89] val_loss=0.7079543471336365\n",
      "INFO:absl:[89] test_loss=0.687356173992157\n",
      "INFO:absl:[90] train_loss=0.47835856676101685, train_x1_loss=0.06591684371232986, train_x2_loss=0.412441223859787\n",
      "INFO:absl:[90] val_loss=0.6996908187866211\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[90] test_loss=0.6835776567459106\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[91] train_loss=0.4831582307815552, train_x1_loss=0.06879768520593643, train_x2_loss=0.4143608808517456\n",
      "INFO:absl:[91] val_loss=0.701838493347168\n",
      "INFO:absl:[91] test_loss=0.6861663460731506\n",
      "INFO:absl:[92] train_loss=0.4765247702598572, train_x1_loss=0.0659848004579544, train_x2_loss=0.4105408191680908\n",
      "INFO:absl:[92] val_loss=0.7373821139335632\n",
      "INFO:absl:[92] test_loss=0.7158008813858032\n",
      "INFO:absl:[93] train_loss=0.477016419172287, train_x1_loss=0.06626202911138535, train_x2_loss=0.41075408458709717\n",
      "INFO:absl:[93] val_loss=0.7028673887252808\n",
      "INFO:absl:[93] test_loss=0.6848886013031006\n",
      "INFO:absl:[94] train_loss=0.4764525890350342, train_x1_loss=0.06724857538938522, train_x2_loss=0.4092048406600952\n",
      "INFO:absl:[94] val_loss=0.7053369879722595\n",
      "INFO:absl:[94] test_loss=0.6852415800094604\n",
      "INFO:absl:[95] train_loss=0.47547203302383423, train_x1_loss=0.06707222014665604, train_x2_loss=0.4083997309207916\n",
      "INFO:absl:[95] val_loss=0.689420759677887\n",
      "INFO:absl:[95] test_loss=0.671617329120636\n",
      "INFO:absl:[96] train_loss=0.47645992040634155, train_x1_loss=0.06759344786405563, train_x2_loss=0.40886589884757996\n",
      "INFO:absl:[96] val_loss=0.6893147826194763\n",
      "INFO:absl:[96] test_loss=0.6719275116920471\n",
      "INFO:absl:[97] train_loss=0.47539326548576355, train_x1_loss=0.06608621031045914, train_x2_loss=0.4093080759048462\n",
      "INFO:absl:[97] val_loss=0.6993119120597839\n",
      "INFO:absl:[97] test_loss=0.6807430386543274\n",
      "INFO:absl:[98] train_loss=0.4761083424091339, train_x1_loss=0.06756293773651123, train_x2_loss=0.40854594111442566\n",
      "INFO:absl:[98] val_loss=0.7121539115905762\n",
      "INFO:absl:[98] test_loss=0.6953273415565491\n",
      "INFO:absl:[99] train_loss=0.4756891131401062, train_x1_loss=0.0676395446062088, train_x2_loss=0.40804991126060486\n",
      "INFO:absl:[99] val_loss=0.6887823939323425\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[99] test_loss=0.6721538305282593\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n"
     ]
    }
   ],
   "source": [
    "for trial in range(num_trials):\n",
    "    _, _, connected_5_eval_metric, connected_5_epoch_losses = train_and_evaluate_with_data(\n",
    "        config=connected_configs_5[trial], workdir=connected_workdirs_5[trial], datasets=all_connected_5_datasets[trial])\n",
    "    \n",
    "    connected_5_eval_metrics.append(connected_5_eval_metric)\n",
    "    \n",
    "    all_connected_5_epoch_losses.append(connected_5_epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'elu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 10, 'dropout_rate': 0.21813956053366504, 'edge_features': (4, 2), 'epochs': 100, 'eval_every_epochs': 1, 'fully_connected_edges': 3, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 1, 'layer_norm': False, 'learning_rate': 3.873531888351175e-05, 'log_every_epochs': 1, 'max_checkpts_to_keep': 2, 'model': 'MLPGraphNetwork', 'n_blocks': 1, 'n_samples': 5000, 'node_features': (4, 2), 'normalize': True, 'optimizer': 'adam', 'output_delay': 0, 'output_steps': 4, 'sample_buffer': -4, 'seed': 65, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 120, 'timestep_duration': 3, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+--------+------+---------+-------+\n",
      "| Name                                   | Shape  | Size | Mean    | Std   |\n",
      "+----------------------------------------+--------+------+---------+-------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (4,)   | 4    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 4) | 24   | -0.0819 | 0.36  |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (2,)   | 2    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (4, 2) | 8    | 0.289   | 0.469 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (4,)   | 4    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (7, 4) | 28   | -0.0395 | 0.333 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)   | 2    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (4, 2) | 8    | -0.135  | 0.542 |\n",
      "+----------------------------------------+--------+------+---------+-------+\n",
      "Total: 80\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Restoring checkpoint: tests/outputs/connected_3/trial-0/checkpoints/ckpt-12\n",
      "INFO:absl:Restored save_counter=12 restored_checkpoint=tests/outputs/connected_3/trial-0/checkpoints/ckpt-12\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.01s.\n",
      "INFO:absl:Starting training.\n",
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'elu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 10, 'dropout_rate': 0.21813956053366504, 'edge_features': (4, 2), 'epochs': 100, 'eval_every_epochs': 1, 'fully_connected_edges': 3, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 1, 'layer_norm': False, 'learning_rate': 3.873531888351175e-05, 'log_every_epochs': 1, 'max_checkpts_to_keep': 2, 'model': 'MLPGraphNetwork', 'n_blocks': 1, 'n_samples': 5000, 'node_features': (4, 2), 'normalize': True, 'optimizer': 'adam', 'output_delay': 0, 'output_steps': 4, 'sample_buffer': -4, 'seed': 66, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 120, 'timestep_duration': 3, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+--------+------+---------+-------+\n",
      "| Name                                   | Shape  | Size | Mean    | Std   |\n",
      "+----------------------------------------+--------+------+---------+-------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (4,)   | 4    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 4) | 24   | -0.0819 | 0.36  |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (2,)   | 2    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (4, 2) | 8    | 0.289   | 0.469 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (4,)   | 4    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (7, 4) | 28   | -0.0395 | 0.333 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)   | 2    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (4, 2) | 8    | -0.135  | 0.542 |\n",
      "+----------------------------------------+--------+------+---------+-------+\n",
      "Total: 80\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Checkpoint None does not exist.\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.01s.\n",
      "INFO:absl:Starting training.\n",
      "INFO:absl:[0] train_loss=3.088252305984497, train_x1_loss=1.7377716302871704, train_x2_loss=1.3504769802093506\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[0] val_loss=2.9922237396240234\n",
      "INFO:absl:[0] test_loss=3.0328478813171387\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Created artifact [10] Profile of type ArtifactType.URL and value None.\n",
      "INFO:absl:[1] train_loss=2.1215128898620605, train_x1_loss=1.086387038230896, train_x2_loss=1.0351247787475586\n",
      "INFO:absl:[1] val_loss=2.4935874938964844\n",
      "INFO:absl:[1] test_loss=2.533302068710327\n",
      "INFO:absl:[2] train_loss=1.6707770824432373, train_x1_loss=0.7586538791656494, train_x2_loss=0.9121224880218506\n",
      "INFO:absl:[2] val_loss=2.182269334793091\n",
      "INFO:absl:[2] test_loss=2.22078537940979\n",
      "INFO:absl:[3] train_loss=1.4533054828643799, train_x1_loss=0.5763273239135742, train_x2_loss=0.8769771456718445\n",
      "INFO:absl:[3] val_loss=2.1082468032836914\n",
      "INFO:absl:[3] test_loss=2.1502933502197266\n",
      "INFO:absl:[4] train_loss=1.3591241836547852, train_x1_loss=0.5087433457374573, train_x2_loss=0.8503814339637756\n",
      "INFO:absl:[4] val_loss=2.051237106323242\n",
      "INFO:absl:[4] test_loss=2.095275402069092\n",
      "INFO:absl:[5] train_loss=1.311151385307312, train_x1_loss=0.486645370721817, train_x2_loss=0.8245062232017517\n",
      "INFO:absl:[5] val_loss=1.99848473072052\n",
      "INFO:absl:[5] test_loss=2.042916774749756\n",
      "INFO:absl:[6] train_loss=1.274613380432129, train_x1_loss=0.47157227993011475, train_x2_loss=0.8030421733856201\n",
      "INFO:absl:[6] val_loss=1.9648123979568481\n",
      "INFO:absl:[6] test_loss=2.0097901821136475\n",
      "INFO:absl:[7] train_loss=1.2515180110931396, train_x1_loss=0.4652276337146759, train_x2_loss=0.7862908840179443\n",
      "INFO:absl:[7] val_loss=1.9416674375534058\n",
      "INFO:absl:[7] test_loss=1.986824870109558\n",
      "INFO:absl:[8] train_loss=1.2361339330673218, train_x1_loss=0.460044264793396, train_x2_loss=0.7760862112045288\n",
      "INFO:absl:[8] val_loss=1.9426952600479126\n",
      "INFO:absl:[8] test_loss=1.9878274202346802\n",
      "INFO:absl:[9] train_loss=1.226149082183838, train_x1_loss=0.45730870962142944, train_x2_loss=0.7688402533531189\n",
      "INFO:absl:[9] val_loss=1.9264243841171265\n",
      "INFO:absl:[9] test_loss=1.9713577032089233\n",
      "INFO:absl:[10] train_loss=1.2217094898223877, train_x1_loss=0.45674511790275574, train_x2_loss=0.7649660110473633\n",
      "INFO:absl:[10] val_loss=1.923744797706604\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[10] test_loss=1.9684991836547852\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[11] train_loss=1.2204174995422363, train_x1_loss=0.4562666118144989, train_x2_loss=0.7641522288322449\n",
      "INFO:absl:[11] val_loss=1.9191466569900513\n",
      "INFO:absl:[11] test_loss=1.9641481637954712\n",
      "INFO:absl:[12] train_loss=1.2155513763427734, train_x1_loss=0.45324957370758057, train_x2_loss=0.7622987627983093\n",
      "INFO:absl:[12] val_loss=1.9188621044158936\n",
      "INFO:absl:[12] test_loss=1.9639561176300049\n",
      "INFO:absl:[13] train_loss=1.2133843898773193, train_x1_loss=0.45349225401878357, train_x2_loss=0.7598885893821716\n",
      "INFO:absl:[13] val_loss=1.915855050086975\n",
      "INFO:absl:[13] test_loss=1.9609471559524536\n",
      "INFO:absl:[14] train_loss=1.212789535522461, train_x1_loss=0.4534318447113037, train_x2_loss=0.759356677532196\n",
      "INFO:absl:[14] val_loss=1.90996253490448\n",
      "INFO:absl:[14] test_loss=1.9546762704849243\n",
      "INFO:absl:[15] train_loss=1.2130366563796997, train_x1_loss=0.4568958282470703, train_x2_loss=0.756141722202301\n",
      "INFO:absl:[15] val_loss=1.9164841175079346\n",
      "INFO:absl:[15] test_loss=1.9614195823669434\n",
      "INFO:absl:[16] train_loss=1.2103617191314697, train_x1_loss=0.4555375576019287, train_x2_loss=0.7548249959945679\n",
      "INFO:absl:[16] val_loss=1.9132684469223022\n",
      "INFO:absl:[16] test_loss=1.957870602607727\n",
      "INFO:absl:[17] train_loss=1.2102534770965576, train_x1_loss=0.45441827178001404, train_x2_loss=0.7558368444442749\n",
      "INFO:absl:[17] val_loss=1.9074101448059082\n",
      "INFO:absl:[17] test_loss=1.9521411657333374\n",
      "INFO:absl:[18] train_loss=1.2144581079483032, train_x1_loss=0.4576992988586426, train_x2_loss=0.7567596435546875\n",
      "INFO:absl:[18] val_loss=1.912505865097046\n",
      "INFO:absl:[18] test_loss=1.957056999206543\n",
      "INFO:absl:[19] train_loss=1.2101274728775024, train_x1_loss=0.45508894324302673, train_x2_loss=0.7550392150878906\n",
      "INFO:absl:[19] val_loss=1.9137437343597412\n",
      "INFO:absl:[19] test_loss=1.9583338499069214\n",
      "INFO:absl:[20] train_loss=1.2094871997833252, train_x1_loss=0.4570225775241852, train_x2_loss=0.752463161945343\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[20] val_loss=1.9061681032180786\n",
      "INFO:absl:[20] test_loss=1.9507381916046143\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[21] train_loss=1.2082345485687256, train_x1_loss=0.4544994831085205, train_x2_loss=0.753735363483429\n",
      "INFO:absl:[21] val_loss=1.9138238430023193\n",
      "INFO:absl:[21] test_loss=1.9584623575210571\n",
      "INFO:absl:[22] train_loss=1.2071354389190674, train_x1_loss=0.4536840617656708, train_x2_loss=0.7534533143043518\n",
      "INFO:absl:[22] val_loss=1.9146775007247925\n",
      "INFO:absl:[22] test_loss=1.9594721794128418\n",
      "INFO:absl:[23] train_loss=1.214299201965332, train_x1_loss=0.45824411511421204, train_x2_loss=0.7560547590255737\n",
      "INFO:absl:[23] val_loss=1.9153010845184326\n",
      "INFO:absl:[23] test_loss=1.960029125213623\n",
      "INFO:absl:[24] train_loss=1.2097806930541992, train_x1_loss=0.4557325541973114, train_x2_loss=0.7540478110313416\n",
      "INFO:absl:[24] val_loss=1.9107712507247925\n",
      "INFO:absl:[24] test_loss=1.9555315971374512\n",
      "INFO:absl:[25] train_loss=1.2103848457336426, train_x1_loss=0.456172376871109, train_x2_loss=0.7542115449905396\n",
      "INFO:absl:[25] val_loss=1.9146795272827148\n",
      "INFO:absl:[25] test_loss=1.959409475326538\n",
      "INFO:absl:[26] train_loss=1.2072718143463135, train_x1_loss=0.4540632367134094, train_x2_loss=0.7532097697257996\n",
      "INFO:absl:[26] val_loss=1.9148825407028198\n",
      "INFO:absl:[26] test_loss=1.9598206281661987\n",
      "INFO:absl:[27] train_loss=1.20828115940094, train_x1_loss=0.45445123314857483, train_x2_loss=0.7538301348686218\n",
      "INFO:absl:[27] val_loss=1.914333462715149\n",
      "INFO:absl:[27] test_loss=1.9590388536453247\n",
      "INFO:absl:[28] train_loss=1.2082955837249756, train_x1_loss=0.4545937776565552, train_x2_loss=0.7537010312080383\n",
      "INFO:absl:[28] val_loss=1.9131227731704712\n",
      "INFO:absl:[28] test_loss=1.9578543901443481\n",
      "INFO:absl:[29] train_loss=1.207254409790039, train_x1_loss=0.4544455111026764, train_x2_loss=0.7528095841407776\n",
      "INFO:absl:[29] val_loss=1.9202404022216797\n",
      "INFO:absl:[29] test_loss=1.9650766849517822\n",
      "INFO:absl:[30] train_loss=1.2084081172943115, train_x1_loss=0.45502138137817383, train_x2_loss=0.7533865571022034\n",
      "INFO:absl:[30] val_loss=1.9055949449539185\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[30] test_loss=1.9502898454666138\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[31] train_loss=1.206817388534546, train_x1_loss=0.4525541365146637, train_x2_loss=0.7542641162872314\n",
      "INFO:absl:[31] val_loss=1.91348135471344\n",
      "INFO:absl:[31] test_loss=1.9583524465560913\n",
      "INFO:absl:[32] train_loss=1.2077240943908691, train_x1_loss=0.4538807272911072, train_x2_loss=0.7538431882858276\n",
      "INFO:absl:[32] val_loss=1.9173710346221924\n",
      "INFO:absl:[32] test_loss=1.9622271060943604\n",
      "INFO:absl:[33] train_loss=1.2027455568313599, train_x1_loss=0.4520135223865509, train_x2_loss=0.7507332563400269\n",
      "INFO:absl:[33] val_loss=1.910247802734375\n",
      "INFO:absl:[33] test_loss=1.9550318717956543\n",
      "INFO:absl:[34] train_loss=1.2029194831848145, train_x1_loss=0.4518994092941284, train_x2_loss=0.7510185837745667\n",
      "INFO:absl:[34] val_loss=1.9131126403808594\n",
      "INFO:absl:[34] test_loss=1.9577630758285522\n",
      "INFO:absl:[35] train_loss=1.1983164548873901, train_x1_loss=0.44682592153549194, train_x2_loss=0.7514898777008057\n",
      "INFO:absl:[35] val_loss=1.912678837776184\n",
      "INFO:absl:[35] test_loss=1.957285761833191\n",
      "INFO:absl:[36] train_loss=1.204848051071167, train_x1_loss=0.45309075713157654, train_x2_loss=0.7517574429512024\n",
      "INFO:absl:[36] val_loss=1.908076524734497\n",
      "INFO:absl:[36] test_loss=1.9526656866073608\n",
      "INFO:absl:[37] train_loss=1.204554557800293, train_x1_loss=0.452095627784729, train_x2_loss=0.7524610757827759\n",
      "INFO:absl:[37] val_loss=1.9115080833435059\n",
      "INFO:absl:[37] test_loss=1.956490397453308\n",
      "INFO:absl:[38] train_loss=1.2037659883499146, train_x1_loss=0.452077180147171, train_x2_loss=0.7516899704933167\n",
      "INFO:absl:[38] val_loss=1.9139610528945923\n",
      "INFO:absl:[38] test_loss=1.9589567184448242\n",
      "INFO:absl:[39] train_loss=1.2011405229568481, train_x1_loss=0.4493369162082672, train_x2_loss=0.7518039345741272\n",
      "INFO:absl:[39] val_loss=1.908259630203247\n",
      "INFO:absl:[39] test_loss=1.953075647354126\n",
      "INFO:absl:[40] train_loss=1.2012896537780762, train_x1_loss=0.4492126703262329, train_x2_loss=0.7520766854286194\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[40] val_loss=1.910089373588562\n",
      "INFO:absl:[40] test_loss=1.9548791646957397\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[41] train_loss=1.1986595392227173, train_x1_loss=0.4484635591506958, train_x2_loss=0.7501951456069946\n",
      "INFO:absl:[41] val_loss=1.9053438901901245\n",
      "INFO:absl:[41] test_loss=1.9503730535507202\n",
      "INFO:absl:[42] train_loss=1.1957204341888428, train_x1_loss=0.4482368230819702, train_x2_loss=0.7474835515022278\n",
      "INFO:absl:[42] val_loss=1.9081758260726929\n",
      "INFO:absl:[42] test_loss=1.9530686140060425\n",
      "INFO:absl:[43] train_loss=1.2010931968688965, train_x1_loss=0.44877371191978455, train_x2_loss=0.7523199915885925\n",
      "INFO:absl:[43] val_loss=1.9057352542877197\n",
      "INFO:absl:[43] test_loss=1.9508185386657715\n",
      "INFO:absl:[44] train_loss=1.1994717121124268, train_x1_loss=0.4478369951248169, train_x2_loss=0.7516360878944397\n",
      "INFO:absl:[44] val_loss=1.9077824354171753\n",
      "INFO:absl:[44] test_loss=1.9530072212219238\n",
      "INFO:absl:[45] train_loss=1.1944425106048584, train_x1_loss=0.4425228238105774, train_x2_loss=0.7519204616546631\n",
      "INFO:absl:[45] val_loss=1.9048900604248047\n",
      "INFO:absl:[45] test_loss=1.9500179290771484\n",
      "INFO:absl:[46] train_loss=1.2006696462631226, train_x1_loss=0.44892287254333496, train_x2_loss=0.7517455220222473\n",
      "INFO:absl:[46] val_loss=1.9081058502197266\n",
      "INFO:absl:[46] test_loss=1.9533385038375854\n",
      "INFO:absl:[47] train_loss=1.1943165063858032, train_x1_loss=0.4456251263618469, train_x2_loss=0.7486894726753235\n",
      "INFO:absl:[47] val_loss=1.9043306112289429\n",
      "INFO:absl:[47] test_loss=1.9494938850402832\n",
      "INFO:absl:[48] train_loss=1.1973655223846436, train_x1_loss=0.4450657069683075, train_x2_loss=0.7523009181022644\n",
      "INFO:absl:[48] val_loss=1.9092060327529907\n",
      "INFO:absl:[48] test_loss=1.9545354843139648\n",
      "INFO:absl:[49] train_loss=1.1938533782958984, train_x1_loss=0.4436359107494354, train_x2_loss=0.7502180337905884\n",
      "INFO:absl:[49] val_loss=1.9061713218688965\n",
      "INFO:absl:[49] test_loss=1.9515544176101685\n",
      "INFO:absl:[50] train_loss=1.1973797082901, train_x1_loss=0.4469177722930908, train_x2_loss=0.7504627704620361\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[50] val_loss=1.9071649312973022\n",
      "INFO:absl:[50] test_loss=1.9525161981582642\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[51] train_loss=1.1947135925292969, train_x1_loss=0.4466421604156494, train_x2_loss=0.7480701208114624\n",
      "INFO:absl:[51] val_loss=1.9006283283233643\n",
      "INFO:absl:[51] test_loss=1.945939540863037\n",
      "INFO:absl:[52] train_loss=1.1930592060089111, train_x1_loss=0.44452375173568726, train_x2_loss=0.748536229133606\n",
      "INFO:absl:[52] val_loss=1.9029057025909424\n",
      "INFO:absl:[52] test_loss=1.9484983682632446\n",
      "INFO:absl:[53] train_loss=1.1899744272232056, train_x1_loss=0.4442957937717438, train_x2_loss=0.7456813454627991\n",
      "INFO:absl:[53] val_loss=1.9008264541625977\n",
      "INFO:absl:[53] test_loss=1.9462257623672485\n",
      "INFO:absl:[54] train_loss=1.1977617740631104, train_x1_loss=0.44723138213157654, train_x2_loss=0.7505313754081726\n",
      "INFO:absl:[54] val_loss=1.9026778936386108\n",
      "INFO:absl:[54] test_loss=1.9482077360153198\n",
      "INFO:absl:[55] train_loss=1.1904906034469604, train_x1_loss=0.4413767158985138, train_x2_loss=0.7491118907928467\n",
      "INFO:absl:[55] val_loss=1.9073039293289185\n",
      "INFO:absl:[55] test_loss=1.9529471397399902\n",
      "INFO:absl:[56] train_loss=1.1927659511566162, train_x1_loss=0.4444526731967926, train_x2_loss=0.7483108043670654\n",
      "INFO:absl:[56] val_loss=1.904018521308899\n",
      "INFO:absl:[56] test_loss=1.9495424032211304\n",
      "INFO:absl:[57] train_loss=1.191083550453186, train_x1_loss=0.44257691502571106, train_x2_loss=0.7485054135322571\n",
      "INFO:absl:[57] val_loss=1.9016401767730713\n",
      "INFO:absl:[57] test_loss=1.947312831878662\n",
      "INFO:absl:[58] train_loss=1.191715121269226, train_x1_loss=0.4431818723678589, train_x2_loss=0.7485350966453552\n",
      "INFO:absl:[58] val_loss=1.8990960121154785\n",
      "INFO:absl:[58] test_loss=1.9449865818023682\n",
      "INFO:absl:Setting work unit notes: 3448.5 steps/s, 59.1% (206913/350000), ETA: 0m (1m : 0.1% checkpoint, 12.6% eval)\n",
      "INFO:absl:[206913] steps_per_sec=3448.549424\n",
      "INFO:absl:[59] train_loss=1.1882965564727783, train_x1_loss=0.44134026765823364, train_x2_loss=0.7469540238380432\n",
      "INFO:absl:[59] val_loss=1.8992104530334473\n",
      "INFO:absl:[59] test_loss=1.9448412656784058\n",
      "INFO:absl:[60] train_loss=1.183122992515564, train_x1_loss=0.43908530473709106, train_x2_loss=0.7440378665924072\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[60] val_loss=1.8971186876296997\n",
      "INFO:absl:[60] test_loss=1.9429309368133545\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[61] train_loss=1.1868488788604736, train_x1_loss=0.43835169076919556, train_x2_loss=0.7484981417655945\n",
      "INFO:absl:[61] val_loss=1.9065752029418945\n",
      "INFO:absl:[61] test_loss=1.9524822235107422\n",
      "INFO:absl:[62] train_loss=1.1892420053482056, train_x1_loss=0.4417497515678406, train_x2_loss=0.7474919557571411\n",
      "INFO:absl:[62] val_loss=1.8991167545318604\n",
      "INFO:absl:[62] test_loss=1.9449238777160645\n",
      "INFO:absl:[63] train_loss=1.1900453567504883, train_x1_loss=0.4420274496078491, train_x2_loss=0.7480194568634033\n",
      "INFO:absl:[63] val_loss=1.8974946737289429\n",
      "INFO:absl:[63] test_loss=1.9431663751602173\n",
      "INFO:absl:[64] train_loss=1.1859424114227295, train_x1_loss=0.44005823135375977, train_x2_loss=0.745883584022522\n",
      "INFO:absl:[64] val_loss=1.901646375656128\n",
      "INFO:absl:[64] test_loss=1.947476863861084\n",
      "INFO:absl:[65] train_loss=1.1873974800109863, train_x1_loss=0.43868622183799744, train_x2_loss=0.7487095594406128\n",
      "INFO:absl:[65] val_loss=1.8974424600601196\n",
      "INFO:absl:[65] test_loss=1.9432034492492676\n",
      "INFO:absl:[66] train_loss=1.1840558052062988, train_x1_loss=0.4366645812988281, train_x2_loss=0.7473917007446289\n",
      "INFO:absl:[66] val_loss=1.900371789932251\n",
      "INFO:absl:[66] test_loss=1.946269154548645\n",
      "INFO:absl:[67] train_loss=1.188541054725647, train_x1_loss=0.4425352215766907, train_x2_loss=0.7460071444511414\n",
      "INFO:absl:[67] val_loss=1.8961820602416992\n",
      "INFO:absl:[67] test_loss=1.9420678615570068\n",
      "INFO:absl:[68] train_loss=1.186127781867981, train_x1_loss=0.4404216706752777, train_x2_loss=0.7457062005996704\n",
      "INFO:absl:[68] val_loss=1.8969721794128418\n",
      "INFO:absl:[68] test_loss=1.9428635835647583\n",
      "INFO:absl:[69] train_loss=1.1840327978134155, train_x1_loss=0.4387674033641815, train_x2_loss=0.7452648282051086\n",
      "INFO:absl:[69] val_loss=1.8946101665496826\n",
      "INFO:absl:[69] test_loss=1.940659999847412\n",
      "INFO:absl:[70] train_loss=1.1928828954696655, train_x1_loss=0.4441368877887726, train_x2_loss=0.7487419247627258\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[70] val_loss=1.8954228162765503\n",
      "INFO:absl:[70] test_loss=1.941561222076416\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[71] train_loss=1.1826066970825195, train_x1_loss=0.4399278461933136, train_x2_loss=0.7426795959472656\n",
      "INFO:absl:[71] val_loss=1.8961340188980103\n",
      "INFO:absl:[71] test_loss=1.9420456886291504\n",
      "INFO:absl:[72] train_loss=1.1842483282089233, train_x1_loss=0.4386129677295685, train_x2_loss=0.7456359267234802\n",
      "INFO:absl:[72] val_loss=1.8945618867874146\n",
      "INFO:absl:[72] test_loss=1.9403928518295288\n",
      "INFO:absl:[73] train_loss=1.1822575330734253, train_x1_loss=0.4354653060436249, train_x2_loss=0.7467929124832153\n",
      "INFO:absl:[73] val_loss=1.8973332643508911\n",
      "INFO:absl:[73] test_loss=1.9432908296585083\n",
      "INFO:absl:[74] train_loss=1.1878784894943237, train_x1_loss=0.44074660539627075, train_x2_loss=0.7471323013305664\n",
      "INFO:absl:[74] val_loss=1.899189829826355\n",
      "INFO:absl:[74] test_loss=1.9451967477798462\n",
      "INFO:absl:[75] train_loss=1.1822288036346436, train_x1_loss=0.43559402227401733, train_x2_loss=0.7466362714767456\n",
      "INFO:absl:[75] val_loss=1.89363431930542\n",
      "INFO:absl:[75] test_loss=1.9398078918457031\n",
      "INFO:absl:[76] train_loss=1.1823323965072632, train_x1_loss=0.4352709650993347, train_x2_loss=0.7470620274543762\n",
      "INFO:absl:[76] val_loss=1.8955752849578857\n",
      "INFO:absl:[76] test_loss=1.9415156841278076\n",
      "INFO:absl:[77] train_loss=1.1850056648254395, train_x1_loss=0.43770459294319153, train_x2_loss=0.7473020553588867\n",
      "INFO:absl:[77] val_loss=1.8975589275360107\n",
      "INFO:absl:[77] test_loss=1.9436999559402466\n",
      "INFO:absl:[78] train_loss=1.1819989681243896, train_x1_loss=0.43761536478996277, train_x2_loss=0.744381844997406\n",
      "INFO:absl:[78] val_loss=1.8957217931747437\n",
      "INFO:absl:[78] test_loss=1.9418061971664429\n",
      "INFO:absl:[79] train_loss=1.179382085800171, train_x1_loss=0.4357835054397583, train_x2_loss=0.7435973882675171\n",
      "INFO:absl:[79] val_loss=1.8939288854599\n",
      "INFO:absl:[79] test_loss=1.9398949146270752\n",
      "INFO:absl:[80] train_loss=1.1815980672836304, train_x1_loss=0.43709588050842285, train_x2_loss=0.7445014715194702\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[80] val_loss=1.8939117193222046\n",
      "INFO:absl:[80] test_loss=1.940164566040039\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[81] train_loss=1.1841458082199097, train_x1_loss=0.43869325518608093, train_x2_loss=0.745453417301178\n",
      "INFO:absl:[81] val_loss=1.8952689170837402\n",
      "INFO:absl:[81] test_loss=1.9413279294967651\n",
      "INFO:absl:[82] train_loss=1.1825066804885864, train_x1_loss=0.43734467029571533, train_x2_loss=0.7451621890068054\n",
      "INFO:absl:[82] val_loss=1.8995311260223389\n",
      "INFO:absl:[82] test_loss=1.9456180334091187\n",
      "INFO:absl:[83] train_loss=1.1803208589553833, train_x1_loss=0.4371197819709778, train_x2_loss=0.7432034611701965\n",
      "INFO:absl:[83] val_loss=1.8929295539855957\n",
      "INFO:absl:[83] test_loss=1.9387418031692505\n",
      "INFO:absl:[84] train_loss=1.1773930788040161, train_x1_loss=0.43505120277404785, train_x2_loss=0.742339551448822\n",
      "INFO:absl:[84] val_loss=1.8934720754623413\n",
      "INFO:absl:[84] test_loss=1.9395484924316406\n",
      "INFO:absl:[85] train_loss=1.178521752357483, train_x1_loss=0.43252238631248474, train_x2_loss=0.7460008859634399\n",
      "INFO:absl:[85] val_loss=1.8948599100112915\n",
      "INFO:absl:[85] test_loss=1.940753698348999\n",
      "INFO:absl:[86] train_loss=1.1822502613067627, train_x1_loss=0.4359305799007416, train_x2_loss=0.7463198304176331\n",
      "INFO:absl:[86] val_loss=1.8940895795822144\n",
      "INFO:absl:[86] test_loss=1.9399917125701904\n",
      "INFO:absl:[87] train_loss=1.1839983463287354, train_x1_loss=0.4382695257663727, train_x2_loss=0.7457302808761597\n",
      "INFO:absl:[87] val_loss=1.8949291706085205\n",
      "INFO:absl:[87] test_loss=1.941017985343933\n",
      "INFO:absl:[88] train_loss=1.1770484447479248, train_x1_loss=0.4323744773864746, train_x2_loss=0.7446722388267517\n",
      "INFO:absl:[88] val_loss=1.8946548700332642\n",
      "INFO:absl:[88] test_loss=1.940862774848938\n",
      "INFO:absl:[89] train_loss=1.179938793182373, train_x1_loss=0.4344336986541748, train_x2_loss=0.7455061674118042\n",
      "INFO:absl:[89] val_loss=1.899678349494934\n",
      "INFO:absl:[89] test_loss=1.945803165435791\n",
      "INFO:absl:[90] train_loss=1.178777813911438, train_x1_loss=0.43405842781066895, train_x2_loss=0.7447198629379272\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[90] val_loss=1.888588309288025\n",
      "INFO:absl:[90] test_loss=1.9348348379135132\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[91] train_loss=1.1777197122573853, train_x1_loss=0.43349480628967285, train_x2_loss=0.7442269325256348\n",
      "INFO:absl:[91] val_loss=1.8904908895492554\n",
      "INFO:absl:[91] test_loss=1.936893105506897\n",
      "INFO:absl:[92] train_loss=1.1820579767227173, train_x1_loss=0.4353202283382416, train_x2_loss=0.7467383742332458\n",
      "INFO:absl:[92] val_loss=1.8911312818527222\n",
      "INFO:absl:[92] test_loss=1.9374797344207764\n",
      "INFO:absl:[93] train_loss=1.1774318218231201, train_x1_loss=0.43344447016716003, train_x2_loss=0.7439863085746765\n",
      "INFO:absl:[93] val_loss=1.8903435468673706\n",
      "INFO:absl:[93] test_loss=1.9364718198776245\n",
      "INFO:absl:[94] train_loss=1.1797969341278076, train_x1_loss=0.4371635615825653, train_x2_loss=0.7426304221153259\n",
      "INFO:absl:[94] val_loss=1.8964413404464722\n",
      "INFO:absl:[94] test_loss=1.942542314529419\n",
      "INFO:absl:[95] train_loss=1.1763628721237183, train_x1_loss=0.4332455098628998, train_x2_loss=0.7431173920631409\n",
      "INFO:absl:[95] val_loss=1.8929448127746582\n",
      "INFO:absl:[95] test_loss=1.9389629364013672\n",
      "INFO:absl:[96] train_loss=1.1790188550949097, train_x1_loss=0.4357767105102539, train_x2_loss=0.7432434558868408\n",
      "INFO:absl:[96] val_loss=1.898891568183899\n",
      "INFO:absl:[96] test_loss=1.9450706243515015\n",
      "INFO:absl:[97] train_loss=1.1779834032058716, train_x1_loss=0.43282222747802734, train_x2_loss=0.7451625466346741\n",
      "INFO:absl:[97] val_loss=1.8927747011184692\n",
      "INFO:absl:[97] test_loss=1.9391025304794312\n",
      "INFO:absl:[98] train_loss=1.1861850023269653, train_x1_loss=0.4399097263813019, train_x2_loss=0.7462757229804993\n",
      "INFO:absl:[98] val_loss=1.8926074504852295\n",
      "INFO:absl:[98] test_loss=1.9389111995697021\n",
      "INFO:absl:[99] train_loss=1.1791404485702515, train_x1_loss=0.43536376953125, train_x2_loss=0.7437762022018433\n",
      "INFO:absl:[99] val_loss=1.8933931589126587\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[99] test_loss=1.939582109451294\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'elu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 10, 'dropout_rate': 0.21813956053366504, 'edge_features': (4, 2), 'epochs': 100, 'eval_every_epochs': 1, 'fully_connected_edges': 3, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 1, 'layer_norm': False, 'learning_rate': 3.873531888351175e-05, 'log_every_epochs': 1, 'max_checkpts_to_keep': 2, 'model': 'MLPGraphNetwork', 'n_blocks': 1, 'n_samples': 5000, 'node_features': (4, 2), 'normalize': True, 'optimizer': 'adam', 'output_delay': 0, 'output_steps': 4, 'sample_buffer': -4, 'seed': 67, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 120, 'timestep_duration': 3, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+--------+------+---------+-------+\n",
      "| Name                                   | Shape  | Size | Mean    | Std   |\n",
      "+----------------------------------------+--------+------+---------+-------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (4,)   | 4    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 4) | 24   | -0.0819 | 0.36  |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (2,)   | 2    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (4, 2) | 8    | 0.289   | 0.469 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (4,)   | 4    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (7, 4) | 28   | -0.0395 | 0.333 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)   | 2    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (4, 2) | 8    | -0.135  | 0.542 |\n",
      "+----------------------------------------+--------+------+---------+-------+\n",
      "Total: 80\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Checkpoint None does not exist.\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.01s.\n",
      "INFO:absl:Starting training.\n",
      "INFO:absl:[0] train_loss=3.2044003009796143, train_x1_loss=1.823408603668213, train_x2_loss=1.3809921741485596\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[0] val_loss=3.002687692642212\n",
      "INFO:absl:[0] test_loss=3.0914604663848877\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Created artifact [10] Profile of type ArtifactType.URL and value None.\n",
      "INFO:absl:[1] train_loss=2.213911294937134, train_x1_loss=1.1539537906646729, train_x2_loss=1.0599583387374878\n",
      "INFO:absl:[1] val_loss=2.5634801387786865\n",
      "INFO:absl:[1] test_loss=2.651629686355591\n",
      "INFO:absl:[2] train_loss=1.7521172761917114, train_x1_loss=0.8209035396575928, train_x2_loss=0.9312127232551575\n",
      "INFO:absl:[2] val_loss=2.184957981109619\n",
      "INFO:absl:[2] test_loss=2.277578353881836\n",
      "INFO:absl:[3] train_loss=1.4922925233840942, train_x1_loss=0.606008768081665, train_x2_loss=0.886281430721283\n",
      "INFO:absl:[3] val_loss=2.0873026847839355\n",
      "INFO:absl:[3] test_loss=2.1768455505371094\n",
      "INFO:absl:[4] train_loss=1.387934684753418, train_x1_loss=0.5246988534927368, train_x2_loss=0.8632358312606812\n",
      "INFO:absl:[4] val_loss=2.030592679977417\n",
      "INFO:absl:[4] test_loss=2.1176655292510986\n",
      "INFO:absl:[5] train_loss=1.331618309020996, train_x1_loss=0.4938352108001709, train_x2_loss=0.8377817273139954\n",
      "INFO:absl:[5] val_loss=1.9722470045089722\n",
      "INFO:absl:[5] test_loss=2.058783769607544\n",
      "INFO:absl:[6] train_loss=1.28714120388031, train_x1_loss=0.4736960828304291, train_x2_loss=0.8134452700614929\n",
      "INFO:absl:[6] val_loss=1.9354374408721924\n",
      "INFO:absl:[6] test_loss=2.0218801498413086\n",
      "INFO:absl:[7] train_loss=1.2636390924453735, train_x1_loss=0.4664817154407501, train_x2_loss=0.7971572279930115\n",
      "INFO:absl:[7] val_loss=1.9131813049316406\n",
      "INFO:absl:[7] test_loss=1.999385952949524\n",
      "INFO:absl:[8] train_loss=1.2471539974212646, train_x1_loss=0.46057140827178955, train_x2_loss=0.7865837812423706\n",
      "INFO:absl:[8] val_loss=1.897850751876831\n",
      "INFO:absl:[8] test_loss=1.9842510223388672\n",
      "INFO:absl:[9] train_loss=1.2399907112121582, train_x1_loss=0.4606100618839264, train_x2_loss=0.7793800830841064\n",
      "INFO:absl:[9] val_loss=1.8834398984909058\n",
      "INFO:absl:[9] test_loss=1.9702152013778687\n",
      "INFO:absl:[10] train_loss=1.2320107221603394, train_x1_loss=0.45537158846855164, train_x2_loss=0.7766405344009399\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[10] val_loss=1.8853451013565063\n",
      "INFO:absl:[10] test_loss=1.9722230434417725\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[11] train_loss=1.2283107042312622, train_x1_loss=0.4548495411872864, train_x2_loss=0.773463249206543\n",
      "INFO:absl:[11] val_loss=1.8723928928375244\n",
      "INFO:absl:[11] test_loss=1.9594769477844238\n",
      "INFO:absl:[12] train_loss=1.228326439857483, train_x1_loss=0.45493438839912415, train_x2_loss=0.773391604423523\n",
      "INFO:absl:[12] val_loss=1.8777846097946167\n",
      "INFO:absl:[12] test_loss=1.9648728370666504\n",
      "INFO:absl:[13] train_loss=1.2267601490020752, train_x1_loss=0.4574755132198334, train_x2_loss=0.7692852020263672\n",
      "INFO:absl:[13] val_loss=1.8755515813827515\n",
      "INFO:absl:[13] test_loss=1.9627528190612793\n",
      "INFO:absl:[14] train_loss=1.2229087352752686, train_x1_loss=0.45407143235206604, train_x2_loss=0.768838107585907\n",
      "INFO:absl:[14] val_loss=1.8725934028625488\n",
      "INFO:absl:[14] test_loss=1.9597023725509644\n",
      "INFO:absl:[15] train_loss=1.2207205295562744, train_x1_loss=0.4546646177768707, train_x2_loss=0.766058087348938\n",
      "INFO:absl:[15] val_loss=1.868823528289795\n",
      "INFO:absl:[15] test_loss=1.956145167350769\n",
      "INFO:absl:[16] train_loss=1.2220134735107422, train_x1_loss=0.4569395184516907, train_x2_loss=0.7650717496871948\n",
      "INFO:absl:[16] val_loss=1.8742468357086182\n",
      "INFO:absl:[16] test_loss=1.9615498781204224\n",
      "INFO:absl:[17] train_loss=1.2174839973449707, train_x1_loss=0.4526611566543579, train_x2_loss=0.764824628829956\n",
      "INFO:absl:[17] val_loss=1.874974250793457\n",
      "INFO:absl:[17] test_loss=1.9620065689086914\n",
      "INFO:absl:[18] train_loss=1.2189935445785522, train_x1_loss=0.45210134983062744, train_x2_loss=0.7668925523757935\n",
      "INFO:absl:[18] val_loss=1.8749539852142334\n",
      "INFO:absl:[18] test_loss=1.962224006652832\n",
      "INFO:absl:[19] train_loss=1.2245521545410156, train_x1_loss=0.4582955539226532, train_x2_loss=0.7662556767463684\n",
      "INFO:absl:[19] val_loss=1.8740204572677612\n",
      "INFO:absl:[19] test_loss=1.9612300395965576\n",
      "INFO:absl:[20] train_loss=1.2206205129623413, train_x1_loss=0.45589306950569153, train_x2_loss=0.7647250294685364\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[20] val_loss=1.873142957687378\n",
      "INFO:absl:[20] test_loss=1.9604089260101318\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[21] train_loss=1.21750009059906, train_x1_loss=0.4534038007259369, train_x2_loss=0.7640972137451172\n",
      "INFO:absl:[21] val_loss=1.8772295713424683\n",
      "INFO:absl:[21] test_loss=1.964201807975769\n",
      "INFO:absl:[22] train_loss=1.2178587913513184, train_x1_loss=0.4543076455593109, train_x2_loss=0.7635515928268433\n",
      "INFO:absl:[22] val_loss=1.8702820539474487\n",
      "INFO:absl:[22] test_loss=1.9575773477554321\n",
      "INFO:absl:[23] train_loss=1.2132283449172974, train_x1_loss=0.4496648609638214, train_x2_loss=0.7635648250579834\n",
      "INFO:absl:[23] val_loss=1.8685333728790283\n",
      "INFO:absl:[23] test_loss=1.9557645320892334\n",
      "INFO:absl:[24] train_loss=1.2175064086914062, train_x1_loss=0.45376890897750854, train_x2_loss=0.7637368440628052\n",
      "INFO:absl:[24] val_loss=1.868656873703003\n",
      "INFO:absl:[24] test_loss=1.9559791088104248\n",
      "INFO:absl:[25] train_loss=1.2207529544830322, train_x1_loss=0.4575406014919281, train_x2_loss=0.7632105350494385\n",
      "INFO:absl:[25] val_loss=1.8702470064163208\n",
      "INFO:absl:[25] test_loss=1.95746648311615\n",
      "INFO:absl:[26] train_loss=1.215562343597412, train_x1_loss=0.4529285430908203, train_x2_loss=0.7626327872276306\n",
      "INFO:absl:[26] val_loss=1.8724241256713867\n",
      "INFO:absl:[26] test_loss=1.9596915245056152\n",
      "INFO:absl:[27] train_loss=1.2143186330795288, train_x1_loss=0.45040807127952576, train_x2_loss=0.7639122605323792\n",
      "INFO:absl:[27] val_loss=1.86759352684021\n",
      "INFO:absl:[27] test_loss=1.9548490047454834\n",
      "INFO:absl:[28] train_loss=1.215341329574585, train_x1_loss=0.4514523446559906, train_x2_loss=0.7638891339302063\n",
      "INFO:absl:[28] val_loss=1.873839259147644\n",
      "INFO:absl:[28] test_loss=1.9608490467071533\n",
      "INFO:absl:[29] train_loss=1.2137854099273682, train_x1_loss=0.4500148594379425, train_x2_loss=0.7637687921524048\n",
      "INFO:absl:[29] val_loss=1.8750451803207397\n",
      "INFO:absl:[29] test_loss=1.9620939493179321\n",
      "INFO:absl:[30] train_loss=1.217941403388977, train_x1_loss=0.45456138253211975, train_x2_loss=0.7633782625198364\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[30] val_loss=1.872899055480957\n",
      "INFO:absl:[30] test_loss=1.9598901271820068\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[31] train_loss=1.2147928476333618, train_x1_loss=0.45210784673690796, train_x2_loss=0.762683093547821\n",
      "INFO:absl:[31] val_loss=1.8710194826126099\n",
      "INFO:absl:[31] test_loss=1.9581365585327148\n",
      "INFO:absl:[32] train_loss=1.2163079977035522, train_x1_loss=0.4513097405433655, train_x2_loss=0.7649986743927002\n",
      "INFO:absl:[32] val_loss=1.8722056150436401\n",
      "INFO:absl:[32] test_loss=1.9593039751052856\n",
      "INFO:absl:[33] train_loss=1.212862253189087, train_x1_loss=0.4519689679145813, train_x2_loss=0.7608922123908997\n",
      "INFO:absl:[33] val_loss=1.8683969974517822\n",
      "INFO:absl:[33] test_loss=1.9555771350860596\n",
      "INFO:absl:[34] train_loss=1.213082194328308, train_x1_loss=0.45006585121154785, train_x2_loss=0.7630152702331543\n",
      "INFO:absl:[34] val_loss=1.8709396123886108\n",
      "INFO:absl:[34] test_loss=1.9579381942749023\n",
      "INFO:absl:[35] train_loss=1.2137954235076904, train_x1_loss=0.450013667345047, train_x2_loss=0.7637824416160583\n",
      "INFO:absl:[35] val_loss=1.8710213899612427\n",
      "INFO:absl:[35] test_loss=1.9579432010650635\n",
      "INFO:absl:[36] train_loss=1.212483525276184, train_x1_loss=0.45133715867996216, train_x2_loss=0.7611486315727234\n",
      "INFO:absl:[36] val_loss=1.8712053298950195\n",
      "INFO:absl:[36] test_loss=1.9581705331802368\n",
      "INFO:absl:[37] train_loss=1.214638590812683, train_x1_loss=0.4511931836605072, train_x2_loss=0.7634456157684326\n",
      "INFO:absl:[37] val_loss=1.868260383605957\n",
      "INFO:absl:[37] test_loss=1.9551984071731567\n",
      "INFO:absl:[38] train_loss=1.2143486738204956, train_x1_loss=0.4521646797657013, train_x2_loss=0.7621829509735107\n",
      "INFO:absl:[38] val_loss=1.8761545419692993\n",
      "INFO:absl:[38] test_loss=1.9629201889038086\n",
      "INFO:absl:[39] train_loss=1.2092646360397339, train_x1_loss=0.4475034475326538, train_x2_loss=0.7617619037628174\n",
      "INFO:absl:[39] val_loss=1.8695592880249023\n",
      "INFO:absl:[39] test_loss=1.9565176963806152\n",
      "INFO:absl:[40] train_loss=1.2104204893112183, train_x1_loss=0.44821980595588684, train_x2_loss=0.7621980309486389\n",
      "INFO:absl:[40] val_loss=1.8704063892364502\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[40] test_loss=1.9571033716201782\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[41] train_loss=1.209276556968689, train_x1_loss=0.44811227917671204, train_x2_loss=0.7611609101295471\n",
      "INFO:absl:[41] val_loss=1.8759506940841675\n",
      "INFO:absl:[41] test_loss=1.9624801874160767\n",
      "INFO:absl:[42] train_loss=1.2092211246490479, train_x1_loss=0.44728729128837585, train_x2_loss=0.7619317770004272\n",
      "INFO:absl:[42] val_loss=1.8669719696044922\n",
      "INFO:absl:[42] test_loss=1.9536628723144531\n",
      "INFO:absl:[43] train_loss=1.2118431329727173, train_x1_loss=0.4502296447753906, train_x2_loss=0.7616130113601685\n",
      "INFO:absl:[43] val_loss=1.865929365158081\n",
      "INFO:absl:[43] test_loss=1.9526678323745728\n",
      "INFO:absl:[44] train_loss=1.2092881202697754, train_x1_loss=0.4474582374095917, train_x2_loss=0.7618293166160583\n",
      "INFO:absl:[44] val_loss=1.8727384805679321\n",
      "INFO:absl:[44] test_loss=1.959089756011963\n",
      "INFO:absl:[45] train_loss=1.2075531482696533, train_x1_loss=0.44485968351364136, train_x2_loss=0.7626920342445374\n",
      "INFO:absl:[45] val_loss=1.871671438217163\n",
      "INFO:absl:[45] test_loss=1.9580353498458862\n",
      "INFO:absl:[46] train_loss=1.2065601348876953, train_x1_loss=0.44676196575164795, train_x2_loss=0.7598007321357727\n",
      "INFO:absl:[46] val_loss=1.869415283203125\n",
      "INFO:absl:[46] test_loss=1.955787181854248\n",
      "INFO:absl:[47] train_loss=1.2062816619873047, train_x1_loss=0.44520729780197144, train_x2_loss=0.7610751986503601\n",
      "INFO:absl:[47] val_loss=1.8634730577468872\n",
      "INFO:absl:[47] test_loss=1.9499109983444214\n",
      "INFO:absl:[48] train_loss=1.2062784433364868, train_x1_loss=0.44555166363716125, train_x2_loss=0.7607253789901733\n",
      "INFO:absl:[48] val_loss=1.8668534755706787\n",
      "INFO:absl:[48] test_loss=1.9531078338623047\n",
      "INFO:absl:[49] train_loss=1.205781102180481, train_x1_loss=0.44512465596199036, train_x2_loss=0.760655403137207\n",
      "INFO:absl:[49] val_loss=1.8715651035308838\n",
      "INFO:absl:[49] test_loss=1.9575738906860352\n",
      "INFO:absl:[50] train_loss=1.2009910345077515, train_x1_loss=0.44195449352264404, train_x2_loss=0.7590352296829224\n",
      "INFO:absl:[50] val_loss=1.8671432733535767\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[50] test_loss=1.9532995223999023\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[51] train_loss=1.2043284177780151, train_x1_loss=0.4449709355831146, train_x2_loss=0.7593570947647095\n",
      "INFO:absl:[51] val_loss=1.8645371198654175\n",
      "INFO:absl:[51] test_loss=1.9506584405899048\n",
      "INFO:absl:[52] train_loss=1.202428936958313, train_x1_loss=0.44433099031448364, train_x2_loss=0.7580981850624084\n",
      "INFO:absl:[52] val_loss=1.865756869316101\n",
      "INFO:absl:[52] test_loss=1.9517147541046143\n",
      "INFO:absl:[53] train_loss=1.2046176195144653, train_x1_loss=0.44565215706825256, train_x2_loss=0.7589642405509949\n",
      "INFO:absl:[53] val_loss=1.863646149635315\n",
      "INFO:absl:[53] test_loss=1.9497474431991577\n",
      "INFO:absl:[54] train_loss=1.2008955478668213, train_x1_loss=0.44343820214271545, train_x2_loss=0.7574582099914551\n",
      "INFO:absl:[54] val_loss=1.8616230487823486\n",
      "INFO:absl:[54] test_loss=1.947715401649475\n",
      "INFO:absl:[55] train_loss=1.1992974281311035, train_x1_loss=0.44113263487815857, train_x2_loss=0.7581652998924255\n",
      "INFO:absl:[55] val_loss=1.8659706115722656\n",
      "INFO:absl:[55] test_loss=1.951697826385498\n",
      "INFO:absl:[56] train_loss=1.2051795721054077, train_x1_loss=0.4469536244869232, train_x2_loss=0.758224606513977\n",
      "INFO:absl:[56] val_loss=1.8657066822052002\n",
      "INFO:absl:[56] test_loss=1.9514950513839722\n",
      "INFO:absl:[57] train_loss=1.2022969722747803, train_x1_loss=0.4412052631378174, train_x2_loss=0.7610915899276733\n",
      "INFO:absl:[57] val_loss=1.8638124465942383\n",
      "INFO:absl:[57] test_loss=1.9496392011642456\n",
      "INFO:absl:Setting work unit notes: 3399.7 steps/s, 58.3% (203982/350000), ETA: 0m (1m : 0.1% checkpoint, 12.3% eval)\n",
      "INFO:absl:[203982] steps_per_sec=3399.691381\n",
      "INFO:absl:[58] train_loss=1.2009714841842651, train_x1_loss=0.4410102963447571, train_x2_loss=0.7599627375602722\n",
      "INFO:absl:[58] val_loss=1.8697199821472168\n",
      "INFO:absl:[58] test_loss=1.9552870988845825\n",
      "INFO:absl:[59] train_loss=1.2013071775436401, train_x1_loss=0.44334545731544495, train_x2_loss=0.7579616904258728\n",
      "INFO:absl:[59] val_loss=1.8648508787155151\n",
      "INFO:absl:[59] test_loss=1.9504705667495728\n",
      "INFO:absl:[60] train_loss=1.201179027557373, train_x1_loss=0.44339823722839355, train_x2_loss=0.7577832937240601\n",
      "INFO:absl:[60] val_loss=1.8658993244171143\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[60] test_loss=1.951459527015686\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[61] train_loss=1.2007133960723877, train_x1_loss=0.44094210863113403, train_x2_loss=0.759771466255188\n",
      "INFO:absl:[61] val_loss=1.8642604351043701\n",
      "INFO:absl:[61] test_loss=1.9498608112335205\n",
      "INFO:absl:[62] train_loss=1.2011018991470337, train_x1_loss=0.442551851272583, train_x2_loss=0.7585494518280029\n",
      "INFO:absl:[62] val_loss=1.864297866821289\n",
      "INFO:absl:[62] test_loss=1.9497520923614502\n",
      "INFO:absl:[63] train_loss=1.2010196447372437, train_x1_loss=0.44183263182640076, train_x2_loss=0.7591851353645325\n",
      "INFO:absl:[63] val_loss=1.8633590936660767\n",
      "INFO:absl:[63] test_loss=1.9488853216171265\n",
      "INFO:absl:[64] train_loss=1.1936450004577637, train_x1_loss=0.4379904866218567, train_x2_loss=0.7556537985801697\n",
      "INFO:absl:[64] val_loss=1.8607358932495117\n",
      "INFO:absl:[64] test_loss=1.9462677240371704\n",
      "INFO:absl:[65] train_loss=1.1997696161270142, train_x1_loss=0.44231927394866943, train_x2_loss=0.7574496865272522\n",
      "INFO:absl:[65] val_loss=1.8635575771331787\n",
      "INFO:absl:[65] test_loss=1.9489132165908813\n",
      "INFO:absl:[66] train_loss=1.1985468864440918, train_x1_loss=0.44150054454803467, train_x2_loss=0.7570447325706482\n",
      "INFO:absl:[66] val_loss=1.8617631196975708\n",
      "INFO:absl:[66] test_loss=1.9470363855361938\n",
      "INFO:absl:[67] train_loss=1.1993261575698853, train_x1_loss=0.44097697734832764, train_x2_loss=0.7583488821983337\n",
      "INFO:absl:[67] val_loss=1.8625767230987549\n",
      "INFO:absl:[67] test_loss=1.9479684829711914\n",
      "INFO:absl:[68] train_loss=1.1964244842529297, train_x1_loss=0.43852365016937256, train_x2_loss=0.7578996419906616\n",
      "INFO:absl:[68] val_loss=1.8646800518035889\n",
      "INFO:absl:[68] test_loss=1.9499307870864868\n",
      "INFO:absl:[69] train_loss=1.1939425468444824, train_x1_loss=0.43705224990844727, train_x2_loss=0.7568917274475098\n",
      "INFO:absl:[69] val_loss=1.8665674924850464\n",
      "INFO:absl:[69] test_loss=1.9514987468719482\n",
      "INFO:absl:[70] train_loss=1.1944760084152222, train_x1_loss=0.43959468603134155, train_x2_loss=0.7548795938491821\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[70] val_loss=1.8622233867645264\n",
      "INFO:absl:[70] test_loss=1.9471005201339722\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[71] train_loss=1.1937724351882935, train_x1_loss=0.43702933192253113, train_x2_loss=0.7567432522773743\n",
      "INFO:absl:[71] val_loss=1.8606235980987549\n",
      "INFO:absl:[71] test_loss=1.9456254243850708\n",
      "INFO:absl:[72] train_loss=1.1933495998382568, train_x1_loss=0.4377666711807251, train_x2_loss=0.7555824518203735\n",
      "INFO:absl:[72] val_loss=1.8582026958465576\n",
      "INFO:absl:[72] test_loss=1.9432164430618286\n",
      "INFO:absl:[73] train_loss=1.1929235458374023, train_x1_loss=0.4369220435619354, train_x2_loss=0.7560026049613953\n",
      "INFO:absl:[73] val_loss=1.8646413087844849\n",
      "INFO:absl:[73] test_loss=1.949603796005249\n",
      "INFO:absl:[74] train_loss=1.1934322118759155, train_x1_loss=0.4383906126022339, train_x2_loss=0.7550424933433533\n",
      "INFO:absl:[74] val_loss=1.8615844249725342\n",
      "INFO:absl:[74] test_loss=1.946488857269287\n",
      "INFO:absl:[75] train_loss=1.1946369409561157, train_x1_loss=0.43724513053894043, train_x2_loss=0.757390022277832\n",
      "INFO:absl:[75] val_loss=1.8619635105133057\n",
      "INFO:absl:[75] test_loss=1.9467034339904785\n",
      "INFO:absl:[76] train_loss=1.191182017326355, train_x1_loss=0.43394845724105835, train_x2_loss=0.7572325468063354\n",
      "INFO:absl:[76] val_loss=1.8608702421188354\n",
      "INFO:absl:[76] test_loss=1.9455828666687012\n",
      "INFO:absl:[77] train_loss=1.1933202743530273, train_x1_loss=0.43585389852523804, train_x2_loss=0.7574655413627625\n",
      "INFO:absl:[77] val_loss=1.8636233806610107\n",
      "INFO:absl:[77] test_loss=1.9482630491256714\n",
      "INFO:absl:[78] train_loss=1.1955232620239258, train_x1_loss=0.4398813247680664, train_x2_loss=0.7556425929069519\n",
      "INFO:absl:[78] val_loss=1.8608579635620117\n",
      "INFO:absl:[78] test_loss=1.9456064701080322\n",
      "INFO:absl:[79] train_loss=1.1961489915847778, train_x1_loss=0.4383980631828308, train_x2_loss=0.7577507495880127\n",
      "INFO:absl:[79] val_loss=1.856264591217041\n",
      "INFO:absl:[79] test_loss=1.9411218166351318\n",
      "INFO:absl:[80] train_loss=1.192037582397461, train_x1_loss=0.4374947249889374, train_x2_loss=0.754542350769043\n",
      "INFO:absl:[80] val_loss=1.8600646257400513\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[80] test_loss=1.9448301792144775\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[81] train_loss=1.1895779371261597, train_x1_loss=0.43558067083358765, train_x2_loss=0.7539961338043213\n",
      "INFO:absl:[81] val_loss=1.859655737876892\n",
      "INFO:absl:[81] test_loss=1.9444868564605713\n",
      "INFO:absl:[82] train_loss=1.1940622329711914, train_x1_loss=0.4373570680618286, train_x2_loss=0.7567070126533508\n",
      "INFO:absl:[82] val_loss=1.8602101802825928\n",
      "INFO:absl:[82] test_loss=1.9448527097702026\n",
      "INFO:absl:[83] train_loss=1.191510796546936, train_x1_loss=0.4363049268722534, train_x2_loss=0.7552074790000916\n",
      "INFO:absl:[83] val_loss=1.8574360609054565\n",
      "INFO:absl:[83] test_loss=1.941997766494751\n",
      "INFO:absl:[84] train_loss=1.1921898126602173, train_x1_loss=0.4359590411186218, train_x2_loss=0.7562318444252014\n",
      "INFO:absl:[84] val_loss=1.8619046211242676\n",
      "INFO:absl:[84] test_loss=1.9465394020080566\n",
      "INFO:absl:[85] train_loss=1.1905213594436646, train_x1_loss=0.433956503868103, train_x2_loss=0.7565646767616272\n",
      "INFO:absl:[85] val_loss=1.8595466613769531\n",
      "INFO:absl:[85] test_loss=1.9442845582962036\n",
      "INFO:absl:[86] train_loss=1.190696120262146, train_x1_loss=0.4372480511665344, train_x2_loss=0.7534493803977966\n",
      "INFO:absl:[86] val_loss=1.8625783920288086\n",
      "INFO:absl:[86] test_loss=1.9470611810684204\n",
      "INFO:absl:[87] train_loss=1.1955703496932983, train_x1_loss=0.43867257237434387, train_x2_loss=0.7568970918655396\n",
      "INFO:absl:[87] val_loss=1.8609284162521362\n",
      "INFO:absl:[87] test_loss=1.9454443454742432\n",
      "INFO:absl:[88] train_loss=1.1863765716552734, train_x1_loss=0.4293975234031677, train_x2_loss=0.7569787502288818\n",
      "INFO:absl:[88] val_loss=1.8607622385025024\n",
      "INFO:absl:[88] test_loss=1.9452003240585327\n",
      "INFO:absl:[89] train_loss=1.189640760421753, train_x1_loss=0.4357474446296692, train_x2_loss=0.753893256187439\n",
      "INFO:absl:[89] val_loss=1.857847809791565\n",
      "INFO:absl:[89] test_loss=1.942509412765503\n",
      "INFO:absl:[90] train_loss=1.189219355583191, train_x1_loss=0.4338752031326294, train_x2_loss=0.7553443908691406\n",
      "INFO:absl:[90] val_loss=1.8609331846237183\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[90] test_loss=1.9453589916229248\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[91] train_loss=1.1890673637390137, train_x1_loss=0.4334487318992615, train_x2_loss=0.7556189298629761\n",
      "INFO:absl:[91] val_loss=1.8604259490966797\n",
      "INFO:absl:[91] test_loss=1.945059061050415\n",
      "INFO:absl:[92] train_loss=1.188706874847412, train_x1_loss=0.43284061551094055, train_x2_loss=0.7558649778366089\n",
      "INFO:absl:[92] val_loss=1.8567018508911133\n",
      "INFO:absl:[92] test_loss=1.9412530660629272\n",
      "INFO:absl:[93] train_loss=1.18912672996521, train_x1_loss=0.43397560715675354, train_x2_loss=0.7551510334014893\n",
      "INFO:absl:[93] val_loss=1.864393949508667\n",
      "INFO:absl:[93] test_loss=1.9486521482467651\n",
      "INFO:absl:[94] train_loss=1.1915719509124756, train_x1_loss=0.4358879625797272, train_x2_loss=0.7556849718093872\n",
      "INFO:absl:[94] val_loss=1.856914758682251\n",
      "INFO:absl:[94] test_loss=1.9413901567459106\n",
      "INFO:absl:[95] train_loss=1.1894587278366089, train_x1_loss=0.4351573884487152, train_x2_loss=0.754301905632019\n",
      "INFO:absl:[95] val_loss=1.8514467477798462\n",
      "INFO:absl:[95] test_loss=1.9360020160675049\n",
      "INFO:absl:[96] train_loss=1.1859015226364136, train_x1_loss=0.43318819999694824, train_x2_loss=0.7527151107788086\n",
      "INFO:absl:[96] val_loss=1.8552894592285156\n",
      "INFO:absl:[96] test_loss=1.9399091005325317\n",
      "INFO:absl:[97] train_loss=1.188419222831726, train_x1_loss=0.4330365061759949, train_x2_loss=0.7553825974464417\n",
      "INFO:absl:[97] val_loss=1.8584446907043457\n",
      "INFO:absl:[97] test_loss=1.9428410530090332\n",
      "INFO:absl:[98] train_loss=1.1898986101150513, train_x1_loss=0.4359833598136902, train_x2_loss=0.7539135813713074\n",
      "INFO:absl:[98] val_loss=1.8522717952728271\n",
      "INFO:absl:[98] test_loss=1.93677818775177\n",
      "INFO:absl:[99] train_loss=1.1923692226409912, train_x1_loss=0.4360445439815521, train_x2_loss=0.7563254833221436\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[99] val_loss=1.8514204025268555\n",
      "INFO:absl:[99] test_loss=1.9361414909362793\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'elu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 10, 'dropout_rate': 0.21813956053366504, 'edge_features': (4, 2), 'epochs': 100, 'eval_every_epochs': 1, 'fully_connected_edges': 3, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 1, 'layer_norm': False, 'learning_rate': 3.873531888351175e-05, 'log_every_epochs': 1, 'max_checkpts_to_keep': 2, 'model': 'MLPGraphNetwork', 'n_blocks': 1, 'n_samples': 5000, 'node_features': (4, 2), 'normalize': True, 'optimizer': 'adam', 'output_delay': 0, 'output_steps': 4, 'sample_buffer': -4, 'seed': 68, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 120, 'timestep_duration': 3, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+--------+------+---------+-------+\n",
      "| Name                                   | Shape  | Size | Mean    | Std   |\n",
      "+----------------------------------------+--------+------+---------+-------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (4,)   | 4    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 4) | 24   | -0.0819 | 0.36  |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (2,)   | 2    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (4, 2) | 8    | 0.289   | 0.469 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (4,)   | 4    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (7, 4) | 28   | -0.0395 | 0.333 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)   | 2    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (4, 2) | 8    | -0.135  | 0.542 |\n",
      "+----------------------------------------+--------+------+---------+-------+\n",
      "Total: 80\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Checkpoint None does not exist.\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.00s.\n",
      "INFO:absl:Starting training.\n",
      "INFO:absl:[0] train_loss=3.1809709072113037, train_x1_loss=1.8119099140167236, train_x2_loss=1.3690563440322876\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[0] val_loss=3.0265958309173584\n",
      "INFO:absl:[0] test_loss=3.076176166534424\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[1] train_loss=2.1979258060455322, train_x1_loss=1.1442748308181763, train_x2_loss=1.0536502599716187\n",
      "INFO:absl:[1] val_loss=2.5718672275543213\n",
      "INFO:absl:Created artifact [10] Profile of type ArtifactType.URL and value None.\n",
      "INFO:absl:[1] test_loss=2.6129298210144043\n",
      "INFO:absl:[2] train_loss=1.7482374906539917, train_x1_loss=0.8203181028366089, train_x2_loss=0.9279168248176575\n",
      "INFO:absl:[2] val_loss=2.216769218444824\n",
      "INFO:absl:[2] test_loss=2.2491843700408936\n",
      "INFO:absl:[3] train_loss=1.488830327987671, train_x1_loss=0.6021841764450073, train_x2_loss=0.8866447806358337\n",
      "INFO:absl:[3] val_loss=2.1223397254943848\n",
      "INFO:absl:[3] test_loss=2.1505680084228516\n",
      "INFO:absl:[4] train_loss=1.3883018493652344, train_x1_loss=0.5266319513320923, train_x2_loss=0.8616741299629211\n",
      "INFO:absl:[4] val_loss=2.0717642307281494\n",
      "INFO:absl:[4] test_loss=2.099184274673462\n",
      "INFO:absl:[5] train_loss=1.328385591506958, train_x1_loss=0.496608167886734, train_x2_loss=0.8317747712135315\n",
      "INFO:absl:[5] val_loss=2.008769989013672\n",
      "INFO:absl:[5] test_loss=2.0361268520355225\n",
      "INFO:absl:[6] train_loss=1.281424641609192, train_x1_loss=0.4724747836589813, train_x2_loss=0.8089509010314941\n",
      "INFO:absl:[6] val_loss=1.9743947982788086\n",
      "INFO:absl:[6] test_loss=2.0022189617156982\n",
      "INFO:absl:[7] train_loss=1.2575373649597168, train_x1_loss=0.46648186445236206, train_x2_loss=0.7910564541816711\n",
      "INFO:absl:[7] val_loss=1.9427164793014526\n",
      "INFO:absl:[7] test_loss=1.9713040590286255\n",
      "INFO:absl:[8] train_loss=1.2424801588058472, train_x1_loss=0.4608684778213501, train_x2_loss=0.7816109657287598\n",
      "INFO:absl:[8] val_loss=1.9264475107192993\n",
      "INFO:absl:[8] test_loss=1.9559452533721924\n",
      "INFO:absl:[9] train_loss=1.233211636543274, train_x1_loss=0.4590374231338501, train_x2_loss=0.774175763130188\n",
      "INFO:absl:[9] val_loss=1.9214345216751099\n",
      "INFO:absl:[9] test_loss=1.9516407251358032\n",
      "INFO:absl:[10] train_loss=1.2291259765625, train_x1_loss=0.4562890827655792, train_x2_loss=0.7728370428085327\n",
      "INFO:absl:[10] val_loss=1.9180080890655518\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[10] test_loss=1.9487465620040894\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[11] train_loss=1.2288837432861328, train_x1_loss=0.45768535137176514, train_x2_loss=0.7711991667747498\n",
      "INFO:absl:[11] val_loss=1.9160031080245972\n",
      "INFO:absl:[11] test_loss=1.9471104145050049\n",
      "INFO:absl:[12] train_loss=1.2208479642868042, train_x1_loss=0.45442885160446167, train_x2_loss=0.7664191722869873\n",
      "INFO:absl:[12] val_loss=1.9079915285110474\n",
      "INFO:absl:[12] test_loss=1.9394265413284302\n",
      "INFO:absl:[13] train_loss=1.2198797464370728, train_x1_loss=0.45645126700401306, train_x2_loss=0.7634310722351074\n",
      "INFO:absl:[13] val_loss=1.9090890884399414\n",
      "INFO:absl:[13] test_loss=1.940747857093811\n",
      "INFO:absl:[14] train_loss=1.221053957939148, train_x1_loss=0.455768883228302, train_x2_loss=0.765286386013031\n",
      "INFO:absl:[14] val_loss=1.9088762998580933\n",
      "INFO:absl:[14] test_loss=1.9406851530075073\n",
      "INFO:absl:[15] train_loss=1.21969473361969, train_x1_loss=0.45781347155570984, train_x2_loss=0.7618800401687622\n",
      "INFO:absl:[15] val_loss=1.9057385921478271\n",
      "INFO:absl:[15] test_loss=1.9376121759414673\n",
      "INFO:absl:[16] train_loss=1.2146260738372803, train_x1_loss=0.4551807641983032, train_x2_loss=0.7594460844993591\n",
      "INFO:absl:[16] val_loss=1.9085586071014404\n",
      "INFO:absl:[16] test_loss=1.9406349658966064\n",
      "INFO:absl:[17] train_loss=1.2117691040039062, train_x1_loss=0.45151153206825256, train_x2_loss=0.7602564692497253\n",
      "INFO:absl:[17] val_loss=1.9065158367156982\n",
      "INFO:absl:[17] test_loss=1.9385899305343628\n",
      "INFO:absl:[18] train_loss=1.2168776988983154, train_x1_loss=0.45633891224861145, train_x2_loss=0.7605365514755249\n",
      "INFO:absl:[18] val_loss=1.9088867902755737\n",
      "INFO:absl:[18] test_loss=1.941070795059204\n",
      "INFO:absl:[19] train_loss=1.215518593788147, train_x1_loss=0.45589780807495117, train_x2_loss=0.7596213817596436\n",
      "INFO:absl:[19] val_loss=1.9040825366973877\n",
      "INFO:absl:[19] test_loss=1.9362908601760864\n",
      "INFO:absl:[20] train_loss=1.2176105976104736, train_x1_loss=0.45632100105285645, train_x2_loss=0.7612881064414978\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[20] val_loss=1.9029558897018433\n",
      "INFO:absl:[20] test_loss=1.9352537393569946\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[21] train_loss=1.2150877714157104, train_x1_loss=0.4548671543598175, train_x2_loss=0.7602206468582153\n",
      "INFO:absl:[21] val_loss=1.907968521118164\n",
      "INFO:absl:[21] test_loss=1.9402538537979126\n",
      "INFO:absl:[22] train_loss=1.2102441787719727, train_x1_loss=0.4513830244541168, train_x2_loss=0.7588616609573364\n",
      "INFO:absl:[22] val_loss=1.9041334390640259\n",
      "INFO:absl:[22] test_loss=1.9364333152770996\n",
      "INFO:absl:[23] train_loss=1.2106800079345703, train_x1_loss=0.452188104391098, train_x2_loss=0.7584917545318604\n",
      "INFO:absl:[23] val_loss=1.904261827468872\n",
      "INFO:absl:[23] test_loss=1.9364944696426392\n",
      "INFO:absl:[24] train_loss=1.2159258127212524, train_x1_loss=0.45723757147789, train_x2_loss=0.7586895227432251\n",
      "INFO:absl:[24] val_loss=1.9014928340911865\n",
      "INFO:absl:[24] test_loss=1.9338840246200562\n",
      "INFO:absl:[25] train_loss=1.211778163909912, train_x1_loss=0.4527225196361542, train_x2_loss=0.7590562105178833\n",
      "INFO:absl:[25] val_loss=1.9079015254974365\n",
      "INFO:absl:[25] test_loss=1.94027578830719\n",
      "INFO:absl:[26] train_loss=1.2124195098876953, train_x1_loss=0.4536004960536957, train_x2_loss=0.7588202357292175\n",
      "INFO:absl:[26] val_loss=1.9101579189300537\n",
      "INFO:absl:[26] test_loss=1.9425771236419678\n",
      "INFO:absl:[27] train_loss=1.207810878753662, train_x1_loss=0.44936883449554443, train_x2_loss=0.7584409117698669\n",
      "INFO:absl:[27] val_loss=1.9084646701812744\n",
      "INFO:absl:[27] test_loss=1.9409141540527344\n",
      "INFO:absl:[28] train_loss=1.2124364376068115, train_x1_loss=0.4535719156265259, train_x2_loss=0.7588663697242737\n",
      "INFO:absl:[28] val_loss=1.9059240818023682\n",
      "INFO:absl:[28] test_loss=1.938321590423584\n",
      "INFO:absl:[29] train_loss=1.21697199344635, train_x1_loss=0.4578663110733032, train_x2_loss=0.7591054439544678\n",
      "INFO:absl:[29] val_loss=1.9151849746704102\n",
      "INFO:absl:[29] test_loss=1.9476150274276733\n",
      "INFO:absl:[30] train_loss=1.2071603536605835, train_x1_loss=0.45014384388923645, train_x2_loss=0.7570146918296814\n",
      "INFO:absl:[30] val_loss=1.898733139038086\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[30] test_loss=1.9311190843582153\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[31] train_loss=1.2113330364227295, train_x1_loss=0.45255333185195923, train_x2_loss=0.7587781548500061\n",
      "INFO:absl:[31] val_loss=1.9122923612594604\n",
      "INFO:absl:[31] test_loss=1.9447436332702637\n",
      "INFO:absl:[32] train_loss=1.2109628915786743, train_x1_loss=0.453715056180954, train_x2_loss=0.7572477459907532\n",
      "INFO:absl:[32] val_loss=1.9027132987976074\n",
      "INFO:absl:[32] test_loss=1.935025691986084\n",
      "INFO:absl:[33] train_loss=1.2104705572128296, train_x1_loss=0.45313483476638794, train_x2_loss=0.7573357820510864\n",
      "INFO:absl:[33] val_loss=1.9079848527908325\n",
      "INFO:absl:[33] test_loss=1.9403454065322876\n",
      "INFO:absl:[34] train_loss=1.2077233791351318, train_x1_loss=0.44995322823524475, train_x2_loss=0.7577698826789856\n",
      "INFO:absl:[34] val_loss=1.902580976486206\n",
      "INFO:absl:[34] test_loss=1.9348523616790771\n",
      "INFO:absl:[35] train_loss=1.2048803567886353, train_x1_loss=0.44706082344055176, train_x2_loss=0.7578193545341492\n",
      "INFO:absl:[35] val_loss=1.9071043729782104\n",
      "INFO:absl:[35] test_loss=1.9394210577011108\n",
      "INFO:absl:[36] train_loss=1.2069207429885864, train_x1_loss=0.4483981132507324, train_x2_loss=0.7585235834121704\n",
      "INFO:absl:[36] val_loss=1.9073837995529175\n",
      "INFO:absl:[36] test_loss=1.9396536350250244\n",
      "INFO:absl:[37] train_loss=1.2074403762817383, train_x1_loss=0.44947731494903564, train_x2_loss=0.7579631805419922\n",
      "INFO:absl:[37] val_loss=1.9053236246109009\n",
      "INFO:absl:[37] test_loss=1.9375698566436768\n",
      "INFO:absl:[38] train_loss=1.2054846286773682, train_x1_loss=0.44999828934669495, train_x2_loss=0.755487322807312\n",
      "INFO:absl:[38] val_loss=1.9044607877731323\n",
      "INFO:absl:[38] test_loss=1.9367355108261108\n",
      "INFO:absl:[39] train_loss=1.2048298120498657, train_x1_loss=0.4477207064628601, train_x2_loss=0.7571086883544922\n",
      "INFO:absl:[39] val_loss=1.906565546989441\n",
      "INFO:absl:[39] test_loss=1.9388495683670044\n",
      "INFO:absl:[40] train_loss=1.2030268907546997, train_x1_loss=0.44769206643104553, train_x2_loss=0.7553355693817139\n",
      "INFO:absl:[40] val_loss=1.9012097120285034\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[40] test_loss=1.9333997964859009\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[41] train_loss=1.202524185180664, train_x1_loss=0.44707727432250977, train_x2_loss=0.7554479837417603\n",
      "INFO:absl:[41] val_loss=1.9007022380828857\n",
      "INFO:absl:[41] test_loss=1.9329618215560913\n",
      "INFO:absl:[42] train_loss=1.2017526626586914, train_x1_loss=0.44511592388153076, train_x2_loss=0.7566379308700562\n",
      "INFO:absl:[42] val_loss=1.9031715393066406\n",
      "INFO:absl:[42] test_loss=1.935301661491394\n",
      "INFO:absl:[43] train_loss=1.2004454135894775, train_x1_loss=0.44500061869621277, train_x2_loss=0.7554448843002319\n",
      "INFO:absl:[43] val_loss=1.9004719257354736\n",
      "INFO:absl:[43] test_loss=1.9326273202896118\n",
      "INFO:absl:[44] train_loss=1.2044298648834229, train_x1_loss=0.45043882727622986, train_x2_loss=0.7539919018745422\n",
      "INFO:absl:[44] val_loss=1.9045066833496094\n",
      "INFO:absl:[44] test_loss=1.9367328882217407\n",
      "INFO:absl:[45] train_loss=1.1988142728805542, train_x1_loss=0.4427851438522339, train_x2_loss=0.7560265064239502\n",
      "INFO:absl:[45] val_loss=1.9011147022247314\n",
      "INFO:absl:[45] test_loss=1.9332329034805298\n",
      "INFO:absl:[46] train_loss=1.198481559753418, train_x1_loss=0.4442160725593567, train_x2_loss=0.7542656064033508\n",
      "INFO:absl:[46] val_loss=1.901231288909912\n",
      "INFO:absl:[46] test_loss=1.93332839012146\n",
      "INFO:absl:[47] train_loss=1.1999465227127075, train_x1_loss=0.443805456161499, train_x2_loss=0.756137490272522\n",
      "INFO:absl:[47] val_loss=1.9009133577346802\n",
      "INFO:absl:[47] test_loss=1.9329936504364014\n",
      "INFO:absl:[48] train_loss=1.1989225149154663, train_x1_loss=0.4419556260108948, train_x2_loss=0.7569665908813477\n",
      "INFO:absl:[48] val_loss=1.8992379903793335\n",
      "INFO:absl:[48] test_loss=1.9313147068023682\n",
      "INFO:absl:[49] train_loss=1.1997871398925781, train_x1_loss=0.44506287574768066, train_x2_loss=0.7547251582145691\n",
      "INFO:absl:[49] val_loss=1.8970882892608643\n",
      "INFO:absl:[49] test_loss=1.92905855178833\n",
      "INFO:absl:[50] train_loss=1.200608253479004, train_x1_loss=0.4414297938346863, train_x2_loss=0.7591793537139893\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[50] val_loss=1.9021058082580566\n",
      "INFO:absl:[50] test_loss=1.934117317199707\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[51] train_loss=1.1989388465881348, train_x1_loss=0.4450252056121826, train_x2_loss=0.7539142966270447\n",
      "INFO:absl:[51] val_loss=1.8991619348526\n",
      "INFO:absl:[51] test_loss=1.9312410354614258\n",
      "INFO:absl:[52] train_loss=1.1946901082992554, train_x1_loss=0.4416196942329407, train_x2_loss=0.7530698180198669\n",
      "INFO:absl:[52] val_loss=1.894029140472412\n",
      "INFO:absl:[52] test_loss=1.9260458946228027\n",
      "INFO:absl:[53] train_loss=1.1950738430023193, train_x1_loss=0.44201287627220154, train_x2_loss=0.7530596852302551\n",
      "INFO:absl:[53] val_loss=1.9015002250671387\n",
      "INFO:absl:[53] test_loss=1.9335739612579346\n",
      "INFO:absl:[54] train_loss=1.1934764385223389, train_x1_loss=0.4404577612876892, train_x2_loss=0.7530214786529541\n",
      "INFO:absl:[54] val_loss=1.895317554473877\n",
      "INFO:absl:[54] test_loss=1.927364706993103\n",
      "INFO:absl:[55] train_loss=1.1936980485916138, train_x1_loss=0.44083091616630554, train_x2_loss=0.7528679370880127\n",
      "INFO:absl:[55] val_loss=1.8959416151046753\n",
      "INFO:absl:[55] test_loss=1.9280586242675781\n",
      "INFO:absl:[56] train_loss=1.1914921998977661, train_x1_loss=0.4406588077545166, train_x2_loss=0.750834584236145\n",
      "INFO:absl:[56] val_loss=1.8950269222259521\n",
      "INFO:absl:[56] test_loss=1.9271167516708374\n",
      "INFO:absl:[57] train_loss=1.1941763162612915, train_x1_loss=0.4407096207141876, train_x2_loss=0.7534673810005188\n",
      "INFO:absl:[57] val_loss=1.8950501680374146\n",
      "INFO:absl:[57] test_loss=1.9271758794784546\n",
      "INFO:absl:[58] train_loss=1.192183017730713, train_x1_loss=0.4378528296947479, train_x2_loss=0.7543293237686157\n",
      "INFO:absl:[58] val_loss=1.8993312120437622\n",
      "INFO:absl:[58] test_loss=1.9312876462936401\n",
      "INFO:absl:[59] train_loss=1.1893856525421143, train_x1_loss=0.438961386680603, train_x2_loss=0.7504252195358276\n",
      "INFO:absl:[59] val_loss=1.893558144569397\n",
      "INFO:absl:[59] test_loss=1.9255414009094238\n",
      "INFO:absl:[60] train_loss=1.195092797279358, train_x1_loss=0.43967658281326294, train_x2_loss=0.755416214466095\n",
      "INFO:absl:[60] val_loss=1.8954166173934937\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[60] test_loss=1.9275370836257935\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Setting work unit notes: 3556.0 steps/s, 61.0% (213500/350000), ETA: 0m (1m : 0.1% checkpoint, 23.6% eval)\n",
      "INFO:absl:[213500] steps_per_sec=3555.976673\n",
      "INFO:absl:[61] train_loss=1.1908150911331177, train_x1_loss=0.43726781010627747, train_x2_loss=0.7535481452941895\n",
      "INFO:absl:[61] val_loss=1.8989654779434204\n",
      "INFO:absl:[61] test_loss=1.9310108423233032\n",
      "INFO:absl:[62] train_loss=1.1887718439102173, train_x1_loss=0.4386039078235626, train_x2_loss=0.7501664161682129\n",
      "INFO:absl:[62] val_loss=1.8948044776916504\n",
      "INFO:absl:[62] test_loss=1.926871418952942\n",
      "INFO:absl:[63] train_loss=1.1936888694763184, train_x1_loss=0.4405159056186676, train_x2_loss=0.7531713843345642\n",
      "INFO:absl:[63] val_loss=1.8960236310958862\n",
      "INFO:absl:[63] test_loss=1.9280130863189697\n",
      "INFO:absl:[64] train_loss=1.1936486959457397, train_x1_loss=0.4394276738166809, train_x2_loss=0.7542193531990051\n",
      "INFO:absl:[64] val_loss=1.8985354900360107\n",
      "INFO:absl:[64] test_loss=1.9305130243301392\n",
      "INFO:absl:[65] train_loss=1.1925075054168701, train_x1_loss=0.43935438990592957, train_x2_loss=0.7531517148017883\n",
      "INFO:absl:[65] val_loss=1.8969670534133911\n",
      "INFO:absl:[65] test_loss=1.9289720058441162\n",
      "INFO:absl:[66] train_loss=1.1890949010849, train_x1_loss=0.43677788972854614, train_x2_loss=0.7523174285888672\n",
      "INFO:absl:[66] val_loss=1.8915574550628662\n",
      "INFO:absl:[66] test_loss=1.9235260486602783\n",
      "INFO:absl:[67] train_loss=1.190334677696228, train_x1_loss=0.4396401047706604, train_x2_loss=0.750696063041687\n",
      "INFO:absl:[67] val_loss=1.8887945413589478\n",
      "INFO:absl:[67] test_loss=1.920790433883667\n",
      "INFO:absl:[68] train_loss=1.1882215738296509, train_x1_loss=0.4368356168270111, train_x2_loss=0.7513856887817383\n",
      "INFO:absl:[68] val_loss=1.8960639238357544\n",
      "INFO:absl:[68] test_loss=1.9280730485916138\n",
      "INFO:absl:[69] train_loss=1.1884469985961914, train_x1_loss=0.4376487731933594, train_x2_loss=0.7507992386817932\n",
      "INFO:absl:[69] val_loss=1.89166259765625\n",
      "INFO:absl:[69] test_loss=1.9236160516738892\n",
      "INFO:absl:[70] train_loss=1.1890336275100708, train_x1_loss=0.4375306963920593, train_x2_loss=0.7515026926994324\n",
      "INFO:absl:[70] val_loss=1.8975772857666016\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[70] test_loss=1.9296295642852783\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[71] train_loss=1.1862306594848633, train_x1_loss=0.43725845217704773, train_x2_loss=0.7489719986915588\n",
      "INFO:absl:[71] val_loss=1.8916850090026855\n",
      "INFO:absl:[71] test_loss=1.9235291481018066\n",
      "INFO:absl:[72] train_loss=1.1884124279022217, train_x1_loss=0.4367908835411072, train_x2_loss=0.751623272895813\n",
      "INFO:absl:[72] val_loss=1.9003368616104126\n",
      "INFO:absl:[72] test_loss=1.9323999881744385\n",
      "INFO:absl:[73] train_loss=1.1854572296142578, train_x1_loss=0.4361377954483032, train_x2_loss=0.7493200302124023\n",
      "INFO:absl:[73] val_loss=1.897213339805603\n",
      "INFO:absl:[73] test_loss=1.9292418956756592\n",
      "INFO:absl:[74] train_loss=1.1890106201171875, train_x1_loss=0.4369506537914276, train_x2_loss=0.7520613074302673\n",
      "INFO:absl:[74] val_loss=1.8981413841247559\n",
      "INFO:absl:[74] test_loss=1.930026650428772\n",
      "INFO:absl:[75] train_loss=1.185115933418274, train_x1_loss=0.4345076084136963, train_x2_loss=0.7506093978881836\n",
      "INFO:absl:[75] val_loss=1.8914291858673096\n",
      "INFO:absl:[75] test_loss=1.9234539270401\n",
      "INFO:absl:[76] train_loss=1.1872117519378662, train_x1_loss=0.4367747902870178, train_x2_loss=0.7504370212554932\n",
      "INFO:absl:[76] val_loss=1.8918559551239014\n",
      "INFO:absl:[76] test_loss=1.9238429069519043\n",
      "INFO:absl:[77] train_loss=1.1842247247695923, train_x1_loss=0.43427982926368713, train_x2_loss=0.7499440312385559\n",
      "INFO:absl:[77] val_loss=1.8957048654556274\n",
      "INFO:absl:[77] test_loss=1.927703857421875\n",
      "INFO:absl:[78] train_loss=1.1872575283050537, train_x1_loss=0.43572717905044556, train_x2_loss=0.7515290379524231\n",
      "INFO:absl:[78] val_loss=1.8951741456985474\n",
      "INFO:absl:[78] test_loss=1.927291989326477\n",
      "INFO:absl:[79] train_loss=1.189329981803894, train_x1_loss=0.4375031590461731, train_x2_loss=0.7518251538276672\n",
      "INFO:absl:[79] val_loss=1.8907597064971924\n",
      "INFO:absl:[79] test_loss=1.9228215217590332\n",
      "INFO:absl:[80] train_loss=1.1835612058639526, train_x1_loss=0.43212687969207764, train_x2_loss=0.7514353394508362\n",
      "INFO:absl:[80] val_loss=1.8912992477416992\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[80] test_loss=1.9233323335647583\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[81] train_loss=1.1813204288482666, train_x1_loss=0.4319749176502228, train_x2_loss=0.7493463754653931\n",
      "INFO:absl:[81] val_loss=1.8959362506866455\n",
      "INFO:absl:[81] test_loss=1.9278568029403687\n",
      "INFO:absl:[82] train_loss=1.1825120449066162, train_x1_loss=0.4350901246070862, train_x2_loss=0.7474208474159241\n",
      "INFO:absl:[82] val_loss=1.8896229267120361\n",
      "INFO:absl:[82] test_loss=1.9217151403427124\n",
      "INFO:absl:[83] train_loss=1.18157160282135, train_x1_loss=0.43304967880249023, train_x2_loss=0.7485217452049255\n",
      "INFO:absl:[83] val_loss=1.887361764907837\n",
      "INFO:absl:[83] test_loss=1.9192825555801392\n",
      "INFO:absl:[84] train_loss=1.1847333908081055, train_x1_loss=0.4352557063102722, train_x2_loss=0.7494784593582153\n",
      "INFO:absl:[84] val_loss=1.8914744853973389\n",
      "INFO:absl:[84] test_loss=1.9235049486160278\n",
      "INFO:absl:[85] train_loss=1.1816424131393433, train_x1_loss=0.43109679222106934, train_x2_loss=0.7505472898483276\n",
      "INFO:absl:[85] val_loss=1.8917125463485718\n",
      "INFO:absl:[85] test_loss=1.9236159324645996\n",
      "INFO:absl:[86] train_loss=1.1849496364593506, train_x1_loss=0.4345955550670624, train_x2_loss=0.7503532767295837\n",
      "INFO:absl:[86] val_loss=1.8896735906600952\n",
      "INFO:absl:[86] test_loss=1.9216125011444092\n",
      "INFO:absl:[87] train_loss=1.1875051259994507, train_x1_loss=0.4369756877422333, train_x2_loss=0.7505303621292114\n",
      "INFO:absl:[87] val_loss=1.89187490940094\n",
      "INFO:absl:[87] test_loss=1.9240832328796387\n",
      "INFO:absl:[88] train_loss=1.1831316947937012, train_x1_loss=0.4325518012046814, train_x2_loss=0.7505812644958496\n",
      "INFO:absl:[88] val_loss=1.8936505317687988\n",
      "INFO:absl:[88] test_loss=1.9258087873458862\n",
      "INFO:absl:[89] train_loss=1.1804232597351074, train_x1_loss=0.4309784173965454, train_x2_loss=0.7494435906410217\n",
      "INFO:absl:[89] val_loss=1.8899496793746948\n",
      "INFO:absl:[89] test_loss=1.9220350980758667\n",
      "INFO:absl:[90] train_loss=1.183276653289795, train_x1_loss=0.4346882700920105, train_x2_loss=0.7485888600349426\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[90] val_loss=1.891628384590149\n",
      "INFO:absl:[90] test_loss=1.9236949682235718\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[91] train_loss=1.183679461479187, train_x1_loss=0.4342755973339081, train_x2_loss=0.7494049668312073\n",
      "INFO:absl:[91] val_loss=1.8890827894210815\n",
      "INFO:absl:[91] test_loss=1.9212628602981567\n",
      "INFO:absl:[92] train_loss=1.1815013885498047, train_x1_loss=0.43295037746429443, train_x2_loss=0.7485523223876953\n",
      "INFO:absl:[92] val_loss=1.889573574066162\n",
      "INFO:absl:[92] test_loss=1.9218621253967285\n",
      "INFO:absl:[93] train_loss=1.1834789514541626, train_x1_loss=0.43399086594581604, train_x2_loss=0.7494890689849854\n",
      "INFO:absl:[93] val_loss=1.8949917554855347\n",
      "INFO:absl:[93] test_loss=1.9272539615631104\n",
      "INFO:absl:[94] train_loss=1.1789395809173584, train_x1_loss=0.4310946464538574, train_x2_loss=0.7478447556495667\n",
      "INFO:absl:[94] val_loss=1.8890256881713867\n",
      "INFO:absl:[94] test_loss=1.9212405681610107\n",
      "INFO:absl:[95] train_loss=1.1790748834609985, train_x1_loss=0.4318729341030121, train_x2_loss=0.7472015619277954\n",
      "INFO:absl:[95] val_loss=1.8874878883361816\n",
      "INFO:absl:[95] test_loss=1.9195916652679443\n",
      "INFO:absl:[96] train_loss=1.1797603368759155, train_x1_loss=0.43179070949554443, train_x2_loss=0.7479703426361084\n",
      "INFO:absl:[96] val_loss=1.8909413814544678\n",
      "INFO:absl:[96] test_loss=1.9232521057128906\n",
      "INFO:absl:[97] train_loss=1.180661678314209, train_x1_loss=0.43151217699050903, train_x2_loss=0.7491498589515686\n",
      "INFO:absl:[97] val_loss=1.8920292854309082\n",
      "INFO:absl:[97] test_loss=1.924251914024353\n",
      "INFO:absl:[98] train_loss=1.1841214895248413, train_x1_loss=0.4342207610607147, train_x2_loss=0.7498999834060669\n",
      "INFO:absl:[98] val_loss=1.8909186124801636\n",
      "INFO:absl:[98] test_loss=1.9231401681900024\n",
      "INFO:absl:[99] train_loss=1.177971363067627, train_x1_loss=0.42990079522132874, train_x2_loss=0.7480696439743042\n",
      "INFO:absl:[99] val_loss=1.8836439847946167\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[99] test_loss=1.9157954454421997\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'elu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 10, 'dropout_rate': 0.21813956053366504, 'edge_features': (4, 2), 'epochs': 100, 'eval_every_epochs': 1, 'fully_connected_edges': 3, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 1, 'layer_norm': False, 'learning_rate': 3.873531888351175e-05, 'log_every_epochs': 1, 'max_checkpts_to_keep': 2, 'model': 'MLPGraphNetwork', 'n_blocks': 1, 'n_samples': 5000, 'node_features': (4, 2), 'normalize': True, 'optimizer': 'adam', 'output_delay': 0, 'output_steps': 4, 'sample_buffer': -4, 'seed': 69, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 120, 'timestep_duration': 3, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+--------+------+---------+-------+\n",
      "| Name                                   | Shape  | Size | Mean    | Std   |\n",
      "+----------------------------------------+--------+------+---------+-------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (4,)   | 4    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 4) | 24   | -0.0819 | 0.36  |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (2,)   | 2    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (4, 2) | 8    | 0.289   | 0.469 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (4,)   | 4    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (7, 4) | 28   | -0.0395 | 0.333 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)   | 2    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (4, 2) | 8    | -0.135  | 0.542 |\n",
      "+----------------------------------------+--------+------+---------+-------+\n",
      "Total: 80\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Checkpoint None does not exist.\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.00s.\n",
      "INFO:absl:Starting training.\n",
      "INFO:absl:[0] train_loss=3.1109890937805176, train_x1_loss=1.7500977516174316, train_x2_loss=1.3608888387680054\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[0] val_loss=3.017913818359375\n",
      "INFO:absl:[0] test_loss=2.869574785232544\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Created artifact [10] Profile of type ArtifactType.URL and value None.\n",
      "INFO:absl:[1] train_loss=2.1407957077026367, train_x1_loss=1.0991290807724, train_x2_loss=1.041664958000183\n",
      "INFO:absl:[1] val_loss=2.530280590057373\n",
      "INFO:absl:[1] test_loss=2.3941421508789062\n",
      "INFO:absl:[2] train_loss=1.6945562362670898, train_x1_loss=0.7726006507873535, train_x2_loss=0.9219570159912109\n",
      "INFO:absl:[2] val_loss=2.2356159687042236\n",
      "INFO:absl:[2] test_loss=2.1035947799682617\n",
      "INFO:absl:[3] train_loss=1.4648947715759277, train_x1_loss=0.5825079083442688, train_x2_loss=0.8823867440223694\n",
      "INFO:absl:[3] val_loss=2.149311065673828\n",
      "INFO:absl:[3] test_loss=2.0176215171813965\n",
      "INFO:absl:[4] train_loss=1.374943733215332, train_x1_loss=0.5154057145118713, train_x2_loss=0.8595389723777771\n",
      "INFO:absl:[4] val_loss=2.0988762378692627\n",
      "INFO:absl:[4] test_loss=1.967848777770996\n",
      "INFO:absl:[5] train_loss=1.3237069845199585, train_x1_loss=0.48833325505256653, train_x2_loss=0.8353763222694397\n",
      "INFO:absl:[5] val_loss=2.0368783473968506\n",
      "INFO:absl:[5] test_loss=1.9081019163131714\n",
      "INFO:absl:[6] train_loss=1.2826604843139648, train_x1_loss=0.4714609980583191, train_x2_loss=0.8112000823020935\n",
      "INFO:absl:[6] val_loss=2.0098633766174316\n",
      "INFO:absl:[6] test_loss=1.8828109502792358\n",
      "INFO:absl:[7] train_loss=1.260187029838562, train_x1_loss=0.46467268466949463, train_x2_loss=0.7955132722854614\n",
      "INFO:absl:[7] val_loss=1.980981469154358\n",
      "INFO:absl:[7] test_loss=1.8553249835968018\n",
      "INFO:absl:[8] train_loss=1.2453206777572632, train_x1_loss=0.4605391323566437, train_x2_loss=0.784782350063324\n",
      "INFO:absl:[8] val_loss=1.9687442779541016\n",
      "INFO:absl:[8] test_loss=1.8444807529449463\n",
      "INFO:absl:[9] train_loss=1.239909052848816, train_x1_loss=0.4602503478527069, train_x2_loss=0.7796605825424194\n",
      "INFO:absl:[9] val_loss=1.9628034830093384\n",
      "INFO:absl:[9] test_loss=1.8382172584533691\n",
      "INFO:absl:[10] train_loss=1.237801194190979, train_x1_loss=0.45928245782852173, train_x2_loss=0.7785190939903259\n",
      "INFO:absl:[10] val_loss=1.9597864151000977\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[10] test_loss=1.835616111755371\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[11] train_loss=1.2338671684265137, train_x1_loss=0.4576464593410492, train_x2_loss=0.7762205600738525\n",
      "INFO:absl:[11] val_loss=1.9514734745025635\n",
      "INFO:absl:[11] test_loss=1.827701210975647\n",
      "INFO:absl:[12] train_loss=1.2229076623916626, train_x1_loss=0.45329999923706055, train_x2_loss=0.7696070075035095\n",
      "INFO:absl:[12] val_loss=1.953680157661438\n",
      "INFO:absl:[12] test_loss=1.8294413089752197\n",
      "INFO:absl:[13] train_loss=1.2238733768463135, train_x1_loss=0.4567473828792572, train_x2_loss=0.7671269774436951\n",
      "INFO:absl:[13] val_loss=1.9514589309692383\n",
      "INFO:absl:[13] test_loss=1.8275949954986572\n",
      "INFO:absl:[14] train_loss=1.223181962966919, train_x1_loss=0.4552358388900757, train_x2_loss=0.7679480910301208\n",
      "INFO:absl:[14] val_loss=1.9484285116195679\n",
      "INFO:absl:[14] test_loss=1.8247408866882324\n",
      "INFO:absl:[15] train_loss=1.220181941986084, train_x1_loss=0.4559467136859894, train_x2_loss=0.7642356157302856\n",
      "INFO:absl:[15] val_loss=1.9461231231689453\n",
      "INFO:absl:[15] test_loss=1.8223906755447388\n",
      "INFO:absl:[16] train_loss=1.2248224020004272, train_x1_loss=0.45950910449028015, train_x2_loss=0.765315592288971\n",
      "INFO:absl:[16] val_loss=1.9537907838821411\n",
      "INFO:absl:[16] test_loss=1.8305585384368896\n",
      "INFO:absl:[17] train_loss=1.219800591468811, train_x1_loss=0.454141765832901, train_x2_loss=0.7656601071357727\n",
      "INFO:absl:[17] val_loss=1.9486415386199951\n",
      "INFO:absl:[17] test_loss=1.8249595165252686\n",
      "INFO:absl:[18] train_loss=1.2190507650375366, train_x1_loss=0.45618298649787903, train_x2_loss=0.7628676295280457\n",
      "INFO:absl:[18] val_loss=1.9467883110046387\n",
      "INFO:absl:[18] test_loss=1.8237459659576416\n",
      "INFO:absl:[19] train_loss=1.2209808826446533, train_x1_loss=0.45748549699783325, train_x2_loss=0.7634961605072021\n",
      "INFO:absl:[19] val_loss=1.9514881372451782\n",
      "INFO:absl:[19] test_loss=1.8284655809402466\n",
      "INFO:absl:[20] train_loss=1.2175253629684448, train_x1_loss=0.4549010097980499, train_x2_loss=0.7626218199729919\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[20] val_loss=1.949643611907959\n",
      "INFO:absl:[20] test_loss=1.8262869119644165\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[21] train_loss=1.218209981918335, train_x1_loss=0.4559279680252075, train_x2_loss=0.7622812986373901\n",
      "INFO:absl:[21] val_loss=1.9445013999938965\n",
      "INFO:absl:[21] test_loss=1.8211342096328735\n",
      "INFO:absl:[22] train_loss=1.2146263122558594, train_x1_loss=0.4521612226963043, train_x2_loss=0.7624635696411133\n",
      "INFO:absl:[22] val_loss=1.9446865320205688\n",
      "INFO:absl:[22] test_loss=1.821556806564331\n",
      "INFO:absl:[23] train_loss=1.2184172868728638, train_x1_loss=0.45635986328125, train_x2_loss=0.7620580196380615\n",
      "INFO:absl:[23] val_loss=1.9506756067276\n",
      "INFO:absl:[23] test_loss=1.8273650407791138\n",
      "INFO:absl:[24] train_loss=1.217742919921875, train_x1_loss=0.4540882706642151, train_x2_loss=0.7636539340019226\n",
      "INFO:absl:[24] val_loss=1.941956877708435\n",
      "INFO:absl:[24] test_loss=1.8184139728546143\n",
      "INFO:absl:[25] train_loss=1.2167270183563232, train_x1_loss=0.4575929343700409, train_x2_loss=0.7591335773468018\n",
      "INFO:absl:[25] val_loss=1.9430830478668213\n",
      "INFO:absl:[25] test_loss=1.819985270500183\n",
      "INFO:absl:[26] train_loss=1.2161970138549805, train_x1_loss=0.45377281308174133, train_x2_loss=0.7624228596687317\n",
      "INFO:absl:[26] val_loss=1.9451335668563843\n",
      "INFO:absl:[26] test_loss=1.822019338607788\n",
      "INFO:absl:[27] train_loss=1.2147232294082642, train_x1_loss=0.45398178696632385, train_x2_loss=0.7607420682907104\n",
      "INFO:absl:[27] val_loss=1.94374418258667\n",
      "INFO:absl:[27] test_loss=1.8206899166107178\n",
      "INFO:absl:[28] train_loss=1.2098463773727417, train_x1_loss=0.4503675699234009, train_x2_loss=0.7594773173332214\n",
      "INFO:absl:[28] val_loss=1.9443466663360596\n",
      "INFO:absl:[28] test_loss=1.821077823638916\n",
      "INFO:absl:[29] train_loss=1.2174303531646729, train_x1_loss=0.4553861618041992, train_x2_loss=0.762044370174408\n",
      "INFO:absl:[29] val_loss=1.9437909126281738\n",
      "INFO:absl:[29] test_loss=1.820806860923767\n",
      "INFO:absl:[30] train_loss=1.217892050743103, train_x1_loss=0.4584546387195587, train_x2_loss=0.7594383358955383\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[30] val_loss=1.9463187456130981\n",
      "INFO:absl:[30] test_loss=1.823322057723999\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[31] train_loss=1.2160985469818115, train_x1_loss=0.45380866527557373, train_x2_loss=0.7622902989387512\n",
      "INFO:absl:[31] val_loss=1.9478389024734497\n",
      "INFO:absl:[31] test_loss=1.8246957063674927\n",
      "INFO:absl:[32] train_loss=1.2171409130096436, train_x1_loss=0.45552486181259155, train_x2_loss=0.7616140246391296\n",
      "INFO:absl:[32] val_loss=1.943358063697815\n",
      "INFO:absl:[32] test_loss=1.8202763795852661\n",
      "INFO:absl:[33] train_loss=1.2118475437164307, train_x1_loss=0.4502814710140228, train_x2_loss=0.7615663409233093\n",
      "INFO:absl:[33] val_loss=1.947816252708435\n",
      "INFO:absl:[33] test_loss=1.824582576751709\n",
      "INFO:absl:[34] train_loss=1.21632719039917, train_x1_loss=0.453737735748291, train_x2_loss=0.7625887393951416\n",
      "INFO:absl:[34] val_loss=1.9473098516464233\n",
      "INFO:absl:[34] test_loss=1.823976993560791\n",
      "INFO:absl:[35] train_loss=1.2103008031845093, train_x1_loss=0.45067015290260315, train_x2_loss=0.7596284747123718\n",
      "INFO:absl:[35] val_loss=1.9458433389663696\n",
      "INFO:absl:[35] test_loss=1.8227962255477905\n",
      "INFO:absl:[36] train_loss=1.2150224447250366, train_x1_loss=0.45186832547187805, train_x2_loss=0.7631548047065735\n",
      "INFO:absl:[36] val_loss=1.946094274520874\n",
      "INFO:absl:[36] test_loss=1.8229507207870483\n",
      "INFO:absl:[37] train_loss=1.2163606882095337, train_x1_loss=0.4536014795303345, train_x2_loss=0.7627613544464111\n",
      "INFO:absl:[37] val_loss=1.9456101655960083\n",
      "INFO:absl:[37] test_loss=1.8222804069519043\n",
      "INFO:absl:[38] train_loss=1.2107692956924438, train_x1_loss=0.4514928162097931, train_x2_loss=0.759277880191803\n",
      "INFO:absl:[38] val_loss=1.9409829378128052\n",
      "INFO:absl:[38] test_loss=1.81795072555542\n",
      "INFO:absl:[39] train_loss=1.2111479043960571, train_x1_loss=0.450145959854126, train_x2_loss=0.7610014081001282\n",
      "INFO:absl:[39] val_loss=1.944549798965454\n",
      "INFO:absl:[39] test_loss=1.8216265439987183\n",
      "INFO:absl:[40] train_loss=1.211323857307434, train_x1_loss=0.4517227113246918, train_x2_loss=0.7596022486686707\n",
      "INFO:absl:[40] val_loss=1.9443751573562622\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[40] test_loss=1.8215677738189697\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[41] train_loss=1.2128514051437378, train_x1_loss=0.4505225419998169, train_x2_loss=0.7623278498649597\n",
      "INFO:absl:[41] val_loss=1.9457206726074219\n",
      "INFO:absl:[41] test_loss=1.8226053714752197\n",
      "INFO:absl:[42] train_loss=1.2113407850265503, train_x1_loss=0.4499831795692444, train_x2_loss=0.7613566517829895\n",
      "INFO:absl:[42] val_loss=1.941042184829712\n",
      "INFO:absl:[42] test_loss=1.8179799318313599\n",
      "INFO:absl:[43] train_loss=1.2117035388946533, train_x1_loss=0.4505312442779541, train_x2_loss=0.7611743211746216\n",
      "INFO:absl:[43] val_loss=1.940251350402832\n",
      "INFO:absl:[43] test_loss=1.8169183731079102\n",
      "INFO:absl:[44] train_loss=1.2083348035812378, train_x1_loss=0.44973471760749817, train_x2_loss=0.7586018443107605\n",
      "INFO:absl:[44] val_loss=1.9427502155303955\n",
      "INFO:absl:[44] test_loss=1.819830298423767\n",
      "INFO:absl:[45] train_loss=1.203086256980896, train_x1_loss=0.4447629451751709, train_x2_loss=0.7583217024803162\n",
      "INFO:absl:[45] val_loss=1.9417877197265625\n",
      "INFO:absl:[45] test_loss=1.8186179399490356\n",
      "INFO:absl:[46] train_loss=1.2047340869903564, train_x1_loss=0.44676831364631653, train_x2_loss=0.7579658627510071\n",
      "INFO:absl:[46] val_loss=1.9401520490646362\n",
      "INFO:absl:[46] test_loss=1.8170607089996338\n",
      "INFO:absl:[47] train_loss=1.2096383571624756, train_x1_loss=0.4480445384979248, train_x2_loss=0.7615942358970642\n",
      "INFO:absl:[47] val_loss=1.941256046295166\n",
      "INFO:absl:[47] test_loss=1.8182317018508911\n",
      "INFO:absl:[48] train_loss=1.2036888599395752, train_x1_loss=0.4455253481864929, train_x2_loss=0.7581643462181091\n",
      "INFO:absl:[48] val_loss=1.9393256902694702\n",
      "INFO:absl:[48] test_loss=1.8162422180175781\n",
      "INFO:absl:[49] train_loss=1.2063758373260498, train_x1_loss=0.4484793543815613, train_x2_loss=0.7578969597816467\n",
      "INFO:absl:[49] val_loss=1.9367884397506714\n",
      "INFO:absl:[49] test_loss=1.813879370689392\n",
      "INFO:absl:[50] train_loss=1.2019107341766357, train_x1_loss=0.44497618079185486, train_x2_loss=0.7569350600242615\n",
      "INFO:absl:[50] val_loss=1.9333304166793823\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[50] test_loss=1.810271143913269\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[51] train_loss=1.2040715217590332, train_x1_loss=0.4445355534553528, train_x2_loss=0.7595373392105103\n",
      "INFO:absl:[51] val_loss=1.9385806322097778\n",
      "INFO:absl:[51] test_loss=1.815635085105896\n",
      "INFO:absl:[52] train_loss=1.2023770809173584, train_x1_loss=0.44565659761428833, train_x2_loss=0.7567216753959656\n",
      "INFO:absl:[52] val_loss=1.941160798072815\n",
      "INFO:absl:[52] test_loss=1.8184634447097778\n",
      "INFO:absl:[53] train_loss=1.201117992401123, train_x1_loss=0.44312402606010437, train_x2_loss=0.7579954862594604\n",
      "INFO:absl:[53] val_loss=1.9342962503433228\n",
      "INFO:absl:[53] test_loss=1.8111045360565186\n",
      "INFO:absl:[54] train_loss=1.2068209648132324, train_x1_loss=0.4474292993545532, train_x2_loss=0.7593902945518494\n",
      "INFO:absl:[54] val_loss=1.9329509735107422\n",
      "INFO:absl:[54] test_loss=1.8099496364593506\n",
      "INFO:absl:[55] train_loss=1.2013593912124634, train_x1_loss=0.4438200891017914, train_x2_loss=0.7575399279594421\n",
      "INFO:absl:[55] val_loss=1.9358772039413452\n",
      "INFO:absl:[55] test_loss=1.8128888607025146\n",
      "INFO:absl:[56] train_loss=1.2059626579284668, train_x1_loss=0.44692015647888184, train_x2_loss=0.7590411305427551\n",
      "INFO:absl:[56] val_loss=1.9334052801132202\n",
      "INFO:absl:[56] test_loss=1.81015145778656\n",
      "INFO:absl:[57] train_loss=1.201494812965393, train_x1_loss=0.44493117928504944, train_x2_loss=0.7565630674362183\n",
      "INFO:absl:[57] val_loss=1.9348338842391968\n",
      "INFO:absl:[57] test_loss=1.8116788864135742\n",
      "INFO:absl:[58] train_loss=1.1965378522872925, train_x1_loss=0.4414210617542267, train_x2_loss=0.7551165819168091\n",
      "INFO:absl:[58] val_loss=1.9310438632965088\n",
      "INFO:absl:[58] test_loss=1.8077565431594849\n",
      "INFO:absl:[59] train_loss=1.2000514268875122, train_x1_loss=0.4420120120048523, train_x2_loss=0.7580373883247375\n",
      "INFO:absl:[59] val_loss=1.9353090524673462\n",
      "INFO:absl:[59] test_loss=1.8124159574508667\n",
      "INFO:absl:[60] train_loss=1.2027873992919922, train_x1_loss=0.4419547915458679, train_x2_loss=0.760834276676178\n",
      "INFO:absl:[60] val_loss=1.9296036958694458\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[60] test_loss=1.806584119796753\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[61] train_loss=1.1994224786758423, train_x1_loss=0.44184428453445435, train_x2_loss=0.7575774788856506\n",
      "INFO:absl:[61] val_loss=1.9365155696868896\n",
      "INFO:absl:[61] test_loss=1.813629388809204\n",
      "INFO:absl:Setting work unit notes: 3670.2 steps/s, 62.9% (220215/350000), ETA: 0m (1m : 0.1% checkpoint, 12.6% eval)\n",
      "INFO:absl:[220215] steps_per_sec=3670.248104\n",
      "INFO:absl:[62] train_loss=1.201228141784668, train_x1_loss=0.44429826736450195, train_x2_loss=0.7569288611412048\n",
      "INFO:absl:[62] val_loss=1.9340800046920776\n",
      "INFO:absl:[62] test_loss=1.8108289241790771\n",
      "INFO:absl:[63] train_loss=1.201844334602356, train_x1_loss=0.4445189833641052, train_x2_loss=0.7573230862617493\n",
      "INFO:absl:[63] val_loss=1.9301661252975464\n",
      "INFO:absl:[63] test_loss=1.8071681261062622\n",
      "INFO:absl:[64] train_loss=1.2014681100845337, train_x1_loss=0.44158080220222473, train_x2_loss=0.7598863840103149\n",
      "INFO:absl:[64] val_loss=1.9318063259124756\n",
      "INFO:absl:[64] test_loss=1.8091157674789429\n",
      "INFO:absl:[65] train_loss=1.199512243270874, train_x1_loss=0.44106626510620117, train_x2_loss=0.7584457397460938\n",
      "INFO:absl:[65] val_loss=1.936924695968628\n",
      "INFO:absl:[65] test_loss=1.8139411211013794\n",
      "INFO:absl:[66] train_loss=1.1972498893737793, train_x1_loss=0.4404524862766266, train_x2_loss=0.7567985653877258\n",
      "INFO:absl:[66] val_loss=1.9309751987457275\n",
      "INFO:absl:[66] test_loss=1.8081376552581787\n",
      "INFO:absl:[67] train_loss=1.1961430311203003, train_x1_loss=0.4389939606189728, train_x2_loss=0.7571485638618469\n",
      "INFO:absl:[67] val_loss=1.927170991897583\n",
      "INFO:absl:[67] test_loss=1.8044309616088867\n",
      "INFO:absl:[68] train_loss=1.194111704826355, train_x1_loss=0.43915536999702454, train_x2_loss=0.7549572587013245\n",
      "INFO:absl:[68] val_loss=1.9298211336135864\n",
      "INFO:absl:[68] test_loss=1.8069877624511719\n",
      "INFO:absl:[69] train_loss=1.1990008354187012, train_x1_loss=0.44347256422042847, train_x2_loss=0.7555282711982727\n",
      "INFO:absl:[69] val_loss=1.9286731481552124\n",
      "INFO:absl:[69] test_loss=1.806059718132019\n",
      "INFO:absl:[70] train_loss=1.1944093704223633, train_x1_loss=0.4396550953388214, train_x2_loss=0.7547573447227478\n",
      "INFO:absl:[70] val_loss=1.9254196882247925\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[70] test_loss=1.8026155233383179\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[71] train_loss=1.1907174587249756, train_x1_loss=0.43903037905693054, train_x2_loss=0.751686692237854\n",
      "INFO:absl:[71] val_loss=1.927500605583191\n",
      "INFO:absl:[71] test_loss=1.8047515153884888\n",
      "INFO:absl:[72] train_loss=1.1966674327850342, train_x1_loss=0.4394814074039459, train_x2_loss=0.7571853995323181\n",
      "INFO:absl:[72] val_loss=1.930586576461792\n",
      "INFO:absl:[72] test_loss=1.8080923557281494\n",
      "INFO:absl:[73] train_loss=1.1983009576797485, train_x1_loss=0.44107672572135925, train_x2_loss=0.7572243213653564\n",
      "INFO:absl:[73] val_loss=1.9294151067733765\n",
      "INFO:absl:[73] test_loss=1.8067545890808105\n",
      "INFO:absl:[74] train_loss=1.1962860822677612, train_x1_loss=0.43964052200317383, train_x2_loss=0.7566491961479187\n",
      "INFO:absl:[74] val_loss=1.9305270910263062\n",
      "INFO:absl:[74] test_loss=1.8077726364135742\n",
      "INFO:absl:[75] train_loss=1.1928791999816895, train_x1_loss=0.43713515996932983, train_x2_loss=0.7557445764541626\n",
      "INFO:absl:[75] val_loss=1.9274346828460693\n",
      "INFO:absl:[75] test_loss=1.8048309087753296\n",
      "INFO:absl:[76] train_loss=1.1901363134384155, train_x1_loss=0.434818297624588, train_x2_loss=0.7553194761276245\n",
      "INFO:absl:[76] val_loss=1.9333099126815796\n",
      "INFO:absl:[76] test_loss=1.810749888420105\n",
      "INFO:absl:[77] train_loss=1.1946994066238403, train_x1_loss=0.4382670521736145, train_x2_loss=0.7564316987991333\n",
      "INFO:absl:[77] val_loss=1.9304006099700928\n",
      "INFO:absl:[77] test_loss=1.8078449964523315\n",
      "INFO:absl:[78] train_loss=1.1977752447128296, train_x1_loss=0.4405226409435272, train_x2_loss=0.7572541236877441\n",
      "INFO:absl:[78] val_loss=1.9293714761734009\n",
      "INFO:absl:[78] test_loss=1.8068857192993164\n",
      "INFO:absl:[79] train_loss=1.1927509307861328, train_x1_loss=0.43938326835632324, train_x2_loss=0.7533683776855469\n",
      "INFO:absl:[79] val_loss=1.9240633249282837\n",
      "INFO:absl:[79] test_loss=1.8014694452285767\n",
      "INFO:absl:[80] train_loss=1.1926872730255127, train_x1_loss=0.43546414375305176, train_x2_loss=0.7572247385978699\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[80] val_loss=1.927648663520813\n",
      "INFO:absl:[80] test_loss=1.8048956394195557\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[81] train_loss=1.192441463470459, train_x1_loss=0.4361877143383026, train_x2_loss=0.7562534809112549\n",
      "INFO:absl:[81] val_loss=1.9287508726119995\n",
      "INFO:absl:[81] test_loss=1.8059507608413696\n",
      "INFO:absl:[82] train_loss=1.1946557760238647, train_x1_loss=0.44037118554115295, train_x2_loss=0.7542846202850342\n",
      "INFO:absl:[82] val_loss=1.9235138893127441\n",
      "INFO:absl:[82] test_loss=1.8007827997207642\n",
      "INFO:absl:[83] train_loss=1.1899751424789429, train_x1_loss=0.4360058903694153, train_x2_loss=0.7539680600166321\n",
      "INFO:absl:[83] val_loss=1.9225865602493286\n",
      "INFO:absl:[83] test_loss=1.8000388145446777\n",
      "INFO:absl:[84] train_loss=1.189428687095642, train_x1_loss=0.4377140402793884, train_x2_loss=0.7517150044441223\n",
      "INFO:absl:[84] val_loss=1.9222633838653564\n",
      "INFO:absl:[84] test_loss=1.7999248504638672\n",
      "INFO:absl:[85] train_loss=1.1921195983886719, train_x1_loss=0.4355819523334503, train_x2_loss=0.7565375566482544\n",
      "INFO:absl:[85] val_loss=1.9221999645233154\n",
      "INFO:absl:[85] test_loss=1.7996197938919067\n",
      "INFO:absl:[86] train_loss=1.19486403465271, train_x1_loss=0.4381081163883209, train_x2_loss=0.7567570209503174\n",
      "INFO:absl:[86] val_loss=1.9249883890151978\n",
      "INFO:absl:[86] test_loss=1.8025046586990356\n",
      "INFO:absl:[87] train_loss=1.192697525024414, train_x1_loss=0.4387044608592987, train_x2_loss=0.7539946436882019\n",
      "INFO:absl:[87] val_loss=1.922672152519226\n",
      "INFO:absl:[87] test_loss=1.8000701665878296\n",
      "INFO:absl:[88] train_loss=1.188214659690857, train_x1_loss=0.4333731532096863, train_x2_loss=0.7548402547836304\n",
      "INFO:absl:[88] val_loss=1.9282177686691284\n",
      "INFO:absl:[88] test_loss=1.80549955368042\n",
      "INFO:absl:[89] train_loss=1.191742181777954, train_x1_loss=0.43629592657089233, train_x2_loss=0.7554454803466797\n",
      "INFO:absl:[89] val_loss=1.9283084869384766\n",
      "INFO:absl:[89] test_loss=1.8056981563568115\n",
      "INFO:absl:[90] train_loss=1.1908411979675293, train_x1_loss=0.43720871210098267, train_x2_loss=0.753631055355072\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[90] val_loss=1.9252718687057495\n",
      "INFO:absl:[90] test_loss=1.802565336227417\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[91] train_loss=1.1908842325210571, train_x1_loss=0.43672430515289307, train_x2_loss=0.7541599869728088\n",
      "INFO:absl:[91] val_loss=1.9220956563949585\n",
      "INFO:absl:[91] test_loss=1.7993942499160767\n",
      "INFO:absl:[92] train_loss=1.1943954229354858, train_x1_loss=0.4379012882709503, train_x2_loss=0.7564947605133057\n",
      "INFO:absl:[92] val_loss=1.9241812229156494\n",
      "INFO:absl:[92] test_loss=1.801864743232727\n",
      "INFO:absl:[93] train_loss=1.1950792074203491, train_x1_loss=0.4382984936237335, train_x2_loss=0.7567788362503052\n",
      "INFO:absl:[93] val_loss=1.9286307096481323\n",
      "INFO:absl:[93] test_loss=1.806158185005188\n",
      "INFO:absl:[94] train_loss=1.1865837574005127, train_x1_loss=0.4314054548740387, train_x2_loss=0.755176842212677\n",
      "INFO:absl:[94] val_loss=1.9272613525390625\n",
      "INFO:absl:[94] test_loss=1.8046660423278809\n",
      "INFO:absl:[95] train_loss=1.1901190280914307, train_x1_loss=0.4348326325416565, train_x2_loss=0.7552853226661682\n",
      "INFO:absl:[95] val_loss=1.9246777296066284\n",
      "INFO:absl:[95] test_loss=1.802547574043274\n",
      "INFO:absl:[96] train_loss=1.1864964962005615, train_x1_loss=0.4332659840583801, train_x2_loss=0.7532294988632202\n",
      "INFO:absl:[96] val_loss=1.9244171380996704\n",
      "INFO:absl:[96] test_loss=1.8018536567687988\n",
      "INFO:absl:[97] train_loss=1.1898523569107056, train_x1_loss=0.4349380135536194, train_x2_loss=0.7549134492874146\n",
      "INFO:absl:[97] val_loss=1.927465796470642\n",
      "INFO:absl:[97] test_loss=1.8051679134368896\n",
      "INFO:absl:[98] train_loss=1.1919876337051392, train_x1_loss=0.437388151884079, train_x2_loss=0.7546017169952393\n",
      "INFO:absl:[98] val_loss=1.9279924631118774\n",
      "INFO:absl:[98] test_loss=1.8051459789276123\n",
      "INFO:absl:[99] train_loss=1.186061978340149, train_x1_loss=0.43202775716781616, train_x2_loss=0.7540327906608582\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[99] val_loss=1.924105167388916\n",
      "INFO:absl:[99] test_loss=1.801708459854126\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'elu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 10, 'dropout_rate': 0.21813956053366504, 'edge_features': (4, 2), 'epochs': 100, 'eval_every_epochs': 1, 'fully_connected_edges': 3, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 1, 'layer_norm': False, 'learning_rate': 3.873531888351175e-05, 'log_every_epochs': 1, 'max_checkpts_to_keep': 2, 'model': 'MLPGraphNetwork', 'n_blocks': 1, 'n_samples': 5000, 'node_features': (4, 2), 'normalize': True, 'optimizer': 'adam', 'output_delay': 0, 'output_steps': 4, 'sample_buffer': -4, 'seed': 70, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 120, 'timestep_duration': 3, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+--------+------+---------+-------+\n",
      "| Name                                   | Shape  | Size | Mean    | Std   |\n",
      "+----------------------------------------+--------+------+---------+-------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (4,)   | 4    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 4) | 24   | -0.0819 | 0.36  |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (2,)   | 2    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (4, 2) | 8    | 0.289   | 0.469 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (4,)   | 4    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (7, 4) | 28   | -0.0395 | 0.333 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)   | 2    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (4, 2) | 8    | -0.135  | 0.542 |\n",
      "+----------------------------------------+--------+------+---------+-------+\n",
      "Total: 80\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Checkpoint None does not exist.\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.00s.\n",
      "INFO:absl:Starting training.\n",
      "INFO:absl:[0] train_loss=3.0573110580444336, train_x1_loss=1.7165501117706299, train_x2_loss=1.3407608270645142\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[0] val_loss=2.9974799156188965\n",
      "INFO:absl:[0] test_loss=3.071308135986328\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Created artifact [10] Profile of type ArtifactType.URL and value None.\n",
      "INFO:absl:[1] train_loss=2.1154685020446777, train_x1_loss=1.0783283710479736, train_x2_loss=1.03713858127594\n",
      "INFO:absl:[1] val_loss=2.494128465652466\n",
      "INFO:absl:[1] test_loss=2.5541722774505615\n",
      "INFO:absl:[2] train_loss=1.6597583293914795, train_x1_loss=0.741492748260498, train_x2_loss=0.9182636737823486\n",
      "INFO:absl:[2] val_loss=2.2092504501342773\n",
      "INFO:absl:[2] test_loss=2.2738780975341797\n",
      "INFO:absl:[3] train_loss=1.4491428136825562, train_x1_loss=0.5701047778129578, train_x2_loss=0.8790382146835327\n",
      "INFO:absl:[3] val_loss=2.1361312866210938\n",
      "INFO:absl:[3] test_loss=2.2024829387664795\n",
      "INFO:absl:[4] train_loss=1.3656136989593506, train_x1_loss=0.5107980370521545, train_x2_loss=0.8548176884651184\n",
      "INFO:absl:[4] val_loss=2.08280611038208\n",
      "INFO:absl:[4] test_loss=2.148167133331299\n",
      "INFO:absl:[5] train_loss=1.3175386190414429, train_x1_loss=0.48776933550834656, train_x2_loss=0.8297702074050903\n",
      "INFO:absl:[5] val_loss=2.027794122695923\n",
      "INFO:absl:[5] test_loss=2.0919322967529297\n",
      "INFO:absl:[6] train_loss=1.2797507047653198, train_x1_loss=0.47022345662117004, train_x2_loss=0.8095251321792603\n",
      "INFO:absl:[6] val_loss=2.001800298690796\n",
      "INFO:absl:[6] test_loss=2.0650579929351807\n",
      "INFO:absl:[7] train_loss=1.2611101865768433, train_x1_loss=0.466594398021698, train_x2_loss=0.7945149540901184\n",
      "INFO:absl:[7] val_loss=1.9744298458099365\n",
      "INFO:absl:[7] test_loss=2.036872148513794\n",
      "INFO:absl:[8] train_loss=1.2468602657318115, train_x1_loss=0.4601641297340393, train_x2_loss=0.7866969108581543\n",
      "INFO:absl:[8] val_loss=1.9667601585388184\n",
      "INFO:absl:[8] test_loss=2.029059886932373\n",
      "INFO:absl:[9] train_loss=1.2374882698059082, train_x1_loss=0.45887574553489685, train_x2_loss=0.7786111235618591\n",
      "INFO:absl:[9] val_loss=1.9534682035446167\n",
      "INFO:absl:[9] test_loss=2.015214681625366\n",
      "INFO:absl:[10] train_loss=1.2298097610473633, train_x1_loss=0.45558807253837585, train_x2_loss=0.7742231488227844\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[10] val_loss=1.949021339416504\n",
      "INFO:absl:[10] test_loss=2.0105624198913574\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[11] train_loss=1.2290849685668945, train_x1_loss=0.45766720175743103, train_x2_loss=0.7714189887046814\n",
      "INFO:absl:[11] val_loss=1.9421688318252563\n",
      "INFO:absl:[11] test_loss=2.0033271312713623\n",
      "INFO:absl:[12] train_loss=1.222675085067749, train_x1_loss=0.4524860084056854, train_x2_loss=0.7701869010925293\n",
      "INFO:absl:[12] val_loss=1.9443358182907104\n",
      "INFO:absl:[12] test_loss=2.0053353309631348\n",
      "INFO:absl:[13] train_loss=1.2200862169265747, train_x1_loss=0.4532245397567749, train_x2_loss=0.7668626308441162\n",
      "INFO:absl:[13] val_loss=1.9391969442367554\n",
      "INFO:absl:[13] test_loss=1.9999805688858032\n",
      "INFO:absl:[14] train_loss=1.2222973108291626, train_x1_loss=0.4552159011363983, train_x2_loss=0.7670814990997314\n",
      "INFO:absl:[14] val_loss=1.9396637678146362\n",
      "INFO:absl:[14] test_loss=2.0002377033233643\n",
      "INFO:absl:[15] train_loss=1.2239881753921509, train_x1_loss=0.45758041739463806, train_x2_loss=0.7664069533348083\n",
      "INFO:absl:[15] val_loss=1.9409010410308838\n",
      "INFO:absl:[15] test_loss=2.0014219284057617\n",
      "INFO:absl:[16] train_loss=1.2222092151641846, train_x1_loss=0.45856964588165283, train_x2_loss=0.7636410593986511\n",
      "INFO:absl:[16] val_loss=1.9381848573684692\n",
      "INFO:absl:[16] test_loss=1.998262643814087\n",
      "INFO:absl:[17] train_loss=1.218468427658081, train_x1_loss=0.45384493470191956, train_x2_loss=0.7646235823631287\n",
      "INFO:absl:[17] val_loss=1.9382174015045166\n",
      "INFO:absl:[17] test_loss=1.9985113143920898\n",
      "INFO:absl:[18] train_loss=1.217473030090332, train_x1_loss=0.45388779044151306, train_x2_loss=0.7635830640792847\n",
      "INFO:absl:[18] val_loss=1.9406450986862183\n",
      "INFO:absl:[18] test_loss=2.0008091926574707\n",
      "INFO:absl:[19] train_loss=1.2145971059799194, train_x1_loss=0.452545702457428, train_x2_loss=0.7620545625686646\n",
      "INFO:absl:[19] val_loss=1.937912106513977\n",
      "INFO:absl:[19] test_loss=1.9980809688568115\n",
      "INFO:absl:[20] train_loss=1.2180571556091309, train_x1_loss=0.4572456479072571, train_x2_loss=0.7608123421669006\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[20] val_loss=1.9363304376602173\n",
      "INFO:absl:[20] test_loss=1.9961987733840942\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[21] train_loss=1.2124911546707153, train_x1_loss=0.4523991048336029, train_x2_loss=0.7600902915000916\n",
      "INFO:absl:[21] val_loss=1.9339185953140259\n",
      "INFO:absl:[21] test_loss=1.9939074516296387\n",
      "INFO:absl:[22] train_loss=1.2127219438552856, train_x1_loss=0.451482355594635, train_x2_loss=0.7612389326095581\n",
      "INFO:absl:[22] val_loss=1.939758539199829\n",
      "INFO:absl:[22] test_loss=1.9997061491012573\n",
      "INFO:absl:[23] train_loss=1.217198371887207, train_x1_loss=0.4547725021839142, train_x2_loss=0.7624258399009705\n",
      "INFO:absl:[23] val_loss=1.937186598777771\n",
      "INFO:absl:[23] test_loss=1.9969865083694458\n",
      "INFO:absl:[24] train_loss=1.2173900604248047, train_x1_loss=0.4549335539340973, train_x2_loss=0.7624568939208984\n",
      "INFO:absl:[24] val_loss=1.9379485845565796\n",
      "INFO:absl:[24] test_loss=1.9977781772613525\n",
      "INFO:absl:[25] train_loss=1.219049334526062, train_x1_loss=0.4559379816055298, train_x2_loss=0.7631124258041382\n",
      "INFO:absl:[25] val_loss=1.9374092817306519\n",
      "INFO:absl:[25] test_loss=1.9969875812530518\n",
      "INFO:absl:[26] train_loss=1.2142353057861328, train_x1_loss=0.4549403190612793, train_x2_loss=0.7592948079109192\n",
      "INFO:absl:[26] val_loss=1.935549259185791\n",
      "INFO:absl:[26] test_loss=1.9951251745224\n",
      "INFO:absl:[27] train_loss=1.216012954711914, train_x1_loss=0.45309552550315857, train_x2_loss=0.7629168033599854\n",
      "INFO:absl:[27] val_loss=1.9412736892700195\n",
      "INFO:absl:[27] test_loss=2.001049280166626\n",
      "INFO:absl:[28] train_loss=1.2150298357009888, train_x1_loss=0.452121376991272, train_x2_loss=0.762908399105072\n",
      "INFO:absl:[28] val_loss=1.937134861946106\n",
      "INFO:absl:[28] test_loss=1.9967427253723145\n",
      "INFO:absl:[29] train_loss=1.2163141965866089, train_x1_loss=0.4560229182243347, train_x2_loss=0.7602916359901428\n",
      "INFO:absl:[29] val_loss=1.9440728425979614\n",
      "INFO:absl:[29] test_loss=2.0036771297454834\n",
      "INFO:absl:[30] train_loss=1.2130357027053833, train_x1_loss=0.45257508754730225, train_x2_loss=0.7604610323905945\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[30] val_loss=1.937345266342163\n",
      "INFO:absl:[30] test_loss=1.9969146251678467\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[31] train_loss=1.2134599685668945, train_x1_loss=0.4551536440849304, train_x2_loss=0.7583056688308716\n",
      "INFO:absl:[31] val_loss=1.9374476671218872\n",
      "INFO:absl:[31] test_loss=1.9968359470367432\n",
      "INFO:absl:[32] train_loss=1.2119944095611572, train_x1_loss=0.4536912441253662, train_x2_loss=0.7583044767379761\n",
      "INFO:absl:[32] val_loss=1.934324026107788\n",
      "INFO:absl:[32] test_loss=1.9935773611068726\n",
      "INFO:absl:[33] train_loss=1.2120943069458008, train_x1_loss=0.4525110423564911, train_x2_loss=0.7595816254615784\n",
      "INFO:absl:[33] val_loss=1.9382171630859375\n",
      "INFO:absl:[33] test_loss=1.997610092163086\n",
      "INFO:absl:[34] train_loss=1.21178138256073, train_x1_loss=0.45274195075035095, train_x2_loss=0.7590385675430298\n",
      "INFO:absl:[34] val_loss=1.9355624914169312\n",
      "INFO:absl:[34] test_loss=1.9945522546768188\n",
      "INFO:absl:[35] train_loss=1.208428978919983, train_x1_loss=0.44874629378318787, train_x2_loss=0.759680986404419\n",
      "INFO:absl:[35] val_loss=1.934848427772522\n",
      "INFO:absl:[35] test_loss=1.9939664602279663\n",
      "INFO:absl:[36] train_loss=1.2133896350860596, train_x1_loss=0.452427476644516, train_x2_loss=0.7609625458717346\n",
      "INFO:absl:[36] val_loss=1.9394789934158325\n",
      "INFO:absl:[36] test_loss=1.9985328912734985\n",
      "INFO:absl:[37] train_loss=1.2108545303344727, train_x1_loss=0.45195963978767395, train_x2_loss=0.7588948011398315\n",
      "INFO:absl:[37] val_loss=1.9376566410064697\n",
      "INFO:absl:[37] test_loss=1.9967131614685059\n",
      "INFO:absl:[38] train_loss=1.206762433052063, train_x1_loss=0.44769227504730225, train_x2_loss=0.7590712904930115\n",
      "INFO:absl:[38] val_loss=1.9366567134857178\n",
      "INFO:absl:[38] test_loss=1.9955260753631592\n",
      "INFO:absl:[39] train_loss=1.2073440551757812, train_x1_loss=0.4511123597621918, train_x2_loss=0.7562322020530701\n",
      "INFO:absl:[39] val_loss=1.9324959516525269\n",
      "INFO:absl:[39] test_loss=1.991298794746399\n",
      "INFO:absl:[40] train_loss=1.2091115713119507, train_x1_loss=0.4481644332408905, train_x2_loss=0.7609474062919617\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[40] val_loss=1.9363845586776733\n",
      "INFO:absl:[40] test_loss=1.9952360391616821\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[41] train_loss=1.2091317176818848, train_x1_loss=0.44807595014572144, train_x2_loss=0.7610571980476379\n",
      "INFO:absl:[41] val_loss=1.9373412132263184\n",
      "INFO:absl:[41] test_loss=1.9962364435195923\n",
      "INFO:absl:[42] train_loss=1.2024739980697632, train_x1_loss=0.4466812312602997, train_x2_loss=0.7557917237281799\n",
      "INFO:absl:[42] val_loss=1.9310979843139648\n",
      "INFO:absl:[42] test_loss=1.989652395248413\n",
      "INFO:absl:[43] train_loss=1.2053855657577515, train_x1_loss=0.4476529657840729, train_x2_loss=0.7577326893806458\n",
      "INFO:absl:[43] val_loss=1.9311565160751343\n",
      "INFO:absl:[43] test_loss=1.9898642301559448\n",
      "INFO:absl:[44] train_loss=1.2057005167007446, train_x1_loss=0.44835808873176575, train_x2_loss=0.7573415040969849\n",
      "INFO:absl:[44] val_loss=1.9315136671066284\n",
      "INFO:absl:[44] test_loss=1.9899013042449951\n",
      "INFO:absl:[45] train_loss=1.2067115306854248, train_x1_loss=0.4465767443180084, train_x2_loss=0.7601337432861328\n",
      "INFO:absl:[45] val_loss=1.9351091384887695\n",
      "INFO:absl:[45] test_loss=1.9937525987625122\n",
      "INFO:absl:[46] train_loss=1.2022855281829834, train_x1_loss=0.44506052136421204, train_x2_loss=0.7572253942489624\n",
      "INFO:absl:[46] val_loss=1.9329311847686768\n",
      "INFO:absl:[46] test_loss=1.9913119077682495\n",
      "INFO:absl:[47] train_loss=1.2062180042266846, train_x1_loss=0.44880032539367676, train_x2_loss=0.7574187517166138\n",
      "INFO:absl:[47] val_loss=1.9309582710266113\n",
      "INFO:absl:[47] test_loss=1.9893566370010376\n",
      "INFO:absl:[48] train_loss=1.2019048929214478, train_x1_loss=0.44433000683784485, train_x2_loss=0.7575742602348328\n",
      "INFO:absl:[48] val_loss=1.9276479482650757\n",
      "INFO:absl:[48] test_loss=1.9860811233520508\n",
      "INFO:absl:[49] train_loss=1.2022019624710083, train_x1_loss=0.4443695843219757, train_x2_loss=0.7578309774398804\n",
      "INFO:absl:[49] val_loss=1.9317729473114014\n",
      "INFO:absl:[49] test_loss=1.990148901939392\n",
      "INFO:absl:[50] train_loss=1.1999421119689941, train_x1_loss=0.4438367187976837, train_x2_loss=0.7561060190200806\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[50] val_loss=1.927940845489502\n",
      "INFO:absl:[50] test_loss=1.9863123893737793\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[51] train_loss=1.2077324390411377, train_x1_loss=0.44856956601142883, train_x2_loss=0.7591634392738342\n",
      "INFO:absl:[51] val_loss=1.932841181755066\n",
      "INFO:absl:[51] test_loss=1.9912488460540771\n",
      "INFO:absl:[52] train_loss=1.2040488719940186, train_x1_loss=0.44645994901657104, train_x2_loss=0.7575883269309998\n",
      "INFO:absl:[52] val_loss=1.9271796941757202\n",
      "INFO:absl:[52] test_loss=1.9853767156600952\n",
      "INFO:absl:[53] train_loss=1.2025341987609863, train_x1_loss=0.4445015788078308, train_x2_loss=0.7580329775810242\n",
      "INFO:absl:[53] val_loss=1.9270471334457397\n",
      "INFO:absl:[53] test_loss=1.985195517539978\n",
      "INFO:absl:[54] train_loss=1.2015355825424194, train_x1_loss=0.44410842657089233, train_x2_loss=0.7574272751808167\n",
      "INFO:absl:[54] val_loss=1.9256799221038818\n",
      "INFO:absl:[54] test_loss=1.9839222431182861\n",
      "INFO:absl:[55] train_loss=1.2003241777420044, train_x1_loss=0.44312378764152527, train_x2_loss=0.7572011947631836\n",
      "INFO:absl:[55] val_loss=1.9270139932632446\n",
      "INFO:absl:[55] test_loss=1.9851858615875244\n",
      "INFO:absl:[56] train_loss=1.1975305080413818, train_x1_loss=0.4423608183860779, train_x2_loss=0.7551727294921875\n",
      "INFO:absl:[56] val_loss=1.9260625839233398\n",
      "INFO:absl:[56] test_loss=1.9840023517608643\n",
      "INFO:absl:[57] train_loss=1.2002413272857666, train_x1_loss=0.44363847374916077, train_x2_loss=0.7566034197807312\n",
      "INFO:absl:[57] val_loss=1.924914836883545\n",
      "INFO:absl:[57] test_loss=1.9829083681106567\n",
      "INFO:absl:[58] train_loss=1.1973625421524048, train_x1_loss=0.44116199016571045, train_x2_loss=0.7562023401260376\n",
      "INFO:absl:[58] val_loss=1.9275331497192383\n",
      "INFO:absl:[58] test_loss=1.9854215383529663\n",
      "INFO:absl:[59] train_loss=1.1950266361236572, train_x1_loss=0.4410460591316223, train_x2_loss=0.7539812326431274\n",
      "INFO:absl:[59] val_loss=1.9285335540771484\n",
      "INFO:absl:[59] test_loss=1.9863836765289307\n",
      "INFO:absl:[60] train_loss=1.2022243738174438, train_x1_loss=0.44327619671821594, train_x2_loss=0.7589475512504578\n",
      "INFO:absl:[60] val_loss=1.9238821268081665\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[60] test_loss=1.9815421104431152\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Setting work unit notes: 3600.5 steps/s, 61.7% (216029/350000), ETA: 0m (1m : 0.1% checkpoint, 12.8% eval)\n",
      "INFO:absl:[216029] steps_per_sec=3600.481116\n",
      "INFO:absl:[61] train_loss=1.1990103721618652, train_x1_loss=0.4420727491378784, train_x2_loss=0.7569390535354614\n",
      "INFO:absl:[61] val_loss=1.9273635149002075\n",
      "INFO:absl:[61] test_loss=1.9850324392318726\n",
      "INFO:absl:[62] train_loss=1.1937772035598755, train_x1_loss=0.4388554096221924, train_x2_loss=0.7549228072166443\n",
      "INFO:absl:[62] val_loss=1.924524188041687\n",
      "INFO:absl:[62] test_loss=1.9822361469268799\n",
      "INFO:absl:[63] train_loss=1.199864149093628, train_x1_loss=0.4428834021091461, train_x2_loss=0.7569811940193176\n",
      "INFO:absl:[63] val_loss=1.9223377704620361\n",
      "INFO:absl:[63] test_loss=1.979871392250061\n",
      "INFO:absl:[64] train_loss=1.1941194534301758, train_x1_loss=0.44005855917930603, train_x2_loss=0.7540605664253235\n",
      "INFO:absl:[64] val_loss=1.9290282726287842\n",
      "INFO:absl:[64] test_loss=1.9867064952850342\n",
      "INFO:absl:[65] train_loss=1.2010889053344727, train_x1_loss=0.44310685992240906, train_x2_loss=0.7579799294471741\n",
      "INFO:absl:[65] val_loss=1.9300105571746826\n",
      "INFO:absl:[65] test_loss=1.9874407052993774\n",
      "INFO:absl:[66] train_loss=1.1946896314620972, train_x1_loss=0.4415603578090668, train_x2_loss=0.753128707408905\n",
      "INFO:absl:[66] val_loss=1.9278602600097656\n",
      "INFO:absl:[66] test_loss=1.9853476285934448\n",
      "INFO:absl:[67] train_loss=1.1969482898712158, train_x1_loss=0.44232407212257385, train_x2_loss=0.7546214461326599\n",
      "INFO:absl:[67] val_loss=1.9229894876480103\n",
      "INFO:absl:[67] test_loss=1.9803978204727173\n",
      "INFO:absl:[68] train_loss=1.1971219778060913, train_x1_loss=0.4418509304523468, train_x2_loss=0.7552726864814758\n",
      "INFO:absl:[68] val_loss=1.925368070602417\n",
      "INFO:absl:[68] test_loss=1.9826449155807495\n",
      "INFO:absl:[69] train_loss=1.1951290369033813, train_x1_loss=0.44094017148017883, train_x2_loss=0.7541894316673279\n",
      "INFO:absl:[69] val_loss=1.9252530336380005\n",
      "INFO:absl:[69] test_loss=1.9824938774108887\n",
      "INFO:absl:[70] train_loss=1.1963251829147339, train_x1_loss=0.44404837489128113, train_x2_loss=0.7522767782211304\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[70] val_loss=1.9218297004699707\n",
      "INFO:absl:[70] test_loss=1.9789799451828003\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[71] train_loss=1.1895995140075684, train_x1_loss=0.4397803544998169, train_x2_loss=0.7498202919960022\n",
      "INFO:absl:[71] val_loss=1.9179259538650513\n",
      "INFO:absl:[71] test_loss=1.9748843908309937\n",
      "INFO:absl:[72] train_loss=1.1948716640472412, train_x1_loss=0.44004666805267334, train_x2_loss=0.7548239827156067\n",
      "INFO:absl:[72] val_loss=1.9245641231536865\n",
      "INFO:absl:[72] test_loss=1.9816794395446777\n",
      "INFO:absl:[73] train_loss=1.1923097372055054, train_x1_loss=0.43952110409736633, train_x2_loss=0.7527891397476196\n",
      "INFO:absl:[73] val_loss=1.928351879119873\n",
      "INFO:absl:[73] test_loss=1.9856749773025513\n",
      "INFO:absl:[74] train_loss=1.195409893989563, train_x1_loss=0.4402957856655121, train_x2_loss=0.7551127076148987\n",
      "INFO:absl:[74] val_loss=1.9275972843170166\n",
      "INFO:absl:[74] test_loss=1.9849950075149536\n",
      "INFO:absl:[75] train_loss=1.196869134902954, train_x1_loss=0.4396120607852936, train_x2_loss=0.7572579383850098\n",
      "INFO:absl:[75] val_loss=1.9244582653045654\n",
      "INFO:absl:[75] test_loss=1.9816298484802246\n",
      "INFO:absl:[76] train_loss=1.1879938840866089, train_x1_loss=0.4343128204345703, train_x2_loss=0.7536805868148804\n",
      "INFO:absl:[76] val_loss=1.9223331212997437\n",
      "INFO:absl:[76] test_loss=1.9793733358383179\n",
      "INFO:absl:[77] train_loss=1.1948115825653076, train_x1_loss=0.44129934906959534, train_x2_loss=0.7535141110420227\n",
      "INFO:absl:[77] val_loss=1.9216428995132446\n",
      "INFO:absl:[77] test_loss=1.9785429239273071\n",
      "INFO:absl:[78] train_loss=1.1922067403793335, train_x1_loss=0.438676655292511, train_x2_loss=0.7535293102264404\n",
      "INFO:absl:[78] val_loss=1.9196339845657349\n",
      "INFO:absl:[78] test_loss=1.9765578508377075\n",
      "INFO:absl:[79] train_loss=1.1866834163665771, train_x1_loss=0.4333838224411011, train_x2_loss=0.7532986402511597\n",
      "INFO:absl:[79] val_loss=1.918587327003479\n",
      "INFO:absl:[79] test_loss=1.9754761457443237\n",
      "INFO:absl:[80] train_loss=1.189703106880188, train_x1_loss=0.4370804727077484, train_x2_loss=0.7526232004165649\n",
      "INFO:absl:[80] val_loss=1.9215279817581177\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[80] test_loss=1.9785120487213135\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[81] train_loss=1.1890194416046143, train_x1_loss=0.4359206557273865, train_x2_loss=0.75309818983078\n",
      "INFO:absl:[81] val_loss=1.922622799873352\n",
      "INFO:absl:[81] test_loss=1.9796334505081177\n",
      "INFO:absl:[82] train_loss=1.188425064086914, train_x1_loss=0.43697816133499146, train_x2_loss=0.7514474987983704\n",
      "INFO:absl:[82] val_loss=1.9200046062469482\n",
      "INFO:absl:[82] test_loss=1.9769119024276733\n",
      "INFO:absl:[83] train_loss=1.189713716506958, train_x1_loss=0.438143789768219, train_x2_loss=0.751567542552948\n",
      "INFO:absl:[83] val_loss=1.9171366691589355\n",
      "INFO:absl:[83] test_loss=1.9740958213806152\n",
      "INFO:absl:[84] train_loss=1.1883723735809326, train_x1_loss=0.43630895018577576, train_x2_loss=0.7520617842674255\n",
      "INFO:absl:[84] val_loss=1.921653151512146\n",
      "INFO:absl:[84] test_loss=1.9785404205322266\n",
      "INFO:absl:[85] train_loss=1.1908022165298462, train_x1_loss=0.43589580059051514, train_x2_loss=0.7549058198928833\n",
      "INFO:absl:[85] val_loss=1.9174835681915283\n",
      "INFO:absl:[85] test_loss=1.974625587463379\n",
      "INFO:absl:[86] train_loss=1.1895817518234253, train_x1_loss=0.43761470913887024, train_x2_loss=0.7519663572311401\n",
      "INFO:absl:[86] val_loss=1.9206126928329468\n",
      "INFO:absl:[86] test_loss=1.9775071144104004\n",
      "INFO:absl:[87] train_loss=1.1885193586349487, train_x1_loss=0.43622705340385437, train_x2_loss=0.7522936463356018\n",
      "INFO:absl:[87] val_loss=1.9184600114822388\n",
      "INFO:absl:[87] test_loss=1.9752638339996338\n",
      "INFO:absl:[88] train_loss=1.185954213142395, train_x1_loss=0.43255630135536194, train_x2_loss=0.7533973455429077\n",
      "INFO:absl:[88] val_loss=1.9179954528808594\n",
      "INFO:absl:[88] test_loss=1.9748563766479492\n",
      "INFO:absl:[89] train_loss=1.18876051902771, train_x1_loss=0.43533289432525635, train_x2_loss=0.7534283399581909\n",
      "INFO:absl:[89] val_loss=1.9208241701126099\n",
      "INFO:absl:[89] test_loss=1.9777582883834839\n",
      "INFO:absl:[90] train_loss=1.1910951137542725, train_x1_loss=0.4372914135456085, train_x2_loss=0.7538039088249207\n",
      "INFO:absl:[90] val_loss=1.9204663038253784\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[90] test_loss=1.977514386177063\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[91] train_loss=1.1885782480239868, train_x1_loss=0.43634775280952454, train_x2_loss=0.7522315979003906\n",
      "INFO:absl:[91] val_loss=1.9165782928466797\n",
      "INFO:absl:[91] test_loss=1.9732609987258911\n",
      "INFO:absl:[92] train_loss=1.187251091003418, train_x1_loss=0.43737635016441345, train_x2_loss=0.7498766183853149\n",
      "INFO:absl:[92] val_loss=1.914986252784729\n",
      "INFO:absl:[92] test_loss=1.9714555740356445\n",
      "INFO:absl:[93] train_loss=1.1914013624191284, train_x1_loss=0.4389324486255646, train_x2_loss=0.7524696588516235\n",
      "INFO:absl:[93] val_loss=1.9182450771331787\n",
      "INFO:absl:[93] test_loss=1.975028395652771\n",
      "INFO:absl:[94] train_loss=1.1871269941329956, train_x1_loss=0.43545210361480713, train_x2_loss=0.7516749501228333\n",
      "INFO:absl:[94] val_loss=1.9195679426193237\n",
      "INFO:absl:[94] test_loss=1.976362705230713\n",
      "INFO:absl:[95] train_loss=1.186620831489563, train_x1_loss=0.43383917212486267, train_x2_loss=0.7527810335159302\n",
      "INFO:absl:[95] val_loss=1.9182151556015015\n",
      "INFO:absl:[95] test_loss=1.9750287532806396\n",
      "INFO:absl:[96] train_loss=1.1909165382385254, train_x1_loss=0.4363555610179901, train_x2_loss=0.7545590400695801\n",
      "INFO:absl:[96] val_loss=1.9226536750793457\n",
      "INFO:absl:[96] test_loss=1.9796545505523682\n",
      "INFO:absl:[97] train_loss=1.1909984350204468, train_x1_loss=0.43559032678604126, train_x2_loss=0.755407989025116\n",
      "INFO:absl:[97] val_loss=1.9187426567077637\n",
      "INFO:absl:[97] test_loss=1.9754703044891357\n",
      "INFO:absl:[98] train_loss=1.188287377357483, train_x1_loss=0.43748629093170166, train_x2_loss=0.7508000135421753\n",
      "INFO:absl:[98] val_loss=1.918232798576355\n",
      "INFO:absl:[98] test_loss=1.9749114513397217\n",
      "INFO:absl:[99] train_loss=1.1890449523925781, train_x1_loss=0.43338510394096375, train_x2_loss=0.7556607127189636\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[99] val_loss=1.9175268411636353\n",
      "INFO:absl:[99] test_loss=1.9740948677062988\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'elu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 10, 'dropout_rate': 0.21813956053366504, 'edge_features': (4, 2), 'epochs': 100, 'eval_every_epochs': 1, 'fully_connected_edges': 3, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 1, 'layer_norm': False, 'learning_rate': 3.873531888351175e-05, 'log_every_epochs': 1, 'max_checkpts_to_keep': 2, 'model': 'MLPGraphNetwork', 'n_blocks': 1, 'n_samples': 5000, 'node_features': (4, 2), 'normalize': True, 'optimizer': 'adam', 'output_delay': 0, 'output_steps': 4, 'sample_buffer': -4, 'seed': 71, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 120, 'timestep_duration': 3, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+--------+------+---------+-------+\n",
      "| Name                                   | Shape  | Size | Mean    | Std   |\n",
      "+----------------------------------------+--------+------+---------+-------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (4,)   | 4    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 4) | 24   | -0.0819 | 0.36  |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (2,)   | 2    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (4, 2) | 8    | 0.289   | 0.469 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (4,)   | 4    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (7, 4) | 28   | -0.0395 | 0.333 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)   | 2    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (4, 2) | 8    | -0.135  | 0.542 |\n",
      "+----------------------------------------+--------+------+---------+-------+\n",
      "Total: 80\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Checkpoint None does not exist.\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.00s.\n",
      "INFO:absl:Starting training.\n",
      "INFO:absl:[0] train_loss=3.0573110580444336, train_x1_loss=1.7165501117706299, train_x2_loss=1.3407608270645142\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[0] val_loss=2.9974799156188965\n",
      "INFO:absl:[0] test_loss=3.071308135986328\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Created artifact [10] Profile of type ArtifactType.URL and value None.\n",
      "INFO:absl:[1] train_loss=2.1154685020446777, train_x1_loss=1.0783283710479736, train_x2_loss=1.03713858127594\n",
      "INFO:absl:[1] val_loss=2.494128465652466\n",
      "INFO:absl:[1] test_loss=2.5541722774505615\n",
      "INFO:absl:[2] train_loss=1.6597583293914795, train_x1_loss=0.741492748260498, train_x2_loss=0.9182636737823486\n",
      "INFO:absl:[2] val_loss=2.2092504501342773\n",
      "INFO:absl:[2] test_loss=2.2738780975341797\n",
      "INFO:absl:[3] train_loss=1.4491428136825562, train_x1_loss=0.5701047778129578, train_x2_loss=0.8790382146835327\n",
      "INFO:absl:[3] val_loss=2.1361312866210938\n",
      "INFO:absl:[3] test_loss=2.2024829387664795\n",
      "INFO:absl:[4] train_loss=1.3656136989593506, train_x1_loss=0.5107980370521545, train_x2_loss=0.8548176884651184\n",
      "INFO:absl:[4] val_loss=2.08280611038208\n",
      "INFO:absl:[4] test_loss=2.148167133331299\n",
      "INFO:absl:[5] train_loss=1.3175386190414429, train_x1_loss=0.48776933550834656, train_x2_loss=0.8297702074050903\n",
      "INFO:absl:[5] val_loss=2.027794122695923\n",
      "INFO:absl:[5] test_loss=2.0919322967529297\n",
      "INFO:absl:[6] train_loss=1.2797507047653198, train_x1_loss=0.47022345662117004, train_x2_loss=0.8095251321792603\n",
      "INFO:absl:[6] val_loss=2.001800298690796\n",
      "INFO:absl:[6] test_loss=2.0650579929351807\n",
      "INFO:absl:[7] train_loss=1.2611101865768433, train_x1_loss=0.466594398021698, train_x2_loss=0.7945149540901184\n",
      "INFO:absl:[7] val_loss=1.9744298458099365\n",
      "INFO:absl:[7] test_loss=2.036872148513794\n",
      "INFO:absl:[8] train_loss=1.2468602657318115, train_x1_loss=0.4601641297340393, train_x2_loss=0.7866969108581543\n",
      "INFO:absl:[8] val_loss=1.9667601585388184\n",
      "INFO:absl:[8] test_loss=2.029059886932373\n",
      "INFO:absl:[9] train_loss=1.2374882698059082, train_x1_loss=0.45887574553489685, train_x2_loss=0.7786111235618591\n",
      "INFO:absl:[9] val_loss=1.9534682035446167\n",
      "INFO:absl:[9] test_loss=2.015214681625366\n",
      "INFO:absl:[10] train_loss=1.2298097610473633, train_x1_loss=0.45558807253837585, train_x2_loss=0.7742231488227844\n",
      "INFO:absl:[10] val_loss=1.949021339416504\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[10] test_loss=2.0105624198913574\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[11] train_loss=1.2290849685668945, train_x1_loss=0.45766720175743103, train_x2_loss=0.7714189887046814\n",
      "INFO:absl:[11] val_loss=1.9421688318252563\n",
      "INFO:absl:[11] test_loss=2.0033271312713623\n",
      "INFO:absl:[12] train_loss=1.222675085067749, train_x1_loss=0.4524860084056854, train_x2_loss=0.7701869010925293\n",
      "INFO:absl:[12] val_loss=1.9443358182907104\n",
      "INFO:absl:[12] test_loss=2.0053353309631348\n",
      "INFO:absl:[13] train_loss=1.2200862169265747, train_x1_loss=0.4532245397567749, train_x2_loss=0.7668626308441162\n",
      "INFO:absl:[13] val_loss=1.9391969442367554\n",
      "INFO:absl:[13] test_loss=1.9999805688858032\n",
      "INFO:absl:[14] train_loss=1.2222973108291626, train_x1_loss=0.4552159011363983, train_x2_loss=0.7670814990997314\n",
      "INFO:absl:[14] val_loss=1.9396637678146362\n",
      "INFO:absl:[14] test_loss=2.0002377033233643\n",
      "INFO:absl:[15] train_loss=1.2239881753921509, train_x1_loss=0.45758041739463806, train_x2_loss=0.7664069533348083\n",
      "INFO:absl:[15] val_loss=1.9409010410308838\n",
      "INFO:absl:[15] test_loss=2.0014219284057617\n",
      "INFO:absl:[16] train_loss=1.2222092151641846, train_x1_loss=0.45856964588165283, train_x2_loss=0.7636410593986511\n",
      "INFO:absl:[16] val_loss=1.9381848573684692\n",
      "INFO:absl:[16] test_loss=1.998262643814087\n",
      "INFO:absl:[17] train_loss=1.218468427658081, train_x1_loss=0.45384493470191956, train_x2_loss=0.7646235823631287\n",
      "INFO:absl:[17] val_loss=1.9382174015045166\n",
      "INFO:absl:[17] test_loss=1.9985113143920898\n",
      "INFO:absl:[18] train_loss=1.217473030090332, train_x1_loss=0.45388779044151306, train_x2_loss=0.7635830640792847\n",
      "INFO:absl:[18] val_loss=1.9406450986862183\n",
      "INFO:absl:[18] test_loss=2.0008091926574707\n",
      "INFO:absl:[19] train_loss=1.2145971059799194, train_x1_loss=0.452545702457428, train_x2_loss=0.7620545625686646\n",
      "INFO:absl:[19] val_loss=1.937912106513977\n",
      "INFO:absl:[19] test_loss=1.9980809688568115\n",
      "INFO:absl:[20] train_loss=1.2180571556091309, train_x1_loss=0.4572456479072571, train_x2_loss=0.7608123421669006\n",
      "INFO:absl:[20] val_loss=1.9363304376602173\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[20] test_loss=1.9961987733840942\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[21] train_loss=1.2124911546707153, train_x1_loss=0.4523991048336029, train_x2_loss=0.7600902915000916\n",
      "INFO:absl:[21] val_loss=1.9339185953140259\n",
      "INFO:absl:[21] test_loss=1.9939074516296387\n",
      "INFO:absl:[22] train_loss=1.2127219438552856, train_x1_loss=0.451482355594635, train_x2_loss=0.7612389326095581\n",
      "INFO:absl:[22] val_loss=1.939758539199829\n",
      "INFO:absl:[22] test_loss=1.9997061491012573\n",
      "INFO:absl:[23] train_loss=1.217198371887207, train_x1_loss=0.4547725021839142, train_x2_loss=0.7624258399009705\n",
      "INFO:absl:[23] val_loss=1.937186598777771\n",
      "INFO:absl:[23] test_loss=1.9969865083694458\n",
      "INFO:absl:[24] train_loss=1.2173900604248047, train_x1_loss=0.4549335539340973, train_x2_loss=0.7624568939208984\n",
      "INFO:absl:[24] val_loss=1.9379485845565796\n",
      "INFO:absl:[24] test_loss=1.9977781772613525\n",
      "INFO:absl:[25] train_loss=1.219049334526062, train_x1_loss=0.4559379816055298, train_x2_loss=0.7631124258041382\n",
      "INFO:absl:[25] val_loss=1.9374092817306519\n",
      "INFO:absl:[25] test_loss=1.9969875812530518\n",
      "INFO:absl:[26] train_loss=1.2142353057861328, train_x1_loss=0.4549403190612793, train_x2_loss=0.7592948079109192\n",
      "INFO:absl:[26] val_loss=1.935549259185791\n",
      "INFO:absl:[26] test_loss=1.9951251745224\n",
      "INFO:absl:[27] train_loss=1.216012954711914, train_x1_loss=0.45309552550315857, train_x2_loss=0.7629168033599854\n",
      "INFO:absl:[27] val_loss=1.9412736892700195\n",
      "INFO:absl:[27] test_loss=2.001049280166626\n",
      "INFO:absl:[28] train_loss=1.2150298357009888, train_x1_loss=0.452121376991272, train_x2_loss=0.762908399105072\n",
      "INFO:absl:[28] val_loss=1.937134861946106\n",
      "INFO:absl:[28] test_loss=1.9967427253723145\n",
      "INFO:absl:[29] train_loss=1.2163141965866089, train_x1_loss=0.4560229182243347, train_x2_loss=0.7602916359901428\n",
      "INFO:absl:[29] val_loss=1.9440728425979614\n",
      "INFO:absl:[29] test_loss=2.0036771297454834\n",
      "INFO:absl:[30] train_loss=1.2130357027053833, train_x1_loss=0.45257508754730225, train_x2_loss=0.7604610323905945\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[30] val_loss=1.937345266342163\n",
      "INFO:absl:[30] test_loss=1.9969146251678467\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[31] train_loss=1.2134599685668945, train_x1_loss=0.4551536440849304, train_x2_loss=0.7583056688308716\n",
      "INFO:absl:[31] val_loss=1.9374476671218872\n",
      "INFO:absl:[31] test_loss=1.9968359470367432\n",
      "INFO:absl:[32] train_loss=1.2119944095611572, train_x1_loss=0.4536912441253662, train_x2_loss=0.7583044767379761\n",
      "INFO:absl:[32] val_loss=1.934324026107788\n",
      "INFO:absl:[32] test_loss=1.9935773611068726\n",
      "INFO:absl:[33] train_loss=1.2120943069458008, train_x1_loss=0.4525110423564911, train_x2_loss=0.7595816254615784\n",
      "INFO:absl:[33] val_loss=1.9382171630859375\n",
      "INFO:absl:[33] test_loss=1.997610092163086\n",
      "INFO:absl:[34] train_loss=1.21178138256073, train_x1_loss=0.45274195075035095, train_x2_loss=0.7590385675430298\n",
      "INFO:absl:[34] val_loss=1.9355624914169312\n",
      "INFO:absl:[34] test_loss=1.9945522546768188\n",
      "INFO:absl:[35] train_loss=1.208428978919983, train_x1_loss=0.44874629378318787, train_x2_loss=0.759680986404419\n",
      "INFO:absl:[35] val_loss=1.934848427772522\n",
      "INFO:absl:[35] test_loss=1.9939664602279663\n",
      "INFO:absl:[36] train_loss=1.2133896350860596, train_x1_loss=0.452427476644516, train_x2_loss=0.7609625458717346\n",
      "INFO:absl:[36] val_loss=1.9394789934158325\n",
      "INFO:absl:[36] test_loss=1.9985328912734985\n",
      "INFO:absl:[37] train_loss=1.2108545303344727, train_x1_loss=0.45195963978767395, train_x2_loss=0.7588948011398315\n",
      "INFO:absl:[37] val_loss=1.9376566410064697\n",
      "INFO:absl:[37] test_loss=1.9967131614685059\n",
      "INFO:absl:[38] train_loss=1.206762433052063, train_x1_loss=0.44769227504730225, train_x2_loss=0.7590712904930115\n",
      "INFO:absl:[38] val_loss=1.9366567134857178\n",
      "INFO:absl:[38] test_loss=1.9955260753631592\n",
      "INFO:absl:[39] train_loss=1.2073440551757812, train_x1_loss=0.4511123597621918, train_x2_loss=0.7562322020530701\n",
      "INFO:absl:[39] val_loss=1.9324959516525269\n",
      "INFO:absl:[39] test_loss=1.991298794746399\n",
      "INFO:absl:[40] train_loss=1.2091115713119507, train_x1_loss=0.4481644332408905, train_x2_loss=0.7609474062919617\n",
      "INFO:absl:[40] val_loss=1.9363845586776733\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[40] test_loss=1.9952360391616821\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[41] train_loss=1.2091317176818848, train_x1_loss=0.44807595014572144, train_x2_loss=0.7610571980476379\n",
      "INFO:absl:[41] val_loss=1.9373412132263184\n",
      "INFO:absl:[41] test_loss=1.9962364435195923\n",
      "INFO:absl:[42] train_loss=1.2024739980697632, train_x1_loss=0.4466812312602997, train_x2_loss=0.7557917237281799\n",
      "INFO:absl:[42] val_loss=1.9310979843139648\n",
      "INFO:absl:[42] test_loss=1.989652395248413\n",
      "INFO:absl:[43] train_loss=1.2053855657577515, train_x1_loss=0.4476529657840729, train_x2_loss=0.7577326893806458\n",
      "INFO:absl:[43] val_loss=1.9311565160751343\n",
      "INFO:absl:[43] test_loss=1.9898642301559448\n",
      "INFO:absl:[44] train_loss=1.2057005167007446, train_x1_loss=0.44835808873176575, train_x2_loss=0.7573415040969849\n",
      "INFO:absl:[44] val_loss=1.9315136671066284\n",
      "INFO:absl:[44] test_loss=1.9899013042449951\n",
      "INFO:absl:[45] train_loss=1.2067115306854248, train_x1_loss=0.4465767443180084, train_x2_loss=0.7601337432861328\n",
      "INFO:absl:[45] val_loss=1.9351091384887695\n",
      "INFO:absl:[45] test_loss=1.9937525987625122\n",
      "INFO:absl:[46] train_loss=1.2022855281829834, train_x1_loss=0.44506052136421204, train_x2_loss=0.7572253942489624\n",
      "INFO:absl:[46] val_loss=1.9329311847686768\n",
      "INFO:absl:[46] test_loss=1.9913119077682495\n",
      "INFO:absl:[47] train_loss=1.2062180042266846, train_x1_loss=0.44880032539367676, train_x2_loss=0.7574187517166138\n",
      "INFO:absl:[47] val_loss=1.9309582710266113\n",
      "INFO:absl:[47] test_loss=1.9893566370010376\n",
      "INFO:absl:[48] train_loss=1.2019048929214478, train_x1_loss=0.44433000683784485, train_x2_loss=0.7575742602348328\n",
      "INFO:absl:[48] val_loss=1.9276479482650757\n",
      "INFO:absl:[48] test_loss=1.9860811233520508\n",
      "INFO:absl:[49] train_loss=1.2022019624710083, train_x1_loss=0.4443695843219757, train_x2_loss=0.7578309774398804\n",
      "INFO:absl:[49] val_loss=1.9317729473114014\n",
      "INFO:absl:[49] test_loss=1.990148901939392\n",
      "INFO:absl:[50] train_loss=1.1999421119689941, train_x1_loss=0.4438367187976837, train_x2_loss=0.7561060190200806\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[50] val_loss=1.927940845489502\n",
      "INFO:absl:[50] test_loss=1.9863123893737793\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[51] train_loss=1.2077324390411377, train_x1_loss=0.44856956601142883, train_x2_loss=0.7591634392738342\n",
      "INFO:absl:[51] val_loss=1.932841181755066\n",
      "INFO:absl:[51] test_loss=1.9912488460540771\n",
      "INFO:absl:[52] train_loss=1.2040488719940186, train_x1_loss=0.44645994901657104, train_x2_loss=0.7575883269309998\n",
      "INFO:absl:[52] val_loss=1.9271796941757202\n",
      "INFO:absl:[52] test_loss=1.9853767156600952\n",
      "INFO:absl:[53] train_loss=1.2025341987609863, train_x1_loss=0.4445015788078308, train_x2_loss=0.7580329775810242\n",
      "INFO:absl:[53] val_loss=1.9270471334457397\n",
      "INFO:absl:[53] test_loss=1.985195517539978\n",
      "INFO:absl:[54] train_loss=1.2015355825424194, train_x1_loss=0.44410842657089233, train_x2_loss=0.7574272751808167\n",
      "INFO:absl:[54] val_loss=1.9256799221038818\n",
      "INFO:absl:[54] test_loss=1.9839222431182861\n",
      "INFO:absl:[55] train_loss=1.2003241777420044, train_x1_loss=0.44312378764152527, train_x2_loss=0.7572011947631836\n",
      "INFO:absl:[55] val_loss=1.9270139932632446\n",
      "INFO:absl:[55] test_loss=1.9851858615875244\n",
      "INFO:absl:[56] train_loss=1.1975305080413818, train_x1_loss=0.4423608183860779, train_x2_loss=0.7551727294921875\n",
      "INFO:absl:[56] val_loss=1.9260625839233398\n",
      "INFO:absl:[56] test_loss=1.9840023517608643\n",
      "INFO:absl:[57] train_loss=1.2002413272857666, train_x1_loss=0.44363847374916077, train_x2_loss=0.7566034197807312\n",
      "INFO:absl:[57] val_loss=1.924914836883545\n",
      "INFO:absl:[57] test_loss=1.9829083681106567\n",
      "INFO:absl:[58] train_loss=1.1973625421524048, train_x1_loss=0.44116199016571045, train_x2_loss=0.7562023401260376\n",
      "INFO:absl:[58] val_loss=1.9275331497192383\n",
      "INFO:absl:[58] test_loss=1.9854215383529663\n",
      "INFO:absl:[59] train_loss=1.1950266361236572, train_x1_loss=0.4410460591316223, train_x2_loss=0.7539812326431274\n",
      "INFO:absl:[59] val_loss=1.9285335540771484\n",
      "INFO:absl:[59] test_loss=1.9863836765289307\n",
      "INFO:absl:Setting work unit notes: 3542.4 steps/s, 60.7% (212542/350000), ETA: 0m (1m : 0.1% checkpoint, 9.4% eval)\n",
      "INFO:absl:[212542] steps_per_sec=3542.365245\n",
      "INFO:absl:[60] train_loss=1.2022243738174438, train_x1_loss=0.44327619671821594, train_x2_loss=0.7589475512504578\n",
      "INFO:absl:[60] val_loss=1.9238821268081665\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[60] test_loss=1.9815421104431152\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[61] train_loss=1.1990103721618652, train_x1_loss=0.4420727491378784, train_x2_loss=0.7569390535354614\n",
      "INFO:absl:[61] val_loss=1.9273635149002075\n",
      "INFO:absl:[61] test_loss=1.9850324392318726\n",
      "INFO:absl:[62] train_loss=1.1937772035598755, train_x1_loss=0.4388554096221924, train_x2_loss=0.7549228072166443\n",
      "INFO:absl:[62] val_loss=1.924524188041687\n",
      "INFO:absl:[62] test_loss=1.9822361469268799\n",
      "INFO:absl:[63] train_loss=1.199864149093628, train_x1_loss=0.4428834021091461, train_x2_loss=0.7569811940193176\n",
      "INFO:absl:[63] val_loss=1.9223377704620361\n",
      "INFO:absl:[63] test_loss=1.979871392250061\n",
      "INFO:absl:[64] train_loss=1.1941194534301758, train_x1_loss=0.44005855917930603, train_x2_loss=0.7540605664253235\n",
      "INFO:absl:[64] val_loss=1.9290282726287842\n",
      "INFO:absl:[64] test_loss=1.9867064952850342\n",
      "INFO:absl:[65] train_loss=1.2010889053344727, train_x1_loss=0.44310685992240906, train_x2_loss=0.7579799294471741\n",
      "INFO:absl:[65] val_loss=1.9300105571746826\n",
      "INFO:absl:[65] test_loss=1.9874407052993774\n",
      "INFO:absl:[66] train_loss=1.1946896314620972, train_x1_loss=0.4415603578090668, train_x2_loss=0.753128707408905\n",
      "INFO:absl:[66] val_loss=1.9278602600097656\n",
      "INFO:absl:[66] test_loss=1.9853476285934448\n",
      "INFO:absl:[67] train_loss=1.1969482898712158, train_x1_loss=0.44232407212257385, train_x2_loss=0.7546214461326599\n",
      "INFO:absl:[67] val_loss=1.9229894876480103\n",
      "INFO:absl:[67] test_loss=1.9803978204727173\n",
      "INFO:absl:[68] train_loss=1.1971219778060913, train_x1_loss=0.4418509304523468, train_x2_loss=0.7552726864814758\n",
      "INFO:absl:[68] val_loss=1.925368070602417\n",
      "INFO:absl:[68] test_loss=1.9826449155807495\n",
      "INFO:absl:[69] train_loss=1.1951290369033813, train_x1_loss=0.44094017148017883, train_x2_loss=0.7541894316673279\n",
      "INFO:absl:[69] val_loss=1.9252530336380005\n",
      "INFO:absl:[69] test_loss=1.9824938774108887\n",
      "INFO:absl:[70] train_loss=1.1963251829147339, train_x1_loss=0.44404837489128113, train_x2_loss=0.7522767782211304\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[70] val_loss=1.9218297004699707\n",
      "INFO:absl:[70] test_loss=1.9789799451828003\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[71] train_loss=1.1895995140075684, train_x1_loss=0.4397803544998169, train_x2_loss=0.7498202919960022\n",
      "INFO:absl:[71] val_loss=1.9179259538650513\n",
      "INFO:absl:[71] test_loss=1.9748843908309937\n",
      "INFO:absl:[72] train_loss=1.1948716640472412, train_x1_loss=0.44004666805267334, train_x2_loss=0.7548239827156067\n",
      "INFO:absl:[72] val_loss=1.9245641231536865\n",
      "INFO:absl:[72] test_loss=1.9816794395446777\n",
      "INFO:absl:[73] train_loss=1.1923097372055054, train_x1_loss=0.43952110409736633, train_x2_loss=0.7527891397476196\n",
      "INFO:absl:[73] val_loss=1.928351879119873\n",
      "INFO:absl:[73] test_loss=1.9856749773025513\n",
      "INFO:absl:[74] train_loss=1.195409893989563, train_x1_loss=0.4402957856655121, train_x2_loss=0.7551127076148987\n",
      "INFO:absl:[74] val_loss=1.9275972843170166\n",
      "INFO:absl:[74] test_loss=1.9849950075149536\n",
      "INFO:absl:[75] train_loss=1.196869134902954, train_x1_loss=0.4396120607852936, train_x2_loss=0.7572579383850098\n",
      "INFO:absl:[75] val_loss=1.9244582653045654\n",
      "INFO:absl:[75] test_loss=1.9816298484802246\n",
      "INFO:absl:[76] train_loss=1.1879938840866089, train_x1_loss=0.4343128204345703, train_x2_loss=0.7536805868148804\n",
      "INFO:absl:[76] val_loss=1.9223331212997437\n",
      "INFO:absl:[76] test_loss=1.9793733358383179\n",
      "INFO:absl:[77] train_loss=1.1948115825653076, train_x1_loss=0.44129934906959534, train_x2_loss=0.7535141110420227\n",
      "INFO:absl:[77] val_loss=1.9216428995132446\n",
      "INFO:absl:[77] test_loss=1.9785429239273071\n",
      "INFO:absl:[78] train_loss=1.1922067403793335, train_x1_loss=0.438676655292511, train_x2_loss=0.7535293102264404\n",
      "INFO:absl:[78] val_loss=1.9196339845657349\n",
      "INFO:absl:[78] test_loss=1.9765578508377075\n",
      "INFO:absl:[79] train_loss=1.1866834163665771, train_x1_loss=0.4333838224411011, train_x2_loss=0.7532986402511597\n",
      "INFO:absl:[79] val_loss=1.918587327003479\n",
      "INFO:absl:[79] test_loss=1.9754761457443237\n",
      "INFO:absl:[80] train_loss=1.189703106880188, train_x1_loss=0.4370804727077484, train_x2_loss=0.7526232004165649\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[80] val_loss=1.9215279817581177\n",
      "INFO:absl:[80] test_loss=1.9785120487213135\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[81] train_loss=1.1890194416046143, train_x1_loss=0.4359206557273865, train_x2_loss=0.75309818983078\n",
      "INFO:absl:[81] val_loss=1.922622799873352\n",
      "INFO:absl:[81] test_loss=1.9796334505081177\n",
      "INFO:absl:[82] train_loss=1.188425064086914, train_x1_loss=0.43697816133499146, train_x2_loss=0.7514474987983704\n",
      "INFO:absl:[82] val_loss=1.9200046062469482\n",
      "INFO:absl:[82] test_loss=1.9769119024276733\n",
      "INFO:absl:[83] train_loss=1.189713716506958, train_x1_loss=0.438143789768219, train_x2_loss=0.751567542552948\n",
      "INFO:absl:[83] val_loss=1.9171366691589355\n",
      "INFO:absl:[83] test_loss=1.9740958213806152\n",
      "INFO:absl:[84] train_loss=1.1883723735809326, train_x1_loss=0.43630895018577576, train_x2_loss=0.7520617842674255\n",
      "INFO:absl:[84] val_loss=1.921653151512146\n",
      "INFO:absl:[84] test_loss=1.9785404205322266\n",
      "INFO:absl:[85] train_loss=1.1908022165298462, train_x1_loss=0.43589580059051514, train_x2_loss=0.7549058198928833\n",
      "INFO:absl:[85] val_loss=1.9174835681915283\n",
      "INFO:absl:[85] test_loss=1.974625587463379\n",
      "INFO:absl:[86] train_loss=1.1895817518234253, train_x1_loss=0.43761470913887024, train_x2_loss=0.7519663572311401\n",
      "INFO:absl:[86] val_loss=1.9206126928329468\n",
      "INFO:absl:[86] test_loss=1.9775071144104004\n",
      "INFO:absl:[87] train_loss=1.1885193586349487, train_x1_loss=0.43622705340385437, train_x2_loss=0.7522936463356018\n",
      "INFO:absl:[87] val_loss=1.9184600114822388\n",
      "INFO:absl:[87] test_loss=1.9752638339996338\n",
      "INFO:absl:[88] train_loss=1.185954213142395, train_x1_loss=0.43255630135536194, train_x2_loss=0.7533973455429077\n",
      "INFO:absl:[88] val_loss=1.9179954528808594\n",
      "INFO:absl:[88] test_loss=1.9748563766479492\n",
      "INFO:absl:[89] train_loss=1.18876051902771, train_x1_loss=0.43533289432525635, train_x2_loss=0.7534283399581909\n",
      "INFO:absl:[89] val_loss=1.9208241701126099\n",
      "INFO:absl:[89] test_loss=1.9777582883834839\n",
      "INFO:absl:[90] train_loss=1.1910951137542725, train_x1_loss=0.4372914135456085, train_x2_loss=0.7538039088249207\n",
      "INFO:absl:[90] val_loss=1.9204663038253784\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[90] test_loss=1.977514386177063\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[91] train_loss=1.1885782480239868, train_x1_loss=0.43634775280952454, train_x2_loss=0.7522315979003906\n",
      "INFO:absl:[91] val_loss=1.9165782928466797\n",
      "INFO:absl:[91] test_loss=1.9732609987258911\n",
      "INFO:absl:[92] train_loss=1.187251091003418, train_x1_loss=0.43737635016441345, train_x2_loss=0.7498766183853149\n",
      "INFO:absl:[92] val_loss=1.914986252784729\n",
      "INFO:absl:[92] test_loss=1.9714555740356445\n",
      "INFO:absl:[93] train_loss=1.1914013624191284, train_x1_loss=0.4389324486255646, train_x2_loss=0.7524696588516235\n",
      "INFO:absl:[93] val_loss=1.9182450771331787\n",
      "INFO:absl:[93] test_loss=1.975028395652771\n",
      "INFO:absl:[94] train_loss=1.1871269941329956, train_x1_loss=0.43545210361480713, train_x2_loss=0.7516749501228333\n",
      "INFO:absl:[94] val_loss=1.9195679426193237\n",
      "INFO:absl:[94] test_loss=1.976362705230713\n",
      "INFO:absl:[95] train_loss=1.186620831489563, train_x1_loss=0.43383917212486267, train_x2_loss=0.7527810335159302\n",
      "INFO:absl:[95] val_loss=1.9182151556015015\n",
      "INFO:absl:[95] test_loss=1.9750287532806396\n",
      "INFO:absl:[96] train_loss=1.1909165382385254, train_x1_loss=0.4363555610179901, train_x2_loss=0.7545590400695801\n",
      "INFO:absl:[96] val_loss=1.9226536750793457\n",
      "INFO:absl:[96] test_loss=1.9796545505523682\n",
      "INFO:absl:[97] train_loss=1.1909984350204468, train_x1_loss=0.43559032678604126, train_x2_loss=0.755407989025116\n",
      "INFO:absl:[97] val_loss=1.9187426567077637\n",
      "INFO:absl:[97] test_loss=1.9754703044891357\n",
      "INFO:absl:[98] train_loss=1.188287377357483, train_x1_loss=0.43748629093170166, train_x2_loss=0.7508000135421753\n",
      "INFO:absl:[98] val_loss=1.918232798576355\n",
      "INFO:absl:[98] test_loss=1.9749114513397217\n",
      "INFO:absl:[99] train_loss=1.1890449523925781, train_x1_loss=0.43338510394096375, train_x2_loss=0.7556607127189636\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[99] val_loss=1.9175268411636353\n",
      "INFO:absl:[99] test_loss=1.9740948677062988\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'elu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 10, 'dropout_rate': 0.21813956053366504, 'edge_features': (4, 2), 'epochs': 100, 'eval_every_epochs': 1, 'fully_connected_edges': 3, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 1, 'layer_norm': False, 'learning_rate': 3.873531888351175e-05, 'log_every_epochs': 1, 'max_checkpts_to_keep': 2, 'model': 'MLPGraphNetwork', 'n_blocks': 1, 'n_samples': 5000, 'node_features': (4, 2), 'normalize': True, 'optimizer': 'adam', 'output_delay': 0, 'output_steps': 4, 'sample_buffer': -4, 'seed': 72, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 120, 'timestep_duration': 3, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+--------+------+---------+-------+\n",
      "| Name                                   | Shape  | Size | Mean    | Std   |\n",
      "+----------------------------------------+--------+------+---------+-------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (4,)   | 4    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 4) | 24   | -0.0819 | 0.36  |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (2,)   | 2    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (4, 2) | 8    | 0.289   | 0.469 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (4,)   | 4    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (7, 4) | 28   | -0.0395 | 0.333 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)   | 2    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (4, 2) | 8    | -0.135  | 0.542 |\n",
      "+----------------------------------------+--------+------+---------+-------+\n",
      "Total: 80\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Checkpoint None does not exist.\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.00s.\n",
      "INFO:absl:Starting training.\n",
      "INFO:absl:[0] train_loss=3.1295249462127686, train_x1_loss=1.7693623304367065, train_x2_loss=1.3601605892181396\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[0] val_loss=3.135202646255493\n",
      "INFO:absl:[0] test_loss=3.1130740642547607\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[1] train_loss=2.1693243980407715, train_x1_loss=1.1249666213989258, train_x2_loss=1.0443575382232666\n",
      "INFO:absl:[1] val_loss=2.675877094268799\n",
      "INFO:absl:[1] test_loss=2.6548798084259033\n",
      "INFO:absl:Created artifact [10] Profile of type ArtifactType.URL and value None.\n",
      "INFO:absl:[2] train_loss=1.7268121242523193, train_x1_loss=0.8066549301147461, train_x2_loss=0.9201573729515076\n",
      "INFO:absl:[2] val_loss=2.318148374557495\n",
      "INFO:absl:[2] test_loss=2.31087064743042\n",
      "INFO:absl:[3] train_loss=1.473580241203308, train_x1_loss=0.5923634767532349, train_x2_loss=0.8812171220779419\n",
      "INFO:absl:[3] val_loss=2.2282330989837646\n",
      "INFO:absl:[3] test_loss=2.2272253036499023\n",
      "INFO:absl:[4] train_loss=1.3715298175811768, train_x1_loss=0.5173124074935913, train_x2_loss=0.8542169332504272\n",
      "INFO:absl:[4] val_loss=2.166826009750366\n",
      "INFO:absl:[4] test_loss=2.164896249771118\n",
      "INFO:absl:[5] train_loss=1.3168445825576782, train_x1_loss=0.48949331045150757, train_x2_loss=0.8273510932922363\n",
      "INFO:absl:[5] val_loss=2.0981147289276123\n",
      "INFO:absl:[5] test_loss=2.092979907989502\n",
      "INFO:absl:[6] train_loss=1.2766637802124023, train_x1_loss=0.47421327233314514, train_x2_loss=0.8024489879608154\n",
      "INFO:absl:[6] val_loss=2.0713741779327393\n",
      "INFO:absl:[6] test_loss=2.0622482299804688\n",
      "INFO:absl:[7] train_loss=1.2492003440856934, train_x1_loss=0.46484100818634033, train_x2_loss=0.7843577265739441\n",
      "INFO:absl:[7] val_loss=2.045609951019287\n",
      "INFO:absl:[7] test_loss=2.0325934886932373\n",
      "INFO:absl:[8] train_loss=1.2342150211334229, train_x1_loss=0.45898544788360596, train_x2_loss=0.7752285003662109\n",
      "INFO:absl:[8] val_loss=2.0330111980438232\n",
      "INFO:absl:[8] test_loss=2.017683982849121\n",
      "INFO:absl:[9] train_loss=1.225043535232544, train_x1_loss=0.45684030652046204, train_x2_loss=0.7682023048400879\n",
      "INFO:absl:[9] val_loss=2.0281355381011963\n",
      "INFO:absl:[9] test_loss=2.0111136436462402\n",
      "INFO:absl:[10] train_loss=1.2223055362701416, train_x1_loss=0.457093745470047, train_x2_loss=0.7652109861373901\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[10] val_loss=2.0218122005462646\n",
      "INFO:absl:[10] test_loss=2.0038928985595703\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[11] train_loss=1.2169737815856934, train_x1_loss=0.45448803901672363, train_x2_loss=0.7624857425689697\n",
      "INFO:absl:[11] val_loss=2.015542507171631\n",
      "INFO:absl:[11] test_loss=1.9968688488006592\n",
      "INFO:absl:[12] train_loss=1.2127227783203125, train_x1_loss=0.4519832730293274, train_x2_loss=0.760741114616394\n",
      "INFO:absl:[12] val_loss=2.018763542175293\n",
      "INFO:absl:[12] test_loss=1.9994930028915405\n",
      "INFO:absl:[13] train_loss=1.208221435546875, train_x1_loss=0.45064443349838257, train_x2_loss=0.7575756311416626\n",
      "INFO:absl:[13] val_loss=2.014017105102539\n",
      "INFO:absl:[13] test_loss=1.9943403005599976\n",
      "INFO:absl:[14] train_loss=1.2118937969207764, train_x1_loss=0.4522584080696106, train_x2_loss=0.7596356868743896\n",
      "INFO:absl:[14] val_loss=2.0175998210906982\n",
      "INFO:absl:[14] test_loss=1.997805118560791\n",
      "INFO:absl:[15] train_loss=1.2112855911254883, train_x1_loss=0.45521220564842224, train_x2_loss=0.7560723423957825\n",
      "INFO:absl:[15] val_loss=2.018547534942627\n",
      "INFO:absl:[15] test_loss=1.9984959363937378\n",
      "INFO:absl:[16] train_loss=1.211832046508789, train_x1_loss=0.456025630235672, train_x2_loss=0.7558060884475708\n",
      "INFO:absl:[16] val_loss=2.0178611278533936\n",
      "INFO:absl:[16] test_loss=1.9975383281707764\n",
      "INFO:absl:[17] train_loss=1.208437204360962, train_x1_loss=0.45259106159210205, train_x2_loss=0.7558451890945435\n",
      "INFO:absl:[17] val_loss=2.0154974460601807\n",
      "INFO:absl:[17] test_loss=1.9943126440048218\n",
      "INFO:absl:[18] train_loss=1.2072269916534424, train_x1_loss=0.4537620544433594, train_x2_loss=0.7534643411636353\n",
      "INFO:absl:[18] val_loss=2.0141489505767822\n",
      "INFO:absl:[18] test_loss=1.9933850765228271\n",
      "INFO:absl:[19] train_loss=1.2102359533309937, train_x1_loss=0.4538240432739258, train_x2_loss=0.7564111351966858\n",
      "INFO:absl:[19] val_loss=2.018080234527588\n",
      "INFO:absl:[19] test_loss=1.9974620342254639\n",
      "INFO:absl:[20] train_loss=1.207990288734436, train_x1_loss=0.45328494906425476, train_x2_loss=0.7547063827514648\n",
      "INFO:absl:[20] val_loss=2.0174224376678467\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[20] test_loss=1.996415615081787\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[21] train_loss=1.2040952444076538, train_x1_loss=0.451579749584198, train_x2_loss=0.7525129914283752\n",
      "INFO:absl:[21] val_loss=2.0169575214385986\n",
      "INFO:absl:[21] test_loss=1.9956327676773071\n",
      "INFO:absl:[22] train_loss=1.206135630607605, train_x1_loss=0.4527658224105835, train_x2_loss=0.7533715963363647\n",
      "INFO:absl:[22] val_loss=2.0194785594940186\n",
      "INFO:absl:[22] test_loss=1.9980779886245728\n",
      "INFO:absl:[23] train_loss=1.2030162811279297, train_x1_loss=0.44981884956359863, train_x2_loss=0.7531983852386475\n",
      "INFO:absl:[23] val_loss=2.0177173614501953\n",
      "INFO:absl:[23] test_loss=1.9964491128921509\n",
      "INFO:absl:[24] train_loss=1.2063764333724976, train_x1_loss=0.4518262445926666, train_x2_loss=0.7545506954193115\n",
      "INFO:absl:[24] val_loss=2.017693519592285\n",
      "INFO:absl:[24] test_loss=1.9964909553527832\n",
      "INFO:absl:[25] train_loss=1.2039180994033813, train_x1_loss=0.45342016220092773, train_x2_loss=0.7504973411560059\n",
      "INFO:absl:[25] val_loss=2.0186986923217773\n",
      "INFO:absl:[25] test_loss=1.997269630432129\n",
      "INFO:absl:[26] train_loss=1.2060052156448364, train_x1_loss=0.45237863063812256, train_x2_loss=0.753627598285675\n",
      "INFO:absl:[26] val_loss=2.02103590965271\n",
      "INFO:absl:[26] test_loss=1.999746322631836\n",
      "INFO:absl:[27] train_loss=1.2037264108657837, train_x1_loss=0.4503626823425293, train_x2_loss=0.7533647418022156\n",
      "INFO:absl:[27] val_loss=2.0206665992736816\n",
      "INFO:absl:[27] test_loss=1.9991543292999268\n",
      "INFO:absl:[28] train_loss=1.2022314071655273, train_x1_loss=0.4506080448627472, train_x2_loss=0.7516226172447205\n",
      "INFO:absl:[28] val_loss=2.016282558441162\n",
      "INFO:absl:[28] test_loss=1.9949907064437866\n",
      "INFO:absl:[29] train_loss=1.206000566482544, train_x1_loss=0.45339131355285645, train_x2_loss=0.7526097297668457\n",
      "INFO:absl:[29] val_loss=2.0210037231445312\n",
      "INFO:absl:[29] test_loss=1.9997142553329468\n",
      "INFO:absl:[30] train_loss=1.1985896825790405, train_x1_loss=0.45094600319862366, train_x2_loss=0.7476420998573303\n",
      "INFO:absl:[30] val_loss=2.0138094425201416\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[30] test_loss=1.992181420326233\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[31] train_loss=1.2055786848068237, train_x1_loss=0.45233842730522156, train_x2_loss=0.7532409429550171\n",
      "INFO:absl:[31] val_loss=2.0147533416748047\n",
      "INFO:absl:[31] test_loss=1.9932126998901367\n",
      "INFO:absl:[32] train_loss=1.2003271579742432, train_x1_loss=0.4497028589248657, train_x2_loss=0.7506257891654968\n",
      "INFO:absl:[32] val_loss=2.0208899974823\n",
      "INFO:absl:[32] test_loss=1.9991586208343506\n",
      "INFO:absl:[33] train_loss=1.203184723854065, train_x1_loss=0.4493408799171448, train_x2_loss=0.7538433074951172\n",
      "INFO:absl:[33] val_loss=2.018054723739624\n",
      "INFO:absl:[33] test_loss=1.9961154460906982\n",
      "INFO:absl:[34] train_loss=1.1970264911651611, train_x1_loss=0.4476441442966461, train_x2_loss=0.7493811845779419\n",
      "INFO:absl:[34] val_loss=2.0123510360717773\n",
      "INFO:absl:[34] test_loss=1.990571141242981\n",
      "INFO:absl:[35] train_loss=1.197688341140747, train_x1_loss=0.4460165202617645, train_x2_loss=0.7516719698905945\n",
      "INFO:absl:[35] val_loss=2.014810800552368\n",
      "INFO:absl:[35] test_loss=1.9925110340118408\n",
      "INFO:absl:[36] train_loss=1.1961129903793335, train_x1_loss=0.44597193598747253, train_x2_loss=0.7501397728919983\n",
      "INFO:absl:[36] val_loss=2.0095102787017822\n",
      "INFO:absl:[36] test_loss=1.9875049591064453\n",
      "INFO:absl:[37] train_loss=1.1973448991775513, train_x1_loss=0.447528600692749, train_x2_loss=0.7498164772987366\n",
      "INFO:absl:[37] val_loss=2.0140514373779297\n",
      "INFO:absl:[37] test_loss=1.991869330406189\n",
      "INFO:absl:[38] train_loss=1.1988123655319214, train_x1_loss=0.4473169147968292, train_x2_loss=0.751494288444519\n",
      "INFO:absl:[38] val_loss=2.019195795059204\n",
      "INFO:absl:[38] test_loss=1.9968401193618774\n",
      "INFO:absl:[39] train_loss=1.1965906620025635, train_x1_loss=0.44664466381073, train_x2_loss=0.7499476075172424\n",
      "INFO:absl:[39] val_loss=2.013709545135498\n",
      "INFO:absl:[39] test_loss=1.9913758039474487\n",
      "INFO:absl:[40] train_loss=1.192198634147644, train_x1_loss=0.4431304633617401, train_x2_loss=0.7490674257278442\n",
      "INFO:absl:[40] val_loss=2.014996290206909\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[40] test_loss=1.9924269914627075\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[41] train_loss=1.1924145221710205, train_x1_loss=0.44499894976615906, train_x2_loss=0.7474163770675659\n",
      "INFO:absl:[41] val_loss=2.010754346847534\n",
      "INFO:absl:[41] test_loss=1.9876651763916016\n",
      "INFO:absl:[42] train_loss=1.194761037826538, train_x1_loss=0.44519519805908203, train_x2_loss=0.749567449092865\n",
      "INFO:absl:[42] val_loss=2.0121278762817383\n",
      "INFO:absl:[42] test_loss=1.9891501665115356\n",
      "INFO:absl:[43] train_loss=1.1931062936782837, train_x1_loss=0.4431498050689697, train_x2_loss=0.7499586939811707\n",
      "INFO:absl:[43] val_loss=2.0113141536712646\n",
      "INFO:absl:[43] test_loss=1.988771915435791\n",
      "INFO:absl:[44] train_loss=1.19821035861969, train_x1_loss=0.44573959708213806, train_x2_loss=0.7524703741073608\n",
      "INFO:absl:[44] val_loss=2.0138697624206543\n",
      "INFO:absl:[44] test_loss=1.9908909797668457\n",
      "INFO:absl:[45] train_loss=1.1924694776535034, train_x1_loss=0.4422018826007843, train_x2_loss=0.7502658367156982\n",
      "INFO:absl:[45] val_loss=2.013960599899292\n",
      "INFO:absl:[45] test_loss=1.991439938545227\n",
      "INFO:absl:[46] train_loss=1.190172553062439, train_x1_loss=0.44237685203552246, train_x2_loss=0.747796893119812\n",
      "INFO:absl:[46] val_loss=2.0111324787139893\n",
      "INFO:absl:[46] test_loss=1.987905740737915\n",
      "INFO:absl:[47] train_loss=1.1926827430725098, train_x1_loss=0.4431094229221344, train_x2_loss=0.7495743632316589\n",
      "INFO:absl:[47] val_loss=2.0095231533050537\n",
      "INFO:absl:[47] test_loss=1.9858026504516602\n",
      "INFO:absl:[48] train_loss=1.1902759075164795, train_x1_loss=0.4429638683795929, train_x2_loss=0.7473132014274597\n",
      "INFO:absl:[48] val_loss=2.0099613666534424\n",
      "INFO:absl:[48] test_loss=1.9865078926086426\n",
      "INFO:absl:[49] train_loss=1.189488172531128, train_x1_loss=0.43952155113220215, train_x2_loss=0.7499664425849915\n",
      "INFO:absl:[49] val_loss=2.005906105041504\n",
      "INFO:absl:[49] test_loss=1.9824894666671753\n",
      "INFO:absl:[50] train_loss=1.1892129182815552, train_x1_loss=0.44142627716064453, train_x2_loss=0.7477850914001465\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[50] val_loss=2.0105814933776855\n",
      "INFO:absl:[50] test_loss=1.9871985912322998\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[51] train_loss=1.1861974000930786, train_x1_loss=0.43916964530944824, train_x2_loss=0.7470292448997498\n",
      "INFO:absl:[51] val_loss=2.0082638263702393\n",
      "INFO:absl:[51] test_loss=1.9848729372024536\n",
      "INFO:absl:[52] train_loss=1.1952118873596191, train_x1_loss=0.44553396105766296, train_x2_loss=0.7496783137321472\n",
      "INFO:absl:[52] val_loss=2.009305477142334\n",
      "INFO:absl:[52] test_loss=1.9860349893569946\n",
      "INFO:absl:[53] train_loss=1.188766598701477, train_x1_loss=0.4424521028995514, train_x2_loss=0.7463148236274719\n",
      "INFO:absl:[53] val_loss=2.005992889404297\n",
      "INFO:absl:[53] test_loss=1.9827091693878174\n",
      "INFO:absl:[54] train_loss=1.1875165700912476, train_x1_loss=0.43883195519447327, train_x2_loss=0.7486823201179504\n",
      "INFO:absl:[54] val_loss=2.0033721923828125\n",
      "INFO:absl:[54] test_loss=1.979955792427063\n",
      "INFO:absl:[55] train_loss=1.1853984594345093, train_x1_loss=0.43797335028648376, train_x2_loss=0.7474251985549927\n",
      "INFO:absl:[55] val_loss=2.0097806453704834\n",
      "INFO:absl:[55] test_loss=1.9860234260559082\n",
      "INFO:absl:[56] train_loss=1.187584400177002, train_x1_loss=0.441514253616333, train_x2_loss=0.746069073677063\n",
      "INFO:absl:[56] val_loss=2.009720802307129\n",
      "INFO:absl:[56] test_loss=1.9858235120773315\n",
      "INFO:absl:[57] train_loss=1.186647653579712, train_x1_loss=0.43957462906837463, train_x2_loss=0.7470716238021851\n",
      "INFO:absl:[57] val_loss=2.0076749324798584\n",
      "INFO:absl:[57] test_loss=1.984018087387085\n",
      "INFO:absl:[58] train_loss=1.1869171857833862, train_x1_loss=0.439754843711853, train_x2_loss=0.7471620440483093\n",
      "INFO:absl:[58] val_loss=2.0090105533599854\n",
      "INFO:absl:[58] test_loss=1.9855360984802246\n",
      "INFO:absl:[59] train_loss=1.1830593347549438, train_x1_loss=0.4372895061969757, train_x2_loss=0.7457719445228577\n",
      "INFO:absl:[59] val_loss=2.00462007522583\n",
      "INFO:absl:[59] test_loss=1.9806206226348877\n",
      "INFO:absl:[60] train_loss=1.1860885620117188, train_x1_loss=0.438467413187027, train_x2_loss=0.7476204037666321\n",
      "INFO:absl:[60] val_loss=2.006456136703491\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[60] test_loss=1.9824713468551636\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Setting work unit notes: 3594.3 steps/s, 61.6% (215656/350000), ETA: 0m (1m : 0.1% checkpoint, 12.8% eval)\n",
      "INFO:absl:[215656] steps_per_sec=3594.265710\n",
      "INFO:absl:[61] train_loss=1.1845672130584717, train_x1_loss=0.4379028081893921, train_x2_loss=0.7466644644737244\n",
      "INFO:absl:[61] val_loss=2.0043399333953857\n",
      "INFO:absl:[61] test_loss=1.9803320169448853\n",
      "INFO:absl:[62] train_loss=1.1854115724563599, train_x1_loss=0.4418962001800537, train_x2_loss=0.7435160279273987\n",
      "INFO:absl:[62] val_loss=2.004978656768799\n",
      "INFO:absl:[62] test_loss=1.9805620908737183\n",
      "INFO:absl:[63] train_loss=1.1853135824203491, train_x1_loss=0.44083917140960693, train_x2_loss=0.7444729208946228\n",
      "INFO:absl:[63] val_loss=2.007481575012207\n",
      "INFO:absl:[63] test_loss=1.9834104776382446\n",
      "INFO:absl:[64] train_loss=1.1855289936065674, train_x1_loss=0.43979865312576294, train_x2_loss=0.7457275986671448\n",
      "INFO:absl:[64] val_loss=2.0048131942749023\n",
      "INFO:absl:[64] test_loss=1.9804863929748535\n",
      "INFO:absl:[65] train_loss=1.185808539390564, train_x1_loss=0.43971168994903564, train_x2_loss=0.7460944652557373\n",
      "INFO:absl:[65] val_loss=2.0047953128814697\n",
      "INFO:absl:[65] test_loss=1.9803295135498047\n",
      "INFO:absl:[66] train_loss=1.185133695602417, train_x1_loss=0.4392252266407013, train_x2_loss=0.7459080219268799\n",
      "INFO:absl:[66] val_loss=2.005573272705078\n",
      "INFO:absl:[66] test_loss=1.9812023639678955\n",
      "INFO:absl:[67] train_loss=1.1855449676513672, train_x1_loss=0.4379131495952606, train_x2_loss=0.7476305961608887\n",
      "INFO:absl:[67] val_loss=2.0033652782440186\n",
      "INFO:absl:[67] test_loss=1.9791185855865479\n",
      "INFO:absl:[68] train_loss=1.1832020282745361, train_x1_loss=0.43753671646118164, train_x2_loss=0.7456652522087097\n",
      "INFO:absl:[68] val_loss=2.0061910152435303\n",
      "INFO:absl:[68] test_loss=1.98227858543396\n",
      "INFO:absl:[69] train_loss=1.1811729669570923, train_x1_loss=0.4368061423301697, train_x2_loss=0.7443649768829346\n",
      "INFO:absl:[69] val_loss=2.0035760402679443\n",
      "INFO:absl:[69] test_loss=1.9788984060287476\n",
      "INFO:absl:[70] train_loss=1.1832950115203857, train_x1_loss=0.4395741820335388, train_x2_loss=0.7437231540679932\n",
      "INFO:absl:[70] val_loss=2.0031824111938477\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[70] test_loss=1.9788413047790527\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[71] train_loss=1.1831464767456055, train_x1_loss=0.43811818957328796, train_x2_loss=0.7450265288352966\n",
      "INFO:absl:[71] val_loss=2.000852584838867\n",
      "INFO:absl:[71] test_loss=1.9764461517333984\n",
      "INFO:absl:[72] train_loss=1.1829243898391724, train_x1_loss=0.43627023696899414, train_x2_loss=0.7466527819633484\n",
      "INFO:absl:[72] val_loss=2.000115394592285\n",
      "INFO:absl:[72] test_loss=1.9753882884979248\n",
      "INFO:absl:[73] train_loss=1.1803616285324097, train_x1_loss=0.4355897307395935, train_x2_loss=0.7447735667228699\n",
      "INFO:absl:[73] val_loss=2.0028369426727295\n",
      "INFO:absl:[73] test_loss=1.978129267692566\n",
      "INFO:absl:[74] train_loss=1.180625557899475, train_x1_loss=0.43754473328590393, train_x2_loss=0.7430814504623413\n",
      "INFO:absl:[74] val_loss=2.000537633895874\n",
      "INFO:absl:[74] test_loss=1.9757391214370728\n",
      "INFO:absl:[75] train_loss=1.182449460029602, train_x1_loss=0.43566733598709106, train_x2_loss=0.7467827796936035\n",
      "INFO:absl:[75] val_loss=2.0045347213745117\n",
      "INFO:absl:[75] test_loss=1.9795687198638916\n",
      "INFO:absl:[76] train_loss=1.1819947957992554, train_x1_loss=0.4356665313243866, train_x2_loss=0.7463276386260986\n",
      "INFO:absl:[76] val_loss=2.005854606628418\n",
      "INFO:absl:[76] test_loss=1.9814385175704956\n",
      "INFO:absl:[77] train_loss=1.185184121131897, train_x1_loss=0.44064968824386597, train_x2_loss=0.744535505771637\n",
      "INFO:absl:[77] val_loss=2.003159523010254\n",
      "INFO:absl:[77] test_loss=1.9784080982208252\n",
      "INFO:absl:[78] train_loss=1.180023193359375, train_x1_loss=0.43486660718917847, train_x2_loss=0.7451584935188293\n",
      "INFO:absl:[78] val_loss=2.005681037902832\n",
      "INFO:absl:[78] test_loss=1.9807744026184082\n",
      "INFO:absl:[79] train_loss=1.1810239553451538, train_x1_loss=0.4375320076942444, train_x2_loss=0.7434912919998169\n",
      "INFO:absl:[79] val_loss=2.001481056213379\n",
      "INFO:absl:[79] test_loss=1.9765340089797974\n",
      "INFO:absl:[80] train_loss=1.1763745546340942, train_x1_loss=0.43413493037223816, train_x2_loss=0.7422384023666382\n",
      "INFO:absl:[80] val_loss=1.9985753297805786\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[80] test_loss=1.9735829830169678\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[81] train_loss=1.1794111728668213, train_x1_loss=0.43383973836898804, train_x2_loss=0.7455727458000183\n",
      "INFO:absl:[81] val_loss=2.003800392150879\n",
      "INFO:absl:[81] test_loss=1.9787278175354004\n",
      "INFO:absl:[82] train_loss=1.1801964044570923, train_x1_loss=0.4358144998550415, train_x2_loss=0.744381844997406\n",
      "INFO:absl:[82] val_loss=2.0039992332458496\n",
      "INFO:absl:[82] test_loss=1.9792118072509766\n",
      "INFO:absl:[83] train_loss=1.1815977096557617, train_x1_loss=0.4377008378505707, train_x2_loss=0.743896484375\n",
      "INFO:absl:[83] val_loss=2.00189471244812\n",
      "INFO:absl:[83] test_loss=1.976994514465332\n",
      "INFO:absl:[84] train_loss=1.1758441925048828, train_x1_loss=0.4326891005039215, train_x2_loss=0.7431536912918091\n",
      "INFO:absl:[84] val_loss=2.0064234733581543\n",
      "INFO:absl:[84] test_loss=1.9817448854446411\n",
      "INFO:absl:[85] train_loss=1.180264949798584, train_x1_loss=0.4350126385688782, train_x2_loss=0.7452520728111267\n",
      "INFO:absl:[85] val_loss=2.000966787338257\n",
      "INFO:absl:[85] test_loss=1.9755228757858276\n",
      "INFO:absl:[86] train_loss=1.180841326713562, train_x1_loss=0.43608197569847107, train_x2_loss=0.7447587847709656\n",
      "INFO:absl:[86] val_loss=1.9985231161117554\n",
      "INFO:absl:[86] test_loss=1.9732547998428345\n",
      "INFO:absl:[87] train_loss=1.17917799949646, train_x1_loss=0.43702366948127747, train_x2_loss=0.7421533465385437\n",
      "INFO:absl:[87] val_loss=2.0012600421905518\n",
      "INFO:absl:[87] test_loss=1.9760562181472778\n",
      "INFO:absl:[88] train_loss=1.1803348064422607, train_x1_loss=0.43584781885147095, train_x2_loss=0.7444884181022644\n",
      "INFO:absl:[88] val_loss=2.0066168308258057\n",
      "INFO:absl:[88] test_loss=1.9813028573989868\n",
      "INFO:absl:[89] train_loss=1.1768596172332764, train_x1_loss=0.4323064088821411, train_x2_loss=0.7445535063743591\n",
      "INFO:absl:[89] val_loss=2.00431752204895\n",
      "INFO:absl:[89] test_loss=1.9792728424072266\n",
      "INFO:absl:[90] train_loss=1.1739168167114258, train_x1_loss=0.43089959025382996, train_x2_loss=0.7430167198181152\n",
      "INFO:absl:[90] val_loss=2.000626802444458\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[90] test_loss=1.9754743576049805\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[91] train_loss=1.178710699081421, train_x1_loss=0.4351343810558319, train_x2_loss=0.7435750365257263\n",
      "INFO:absl:[91] val_loss=2.000128984451294\n",
      "INFO:absl:[91] test_loss=1.9746181964874268\n",
      "INFO:absl:[92] train_loss=1.1781506538391113, train_x1_loss=0.43444064259529114, train_x2_loss=0.7437102198600769\n",
      "INFO:absl:[92] val_loss=1.9973719120025635\n",
      "INFO:absl:[92] test_loss=1.9720113277435303\n",
      "INFO:absl:[93] train_loss=1.1786295175552368, train_x1_loss=0.4334416389465332, train_x2_loss=0.7451890110969543\n",
      "INFO:absl:[93] val_loss=2.003718137741089\n",
      "INFO:absl:[93] test_loss=1.9784489870071411\n",
      "INFO:absl:[94] train_loss=1.1774749755859375, train_x1_loss=0.4328004717826843, train_x2_loss=0.7446736693382263\n",
      "INFO:absl:[94] val_loss=2.0019986629486084\n",
      "INFO:absl:[94] test_loss=1.9765812158584595\n",
      "INFO:absl:[95] train_loss=1.1774122714996338, train_x1_loss=0.43404901027679443, train_x2_loss=0.7433605194091797\n",
      "INFO:absl:[95] val_loss=2.0032453536987305\n",
      "INFO:absl:[95] test_loss=1.9775999784469604\n",
      "INFO:absl:[96] train_loss=1.1755015850067139, train_x1_loss=0.4344376027584076, train_x2_loss=0.7410609722137451\n",
      "INFO:absl:[96] val_loss=1.9983904361724854\n",
      "INFO:absl:[96] test_loss=1.9726024866104126\n",
      "INFO:absl:[97] train_loss=1.174729824066162, train_x1_loss=0.43113476037979126, train_x2_loss=0.7435945868492126\n",
      "INFO:absl:[97] val_loss=2.0023462772369385\n",
      "INFO:absl:[97] test_loss=1.9769266843795776\n",
      "INFO:absl:[98] train_loss=1.1784847974777222, train_x1_loss=0.4351307451725006, train_x2_loss=0.7433538436889648\n",
      "INFO:absl:[98] val_loss=1.9981693029403687\n",
      "INFO:absl:[98] test_loss=1.9724589586257935\n",
      "INFO:absl:[99] train_loss=1.172736644744873, train_x1_loss=0.4300525188446045, train_x2_loss=0.7426838278770447\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[99] val_loss=1.9945276975631714\n",
      "INFO:absl:[99] test_loss=1.9687139987945557\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'elu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 10, 'dropout_rate': 0.21813956053366504, 'edge_features': (4, 2), 'epochs': 100, 'eval_every_epochs': 1, 'fully_connected_edges': 3, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 1, 'layer_norm': False, 'learning_rate': 3.873531888351175e-05, 'log_every_epochs': 1, 'max_checkpts_to_keep': 2, 'model': 'MLPGraphNetwork', 'n_blocks': 1, 'n_samples': 5000, 'node_features': (4, 2), 'normalize': True, 'optimizer': 'adam', 'output_delay': 0, 'output_steps': 4, 'sample_buffer': -4, 'seed': 73, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 120, 'timestep_duration': 3, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+--------+------+---------+-------+\n",
      "| Name                                   | Shape  | Size | Mean    | Std   |\n",
      "+----------------------------------------+--------+------+---------+-------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (4,)   | 4    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 4) | 24   | -0.0819 | 0.36  |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (2,)   | 2    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (4, 2) | 8    | 0.289   | 0.469 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (4,)   | 4    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (7, 4) | 28   | -0.0395 | 0.333 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)   | 2    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (4, 2) | 8    | -0.135  | 0.542 |\n",
      "+----------------------------------------+--------+------+---------+-------+\n",
      "Total: 80\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Checkpoint None does not exist.\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.00s.\n",
      "INFO:absl:Starting training.\n",
      "INFO:absl:[0] train_loss=3.1118125915527344, train_x1_loss=1.756775975227356, train_x2_loss=1.3550400733947754\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[0] val_loss=2.9057328701019287\n",
      "INFO:absl:[0] test_loss=3.0516040325164795\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Created artifact [10] Profile of type ArtifactType.URL and value None.\n",
      "INFO:absl:[1] train_loss=2.1301355361938477, train_x1_loss=1.0934369564056396, train_x2_loss=1.0367003679275513\n",
      "INFO:absl:[1] val_loss=2.4214165210723877\n",
      "INFO:absl:[1] test_loss=2.554504156112671\n",
      "INFO:absl:[2] train_loss=1.6811598539352417, train_x1_loss=0.7648541331291199, train_x2_loss=0.9163092374801636\n",
      "INFO:absl:[2] val_loss=2.110304355621338\n",
      "INFO:absl:[2] test_loss=2.227550983428955\n",
      "INFO:absl:[3] train_loss=1.4631959199905396, train_x1_loss=0.581859290599823, train_x2_loss=0.8813345432281494\n",
      "INFO:absl:[3] val_loss=2.0390913486480713\n",
      "INFO:absl:[3] test_loss=2.1505415439605713\n",
      "INFO:absl:[4] train_loss=1.368463397026062, train_x1_loss=0.5138434171676636, train_x2_loss=0.8546220660209656\n",
      "INFO:absl:[4] val_loss=1.9871121644973755\n",
      "INFO:absl:[4] test_loss=2.097642183303833\n",
      "INFO:absl:[5] train_loss=1.3197840452194214, train_x1_loss=0.4909402132034302, train_x2_loss=0.8288440704345703\n",
      "INFO:absl:[5] val_loss=1.9317585229873657\n",
      "INFO:absl:[5] test_loss=2.0414485931396484\n",
      "INFO:absl:[6] train_loss=1.272358775138855, train_x1_loss=0.4680207371711731, train_x2_loss=0.8043373823165894\n",
      "INFO:absl:[6] val_loss=1.9016368389129639\n",
      "INFO:absl:[6] test_loss=2.0106041431427\n",
      "INFO:absl:[7] train_loss=1.2541790008544922, train_x1_loss=0.4648260176181793, train_x2_loss=0.7893508672714233\n",
      "INFO:absl:[7] val_loss=1.8830713033676147\n",
      "INFO:absl:[7] test_loss=1.9912290573120117\n",
      "INFO:absl:[8] train_loss=1.2414660453796387, train_x1_loss=0.46185460686683655, train_x2_loss=0.7796118259429932\n",
      "INFO:absl:[8] val_loss=1.8676108121871948\n",
      "INFO:absl:[8] test_loss=1.9759585857391357\n",
      "INFO:absl:[9] train_loss=1.2344391345977783, train_x1_loss=0.4601968228816986, train_x2_loss=0.7742418646812439\n",
      "INFO:absl:[9] val_loss=1.8584812879562378\n",
      "INFO:absl:[9] test_loss=1.967331051826477\n",
      "INFO:absl:[10] train_loss=1.2299463748931885, train_x1_loss=0.4581835865974426, train_x2_loss=0.7717621326446533\n",
      "INFO:absl:[10] val_loss=1.8588385581970215\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[10] test_loss=1.9679841995239258\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[11] train_loss=1.22847318649292, train_x1_loss=0.4588620364665985, train_x2_loss=0.7696112990379333\n",
      "INFO:absl:[11] val_loss=1.8520936965942383\n",
      "INFO:absl:[11] test_loss=1.960649013519287\n",
      "INFO:absl:[12] train_loss=1.2246379852294922, train_x1_loss=0.4575270414352417, train_x2_loss=0.7671120166778564\n",
      "INFO:absl:[12] val_loss=1.8479419946670532\n",
      "INFO:absl:[12] test_loss=1.956903100013733\n",
      "INFO:absl:[13] train_loss=1.2207542657852173, train_x1_loss=0.4562495946884155, train_x2_loss=0.7645035982131958\n",
      "INFO:absl:[13] val_loss=1.848720669746399\n",
      "INFO:absl:[13] test_loss=1.9577239751815796\n",
      "INFO:absl:[14] train_loss=1.2226563692092896, train_x1_loss=0.4569947421550751, train_x2_loss=0.7656611800193787\n",
      "INFO:absl:[14] val_loss=1.8489149808883667\n",
      "INFO:absl:[14] test_loss=1.9582432508468628\n",
      "INFO:absl:[15] train_loss=1.2184404134750366, train_x1_loss=0.45747998356819153, train_x2_loss=0.7609593272209167\n",
      "INFO:absl:[15] val_loss=1.8470146656036377\n",
      "INFO:absl:[15] test_loss=1.9560562372207642\n",
      "INFO:absl:[16] train_loss=1.2198600769042969, train_x1_loss=0.45839887857437134, train_x2_loss=0.7614622116088867\n",
      "INFO:absl:[16] val_loss=1.8450769186019897\n",
      "INFO:absl:[16] test_loss=1.954674243927002\n",
      "INFO:absl:[17] train_loss=1.2173131704330444, train_x1_loss=0.454790323972702, train_x2_loss=0.762525200843811\n",
      "INFO:absl:[17] val_loss=1.8461027145385742\n",
      "INFO:absl:[17] test_loss=1.9555164575576782\n",
      "INFO:absl:[18] train_loss=1.2180806398391724, train_x1_loss=0.45770373940467834, train_x2_loss=0.7603773474693298\n",
      "INFO:absl:[18] val_loss=1.848963975906372\n",
      "INFO:absl:[18] test_loss=1.9578897953033447\n",
      "INFO:absl:[19] train_loss=1.2175006866455078, train_x1_loss=0.4569076895713806, train_x2_loss=0.7605932354927063\n",
      "INFO:absl:[19] val_loss=1.8445531129837036\n",
      "INFO:absl:[19] test_loss=1.9540349245071411\n",
      "INFO:absl:[20] train_loss=1.218647837638855, train_x1_loss=0.4583556056022644, train_x2_loss=0.7602927088737488\n",
      "INFO:absl:[20] val_loss=1.8486552238464355\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[20] test_loss=1.9580553770065308\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[21] train_loss=1.2141482830047607, train_x1_loss=0.4547506272792816, train_x2_loss=0.7593981623649597\n",
      "INFO:absl:[21] val_loss=1.8464763164520264\n",
      "INFO:absl:[21] test_loss=1.9558149576187134\n",
      "INFO:absl:[22] train_loss=1.213239312171936, train_x1_loss=0.4548409581184387, train_x2_loss=0.758399486541748\n",
      "INFO:absl:[22] val_loss=1.850381851196289\n",
      "INFO:absl:[22] test_loss=1.9600414037704468\n",
      "INFO:absl:[23] train_loss=1.2165918350219727, train_x1_loss=0.4577445983886719, train_x2_loss=0.7588493227958679\n",
      "INFO:absl:[23] val_loss=1.8434560298919678\n",
      "INFO:absl:[23] test_loss=1.9531340599060059\n",
      "INFO:absl:[24] train_loss=1.2165641784667969, train_x1_loss=0.4581003487110138, train_x2_loss=0.758465588092804\n",
      "INFO:absl:[24] val_loss=1.8436394929885864\n",
      "INFO:absl:[24] test_loss=1.953160285949707\n",
      "INFO:absl:[25] train_loss=1.2149286270141602, train_x1_loss=0.4575422406196594, train_x2_loss=0.7573862075805664\n",
      "INFO:absl:[25] val_loss=1.848984956741333\n",
      "INFO:absl:[25] test_loss=1.958280324935913\n",
      "INFO:absl:[26] train_loss=1.2098467350006104, train_x1_loss=0.45240747928619385, train_x2_loss=0.757439374923706\n",
      "INFO:absl:[26] val_loss=1.8480026721954346\n",
      "INFO:absl:[26] test_loss=1.9574872255325317\n",
      "INFO:absl:[27] train_loss=1.2166348695755005, train_x1_loss=0.4566855728626251, train_x2_loss=0.7599499225616455\n",
      "INFO:absl:[27] val_loss=1.850348949432373\n",
      "INFO:absl:[27] test_loss=1.9598705768585205\n",
      "INFO:absl:[28] train_loss=1.2128239870071411, train_x1_loss=0.4523886740207672, train_x2_loss=0.7604371309280396\n",
      "INFO:absl:[28] val_loss=1.8455840349197388\n",
      "INFO:absl:[28] test_loss=1.954737901687622\n",
      "INFO:absl:[29] train_loss=1.2147421836853027, train_x1_loss=0.45694899559020996, train_x2_loss=0.7577905058860779\n",
      "INFO:absl:[29] val_loss=1.8425357341766357\n",
      "INFO:absl:[29] test_loss=1.9520145654678345\n",
      "INFO:absl:[30] train_loss=1.2145917415618896, train_x1_loss=0.4555383622646332, train_x2_loss=0.7590512633323669\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[30] val_loss=1.8458365201950073\n",
      "INFO:absl:[30] test_loss=1.9552185535430908\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[31] train_loss=1.208303451538086, train_x1_loss=0.4521408975124359, train_x2_loss=0.7561624050140381\n",
      "INFO:absl:[31] val_loss=1.848394751548767\n",
      "INFO:absl:[31] test_loss=1.9581952095031738\n",
      "INFO:absl:[32] train_loss=1.2124865055084229, train_x1_loss=0.45424774289131165, train_x2_loss=0.7582390904426575\n",
      "INFO:absl:[32] val_loss=1.8477692604064941\n",
      "INFO:absl:[32] test_loss=1.957464337348938\n",
      "INFO:absl:[33] train_loss=1.2069681882858276, train_x1_loss=0.4510088562965393, train_x2_loss=0.7559593319892883\n",
      "INFO:absl:[33] val_loss=1.8445597887039185\n",
      "INFO:absl:[33] test_loss=1.9543708562850952\n",
      "INFO:absl:[34] train_loss=1.2099297046661377, train_x1_loss=0.45351389050483704, train_x2_loss=0.7564162015914917\n",
      "INFO:absl:[34] val_loss=1.8488446474075317\n",
      "INFO:absl:[34] test_loss=1.9584040641784668\n",
      "INFO:absl:[35] train_loss=1.2130022048950195, train_x1_loss=0.4540732204914093, train_x2_loss=0.7589259147644043\n",
      "INFO:absl:[35] val_loss=1.8507599830627441\n",
      "INFO:absl:[35] test_loss=1.9603748321533203\n",
      "INFO:absl:[36] train_loss=1.2086122035980225, train_x1_loss=0.45215773582458496, train_x2_loss=0.7564516067504883\n",
      "INFO:absl:[36] val_loss=1.8502099514007568\n",
      "INFO:absl:[36] test_loss=1.9597049951553345\n",
      "INFO:absl:[37] train_loss=1.2122575044631958, train_x1_loss=0.4533577263355255, train_x2_loss=0.7588999271392822\n",
      "INFO:absl:[37] val_loss=1.8521711826324463\n",
      "INFO:absl:[37] test_loss=1.9618085622787476\n",
      "INFO:absl:[38] train_loss=1.2068904638290405, train_x1_loss=0.45160213112831116, train_x2_loss=0.7552872896194458\n",
      "INFO:absl:[38] val_loss=1.8514645099639893\n",
      "INFO:absl:[38] test_loss=1.9611562490463257\n",
      "INFO:absl:[39] train_loss=1.2049912214279175, train_x1_loss=0.44907647371292114, train_x2_loss=0.7559139728546143\n",
      "INFO:absl:[39] val_loss=1.844825029373169\n",
      "INFO:absl:[39] test_loss=1.9542961120605469\n",
      "INFO:absl:[40] train_loss=1.2019387483596802, train_x1_loss=0.44833052158355713, train_x2_loss=0.7536104917526245\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[40] val_loss=1.8419759273529053\n",
      "INFO:absl:[40] test_loss=1.951980710029602\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[41] train_loss=1.2042899131774902, train_x1_loss=0.4502102732658386, train_x2_loss=0.7540791034698486\n",
      "INFO:absl:[41] val_loss=1.8457067012786865\n",
      "INFO:absl:[41] test_loss=1.9556816816329956\n",
      "INFO:absl:[42] train_loss=1.2034611701965332, train_x1_loss=0.44870975613594055, train_x2_loss=0.7547500133514404\n",
      "INFO:absl:[42] val_loss=1.8397940397262573\n",
      "INFO:absl:[42] test_loss=1.9498553276062012\n",
      "INFO:absl:[43] train_loss=1.2030507326126099, train_x1_loss=0.44689273834228516, train_x2_loss=0.756157636642456\n",
      "INFO:absl:[43] val_loss=1.8428013324737549\n",
      "INFO:absl:[43] test_loss=1.9525628089904785\n",
      "INFO:absl:[44] train_loss=1.2043205499649048, train_x1_loss=0.4487230181694031, train_x2_loss=0.7555971145629883\n",
      "INFO:absl:[44] val_loss=1.8426457643508911\n",
      "INFO:absl:[44] test_loss=1.9527223110198975\n",
      "INFO:absl:[45] train_loss=1.2050397396087646, train_x1_loss=0.44752776622772217, train_x2_loss=0.7575108408927917\n",
      "INFO:absl:[45] val_loss=1.8393125534057617\n",
      "INFO:absl:[45] test_loss=1.9495126008987427\n",
      "INFO:absl:[46] train_loss=1.2032666206359863, train_x1_loss=0.4466550946235657, train_x2_loss=0.756614089012146\n",
      "INFO:absl:[46] val_loss=1.848644495010376\n",
      "INFO:absl:[46] test_loss=1.9584705829620361\n",
      "INFO:absl:[47] train_loss=1.2038689851760864, train_x1_loss=0.4460103213787079, train_x2_loss=0.7578572630882263\n",
      "INFO:absl:[47] val_loss=1.8418738842010498\n",
      "INFO:absl:[47] test_loss=1.9523810148239136\n",
      "INFO:absl:[48] train_loss=1.2004061937332153, train_x1_loss=0.4466434717178345, train_x2_loss=0.7537627816200256\n",
      "INFO:absl:[48] val_loss=1.8378050327301025\n",
      "INFO:absl:[48] test_loss=1.9483869075775146\n",
      "INFO:absl:[49] train_loss=1.203079342842102, train_x1_loss=0.4467911422252655, train_x2_loss=0.7562870979309082\n",
      "INFO:absl:[49] val_loss=1.8431512117385864\n",
      "INFO:absl:[49] test_loss=1.9533042907714844\n",
      "INFO:absl:[50] train_loss=1.2008171081542969, train_x1_loss=0.4439396262168884, train_x2_loss=0.7568762302398682\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[50] val_loss=1.8413410186767578\n",
      "INFO:absl:[50] test_loss=1.9518827199935913\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[51] train_loss=1.2000175714492798, train_x1_loss=0.4453309178352356, train_x2_loss=0.7546852827072144\n",
      "INFO:absl:[51] val_loss=1.8384329080581665\n",
      "INFO:absl:[51] test_loss=1.9488283395767212\n",
      "INFO:absl:[52] train_loss=1.2011505365371704, train_x1_loss=0.44618913531303406, train_x2_loss=0.7549607753753662\n",
      "INFO:absl:[52] val_loss=1.8359332084655762\n",
      "INFO:absl:[52] test_loss=1.9471441507339478\n",
      "INFO:absl:[53] train_loss=1.2019480466842651, train_x1_loss=0.4465272128582001, train_x2_loss=0.7554206848144531\n",
      "INFO:absl:[53] val_loss=1.8354066610336304\n",
      "INFO:absl:[53] test_loss=1.9463051557540894\n",
      "INFO:absl:[54] train_loss=1.2020478248596191, train_x1_loss=0.44538357853889465, train_x2_loss=0.7566659450531006\n",
      "INFO:absl:[54] val_loss=1.842531681060791\n",
      "INFO:absl:[54] test_loss=1.9529459476470947\n",
      "INFO:absl:[55] train_loss=1.1965420246124268, train_x1_loss=0.4429268538951874, train_x2_loss=0.7536158561706543\n",
      "INFO:absl:[55] val_loss=1.8391945362091064\n",
      "INFO:absl:[55] test_loss=1.9494394063949585\n",
      "INFO:absl:[56] train_loss=1.1961556673049927, train_x1_loss=0.4428621530532837, train_x2_loss=0.7532930374145508\n",
      "INFO:absl:[56] val_loss=1.8344838619232178\n",
      "INFO:absl:[56] test_loss=1.945183277130127\n",
      "INFO:absl:[57] train_loss=1.198133945465088, train_x1_loss=0.4429094195365906, train_x2_loss=0.7552240490913391\n",
      "INFO:absl:[57] val_loss=1.8402034044265747\n",
      "INFO:absl:[57] test_loss=1.9507310390472412\n",
      "INFO:absl:[58] train_loss=1.1945481300354004, train_x1_loss=0.44301852583885193, train_x2_loss=0.7515291571617126\n",
      "INFO:absl:[58] val_loss=1.8371849060058594\n",
      "INFO:absl:[58] test_loss=1.9479979276657104\n",
      "INFO:absl:[59] train_loss=1.1952003240585327, train_x1_loss=0.4414631426334381, train_x2_loss=0.7537375092506409\n",
      "INFO:absl:[59] val_loss=1.8430628776550293\n",
      "INFO:absl:[59] test_loss=1.953819990158081\n",
      "INFO:absl:Setting work unit notes: 3553.5 steps/s, 60.9% (213208/350000), ETA: 0m (1m : 0.1% checkpoint, 12.9% eval)\n",
      "INFO:absl:[213208] steps_per_sec=3553.459423\n",
      "INFO:absl:[60] train_loss=1.1958625316619873, train_x1_loss=0.4414021074771881, train_x2_loss=0.7544617056846619\n",
      "INFO:absl:[60] val_loss=1.8340750932693481\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[60] test_loss=1.945291519165039\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[61] train_loss=1.1943564414978027, train_x1_loss=0.44114652276039124, train_x2_loss=0.7532098293304443\n",
      "INFO:absl:[61] val_loss=1.8424991369247437\n",
      "INFO:absl:[61] test_loss=1.9537285566329956\n",
      "INFO:absl:[62] train_loss=1.1958898305892944, train_x1_loss=0.443684458732605, train_x2_loss=0.7522014379501343\n",
      "INFO:absl:[62] val_loss=1.8371514081954956\n",
      "INFO:absl:[62] test_loss=1.9489308595657349\n",
      "INFO:absl:[63] train_loss=1.194898009300232, train_x1_loss=0.44345763325691223, train_x2_loss=0.7514393925666809\n",
      "INFO:absl:[63] val_loss=1.8361045122146606\n",
      "INFO:absl:[63] test_loss=1.9470367431640625\n",
      "INFO:absl:[64] train_loss=1.1955987215042114, train_x1_loss=0.44275879859924316, train_x2_loss=0.7528401017189026\n",
      "INFO:absl:[64] val_loss=1.8287605047225952\n",
      "INFO:absl:[64] test_loss=1.9403572082519531\n",
      "INFO:absl:[65] train_loss=1.1929733753204346, train_x1_loss=0.4388323426246643, train_x2_loss=0.754142165184021\n",
      "INFO:absl:[65] val_loss=1.8354030847549438\n",
      "INFO:absl:[65] test_loss=1.9467275142669678\n",
      "INFO:absl:[66] train_loss=1.1855651140213013, train_x1_loss=0.43636003136634827, train_x2_loss=0.7492062449455261\n",
      "INFO:absl:[66] val_loss=1.8335155248641968\n",
      "INFO:absl:[66] test_loss=1.9447282552719116\n",
      "INFO:absl:[67] train_loss=1.191473364830017, train_x1_loss=0.43766507506370544, train_x2_loss=0.7538099884986877\n",
      "INFO:absl:[67] val_loss=1.8348631858825684\n",
      "INFO:absl:[67] test_loss=1.9461952447891235\n",
      "INFO:absl:[68] train_loss=1.192147135734558, train_x1_loss=0.44090136885643005, train_x2_loss=0.751246452331543\n",
      "INFO:absl:[68] val_loss=1.8390146493911743\n",
      "INFO:absl:[68] test_loss=1.9500641822814941\n",
      "INFO:absl:[69] train_loss=1.1901053190231323, train_x1_loss=0.43905964493751526, train_x2_loss=0.7510465979576111\n",
      "INFO:absl:[69] val_loss=1.8267546892166138\n",
      "INFO:absl:[69] test_loss=1.938477873802185\n",
      "INFO:absl:[70] train_loss=1.1948175430297852, train_x1_loss=0.4434606730937958, train_x2_loss=0.7513592839241028\n",
      "INFO:absl:[70] val_loss=1.835421085357666\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[70] test_loss=1.9469401836395264\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[71] train_loss=1.1867281198501587, train_x1_loss=0.43872615694999695, train_x2_loss=0.748001754283905\n",
      "INFO:absl:[71] val_loss=1.835777997970581\n",
      "INFO:absl:[71] test_loss=1.9469250440597534\n",
      "INFO:absl:[72] train_loss=1.186441421508789, train_x1_loss=0.4368746280670166, train_x2_loss=0.7495659589767456\n",
      "INFO:absl:[72] val_loss=1.8340898752212524\n",
      "INFO:absl:[72] test_loss=1.945695400238037\n",
      "INFO:absl:[73] train_loss=1.1904196739196777, train_x1_loss=0.44082558155059814, train_x2_loss=0.7495948076248169\n",
      "INFO:absl:[73] val_loss=1.8360531330108643\n",
      "INFO:absl:[73] test_loss=1.9475398063659668\n",
      "INFO:absl:[74] train_loss=1.1922717094421387, train_x1_loss=0.44037413597106934, train_x2_loss=0.7518975138664246\n",
      "INFO:absl:[74] val_loss=1.834134817123413\n",
      "INFO:absl:[74] test_loss=1.945522427558899\n",
      "INFO:absl:[75] train_loss=1.1901472806930542, train_x1_loss=0.4359596073627472, train_x2_loss=0.7541888356208801\n",
      "INFO:absl:[75] val_loss=1.8400530815124512\n",
      "INFO:absl:[75] test_loss=1.9512578248977661\n",
      "INFO:absl:[76] train_loss=1.189042091369629, train_x1_loss=0.43613919615745544, train_x2_loss=0.7529047727584839\n",
      "INFO:absl:[76] val_loss=1.8396236896514893\n",
      "INFO:absl:[76] test_loss=1.9506901502609253\n",
      "INFO:absl:[77] train_loss=1.189139485359192, train_x1_loss=0.43749678134918213, train_x2_loss=0.7516400814056396\n",
      "INFO:absl:[77] val_loss=1.8338359594345093\n",
      "INFO:absl:[77] test_loss=1.945542335510254\n",
      "INFO:absl:[78] train_loss=1.1927443742752075, train_x1_loss=0.44200971722602844, train_x2_loss=0.7507346272468567\n",
      "INFO:absl:[78] val_loss=1.8306227922439575\n",
      "INFO:absl:[78] test_loss=1.942042589187622\n",
      "INFO:absl:[79] train_loss=1.1930891275405884, train_x1_loss=0.4411160349845886, train_x2_loss=0.7519730925559998\n",
      "INFO:absl:[79] val_loss=1.8309577703475952\n",
      "INFO:absl:[79] test_loss=1.9424771070480347\n",
      "INFO:absl:[80] train_loss=1.189926028251648, train_x1_loss=0.43890148401260376, train_x2_loss=0.7510252594947815\n",
      "INFO:absl:[80] val_loss=1.8359260559082031\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[80] test_loss=1.947217583656311\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[81] train_loss=1.1851449012756348, train_x1_loss=0.43423110246658325, train_x2_loss=0.7509125471115112\n",
      "INFO:absl:[81] val_loss=1.8315268754959106\n",
      "INFO:absl:[81] test_loss=1.9430017471313477\n",
      "INFO:absl:[82] train_loss=1.189401388168335, train_x1_loss=0.4385468661785126, train_x2_loss=0.7508549690246582\n",
      "INFO:absl:[82] val_loss=1.8281606435775757\n",
      "INFO:absl:[82] test_loss=1.9396892786026\n",
      "INFO:absl:[83] train_loss=1.1858881711959839, train_x1_loss=0.4358863830566406, train_x2_loss=0.7500012516975403\n",
      "INFO:absl:[83] val_loss=1.833451747894287\n",
      "INFO:absl:[83] test_loss=1.945005178451538\n",
      "INFO:absl:[84] train_loss=1.1879462003707886, train_x1_loss=0.4372599124908447, train_x2_loss=0.750683605670929\n",
      "INFO:absl:[84] val_loss=1.8323420286178589\n",
      "INFO:absl:[84] test_loss=1.9438352584838867\n",
      "INFO:absl:[85] train_loss=1.1856673955917358, train_x1_loss=0.4324944317340851, train_x2_loss=0.7531726360321045\n",
      "INFO:absl:[85] val_loss=1.8369333744049072\n",
      "INFO:absl:[85] test_loss=1.9481815099716187\n",
      "INFO:absl:[86] train_loss=1.188546895980835, train_x1_loss=0.43628600239753723, train_x2_loss=0.7522594332695007\n",
      "INFO:absl:[86] val_loss=1.8313398361206055\n",
      "INFO:absl:[86] test_loss=1.9433715343475342\n",
      "INFO:absl:[87] train_loss=1.1866105794906616, train_x1_loss=0.438062846660614, train_x2_loss=0.7485491633415222\n",
      "INFO:absl:[87] val_loss=1.8277337551116943\n",
      "INFO:absl:[87] test_loss=1.9393926858901978\n",
      "INFO:absl:[88] train_loss=1.1820411682128906, train_x1_loss=0.43340039253234863, train_x2_loss=0.7486409544944763\n",
      "INFO:absl:[88] val_loss=1.8313039541244507\n",
      "INFO:absl:[88] test_loss=1.9429504871368408\n",
      "INFO:absl:[89] train_loss=1.1863824129104614, train_x1_loss=0.43579041957855225, train_x2_loss=0.7505928874015808\n",
      "INFO:absl:[89] val_loss=1.8308967351913452\n",
      "INFO:absl:[89] test_loss=1.942284107208252\n",
      "INFO:absl:[90] train_loss=1.1869747638702393, train_x1_loss=0.4357953667640686, train_x2_loss=0.7511782646179199\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[90] val_loss=1.8293602466583252\n",
      "INFO:absl:[90] test_loss=1.9412543773651123\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[91] train_loss=1.1845089197158813, train_x1_loss=0.4358457028865814, train_x2_loss=0.7486644387245178\n",
      "INFO:absl:[91] val_loss=1.828774094581604\n",
      "INFO:absl:[91] test_loss=1.9402536153793335\n",
      "INFO:absl:[92] train_loss=1.1852543354034424, train_x1_loss=0.4364440441131592, train_x2_loss=0.7488107681274414\n",
      "INFO:absl:[92] val_loss=1.8265684843063354\n",
      "INFO:absl:[92] test_loss=1.9384465217590332\n",
      "INFO:absl:[93] train_loss=1.18843412399292, train_x1_loss=0.4361974000930786, train_x2_loss=0.7522356510162354\n",
      "INFO:absl:[93] val_loss=1.8302074670791626\n",
      "INFO:absl:[93] test_loss=1.9419535398483276\n",
      "INFO:absl:[94] train_loss=1.1840986013412476, train_x1_loss=0.4356329143047333, train_x2_loss=0.7484664916992188\n",
      "INFO:absl:[94] val_loss=1.8300836086273193\n",
      "INFO:absl:[94] test_loss=1.941890835762024\n",
      "INFO:absl:[95] train_loss=1.187143087387085, train_x1_loss=0.43746984004974365, train_x2_loss=0.7496739029884338\n",
      "INFO:absl:[95] val_loss=1.8319042921066284\n",
      "INFO:absl:[95] test_loss=1.9435524940490723\n",
      "INFO:absl:[96] train_loss=1.1779861450195312, train_x1_loss=0.42995989322662354, train_x2_loss=0.748028039932251\n",
      "INFO:absl:[96] val_loss=1.8285619020462036\n",
      "INFO:absl:[96] test_loss=1.940218448638916\n",
      "INFO:absl:[97] train_loss=1.1823132038116455, train_x1_loss=0.43535247445106506, train_x2_loss=0.7469608783721924\n",
      "INFO:absl:[97] val_loss=1.8288112878799438\n",
      "INFO:absl:[97] test_loss=1.9404590129852295\n",
      "INFO:absl:[98] train_loss=1.1886366605758667, train_x1_loss=0.4395614266395569, train_x2_loss=0.7490730881690979\n",
      "INFO:absl:[98] val_loss=1.82859206199646\n",
      "INFO:absl:[98] test_loss=1.9402601718902588\n",
      "INFO:absl:[99] train_loss=1.1836131811141968, train_x1_loss=0.4342651069164276, train_x2_loss=0.7493469715118408\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[99] val_loss=1.8225681781768799\n",
      "INFO:absl:[99] test_loss=1.9348751306533813\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'elu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 10, 'dropout_rate': 0.21813956053366504, 'edge_features': (4, 2), 'epochs': 100, 'eval_every_epochs': 1, 'fully_connected_edges': 3, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 1, 'layer_norm': False, 'learning_rate': 3.873531888351175e-05, 'log_every_epochs': 1, 'max_checkpts_to_keep': 2, 'model': 'MLPGraphNetwork', 'n_blocks': 1, 'n_samples': 5000, 'node_features': (4, 2), 'normalize': True, 'optimizer': 'adam', 'output_delay': 0, 'output_steps': 4, 'sample_buffer': -4, 'seed': 74, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 120, 'timestep_duration': 3, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+--------+------+---------+-------+\n",
      "| Name                                   | Shape  | Size | Mean    | Std   |\n",
      "+----------------------------------------+--------+------+---------+-------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (4,)   | 4    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 4) | 24   | -0.0819 | 0.36  |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (2,)   | 2    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (4, 2) | 8    | 0.289   | 0.469 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (4,)   | 4    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (7, 4) | 28   | -0.0395 | 0.333 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)   | 2    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (4, 2) | 8    | -0.135  | 0.542 |\n",
      "+----------------------------------------+--------+------+---------+-------+\n",
      "Total: 80\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Checkpoint None does not exist.\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.00s.\n",
      "INFO:absl:Starting training.\n",
      "INFO:absl:[0] train_loss=3.1493611335754395, train_x1_loss=1.781646728515625, train_x2_loss=1.3677037954330444\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[0] val_loss=2.957442283630371\n",
      "INFO:absl:[0] test_loss=2.9301042556762695\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[1] train_loss=2.1591506004333496, train_x1_loss=1.1136560440063477, train_x2_loss=1.0454952716827393\n",
      "INFO:absl:[1] val_loss=2.487506151199341\n",
      "INFO:absl:Created artifact [10] Profile of type ArtifactType.URL and value None.\n",
      "INFO:absl:[1] test_loss=2.4622201919555664\n",
      "INFO:absl:[2] train_loss=1.698931336402893, train_x1_loss=0.7771119475364685, train_x2_loss=0.9218165874481201\n",
      "INFO:absl:[2] val_loss=2.179495096206665\n",
      "INFO:absl:[2] test_loss=2.154005289077759\n",
      "INFO:absl:[3] train_loss=1.4660489559173584, train_x1_loss=0.5816009640693665, train_x2_loss=0.8844450116157532\n",
      "INFO:absl:[3] val_loss=2.099501848220825\n",
      "INFO:absl:[3] test_loss=2.0759799480438232\n",
      "INFO:absl:[4] train_loss=1.375347375869751, train_x1_loss=0.516447126865387, train_x2_loss=0.8589015603065491\n",
      "INFO:absl:[4] val_loss=2.046104669570923\n",
      "INFO:absl:[4] test_loss=2.023667335510254\n",
      "INFO:absl:[5] train_loss=1.3243440389633179, train_x1_loss=0.4907291829586029, train_x2_loss=0.8336148858070374\n",
      "INFO:absl:[5] val_loss=1.981365442276001\n",
      "INFO:absl:[5] test_loss=1.9589687585830688\n",
      "INFO:absl:[6] train_loss=1.2805676460266113, train_x1_loss=0.4716421365737915, train_x2_loss=0.8089263439178467\n",
      "INFO:absl:[6] val_loss=1.9476627111434937\n",
      "INFO:absl:[6] test_loss=1.9236106872558594\n",
      "INFO:absl:[7] train_loss=1.255200982093811, train_x1_loss=0.46284499764442444, train_x2_loss=0.7923567891120911\n",
      "INFO:absl:[7] val_loss=1.9254469871520996\n",
      "INFO:absl:[7] test_loss=1.8999079465866089\n",
      "INFO:absl:[8] train_loss=1.241576910018921, train_x1_loss=0.45931312441825867, train_x2_loss=0.7822620272636414\n",
      "INFO:absl:[8] val_loss=1.9154161214828491\n",
      "INFO:absl:[8] test_loss=1.8890740871429443\n",
      "INFO:absl:[9] train_loss=1.2355389595031738, train_x1_loss=0.4582664668560028, train_x2_loss=0.7772709131240845\n",
      "INFO:absl:[9] val_loss=1.90908682346344\n",
      "INFO:absl:[9] test_loss=1.8824195861816406\n",
      "INFO:absl:[10] train_loss=1.2325016260147095, train_x1_loss=0.4574451446533203, train_x2_loss=0.7750571966171265\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[10] val_loss=1.9062691926956177\n",
      "INFO:absl:[10] test_loss=1.8790180683135986\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[11] train_loss=1.2254050970077515, train_x1_loss=0.4532466530799866, train_x2_loss=0.772158145904541\n",
      "INFO:absl:[11] val_loss=1.9019774198532104\n",
      "INFO:absl:[11] test_loss=1.8747392892837524\n",
      "INFO:absl:[12] train_loss=1.2211095094680786, train_x1_loss=0.45295944809913635, train_x2_loss=0.7681496739387512\n",
      "INFO:absl:[12] val_loss=1.8995490074157715\n",
      "INFO:absl:[12] test_loss=1.8723257780075073\n",
      "INFO:absl:[13] train_loss=1.222110390663147, train_x1_loss=0.4544319212436676, train_x2_loss=0.7676789164543152\n",
      "INFO:absl:[13] val_loss=1.8962926864624023\n",
      "INFO:absl:[13] test_loss=1.8686093091964722\n",
      "INFO:absl:[14] train_loss=1.2239083051681519, train_x1_loss=0.4554329216480255, train_x2_loss=0.7684754729270935\n",
      "INFO:absl:[14] val_loss=1.8997894525527954\n",
      "INFO:absl:[14] test_loss=1.8720735311508179\n",
      "INFO:absl:[15] train_loss=1.2194241285324097, train_x1_loss=0.4557563066482544, train_x2_loss=0.7636668086051941\n",
      "INFO:absl:[15] val_loss=1.89718496799469\n",
      "INFO:absl:[15] test_loss=1.8693139553070068\n",
      "INFO:absl:[16] train_loss=1.2243543863296509, train_x1_loss=0.45661047101020813, train_x2_loss=0.7677416205406189\n",
      "INFO:absl:[16] val_loss=1.90414297580719\n",
      "INFO:absl:[16] test_loss=1.8760913610458374\n",
      "INFO:absl:[17] train_loss=1.216516375541687, train_x1_loss=0.45236751437187195, train_x2_loss=0.7641459703445435\n",
      "INFO:absl:[17] val_loss=1.89340341091156\n",
      "INFO:absl:[17] test_loss=1.865598440170288\n",
      "INFO:absl:[18] train_loss=1.2162970304489136, train_x1_loss=0.4539188742637634, train_x2_loss=0.7623797655105591\n",
      "INFO:absl:[18] val_loss=1.8992562294006348\n",
      "INFO:absl:[18] test_loss=1.871169924736023\n",
      "INFO:absl:[19] train_loss=1.2218657732009888, train_x1_loss=0.45629414916038513, train_x2_loss=0.7655738592147827\n",
      "INFO:absl:[19] val_loss=1.9004194736480713\n",
      "INFO:absl:[19] test_loss=1.8723386526107788\n",
      "INFO:absl:[20] train_loss=1.2209234237670898, train_x1_loss=0.45420581102371216, train_x2_loss=0.7667167782783508\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[20] val_loss=1.8984500169754028\n",
      "INFO:absl:[20] test_loss=1.870448350906372\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[21] train_loss=1.2151570320129395, train_x1_loss=0.4534682333469391, train_x2_loss=0.7616912126541138\n",
      "INFO:absl:[21] val_loss=1.8996045589447021\n",
      "INFO:absl:[21] test_loss=1.8712719678878784\n",
      "INFO:absl:[22] train_loss=1.2124122381210327, train_x1_loss=0.45214366912841797, train_x2_loss=0.7602702379226685\n",
      "INFO:absl:[22] val_loss=1.8937327861785889\n",
      "INFO:absl:[22] test_loss=1.865500807762146\n",
      "INFO:absl:[23] train_loss=1.215744137763977, train_x1_loss=0.4519056975841522, train_x2_loss=0.7638375163078308\n",
      "INFO:absl:[23] val_loss=1.9006474018096924\n",
      "INFO:absl:[23] test_loss=1.8723348379135132\n",
      "INFO:absl:[24] train_loss=1.218928337097168, train_x1_loss=0.4555342197418213, train_x2_loss=0.7633953094482422\n",
      "INFO:absl:[24] val_loss=1.8979710340499878\n",
      "INFO:absl:[24] test_loss=1.8696004152297974\n",
      "INFO:absl:[25] train_loss=1.216201901435852, train_x1_loss=0.45371079444885254, train_x2_loss=0.7624899744987488\n",
      "INFO:absl:[25] val_loss=1.898183822631836\n",
      "INFO:absl:[25] test_loss=1.8698837757110596\n",
      "INFO:absl:[26] train_loss=1.2139681577682495, train_x1_loss=0.4510902166366577, train_x2_loss=0.7628797888755798\n",
      "INFO:absl:[26] val_loss=1.8944172859191895\n",
      "INFO:absl:[26] test_loss=1.8661803007125854\n",
      "INFO:absl:[27] train_loss=1.215952754020691, train_x1_loss=0.4528788924217224, train_x2_loss=0.7630734443664551\n",
      "INFO:absl:[27] val_loss=1.8966652154922485\n",
      "INFO:absl:[27] test_loss=1.8682775497436523\n",
      "INFO:absl:[28] train_loss=1.2176631689071655, train_x1_loss=0.4539474844932556, train_x2_loss=0.7637139558792114\n",
      "INFO:absl:[28] val_loss=1.8949223756790161\n",
      "INFO:absl:[28] test_loss=1.866386890411377\n",
      "INFO:absl:[29] train_loss=1.2166972160339355, train_x1_loss=0.4542955458164215, train_x2_loss=0.7623987197875977\n",
      "INFO:absl:[29] val_loss=1.9019625186920166\n",
      "INFO:absl:[29] test_loss=1.873504638671875\n",
      "INFO:absl:[30] train_loss=1.213545322418213, train_x1_loss=0.45250189304351807, train_x2_loss=0.7610441446304321\n",
      "INFO:absl:[30] val_loss=1.8952099084854126\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[30] test_loss=1.866761565208435\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[31] train_loss=1.2123523950576782, train_x1_loss=0.45301884412765503, train_x2_loss=0.7593323588371277\n",
      "INFO:absl:[31] val_loss=1.892729640007019\n",
      "INFO:absl:[31] test_loss=1.8643043041229248\n",
      "INFO:absl:[32] train_loss=1.2154203653335571, train_x1_loss=0.45255935192108154, train_x2_loss=0.7628609538078308\n",
      "INFO:absl:[32] val_loss=1.899229645729065\n",
      "INFO:absl:[32] test_loss=1.8707263469696045\n",
      "INFO:absl:[33] train_loss=1.2123512029647827, train_x1_loss=0.4518214166164398, train_x2_loss=0.7605292201042175\n",
      "INFO:absl:[33] val_loss=1.9016920328140259\n",
      "INFO:absl:[33] test_loss=1.8730754852294922\n",
      "INFO:absl:[34] train_loss=1.209435224533081, train_x1_loss=0.4495941698551178, train_x2_loss=0.7598400712013245\n",
      "INFO:absl:[34] val_loss=1.9000002145767212\n",
      "INFO:absl:[34] test_loss=1.8714234828948975\n",
      "INFO:absl:[35] train_loss=1.2085111141204834, train_x1_loss=0.44760289788246155, train_x2_loss=0.7609075903892517\n",
      "INFO:absl:[35] val_loss=1.896767258644104\n",
      "INFO:absl:[35] test_loss=1.8680223226547241\n",
      "INFO:absl:[36] train_loss=1.2097749710083008, train_x1_loss=0.4496253728866577, train_x2_loss=0.7601482272148132\n",
      "INFO:absl:[36] val_loss=1.8949099779129028\n",
      "INFO:absl:[36] test_loss=1.8665047883987427\n",
      "INFO:absl:[37] train_loss=1.2149502038955688, train_x1_loss=0.452961802482605, train_x2_loss=0.7619874477386475\n",
      "INFO:absl:[37] val_loss=1.8965805768966675\n",
      "INFO:absl:[37] test_loss=1.8679652214050293\n",
      "INFO:absl:[38] train_loss=1.208600401878357, train_x1_loss=0.4481230080127716, train_x2_loss=0.7604775428771973\n",
      "INFO:absl:[38] val_loss=1.8978965282440186\n",
      "INFO:absl:[38] test_loss=1.8692508935928345\n",
      "INFO:absl:[39] train_loss=1.2088003158569336, train_x1_loss=0.44852253794670105, train_x2_loss=0.760277509689331\n",
      "INFO:absl:[39] val_loss=1.892646312713623\n",
      "INFO:absl:[39] test_loss=1.863789677619934\n",
      "INFO:absl:[40] train_loss=1.2032945156097412, train_x1_loss=0.4441538453102112, train_x2_loss=0.7591404914855957\n",
      "INFO:absl:[40] val_loss=1.8923684358596802\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[40] test_loss=1.863463282585144\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[41] train_loss=1.2108154296875, train_x1_loss=0.4499838650226593, train_x2_loss=0.7608312368392944\n",
      "INFO:absl:[41] val_loss=1.8952581882476807\n",
      "INFO:absl:[41] test_loss=1.8663791418075562\n",
      "INFO:absl:[42] train_loss=1.2093194723129272, train_x1_loss=0.4492844343185425, train_x2_loss=0.7600346207618713\n",
      "INFO:absl:[42] val_loss=1.8927110433578491\n",
      "INFO:absl:[42] test_loss=1.8641703128814697\n",
      "INFO:absl:[43] train_loss=1.204054594039917, train_x1_loss=0.4450927674770355, train_x2_loss=0.7589592337608337\n",
      "INFO:absl:[43] val_loss=1.8912137746810913\n",
      "INFO:absl:[43] test_loss=1.86264169216156\n",
      "INFO:absl:[44] train_loss=1.2071188688278198, train_x1_loss=0.44795456528663635, train_x2_loss=0.7591633796691895\n",
      "INFO:absl:[44] val_loss=1.8932627439498901\n",
      "INFO:absl:[44] test_loss=1.8642388582229614\n",
      "INFO:absl:[45] train_loss=1.2021979093551636, train_x1_loss=0.44253596663475037, train_x2_loss=0.7596620917320251\n",
      "INFO:absl:[45] val_loss=1.8947149515151978\n",
      "INFO:absl:[45] test_loss=1.866018533706665\n",
      "INFO:absl:[46] train_loss=1.203368067741394, train_x1_loss=0.444694846868515, train_x2_loss=0.7586733102798462\n",
      "INFO:absl:[46] val_loss=1.8962273597717285\n",
      "INFO:absl:[46] test_loss=1.8670121431350708\n",
      "INFO:absl:[47] train_loss=1.2060467004776, train_x1_loss=0.44581615924835205, train_x2_loss=0.7602312564849854\n",
      "INFO:absl:[47] val_loss=1.896365761756897\n",
      "INFO:absl:[47] test_loss=1.8674521446228027\n",
      "INFO:absl:[48] train_loss=1.203194499015808, train_x1_loss=0.44234150648117065, train_x2_loss=0.7608532309532166\n",
      "INFO:absl:[48] val_loss=1.8925578594207764\n",
      "INFO:absl:[48] test_loss=1.863618016242981\n",
      "INFO:absl:[49] train_loss=1.2043850421905518, train_x1_loss=0.4456249177455902, train_x2_loss=0.7587606906890869\n",
      "INFO:absl:[49] val_loss=1.8929051160812378\n",
      "INFO:absl:[49] test_loss=1.8641234636306763\n",
      "INFO:absl:[50] train_loss=1.2028816938400269, train_x1_loss=0.4442618191242218, train_x2_loss=0.7586187720298767\n",
      "INFO:absl:[50] val_loss=1.8895626068115234\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[50] test_loss=1.860656499862671\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[51] train_loss=1.2029203176498413, train_x1_loss=0.4454912841320038, train_x2_loss=0.7574286460876465\n",
      "INFO:absl:[51] val_loss=1.8891700506210327\n",
      "INFO:absl:[51] test_loss=1.8603698015213013\n",
      "INFO:absl:[52] train_loss=1.2057117223739624, train_x1_loss=0.4463936388492584, train_x2_loss=0.759320080280304\n",
      "INFO:absl:[52] val_loss=1.89180326461792\n",
      "INFO:absl:[52] test_loss=1.862741231918335\n",
      "INFO:absl:[53] train_loss=1.202475905418396, train_x1_loss=0.44349661469459534, train_x2_loss=0.7589792609214783\n",
      "INFO:absl:[53] val_loss=1.8856303691864014\n",
      "INFO:absl:[53] test_loss=1.856972098350525\n",
      "INFO:absl:[54] train_loss=1.204237699508667, train_x1_loss=0.44393882155418396, train_x2_loss=0.76030033826828\n",
      "INFO:absl:[54] val_loss=1.8970245122909546\n",
      "INFO:absl:[54] test_loss=1.8677754402160645\n",
      "INFO:absl:[55] train_loss=1.203086495399475, train_x1_loss=0.4427970051765442, train_x2_loss=0.7602890729904175\n",
      "INFO:absl:[55] val_loss=1.8959341049194336\n",
      "INFO:absl:[55] test_loss=1.8670634031295776\n",
      "INFO:absl:[56] train_loss=1.2027039527893066, train_x1_loss=0.4452056288719177, train_x2_loss=0.7574983835220337\n",
      "INFO:absl:[56] val_loss=1.890005350112915\n",
      "INFO:absl:[56] test_loss=1.8611292839050293\n",
      "INFO:absl:[57] train_loss=1.202639102935791, train_x1_loss=0.44345271587371826, train_x2_loss=0.7591860890388489\n",
      "INFO:absl:[57] val_loss=1.89449942111969\n",
      "INFO:absl:[57] test_loss=1.8650506734848022\n",
      "INFO:absl:[58] train_loss=1.200839877128601, train_x1_loss=0.44207730889320374, train_x2_loss=0.7587623596191406\n",
      "INFO:absl:[58] val_loss=1.8886797428131104\n",
      "INFO:absl:[58] test_loss=1.8598922491073608\n",
      "INFO:absl:[59] train_loss=1.2007485628128052, train_x1_loss=0.4428287744522095, train_x2_loss=0.7579172253608704\n",
      "INFO:absl:[59] val_loss=1.8928439617156982\n",
      "INFO:absl:[59] test_loss=1.8635143041610718\n",
      "INFO:absl:Setting work unit notes: 3530.4 steps/s, 60.5% (211822/350000), ETA: 0m (1m : 0.1% checkpoint, 12.9% eval)\n",
      "INFO:absl:[211822] steps_per_sec=3530.358250\n",
      "INFO:absl:[60] train_loss=1.1999397277832031, train_x1_loss=0.441646933555603, train_x2_loss=0.7582924962043762\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[60] val_loss=1.8858532905578613\n",
      "INFO:absl:[60] test_loss=1.856988549232483\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[61] train_loss=1.1973087787628174, train_x1_loss=0.4374244511127472, train_x2_loss=0.7598838806152344\n",
      "INFO:absl:[61] val_loss=1.8938162326812744\n",
      "INFO:absl:[61] test_loss=1.8644732236862183\n",
      "INFO:absl:[62] train_loss=1.198907732963562, train_x1_loss=0.4385748505592346, train_x2_loss=0.7603342533111572\n",
      "INFO:absl:[62] val_loss=1.8942382335662842\n",
      "INFO:absl:[62] test_loss=1.8646423816680908\n",
      "INFO:absl:[63] train_loss=1.1978726387023926, train_x1_loss=0.4423225224018097, train_x2_loss=0.7555485367774963\n",
      "INFO:absl:[63] val_loss=1.8877761363983154\n",
      "INFO:absl:[63] test_loss=1.8586039543151855\n",
      "INFO:absl:[64] train_loss=1.1979299783706665, train_x1_loss=0.4395523965358734, train_x2_loss=0.7583770155906677\n",
      "INFO:absl:[64] val_loss=1.8903756141662598\n",
      "INFO:absl:[64] test_loss=1.861304521560669\n",
      "INFO:absl:[65] train_loss=1.2015131711959839, train_x1_loss=0.4439193606376648, train_x2_loss=0.7575961351394653\n",
      "INFO:absl:[65] val_loss=1.8906410932540894\n",
      "INFO:absl:[65] test_loss=1.861366868019104\n",
      "INFO:absl:[66] train_loss=1.2014540433883667, train_x1_loss=0.44294077157974243, train_x2_loss=0.7585136294364929\n",
      "INFO:absl:[66] val_loss=1.8898857831954956\n",
      "INFO:absl:[66] test_loss=1.860905408859253\n",
      "INFO:absl:[67] train_loss=1.198901891708374, train_x1_loss=0.4404386281967163, train_x2_loss=0.7584612369537354\n",
      "INFO:absl:[67] val_loss=1.8824628591537476\n",
      "INFO:absl:[67] test_loss=1.853729486465454\n",
      "INFO:absl:[68] train_loss=1.1989697217941284, train_x1_loss=0.4406997859477997, train_x2_loss=0.7582695484161377\n",
      "INFO:absl:[68] val_loss=1.8865768909454346\n",
      "INFO:absl:[68] test_loss=1.857441782951355\n",
      "INFO:absl:[69] train_loss=1.197293996810913, train_x1_loss=0.4418536424636841, train_x2_loss=0.7554394006729126\n",
      "INFO:absl:[69] val_loss=1.889270305633545\n",
      "INFO:absl:[69] test_loss=1.8601990938186646\n",
      "INFO:absl:[70] train_loss=1.2026572227478027, train_x1_loss=0.4435645341873169, train_x2_loss=0.7590927481651306\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[70] val_loss=1.8897547721862793\n",
      "INFO:absl:[70] test_loss=1.860442042350769\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[71] train_loss=1.1963382959365845, train_x1_loss=0.4400128722190857, train_x2_loss=0.7563266158103943\n",
      "INFO:absl:[71] val_loss=1.8884174823760986\n",
      "INFO:absl:[71] test_loss=1.8592966794967651\n",
      "INFO:absl:[72] train_loss=1.1963375806808472, train_x1_loss=0.43933141231536865, train_x2_loss=0.7570036053657532\n",
      "INFO:absl:[72] val_loss=1.8930983543395996\n",
      "INFO:absl:[72] test_loss=1.8635174036026\n",
      "INFO:absl:[73] train_loss=1.1927967071533203, train_x1_loss=0.43683385848999023, train_x2_loss=0.7559628486633301\n",
      "INFO:absl:[73] val_loss=1.8912899494171143\n",
      "INFO:absl:[73] test_loss=1.8618907928466797\n",
      "INFO:absl:[74] train_loss=1.1998341083526611, train_x1_loss=0.44217902421951294, train_x2_loss=0.757654070854187\n",
      "INFO:absl:[74] val_loss=1.8907544612884521\n",
      "INFO:absl:[74] test_loss=1.8615704774856567\n",
      "INFO:absl:[75] train_loss=1.19326651096344, train_x1_loss=0.43764033913612366, train_x2_loss=0.7556267976760864\n",
      "INFO:absl:[75] val_loss=1.8856676816940308\n",
      "INFO:absl:[75] test_loss=1.8565354347229004\n",
      "INFO:absl:[76] train_loss=1.1947963237762451, train_x1_loss=0.43963491916656494, train_x2_loss=0.7551621198654175\n",
      "INFO:absl:[76] val_loss=1.8850038051605225\n",
      "INFO:absl:[76] test_loss=1.855905294418335\n",
      "INFO:absl:[77] train_loss=1.2016377449035645, train_x1_loss=0.4421812891960144, train_x2_loss=0.7594568729400635\n",
      "INFO:absl:[77] val_loss=1.890934944152832\n",
      "INFO:absl:[77] test_loss=1.8615304231643677\n",
      "INFO:absl:[78] train_loss=1.1991024017333984, train_x1_loss=0.4420078694820404, train_x2_loss=0.7570943832397461\n",
      "INFO:absl:[78] val_loss=1.8945990800857544\n",
      "INFO:absl:[78] test_loss=1.865091323852539\n",
      "INFO:absl:[79] train_loss=1.1939102411270142, train_x1_loss=0.43883636593818665, train_x2_loss=0.7550752758979797\n",
      "INFO:absl:[79] val_loss=1.8862676620483398\n",
      "INFO:absl:[79] test_loss=1.857033371925354\n",
      "INFO:absl:[80] train_loss=1.1963900327682495, train_x1_loss=0.43968456983566284, train_x2_loss=0.756705641746521\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[80] val_loss=1.8899601697921753\n",
      "INFO:absl:[80] test_loss=1.860311508178711\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[81] train_loss=1.1917080879211426, train_x1_loss=0.43445485830307007, train_x2_loss=0.7572515606880188\n",
      "INFO:absl:[81] val_loss=1.8905177116394043\n",
      "INFO:absl:[81] test_loss=1.861224889755249\n",
      "INFO:absl:[82] train_loss=1.1943981647491455, train_x1_loss=0.4379878640174866, train_x2_loss=0.7564108371734619\n",
      "INFO:absl:[82] val_loss=1.8877605199813843\n",
      "INFO:absl:[82] test_loss=1.8585865497589111\n",
      "INFO:absl:[83] train_loss=1.1917272806167603, train_x1_loss=0.4356484115123749, train_x2_loss=0.756078839302063\n",
      "INFO:absl:[83] val_loss=1.8824795484542847\n",
      "INFO:absl:[83] test_loss=1.853122591972351\n",
      "INFO:absl:[84] train_loss=1.195003867149353, train_x1_loss=0.4385963976383209, train_x2_loss=0.7564051151275635\n",
      "INFO:absl:[84] val_loss=1.8863319158554077\n",
      "INFO:absl:[84] test_loss=1.857189416885376\n",
      "INFO:absl:[85] train_loss=1.1920459270477295, train_x1_loss=0.4358748495578766, train_x2_loss=0.7561706304550171\n",
      "INFO:absl:[85] val_loss=1.8884069919586182\n",
      "INFO:absl:[85] test_loss=1.8589386940002441\n",
      "INFO:absl:[86] train_loss=1.1919602155685425, train_x1_loss=0.43590229749679565, train_x2_loss=0.7560557126998901\n",
      "INFO:absl:[86] val_loss=1.8880308866500854\n",
      "INFO:absl:[86] test_loss=1.8587243556976318\n",
      "INFO:absl:[87] train_loss=1.193505883216858, train_x1_loss=0.4393347203731537, train_x2_loss=0.7541707158088684\n",
      "INFO:absl:[87] val_loss=1.8918054103851318\n",
      "INFO:absl:[87] test_loss=1.8621739149093628\n",
      "INFO:absl:[88] train_loss=1.193272590637207, train_x1_loss=0.43540477752685547, train_x2_loss=0.7578665614128113\n",
      "INFO:absl:[88] val_loss=1.8899182081222534\n",
      "INFO:absl:[88] test_loss=1.8603836297988892\n",
      "INFO:absl:[89] train_loss=1.1914161443710327, train_x1_loss=0.4366970658302307, train_x2_loss=0.7547178268432617\n",
      "INFO:absl:[89] val_loss=1.887541651725769\n",
      "INFO:absl:[89] test_loss=1.8581558465957642\n",
      "INFO:absl:[90] train_loss=1.1915441751480103, train_x1_loss=0.4361426532268524, train_x2_loss=0.7553999423980713\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[90] val_loss=1.883020043373108\n",
      "INFO:absl:[90] test_loss=1.8540444374084473\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[91] train_loss=1.1953281164169312, train_x1_loss=0.43693655729293823, train_x2_loss=0.7583911418914795\n",
      "INFO:absl:[91] val_loss=1.8885843753814697\n",
      "INFO:absl:[91] test_loss=1.8592529296875\n",
      "INFO:absl:[92] train_loss=1.1891213655471802, train_x1_loss=0.4358305335044861, train_x2_loss=0.753292977809906\n",
      "INFO:absl:[92] val_loss=1.885191798210144\n",
      "INFO:absl:[92] test_loss=1.8558967113494873\n",
      "INFO:absl:[93] train_loss=1.1906436681747437, train_x1_loss=0.43729186058044434, train_x2_loss=0.7533515095710754\n",
      "INFO:absl:[93] val_loss=1.8876543045043945\n",
      "INFO:absl:[93] test_loss=1.8580371141433716\n",
      "INFO:absl:[94] train_loss=1.1910090446472168, train_x1_loss=0.43552592396736145, train_x2_loss=0.755480945110321\n",
      "INFO:absl:[94] val_loss=1.8847664594650269\n",
      "INFO:absl:[94] test_loss=1.8554707765579224\n",
      "INFO:absl:[95] train_loss=1.192214012145996, train_x1_loss=0.436740517616272, train_x2_loss=0.7554727792739868\n",
      "INFO:absl:[95] val_loss=1.8829890489578247\n",
      "INFO:absl:[95] test_loss=1.8535503149032593\n",
      "INFO:absl:[96] train_loss=1.188408613204956, train_x1_loss=0.4356437027454376, train_x2_loss=0.7527657151222229\n",
      "INFO:absl:[96] val_loss=1.8888258934020996\n",
      "INFO:absl:[96] test_loss=1.8593640327453613\n",
      "INFO:absl:[97] train_loss=1.1923555135726929, train_x1_loss=0.43623095750808716, train_x2_loss=0.756126880645752\n",
      "INFO:absl:[97] val_loss=1.8865957260131836\n",
      "INFO:absl:[97] test_loss=1.8572393655776978\n",
      "INFO:absl:[98] train_loss=1.1883763074874878, train_x1_loss=0.43553295731544495, train_x2_loss=0.7528445720672607\n",
      "INFO:absl:[98] val_loss=1.886584758758545\n",
      "INFO:absl:[98] test_loss=1.8569724559783936\n",
      "INFO:absl:[99] train_loss=1.1882052421569824, train_x1_loss=0.43436670303344727, train_x2_loss=0.7538385987281799\n",
      "INFO:absl:[99] val_loss=1.87993323802948\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[99] test_loss=1.850864291191101\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n"
     ]
    }
   ],
   "source": [
    "for trial in range(num_trials):\n",
    "    _, _, connected_3_eval_metric, connected_3_epoch_losses = train_and_evaluate_with_data(\n",
    "        config=connected_configs_3[trial], workdir=connected_workdirs_3[trial], datasets=all_connected_3_datasets[trial])\n",
    "    \n",
    "    connected_3_eval_metrics.append(connected_3_eval_metric)\n",
    "    \n",
    "    all_connected_3_epoch_losses.append(connected_3_epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'relu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 10, 'dropout_rate': 0.21664822298158384, 'edge_features': (4, 8), 'epochs': 100, 'eval_every_epochs': 1, 'fully_connected_edges': 1, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 1, 'layer_norm': False, 'learning_rate': 2.9648345553652954e-05, 'log_every_epochs': 1, 'max_checkpts_to_keep': 2, 'model': 'MLPGraphNetwork', 'n_blocks': 1, 'n_samples': 5000, 'node_features': (128, 2), 'normalize': True, 'optimizer': 'adam', 'output_delay': 0, 'output_steps': 4, 'sample_buffer': -4, 'seed': 65, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 120, 'timestep_duration': 3, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+-----------+-------+----------+--------+\n",
      "| Name                                   | Shape     | Size  | Mean     | Std    |\n",
      "+----------------------------------------+-----------+-------+----------+--------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (4,)      | 4     | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 4)    | 24    | -0.0819  | 0.36   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (8,)      | 8     | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (4, 8)    | 32    | -0.0639  | 0.439  |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (128,)    | 128   | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (19, 128) | 2,432 | -0.00462 | 0.228  |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)      | 2     | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (128, 2)  | 256   | -0.00622 | 0.0884 |\n",
      "+----------------------------------------+-----------+-------+----------+--------+\n",
      "Total: 2,886\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Checkpoint None does not exist.\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.01s.\n",
      "INFO:absl:Starting training.\n",
      "INFO:absl:[0] train_loss=1.489132285118103, train_x1_loss=0.6490700840950012, train_x2_loss=0.8400614261627197\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[0] val_loss=1.8077459335327148\n",
      "INFO:absl:[0] test_loss=1.8838838338851929\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Created artifact [10] Profile of type ArtifactType.URL and value None.\n",
      "INFO:absl:[1] train_loss=1.181442379951477, train_x1_loss=0.4247705638408661, train_x2_loss=0.756671667098999\n",
      "INFO:absl:[1] val_loss=1.7224318981170654\n",
      "INFO:absl:[1] test_loss=1.8008416891098022\n",
      "INFO:absl:[2] train_loss=1.1102697849273682, train_x1_loss=0.376824289560318, train_x2_loss=0.7334446310997009\n",
      "INFO:absl:[2] val_loss=1.700047492980957\n",
      "INFO:absl:[2] test_loss=1.7786918878555298\n",
      "INFO:absl:[3] train_loss=1.0784213542938232, train_x1_loss=0.35666754841804504, train_x2_loss=0.7217530608177185\n",
      "INFO:absl:[3] val_loss=1.6913402080535889\n",
      "INFO:absl:[3] test_loss=1.770919680595398\n",
      "INFO:absl:[4] train_loss=1.0620871782302856, train_x1_loss=0.35015442967414856, train_x2_loss=0.711934506893158\n",
      "INFO:absl:[4] val_loss=1.6748803853988647\n",
      "INFO:absl:[4] test_loss=1.7544773817062378\n",
      "INFO:absl:[5] train_loss=1.0559648275375366, train_x1_loss=0.3459275960922241, train_x2_loss=0.7100384831428528\n",
      "INFO:absl:[5] val_loss=1.6721994876861572\n",
      "INFO:absl:[5] test_loss=1.75265371799469\n",
      "INFO:absl:[6] train_loss=1.0449798107147217, train_x1_loss=0.33987611532211304, train_x2_loss=0.705102264881134\n",
      "INFO:absl:[6] val_loss=1.6598564386367798\n",
      "INFO:absl:[6] test_loss=1.7404204607009888\n",
      "INFO:absl:[7] train_loss=1.0409069061279297, train_x1_loss=0.3363652229309082, train_x2_loss=0.7045423984527588\n",
      "INFO:absl:[7] val_loss=1.6605111360549927\n",
      "INFO:absl:[7] test_loss=1.7407424449920654\n",
      "INFO:absl:[8] train_loss=1.03922438621521, train_x1_loss=0.33691418170928955, train_x2_loss=0.7023085951805115\n",
      "INFO:absl:[8] val_loss=1.6600712537765503\n",
      "INFO:absl:[8] test_loss=1.7406586408615112\n",
      "INFO:absl:[9] train_loss=1.0357251167297363, train_x1_loss=0.33584606647491455, train_x2_loss=0.6998785138130188\n",
      "INFO:absl:[9] val_loss=1.65853750705719\n",
      "INFO:absl:[9] test_loss=1.739371418952942\n",
      "INFO:absl:[10] train_loss=1.036615014076233, train_x1_loss=0.3344503343105316, train_x2_loss=0.7021657824516296\n",
      "INFO:absl:[10] val_loss=1.650892972946167\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[10] test_loss=1.7313990592956543\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[11] train_loss=1.03557288646698, train_x1_loss=0.33387482166290283, train_x2_loss=0.7016974687576294\n",
      "INFO:absl:[11] val_loss=1.6612468957901\n",
      "INFO:absl:[11] test_loss=1.7420668601989746\n",
      "INFO:absl:[12] train_loss=1.027809977531433, train_x1_loss=0.32808899879455566, train_x2_loss=0.6997200846672058\n",
      "INFO:absl:[12] val_loss=1.6457006931304932\n",
      "INFO:absl:[12] test_loss=1.726377248764038\n",
      "INFO:absl:[13] train_loss=1.0269204378128052, train_x1_loss=0.3304913640022278, train_x2_loss=0.6964319348335266\n",
      "INFO:absl:[13] val_loss=1.6460177898406982\n",
      "INFO:absl:[13] test_loss=1.726461410522461\n",
      "INFO:absl:[14] train_loss=1.0267906188964844, train_x1_loss=0.3290726840496063, train_x2_loss=0.6977198123931885\n",
      "INFO:absl:[14] val_loss=1.6449437141418457\n",
      "INFO:absl:[14] test_loss=1.7254931926727295\n",
      "INFO:absl:[15] train_loss=1.033103108406067, train_x1_loss=0.3324110507965088, train_x2_loss=0.7006919384002686\n",
      "INFO:absl:[15] val_loss=1.65003502368927\n",
      "INFO:absl:[15] test_loss=1.7306311130523682\n",
      "INFO:absl:[16] train_loss=1.0288691520690918, train_x1_loss=0.33135583996772766, train_x2_loss=0.6975136995315552\n",
      "INFO:absl:[16] val_loss=1.644934058189392\n",
      "INFO:absl:[16] test_loss=1.725354790687561\n",
      "INFO:absl:[17] train_loss=1.0251816511154175, train_x1_loss=0.32799747586250305, train_x2_loss=0.6971832513809204\n",
      "INFO:absl:[17] val_loss=1.643588662147522\n",
      "INFO:absl:[17] test_loss=1.7242271900177002\n",
      "INFO:absl:[18] train_loss=1.0279362201690674, train_x1_loss=0.33037269115448, train_x2_loss=0.6975630521774292\n",
      "INFO:absl:[18] val_loss=1.6436994075775146\n",
      "INFO:absl:[18] test_loss=1.7244327068328857\n",
      "INFO:absl:[19] train_loss=1.027140498161316, train_x1_loss=0.329933762550354, train_x2_loss=0.6972067952156067\n",
      "INFO:absl:[19] val_loss=1.643930196762085\n",
      "INFO:absl:[19] test_loss=1.7244305610656738\n",
      "INFO:absl:[20] train_loss=1.0281774997711182, train_x1_loss=0.3298599421977997, train_x2_loss=0.69831782579422\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[20] val_loss=1.6461920738220215\n",
      "INFO:absl:[20] test_loss=1.72700834274292\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[21] train_loss=1.0252307653427124, train_x1_loss=0.32942259311676025, train_x2_loss=0.6958079934120178\n",
      "INFO:absl:[21] val_loss=1.6409891843795776\n",
      "INFO:absl:[21] test_loss=1.7213499546051025\n",
      "INFO:absl:[22] train_loss=1.024727702140808, train_x1_loss=0.32852351665496826, train_x2_loss=0.6962050795555115\n",
      "INFO:absl:[22] val_loss=1.640998125076294\n",
      "INFO:absl:[22] test_loss=1.7214281558990479\n",
      "INFO:absl:[23] train_loss=1.0269806385040283, train_x1_loss=0.3285885155200958, train_x2_loss=0.6983917355537415\n",
      "INFO:absl:[23] val_loss=1.6414461135864258\n",
      "INFO:absl:[23] test_loss=1.7215650081634521\n",
      "INFO:absl:[24] train_loss=1.026052474975586, train_x1_loss=0.3299044072628021, train_x2_loss=0.6961473226547241\n",
      "INFO:absl:[24] val_loss=1.6459823846817017\n",
      "INFO:absl:[24] test_loss=1.7264784574508667\n",
      "INFO:absl:[25] train_loss=1.0234110355377197, train_x1_loss=0.3282589614391327, train_x2_loss=0.6951512098312378\n",
      "INFO:absl:[25] val_loss=1.6355385780334473\n",
      "INFO:absl:[25] test_loss=1.7158082723617554\n",
      "INFO:absl:[26] train_loss=1.0223091840744019, train_x1_loss=0.3280683159828186, train_x2_loss=0.6942411661148071\n",
      "INFO:absl:[26] val_loss=1.642496943473816\n",
      "INFO:absl:[26] test_loss=1.7227755784988403\n",
      "INFO:absl:[27] train_loss=1.0241751670837402, train_x1_loss=0.3273761570453644, train_x2_loss=0.6967995762825012\n",
      "INFO:absl:[27] val_loss=1.6503993272781372\n",
      "INFO:absl:[27] test_loss=1.730697512626648\n",
      "INFO:absl:[28] train_loss=1.0237208604812622, train_x1_loss=0.32904425263404846, train_x2_loss=0.6946761012077332\n",
      "INFO:absl:[28] val_loss=1.6441737413406372\n",
      "INFO:absl:[28] test_loss=1.7245620489120483\n",
      "INFO:absl:[29] train_loss=1.0249642133712769, train_x1_loss=0.32837849855422974, train_x2_loss=0.6965850591659546\n",
      "INFO:absl:[29] val_loss=1.6433442831039429\n",
      "INFO:absl:[29] test_loss=1.7240294218063354\n",
      "INFO:absl:[30] train_loss=1.0231088399887085, train_x1_loss=0.3271438479423523, train_x2_loss=0.695964515209198\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[30] val_loss=1.6390070915222168\n",
      "INFO:absl:[30] test_loss=1.7192330360412598\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[31] train_loss=1.0253660678863525, train_x1_loss=0.32915666699409485, train_x2_loss=0.6962102651596069\n",
      "INFO:absl:[31] val_loss=1.6441928148269653\n",
      "INFO:absl:[31] test_loss=1.7243614196777344\n",
      "INFO:absl:[32] train_loss=1.02373206615448, train_x1_loss=0.3285227119922638, train_x2_loss=0.695210337638855\n",
      "INFO:absl:[32] val_loss=1.6402506828308105\n",
      "INFO:absl:[32] test_loss=1.72040855884552\n",
      "INFO:absl:[33] train_loss=1.0270322561264038, train_x1_loss=0.3308950960636139, train_x2_loss=0.696137011051178\n",
      "INFO:absl:[33] val_loss=1.649545669555664\n",
      "INFO:absl:[33] test_loss=1.7297202348709106\n",
      "INFO:absl:[34] train_loss=1.0223596096038818, train_x1_loss=0.327834814786911, train_x2_loss=0.6945246458053589\n",
      "INFO:absl:[34] val_loss=1.6452113389968872\n",
      "INFO:absl:[34] test_loss=1.7254610061645508\n",
      "INFO:absl:[35] train_loss=1.0193954706192017, train_x1_loss=0.3254525363445282, train_x2_loss=0.6939437985420227\n",
      "INFO:absl:[35] val_loss=1.6421973705291748\n",
      "INFO:absl:[35] test_loss=1.722627878189087\n",
      "INFO:absl:[36] train_loss=1.020485758781433, train_x1_loss=0.3278641998767853, train_x2_loss=0.6926206350326538\n",
      "INFO:absl:[36] val_loss=1.644153356552124\n",
      "INFO:absl:[36] test_loss=1.7245324850082397\n",
      "INFO:absl:Setting work unit notes: 2192.8 steps/s, 37.6% (131570/350000), ETA: 1m (1m : 0.1% checkpoint, 9.0% eval)\n",
      "INFO:absl:[131570] steps_per_sec=2192.823792\n",
      "INFO:absl:[37] train_loss=1.0214743614196777, train_x1_loss=0.32768410444259644, train_x2_loss=0.6937887668609619\n",
      "INFO:absl:[37] val_loss=1.6464020013809204\n",
      "INFO:absl:[37] test_loss=1.7267624139785767\n",
      "INFO:absl:[38] train_loss=1.0226151943206787, train_x1_loss=0.32773852348327637, train_x2_loss=0.6948761343955994\n",
      "INFO:absl:[38] val_loss=1.6393908262252808\n",
      "INFO:absl:[38] test_loss=1.7195276021957397\n",
      "INFO:absl:[39] train_loss=1.0189622640609741, train_x1_loss=0.3250545859336853, train_x2_loss=0.6939089894294739\n",
      "INFO:absl:[39] val_loss=1.6361100673675537\n",
      "INFO:absl:[39] test_loss=1.7157621383666992\n",
      "INFO:absl:[40] train_loss=1.0181680917739868, train_x1_loss=0.3257320821285248, train_x2_loss=0.6924359798431396\n",
      "INFO:absl:[40] val_loss=1.6390652656555176\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[40] test_loss=1.718864917755127\n",
      "INFO:absl:Checkpoint.save() finished after 0.04s.\n",
      "INFO:absl:[41] train_loss=1.0220589637756348, train_x1_loss=0.32834944128990173, train_x2_loss=0.6937083601951599\n",
      "INFO:absl:[41] val_loss=1.6449365615844727\n",
      "INFO:absl:[41] test_loss=1.7251445055007935\n",
      "INFO:absl:[42] train_loss=1.0213266611099243, train_x1_loss=0.3279738426208496, train_x2_loss=0.6933510303497314\n",
      "INFO:absl:[42] val_loss=1.6384614706039429\n",
      "INFO:absl:[42] test_loss=1.7188206911087036\n",
      "INFO:absl:[43] train_loss=1.0195350646972656, train_x1_loss=0.3258303999900818, train_x2_loss=0.6937021613121033\n",
      "INFO:absl:[43] val_loss=1.6449366807937622\n",
      "INFO:absl:[43] test_loss=1.72507643699646\n",
      "INFO:absl:[44] train_loss=1.0259498357772827, train_x1_loss=0.3307476341724396, train_x2_loss=0.6952017545700073\n",
      "INFO:absl:[44] val_loss=1.6445493698120117\n",
      "INFO:absl:[44] test_loss=1.7245697975158691\n",
      "INFO:absl:[45] train_loss=1.022234559059143, train_x1_loss=0.3266390562057495, train_x2_loss=0.6955984830856323\n",
      "INFO:absl:[45] val_loss=1.6442042589187622\n",
      "INFO:absl:[45] test_loss=1.7244188785552979\n",
      "INFO:absl:[46] train_loss=1.0243422985076904, train_x1_loss=0.32887551188468933, train_x2_loss=0.6954671740531921\n",
      "INFO:absl:[46] val_loss=1.6471021175384521\n",
      "INFO:absl:[46] test_loss=1.7273629903793335\n",
      "INFO:absl:[47] train_loss=1.0205711126327515, train_x1_loss=0.3277665078639984, train_x2_loss=0.692804753780365\n",
      "INFO:absl:[47] val_loss=1.6471874713897705\n",
      "INFO:absl:[47] test_loss=1.727182149887085\n",
      "INFO:absl:[48] train_loss=1.0231797695159912, train_x1_loss=0.32711321115493774, train_x2_loss=0.6960658431053162\n",
      "INFO:absl:[48] val_loss=1.6394320726394653\n",
      "INFO:absl:[48] test_loss=1.719541072845459\n",
      "INFO:absl:[49] train_loss=1.022865891456604, train_x1_loss=0.32672807574272156, train_x2_loss=0.6961367130279541\n",
      "INFO:absl:[49] val_loss=1.6402126550674438\n",
      "INFO:absl:[49] test_loss=1.7202913761138916\n",
      "INFO:absl:[50] train_loss=1.0189412832260132, train_x1_loss=0.32477232813835144, train_x2_loss=0.6941690444946289\n",
      "INFO:absl:[50] val_loss=1.6400607824325562\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[50] test_loss=1.7200508117675781\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[51] train_loss=1.0212879180908203, train_x1_loss=0.32792365550994873, train_x2_loss=0.693365216255188\n",
      "INFO:absl:[51] val_loss=1.6416481733322144\n",
      "INFO:absl:[51] test_loss=1.7217648029327393\n",
      "INFO:absl:[52] train_loss=1.0238122940063477, train_x1_loss=0.32803794741630554, train_x2_loss=0.6957744359970093\n",
      "INFO:absl:[52] val_loss=1.6435192823410034\n",
      "INFO:absl:[52] test_loss=1.723399043083191\n",
      "INFO:absl:[53] train_loss=1.0210881233215332, train_x1_loss=0.32554349303245544, train_x2_loss=0.6955447793006897\n",
      "INFO:absl:[53] val_loss=1.640000820159912\n",
      "INFO:absl:[53] test_loss=1.720086932182312\n",
      "INFO:absl:[54] train_loss=1.0226495265960693, train_x1_loss=0.32781124114990234, train_x2_loss=0.6948357820510864\n",
      "INFO:absl:[54] val_loss=1.637514591217041\n",
      "INFO:absl:[54] test_loss=1.7175081968307495\n",
      "INFO:absl:[55] train_loss=1.0188589096069336, train_x1_loss=0.32472142577171326, train_x2_loss=0.6941385269165039\n",
      "INFO:absl:[55] val_loss=1.644966959953308\n",
      "INFO:absl:[55] test_loss=1.72503662109375\n",
      "INFO:absl:[56] train_loss=1.0234044790267944, train_x1_loss=0.3275816738605499, train_x2_loss=0.6958221793174744\n",
      "INFO:absl:[56] val_loss=1.6448071002960205\n",
      "INFO:absl:[56] test_loss=1.7248317003250122\n",
      "INFO:absl:[57] train_loss=1.0227890014648438, train_x1_loss=0.32718339562416077, train_x2_loss=0.6956034302711487\n",
      "INFO:absl:[57] val_loss=1.6413642168045044\n",
      "INFO:absl:[57] test_loss=1.7214446067810059\n",
      "INFO:absl:[58] train_loss=1.0184184312820435, train_x1_loss=0.32540610432624817, train_x2_loss=0.6930127739906311\n",
      "INFO:absl:[58] val_loss=1.6386789083480835\n",
      "INFO:absl:[58] test_loss=1.7186576128005981\n",
      "INFO:absl:[59] train_loss=1.0232667922973633, train_x1_loss=0.3285121023654938, train_x2_loss=0.6947560906410217\n",
      "INFO:absl:[59] val_loss=1.6427490711212158\n",
      "INFO:absl:[59] test_loss=1.7225133180618286\n",
      "INFO:absl:[60] train_loss=1.0260511636734009, train_x1_loss=0.32905665040016174, train_x2_loss=0.6969931721687317\n",
      "INFO:absl:[60] val_loss=1.6446281671524048\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[60] test_loss=1.7246136665344238\n",
      "INFO:absl:Checkpoint.save() finished after 0.04s.\n",
      "INFO:absl:[61] train_loss=1.024797797203064, train_x1_loss=0.3287912905216217, train_x2_loss=0.6960065364837646\n",
      "INFO:absl:[61] val_loss=1.6420260667800903\n",
      "INFO:absl:[61] test_loss=1.7223010063171387\n",
      "INFO:absl:[62] train_loss=1.0238349437713623, train_x1_loss=0.3267640173435211, train_x2_loss=0.6970689296722412\n",
      "INFO:absl:[62] val_loss=1.6401593685150146\n",
      "INFO:absl:[62] test_loss=1.7201061248779297\n",
      "INFO:absl:[63] train_loss=1.0208652019500732, train_x1_loss=0.3268359303474426, train_x2_loss=0.6940294504165649\n",
      "INFO:absl:[63] val_loss=1.644119381904602\n",
      "INFO:absl:[63] test_loss=1.7240126132965088\n",
      "INFO:absl:[64] train_loss=1.0250235795974731, train_x1_loss=0.328853577375412, train_x2_loss=0.6961709856987\n",
      "INFO:absl:[64] val_loss=1.6447739601135254\n",
      "INFO:absl:[64] test_loss=1.7247915267944336\n",
      "INFO:absl:[65] train_loss=1.0208181142807007, train_x1_loss=0.3262510597705841, train_x2_loss=0.6945669651031494\n",
      "INFO:absl:[65] val_loss=1.6418466567993164\n",
      "INFO:absl:[65] test_loss=1.7217719554901123\n",
      "INFO:absl:[66] train_loss=1.023749828338623, train_x1_loss=0.32810500264167786, train_x2_loss=0.6956451535224915\n",
      "INFO:absl:[66] val_loss=1.64290189743042\n",
      "INFO:absl:[66] test_loss=1.7228878736495972\n",
      "INFO:absl:[67] train_loss=1.0226075649261475, train_x1_loss=0.32642611861228943, train_x2_loss=0.6961786150932312\n",
      "INFO:absl:[67] val_loss=1.641488790512085\n",
      "INFO:absl:[67] test_loss=1.721839427947998\n",
      "INFO:absl:[68] train_loss=1.0207759141921997, train_x1_loss=0.32564932107925415, train_x2_loss=0.6951286196708679\n",
      "INFO:absl:[68] val_loss=1.6422308683395386\n",
      "INFO:absl:[68] test_loss=1.7224034070968628\n",
      "INFO:absl:[69] train_loss=1.0216337442398071, train_x1_loss=0.3274150490760803, train_x2_loss=0.6942191123962402\n",
      "INFO:absl:[69] val_loss=1.642160177230835\n",
      "INFO:absl:[69] test_loss=1.7221667766571045\n",
      "INFO:absl:[70] train_loss=1.02333402633667, train_x1_loss=0.3291773796081543, train_x2_loss=0.6941562294960022\n",
      "INFO:absl:[70] val_loss=1.6402004957199097\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[70] test_loss=1.719988226890564\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[71] train_loss=1.02091646194458, train_x1_loss=0.32493001222610474, train_x2_loss=0.6959872245788574\n",
      "INFO:absl:[71] val_loss=1.6430796384811401\n",
      "INFO:absl:[71] test_loss=1.7232859134674072\n",
      "INFO:absl:[72] train_loss=1.0205177068710327, train_x1_loss=0.325610488653183, train_x2_loss=0.6949074268341064\n",
      "INFO:absl:[72] val_loss=1.6446949243545532\n",
      "INFO:absl:[72] test_loss=1.7249112129211426\n",
      "INFO:absl:[73] train_loss=1.0215816497802734, train_x1_loss=0.3263189196586609, train_x2_loss=0.6952630877494812\n",
      "INFO:absl:[73] val_loss=1.6415650844573975\n",
      "INFO:absl:[73] test_loss=1.7217411994934082\n",
      "INFO:absl:[74] train_loss=1.0210292339324951, train_x1_loss=0.3276401162147522, train_x2_loss=0.6933891773223877\n",
      "INFO:absl:[74] val_loss=1.640318512916565\n",
      "INFO:absl:[74] test_loss=1.7202075719833374\n",
      "INFO:absl:[75] train_loss=1.0203981399536133, train_x1_loss=0.3257125914096832, train_x2_loss=0.6946849822998047\n",
      "INFO:absl:[75] val_loss=1.6410285234451294\n",
      "INFO:absl:[75] test_loss=1.7210768461227417\n",
      "INFO:absl:[76] train_loss=1.0214546918869019, train_x1_loss=0.32622095942497253, train_x2_loss=0.6952330470085144\n",
      "INFO:absl:[76] val_loss=1.6425085067749023\n",
      "INFO:absl:[76] test_loss=1.722744107246399\n",
      "INFO:absl:[77] train_loss=1.0233770608901978, train_x1_loss=0.32794851064682007, train_x2_loss=0.6954299807548523\n",
      "INFO:absl:[77] val_loss=1.646340250968933\n",
      "INFO:absl:[77] test_loss=1.726627230644226\n",
      "INFO:absl:[78] train_loss=1.0206190347671509, train_x1_loss=0.3253448009490967, train_x2_loss=0.6952746510505676\n",
      "INFO:absl:[78] val_loss=1.6478484869003296\n",
      "INFO:absl:[78] test_loss=1.7277213335037231\n",
      "INFO:absl:[79] train_loss=1.0223753452301025, train_x1_loss=0.3278833329677582, train_x2_loss=0.6944934725761414\n",
      "INFO:absl:[79] val_loss=1.6443889141082764\n",
      "INFO:absl:[79] test_loss=1.724682092666626\n",
      "INFO:absl:[80] train_loss=1.0200122594833374, train_x1_loss=0.32460126280784607, train_x2_loss=0.6954103112220764\n",
      "INFO:absl:[80] val_loss=1.6396816968917847\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[80] test_loss=1.7196449041366577\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Setting work unit notes: 2567.6 steps/s, 81.6% (285628/350000), ETA: 0m (2m : 0.1% checkpoint, 8.9% eval)\n",
      "INFO:absl:[285628] steps_per_sec=2567.623355\n",
      "INFO:absl:[81] train_loss=1.0195224285125732, train_x1_loss=0.32575881481170654, train_x2_loss=0.6937617659568787\n",
      "INFO:absl:[81] val_loss=1.6498771905899048\n",
      "INFO:absl:[81] test_loss=1.7298134565353394\n",
      "INFO:absl:[82] train_loss=1.0201799869537354, train_x1_loss=0.32658207416534424, train_x2_loss=0.6935974955558777\n",
      "INFO:absl:[82] val_loss=1.640573501586914\n",
      "INFO:absl:[82] test_loss=1.7207744121551514\n",
      "INFO:absl:[83] train_loss=1.0152286291122437, train_x1_loss=0.32421207427978516, train_x2_loss=0.6910173892974854\n",
      "INFO:absl:[83] val_loss=1.646937370300293\n",
      "INFO:absl:[83] test_loss=1.7271740436553955\n",
      "INFO:absl:[84] train_loss=1.019240140914917, train_x1_loss=0.3266441524028778, train_x2_loss=0.6925955414772034\n",
      "INFO:absl:[84] val_loss=1.6424049139022827\n",
      "INFO:absl:[84] test_loss=1.7228487730026245\n",
      "INFO:absl:[85] train_loss=1.0229908227920532, train_x1_loss=0.32737359404563904, train_x2_loss=0.6956180930137634\n",
      "INFO:absl:[85] val_loss=1.634261131286621\n",
      "INFO:absl:[85] test_loss=1.7144182920455933\n",
      "INFO:absl:[86] train_loss=1.023637294769287, train_x1_loss=0.32773810625076294, train_x2_loss=0.6958986520767212\n",
      "INFO:absl:[86] val_loss=1.6462514400482178\n",
      "INFO:absl:[86] test_loss=1.7265887260437012\n",
      "INFO:absl:[87] train_loss=1.0224071741104126, train_x1_loss=0.32681864500045776, train_x2_loss=0.6955873966217041\n",
      "INFO:absl:[87] val_loss=1.646383285522461\n",
      "INFO:absl:[87] test_loss=1.7265928983688354\n",
      "INFO:absl:[88] train_loss=1.0194077491760254, train_x1_loss=0.32544824481010437, train_x2_loss=0.693959653377533\n",
      "INFO:absl:[88] val_loss=1.6474967002868652\n",
      "INFO:absl:[88] test_loss=1.7277203798294067\n",
      "INFO:absl:[89] train_loss=1.0232938528060913, train_x1_loss=0.3277320861816406, train_x2_loss=0.6955608129501343\n",
      "INFO:absl:[89] val_loss=1.6447563171386719\n",
      "INFO:absl:[89] test_loss=1.7245367765426636\n",
      "INFO:absl:[90] train_loss=1.0180855989456177, train_x1_loss=0.3236270546913147, train_x2_loss=0.6944586634635925\n",
      "INFO:absl:[90] val_loss=1.641967535018921\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[90] test_loss=1.7220436334609985\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[91] train_loss=1.0212868452072144, train_x1_loss=0.32740479707717896, train_x2_loss=0.693882942199707\n",
      "INFO:absl:[91] val_loss=1.642350673675537\n",
      "INFO:absl:[91] test_loss=1.722376823425293\n",
      "INFO:absl:[92] train_loss=1.0206947326660156, train_x1_loss=0.3254157304763794, train_x2_loss=0.6952792406082153\n",
      "INFO:absl:[92] val_loss=1.6483485698699951\n",
      "INFO:absl:[92] test_loss=1.728610634803772\n",
      "INFO:absl:[93] train_loss=1.0171246528625488, train_x1_loss=0.323337197303772, train_x2_loss=0.6937857866287231\n",
      "INFO:absl:[93] val_loss=1.6397234201431274\n",
      "INFO:absl:[93] test_loss=1.71981680393219\n",
      "INFO:absl:[94] train_loss=1.0181182622909546, train_x1_loss=0.325290709733963, train_x2_loss=0.6928276419639587\n",
      "INFO:absl:[94] val_loss=1.6443967819213867\n",
      "INFO:absl:[94] test_loss=1.7246202230453491\n",
      "INFO:absl:[95] train_loss=1.0190584659576416, train_x1_loss=0.3253259062767029, train_x2_loss=0.6937339305877686\n",
      "INFO:absl:[95] val_loss=1.644010066986084\n",
      "INFO:absl:[95] test_loss=1.7240642309188843\n",
      "INFO:absl:[96] train_loss=1.0180968046188354, train_x1_loss=0.32414212822914124, train_x2_loss=0.69395512342453\n",
      "INFO:absl:[96] val_loss=1.646943211555481\n",
      "INFO:absl:[96] test_loss=1.727366328239441\n",
      "INFO:absl:[97] train_loss=1.0217103958129883, train_x1_loss=0.32513710856437683, train_x2_loss=0.6965750455856323\n",
      "INFO:absl:[97] val_loss=1.6450612545013428\n",
      "INFO:absl:[97] test_loss=1.725165843963623\n",
      "INFO:absl:[98] train_loss=1.0192983150482178, train_x1_loss=0.326569139957428, train_x2_loss=0.6927293539047241\n",
      "INFO:absl:[98] val_loss=1.6398886442184448\n",
      "INFO:absl:[98] test_loss=1.7197895050048828\n",
      "INFO:absl:[99] train_loss=1.0161521434783936, train_x1_loss=0.32287389039993286, train_x2_loss=0.6932775974273682\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[99] val_loss=1.6372369527816772\n",
      "INFO:absl:[99] test_loss=1.7171653509140015\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'relu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 10, 'dropout_rate': 0.21664822298158384, 'edge_features': (4, 8), 'epochs': 100, 'eval_every_epochs': 1, 'fully_connected_edges': 1, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 1, 'layer_norm': False, 'learning_rate': 2.9648345553652954e-05, 'log_every_epochs': 1, 'max_checkpts_to_keep': 2, 'model': 'MLPGraphNetwork', 'n_blocks': 1, 'n_samples': 5000, 'node_features': (128, 2), 'normalize': True, 'optimizer': 'adam', 'output_delay': 0, 'output_steps': 4, 'sample_buffer': -4, 'seed': 66, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 120, 'timestep_duration': 3, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+-----------+-------+----------+--------+\n",
      "| Name                                   | Shape     | Size  | Mean     | Std    |\n",
      "+----------------------------------------+-----------+-------+----------+--------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (4,)      | 4     | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 4)    | 24    | -0.0819  | 0.36   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (8,)      | 8     | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (4, 8)    | 32    | -0.0639  | 0.439  |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (128,)    | 128   | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (19, 128) | 2,432 | -0.00462 | 0.228  |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)      | 2     | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (128, 2)  | 256   | -0.00622 | 0.0884 |\n",
      "+----------------------------------------+-----------+-------+----------+--------+\n",
      "Total: 2,886\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Checkpoint None does not exist.\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.00s.\n",
      "INFO:absl:Starting training.\n",
      "INFO:absl:[0] train_loss=1.5157485008239746, train_x1_loss=0.6712642312049866, train_x2_loss=0.8444846272468567\n",
      "INFO:absl:[0] val_loss=1.8378771543502808\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[0] test_loss=1.8746938705444336\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Created artifact [10] Profile of type ArtifactType.URL and value None.\n",
      "INFO:absl:[1] train_loss=1.2020344734191895, train_x1_loss=0.4378174841403961, train_x2_loss=0.7642176151275635\n",
      "INFO:absl:[1] val_loss=1.7154839038848877\n",
      "INFO:absl:[1] test_loss=1.7549062967300415\n",
      "INFO:absl:[2] train_loss=1.1226824522018433, train_x1_loss=0.3868425786495209, train_x2_loss=0.7358399033546448\n",
      "INFO:absl:[2] val_loss=1.7002543210983276\n",
      "INFO:absl:[2] test_loss=1.740081548690796\n",
      "INFO:absl:[3] train_loss=1.0866680145263672, train_x1_loss=0.36581453680992126, train_x2_loss=0.7208523750305176\n",
      "INFO:absl:[3] val_loss=1.6782397031784058\n",
      "INFO:absl:[3] test_loss=1.718014121055603\n",
      "INFO:absl:[4] train_loss=1.0620583295822144, train_x1_loss=0.3509020507335663, train_x2_loss=0.7111557126045227\n",
      "INFO:absl:[4] val_loss=1.6657893657684326\n",
      "INFO:absl:[4] test_loss=1.7058688402175903\n",
      "INFO:absl:[5] train_loss=1.0569263696670532, train_x1_loss=0.348747581243515, train_x2_loss=0.7081787586212158\n",
      "INFO:absl:[5] val_loss=1.657107949256897\n",
      "INFO:absl:[5] test_loss=1.6968419551849365\n",
      "INFO:absl:[6] train_loss=1.049176573753357, train_x1_loss=0.34358322620391846, train_x2_loss=0.7055943608283997\n",
      "INFO:absl:[6] val_loss=1.6597882509231567\n",
      "INFO:absl:[6] test_loss=1.69978928565979\n",
      "INFO:absl:[7] train_loss=1.0450010299682617, train_x1_loss=0.342259019613266, train_x2_loss=0.7027420997619629\n",
      "INFO:absl:[7] val_loss=1.655361533164978\n",
      "INFO:absl:[7] test_loss=1.6947345733642578\n",
      "INFO:absl:[8] train_loss=1.0360217094421387, train_x1_loss=0.33802542090415955, train_x2_loss=0.6979960203170776\n",
      "INFO:absl:[8] val_loss=1.6536641120910645\n",
      "INFO:absl:[8] test_loss=1.6934400796890259\n",
      "INFO:absl:[9] train_loss=1.032755732536316, train_x1_loss=0.3358871340751648, train_x2_loss=0.696869969367981\n",
      "INFO:absl:[9] val_loss=1.6471638679504395\n",
      "INFO:absl:[9] test_loss=1.6867225170135498\n",
      "INFO:absl:[10] train_loss=1.034506916999817, train_x1_loss=0.33632439374923706, train_x2_loss=0.6981825232505798\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[10] val_loss=1.641295075416565\n",
      "INFO:absl:[10] test_loss=1.6809195280075073\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[11] train_loss=1.03517746925354, train_x1_loss=0.33677029609680176, train_x2_loss=0.6984086632728577\n",
      "INFO:absl:[11] val_loss=1.643692135810852\n",
      "INFO:absl:[11] test_loss=1.6835209131240845\n",
      "INFO:absl:[12] train_loss=1.0276647806167603, train_x1_loss=0.33174675703048706, train_x2_loss=0.6959171891212463\n",
      "INFO:absl:[12] val_loss=1.6401058435440063\n",
      "INFO:absl:[12] test_loss=1.6795032024383545\n",
      "INFO:absl:[13] train_loss=1.0300085544586182, train_x1_loss=0.3330036699771881, train_x2_loss=0.6970043182373047\n",
      "INFO:absl:[13] val_loss=1.644688367843628\n",
      "INFO:absl:[13] test_loss=1.6843860149383545\n",
      "INFO:absl:[14] train_loss=1.028550386428833, train_x1_loss=0.33184486627578735, train_x2_loss=0.6967063546180725\n",
      "INFO:absl:[14] val_loss=1.6343843936920166\n",
      "INFO:absl:[14] test_loss=1.6738377809524536\n",
      "INFO:absl:[15] train_loss=1.0287076234817505, train_x1_loss=0.3357575237751007, train_x2_loss=0.6929522752761841\n",
      "INFO:absl:[15] val_loss=1.6342278718948364\n",
      "INFO:absl:[15] test_loss=1.6744434833526611\n",
      "INFO:absl:[16] train_loss=1.025207281112671, train_x1_loss=0.3335849344730377, train_x2_loss=0.691622257232666\n",
      "INFO:absl:[16] val_loss=1.6284109354019165\n",
      "INFO:absl:[16] test_loss=1.6682641506195068\n",
      "INFO:absl:[17] train_loss=1.023108720779419, train_x1_loss=0.3299843668937683, train_x2_loss=0.6931251883506775\n",
      "INFO:absl:[17] val_loss=1.6315070390701294\n",
      "INFO:absl:[17] test_loss=1.6712429523468018\n",
      "INFO:absl:[18] train_loss=1.028586745262146, train_x1_loss=0.3347385823726654, train_x2_loss=0.6938489079475403\n",
      "INFO:absl:[18] val_loss=1.641629934310913\n",
      "INFO:absl:[18] test_loss=1.6815071105957031\n",
      "INFO:absl:[19] train_loss=1.0274288654327393, train_x1_loss=0.3334307074546814, train_x2_loss=0.693999707698822\n",
      "INFO:absl:[19] val_loss=1.638466477394104\n",
      "INFO:absl:[19] test_loss=1.6779143810272217\n",
      "INFO:absl:[20] train_loss=1.0287761688232422, train_x1_loss=0.33512169122695923, train_x2_loss=0.693656861782074\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[20] val_loss=1.636218547821045\n",
      "INFO:absl:[20] test_loss=1.6759754419326782\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[21] train_loss=1.0224575996398926, train_x1_loss=0.3293141722679138, train_x2_loss=0.693142831325531\n",
      "INFO:absl:[21] val_loss=1.643917441368103\n",
      "INFO:absl:[21] test_loss=1.6838935613632202\n",
      "INFO:absl:[22] train_loss=1.0246981382369995, train_x1_loss=0.3310348093509674, train_x2_loss=0.6936632990837097\n",
      "INFO:absl:[22] val_loss=1.628665804862976\n",
      "INFO:absl:[22] test_loss=1.6682740449905396\n",
      "INFO:absl:[23] train_loss=1.030435562133789, train_x1_loss=0.3351004123687744, train_x2_loss=0.6953362822532654\n",
      "INFO:absl:[23] val_loss=1.634605884552002\n",
      "INFO:absl:[23] test_loss=1.6742819547653198\n",
      "INFO:absl:[24] train_loss=1.026801586151123, train_x1_loss=0.3320324122905731, train_x2_loss=0.6947700381278992\n",
      "INFO:absl:[24] val_loss=1.6417535543441772\n",
      "INFO:absl:[24] test_loss=1.6822885274887085\n",
      "INFO:absl:[25] train_loss=1.025396704673767, train_x1_loss=0.3314470648765564, train_x2_loss=0.6939473152160645\n",
      "INFO:absl:[25] val_loss=1.6370973587036133\n",
      "INFO:absl:[25] test_loss=1.676846981048584\n",
      "INFO:absl:[26] train_loss=1.0252187252044678, train_x1_loss=0.3319721817970276, train_x2_loss=0.6932451725006104\n",
      "INFO:absl:[26] val_loss=1.6378486156463623\n",
      "INFO:absl:[26] test_loss=1.6778571605682373\n",
      "INFO:absl:[27] train_loss=1.0216954946517944, train_x1_loss=0.3296891748905182, train_x2_loss=0.6920046210289001\n",
      "INFO:absl:[27] val_loss=1.6353638172149658\n",
      "INFO:absl:[27] test_loss=1.674938440322876\n",
      "INFO:absl:[28] train_loss=1.0274168252944946, train_x1_loss=0.3312778174877167, train_x2_loss=0.6961404085159302\n",
      "INFO:absl:[28] val_loss=1.6376410722732544\n",
      "INFO:absl:[28] test_loss=1.6776634454727173\n",
      "INFO:absl:[29] train_loss=1.0247060060501099, train_x1_loss=0.33148765563964844, train_x2_loss=0.6932196617126465\n",
      "INFO:absl:[29] val_loss=1.6369593143463135\n",
      "INFO:absl:[29] test_loss=1.677229881286621\n",
      "INFO:absl:[30] train_loss=1.0211684703826904, train_x1_loss=0.329831063747406, train_x2_loss=0.6913384199142456\n",
      "INFO:absl:[30] val_loss=1.6344337463378906\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[30] test_loss=1.674173355102539\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[31] train_loss=1.0253565311431885, train_x1_loss=0.3306882977485657, train_x2_loss=0.6946684718132019\n",
      "INFO:absl:[31] val_loss=1.634896993637085\n",
      "INFO:absl:[31] test_loss=1.6749337911605835\n",
      "INFO:absl:[32] train_loss=1.022722840309143, train_x1_loss=0.3298902213573456, train_x2_loss=0.692833423614502\n",
      "INFO:absl:[32] val_loss=1.6321040391921997\n",
      "INFO:absl:[32] test_loss=1.6714576482772827\n",
      "INFO:absl:[33] train_loss=1.0233780145645142, train_x1_loss=0.33029115200042725, train_x2_loss=0.6930879354476929\n",
      "INFO:absl:[33] val_loss=1.633604645729065\n",
      "INFO:absl:[33] test_loss=1.6729182004928589\n",
      "INFO:absl:[34] train_loss=1.0206303596496582, train_x1_loss=0.3300873041152954, train_x2_loss=0.6905422806739807\n",
      "INFO:absl:[34] val_loss=1.6359320878982544\n",
      "INFO:absl:[34] test_loss=1.6754151582717896\n",
      "INFO:absl:[35] train_loss=1.0169156789779663, train_x1_loss=0.3257463872432709, train_x2_loss=0.6911690831184387\n",
      "INFO:absl:[35] val_loss=1.633134126663208\n",
      "INFO:absl:[35] test_loss=1.672814130783081\n",
      "INFO:absl:[36] train_loss=1.0244821310043335, train_x1_loss=0.3316665291786194, train_x2_loss=0.6928142309188843\n",
      "INFO:absl:[36] val_loss=1.6279197931289673\n",
      "INFO:absl:[36] test_loss=1.66726815700531\n",
      "INFO:absl:[37] train_loss=1.0223534107208252, train_x1_loss=0.33070075511932373, train_x2_loss=0.6916520595550537\n",
      "INFO:absl:[37] val_loss=1.6333612203598022\n",
      "INFO:absl:[37] test_loss=1.6726112365722656\n",
      "INFO:absl:[38] train_loss=1.0251109600067139, train_x1_loss=0.3315926790237427, train_x2_loss=0.693519651889801\n",
      "INFO:absl:[38] val_loss=1.6374393701553345\n",
      "INFO:absl:[38] test_loss=1.6774413585662842\n",
      "INFO:absl:Setting work unit notes: 2285.7 steps/s, 39.2% (137145/350000), ETA: 1m (1m : 0.0% checkpoint, 9.2% eval)\n",
      "INFO:absl:[137145] steps_per_sec=2285.748174\n",
      "INFO:absl:[39] train_loss=1.0205522775650024, train_x1_loss=0.3279353380203247, train_x2_loss=0.6926165819168091\n",
      "INFO:absl:[39] val_loss=1.6345921754837036\n",
      "INFO:absl:[39] test_loss=1.674081563949585\n",
      "INFO:absl:[40] train_loss=1.0237700939178467, train_x1_loss=0.3302237391471863, train_x2_loss=0.693547248840332\n",
      "INFO:absl:[40] val_loss=1.6395419836044312\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[40] test_loss=1.6794824600219727\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[41] train_loss=1.0198969841003418, train_x1_loss=0.3288358449935913, train_x2_loss=0.6910625100135803\n",
      "INFO:absl:[41] val_loss=1.633461833000183\n",
      "INFO:absl:[41] test_loss=1.673216462135315\n",
      "INFO:absl:[42] train_loss=1.017032265663147, train_x1_loss=0.3279139995574951, train_x2_loss=0.689118504524231\n",
      "INFO:absl:[42] val_loss=1.6292047500610352\n",
      "INFO:absl:[42] test_loss=1.668373942375183\n",
      "INFO:absl:[43] train_loss=1.0220332145690918, train_x1_loss=0.3306114077568054, train_x2_loss=0.6914206743240356\n",
      "INFO:absl:[43] val_loss=1.6343132257461548\n",
      "INFO:absl:[43] test_loss=1.6736961603164673\n",
      "INFO:absl:[44] train_loss=1.0228543281555176, train_x1_loss=0.32845166325569153, train_x2_loss=0.6944032311439514\n",
      "INFO:absl:[44] val_loss=1.6327204704284668\n",
      "INFO:absl:[44] test_loss=1.6725854873657227\n",
      "INFO:absl:[45] train_loss=1.017045259475708, train_x1_loss=0.3255611062049866, train_x2_loss=0.6914830207824707\n",
      "INFO:absl:[45] val_loss=1.6350325345993042\n",
      "INFO:absl:[45] test_loss=1.6742995977401733\n",
      "INFO:absl:[46] train_loss=1.0274780988693237, train_x1_loss=0.3315756022930145, train_x2_loss=0.6959040760993958\n",
      "INFO:absl:[46] val_loss=1.6395847797393799\n",
      "INFO:absl:[46] test_loss=1.6792668104171753\n",
      "INFO:absl:[47] train_loss=1.0229864120483398, train_x1_loss=0.33088380098342896, train_x2_loss=0.692103385925293\n",
      "INFO:absl:[47] val_loss=1.6273585557937622\n",
      "INFO:absl:[47] test_loss=1.6668035984039307\n",
      "INFO:absl:[48] train_loss=1.022053599357605, train_x1_loss=0.32848629355430603, train_x2_loss=0.6935673356056213\n",
      "INFO:absl:[48] val_loss=1.6346908807754517\n",
      "INFO:absl:[48] test_loss=1.673999547958374\n",
      "INFO:absl:[49] train_loss=1.0232999324798584, train_x1_loss=0.3306814730167389, train_x2_loss=0.69261634349823\n",
      "INFO:absl:[49] val_loss=1.638415813446045\n",
      "INFO:absl:[49] test_loss=1.6785991191864014\n",
      "INFO:absl:[50] train_loss=1.022508978843689, train_x1_loss=0.33140280842781067, train_x2_loss=0.6911059021949768\n",
      "INFO:absl:[50] val_loss=1.635187029838562\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[50] test_loss=1.6743888854980469\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[51] train_loss=1.0233426094055176, train_x1_loss=0.33082476258277893, train_x2_loss=0.6925172805786133\n",
      "INFO:absl:[51] val_loss=1.6347033977508545\n",
      "INFO:absl:[51] test_loss=1.674483060836792\n",
      "INFO:absl:[52] train_loss=1.024577260017395, train_x1_loss=0.33219626545906067, train_x2_loss=0.6923795342445374\n",
      "INFO:absl:[52] val_loss=1.635702133178711\n",
      "INFO:absl:[52] test_loss=1.6752142906188965\n",
      "INFO:absl:[53] train_loss=1.01921546459198, train_x1_loss=0.328538715839386, train_x2_loss=0.690674901008606\n",
      "INFO:absl:[53] val_loss=1.6381769180297852\n",
      "INFO:absl:[53] test_loss=1.6774897575378418\n",
      "INFO:absl:[54] train_loss=1.0219261646270752, train_x1_loss=0.3299707770347595, train_x2_loss=0.6919563412666321\n",
      "INFO:absl:[54] val_loss=1.6410681009292603\n",
      "INFO:absl:[54] test_loss=1.6806167364120483\n",
      "INFO:absl:[55] train_loss=1.017701506614685, train_x1_loss=0.3260304927825928, train_x2_loss=0.6916713118553162\n",
      "INFO:absl:[55] val_loss=1.6383295059204102\n",
      "INFO:absl:[55] test_loss=1.6781420707702637\n",
      "INFO:absl:[56] train_loss=1.0207436084747314, train_x1_loss=0.32990342378616333, train_x2_loss=0.6908395886421204\n",
      "INFO:absl:[56] val_loss=1.642173171043396\n",
      "INFO:absl:[56] test_loss=1.6821341514587402\n",
      "INFO:absl:[57] train_loss=1.0232244729995728, train_x1_loss=0.3296012282371521, train_x2_loss=0.6936227083206177\n",
      "INFO:absl:[57] val_loss=1.6355644464492798\n",
      "INFO:absl:[57] test_loss=1.6754542589187622\n",
      "INFO:absl:[58] train_loss=1.023386836051941, train_x1_loss=0.3297134041786194, train_x2_loss=0.693672776222229\n",
      "INFO:absl:[58] val_loss=1.6361051797866821\n",
      "INFO:absl:[58] test_loss=1.6756365299224854\n",
      "INFO:absl:[59] train_loss=1.0222742557525635, train_x1_loss=0.32973513007164, train_x2_loss=0.6925400495529175\n",
      "INFO:absl:[59] val_loss=1.6358346939086914\n",
      "INFO:absl:[59] test_loss=1.6749792098999023\n",
      "INFO:absl:[60] train_loss=1.0186655521392822, train_x1_loss=0.3270973563194275, train_x2_loss=0.691567599773407\n",
      "INFO:absl:[60] val_loss=1.6328401565551758\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[60] test_loss=1.6726847887039185\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[61] train_loss=1.0211519002914429, train_x1_loss=0.32907408475875854, train_x2_loss=0.6920773983001709\n",
      "INFO:absl:[61] val_loss=1.6357325315475464\n",
      "INFO:absl:[61] test_loss=1.6756840944290161\n",
      "INFO:absl:[62] train_loss=1.0219314098358154, train_x1_loss=0.329104483127594, train_x2_loss=0.6928285360336304\n",
      "INFO:absl:[62] val_loss=1.6334917545318604\n",
      "INFO:absl:[62] test_loss=1.6731274127960205\n",
      "INFO:absl:[63] train_loss=1.0197162628173828, train_x1_loss=0.3285672068595886, train_x2_loss=0.6911489367485046\n",
      "INFO:absl:[63] val_loss=1.634846806526184\n",
      "INFO:absl:[63] test_loss=1.6743333339691162\n",
      "INFO:absl:[64] train_loss=1.020803451538086, train_x1_loss=0.32852330803871155, train_x2_loss=0.692281186580658\n",
      "INFO:absl:[64] val_loss=1.6339190006256104\n",
      "INFO:absl:[64] test_loss=1.6735625267028809\n",
      "INFO:absl:[65] train_loss=1.0231976509094238, train_x1_loss=0.328982949256897, train_x2_loss=0.6942134499549866\n",
      "INFO:absl:[65] val_loss=1.6351221799850464\n",
      "INFO:absl:[65] test_loss=1.6747828722000122\n",
      "INFO:absl:[66] train_loss=1.0210689306259155, train_x1_loss=0.32796481251716614, train_x2_loss=0.6931046843528748\n",
      "INFO:absl:[66] val_loss=1.6378557682037354\n",
      "INFO:absl:[66] test_loss=1.6777299642562866\n",
      "INFO:absl:[67] train_loss=1.0221458673477173, train_x1_loss=0.3303467035293579, train_x2_loss=0.6917994022369385\n",
      "INFO:absl:[67] val_loss=1.6392616033554077\n",
      "INFO:absl:[67] test_loss=1.6787900924682617\n",
      "INFO:absl:[68] train_loss=1.0202512741088867, train_x1_loss=0.32935309410095215, train_x2_loss=0.6908990740776062\n",
      "INFO:absl:[68] val_loss=1.6326204538345337\n",
      "INFO:absl:[68] test_loss=1.6721138954162598\n",
      "INFO:absl:[69] train_loss=1.0184575319290161, train_x1_loss=0.32805415987968445, train_x2_loss=0.6904039978981018\n",
      "INFO:absl:[69] val_loss=1.627408742904663\n",
      "INFO:absl:[69] test_loss=1.6673511266708374\n",
      "INFO:absl:[70] train_loss=1.0250164270401, train_x1_loss=0.3325870633125305, train_x2_loss=0.6924306750297546\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[70] val_loss=1.6364178657531738\n",
      "INFO:absl:[70] test_loss=1.676469326019287\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[71] train_loss=1.0178481340408325, train_x1_loss=0.32794642448425293, train_x2_loss=0.6899040341377258\n",
      "INFO:absl:[71] val_loss=1.6391652822494507\n",
      "INFO:absl:[71] test_loss=1.678996205329895\n",
      "INFO:absl:[72] train_loss=1.0193718671798706, train_x1_loss=0.3281387388706207, train_x2_loss=0.6912309527397156\n",
      "INFO:absl:[72] val_loss=1.637069582939148\n",
      "INFO:absl:[72] test_loss=1.676620364189148\n",
      "INFO:absl:[73] train_loss=1.018355369567871, train_x1_loss=0.32621094584465027, train_x2_loss=0.6921437382698059\n",
      "INFO:absl:[73] val_loss=1.63778817653656\n",
      "INFO:absl:[73] test_loss=1.6775500774383545\n",
      "INFO:absl:[74] train_loss=1.0228275060653687, train_x1_loss=0.33082884550094604, train_x2_loss=0.6919971108436584\n",
      "INFO:absl:[74] val_loss=1.6356173753738403\n",
      "INFO:absl:[74] test_loss=1.6755982637405396\n",
      "INFO:absl:[75] train_loss=1.0213677883148193, train_x1_loss=0.32742759585380554, train_x2_loss=0.6939396858215332\n",
      "INFO:absl:[75] val_loss=1.636873722076416\n",
      "INFO:absl:[75] test_loss=1.6769148111343384\n",
      "INFO:absl:[76] train_loss=1.0195565223693848, train_x1_loss=0.3266095519065857, train_x2_loss=0.6929478645324707\n",
      "INFO:absl:[76] val_loss=1.63675856590271\n",
      "INFO:absl:[76] test_loss=1.6764696836471558\n",
      "INFO:absl:[77] train_loss=1.019592523574829, train_x1_loss=0.3268706500530243, train_x2_loss=0.6927217245101929\n",
      "INFO:absl:[77] val_loss=1.63676118850708\n",
      "INFO:absl:[77] test_loss=1.6765336990356445\n",
      "INFO:absl:[78] train_loss=1.0169049501419067, train_x1_loss=0.3270736038684845, train_x2_loss=0.6898313164710999\n",
      "INFO:absl:[78] val_loss=1.6349236965179443\n",
      "INFO:absl:[78] test_loss=1.6742117404937744\n",
      "INFO:absl:[79] train_loss=1.0205233097076416, train_x1_loss=0.3289833664894104, train_x2_loss=0.6915393471717834\n",
      "INFO:absl:[79] val_loss=1.6341725587844849\n",
      "INFO:absl:[79] test_loss=1.6736512184143066\n",
      "INFO:absl:[80] train_loss=1.0216255187988281, train_x1_loss=0.3274531960487366, train_x2_loss=0.6941725015640259\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[80] val_loss=1.635164499282837\n",
      "INFO:absl:[80] test_loss=1.6750003099441528\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[81] train_loss=1.0234246253967285, train_x1_loss=0.3308393657207489, train_x2_loss=0.692584753036499\n",
      "INFO:absl:[81] val_loss=1.6367892026901245\n",
      "INFO:absl:[81] test_loss=1.6763012409210205\n",
      "INFO:absl:[82] train_loss=1.0178056955337524, train_x1_loss=0.32737502455711365, train_x2_loss=0.6904310584068298\n",
      "INFO:absl:[82] val_loss=1.637617826461792\n",
      "INFO:absl:[82] test_loss=1.6773653030395508\n",
      "INFO:absl:Setting work unit notes: 2566.2 steps/s, 83.2% (291116/350000), ETA: 0m (2m : 0.0% checkpoint, 9.1% eval)\n",
      "INFO:absl:[291116] steps_per_sec=2566.178112\n",
      "INFO:absl:[83] train_loss=1.0181093215942383, train_x1_loss=0.3279467523097992, train_x2_loss=0.6901617050170898\n",
      "INFO:absl:[83] val_loss=1.6374976634979248\n",
      "INFO:absl:[83] test_loss=1.6768931150436401\n",
      "INFO:absl:[84] train_loss=1.0142161846160889, train_x1_loss=0.3248634934425354, train_x2_loss=0.6893495917320251\n",
      "INFO:absl:[84] val_loss=1.6401588916778564\n",
      "INFO:absl:[84] test_loss=1.6798937320709229\n",
      "INFO:absl:[85] train_loss=1.0164415836334229, train_x1_loss=0.32451632618904114, train_x2_loss=0.6919248104095459\n",
      "INFO:absl:[85] val_loss=1.633345603942871\n",
      "INFO:absl:[85] test_loss=1.6728609800338745\n",
      "INFO:absl:[86] train_loss=1.0210050344467163, train_x1_loss=0.3291076719760895, train_x2_loss=0.6918985843658447\n",
      "INFO:absl:[86] val_loss=1.6344876289367676\n",
      "INFO:absl:[86] test_loss=1.6742790937423706\n",
      "INFO:absl:[87] train_loss=1.0195221900939941, train_x1_loss=0.3285333514213562, train_x2_loss=0.6909880638122559\n",
      "INFO:absl:[87] val_loss=1.6323035955429077\n",
      "INFO:absl:[87] test_loss=1.671737551689148\n",
      "INFO:absl:[88] train_loss=1.0178101062774658, train_x1_loss=0.3272159993648529, train_x2_loss=0.6905932426452637\n",
      "INFO:absl:[88] val_loss=1.6351090669631958\n",
      "INFO:absl:[88] test_loss=1.6747784614562988\n",
      "INFO:absl:[89] train_loss=1.0215786695480347, train_x1_loss=0.32927173376083374, train_x2_loss=0.6923085451126099\n",
      "INFO:absl:[89] val_loss=1.6354516744613647\n",
      "INFO:absl:[89] test_loss=1.6750568151474\n",
      "INFO:absl:[90] train_loss=1.0201603174209595, train_x1_loss=0.3265834152698517, train_x2_loss=0.6935765743255615\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[90] val_loss=1.632868766784668\n",
      "INFO:absl:[90] test_loss=1.6724815368652344\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[91] train_loss=1.0212757587432861, train_x1_loss=0.32807809114456177, train_x2_loss=0.69319748878479\n",
      "INFO:absl:[91] val_loss=1.6390228271484375\n",
      "INFO:absl:[91] test_loss=1.6788647174835205\n",
      "INFO:absl:[92] train_loss=1.018855094909668, train_x1_loss=0.326298326253891, train_x2_loss=0.6925553679466248\n",
      "INFO:absl:[92] val_loss=1.6345767974853516\n",
      "INFO:absl:[92] test_loss=1.6747565269470215\n",
      "INFO:absl:[93] train_loss=1.017699122428894, train_x1_loss=0.32624420523643494, train_x2_loss=0.6914557814598083\n",
      "INFO:absl:[93] val_loss=1.6350773572921753\n",
      "INFO:absl:[93] test_loss=1.6745678186416626\n",
      "INFO:absl:[94] train_loss=1.0182381868362427, train_x1_loss=0.32716965675354004, train_x2_loss=0.6910674571990967\n",
      "INFO:absl:[94] val_loss=1.634559988975525\n",
      "INFO:absl:[94] test_loss=1.6741691827774048\n",
      "INFO:absl:[95] train_loss=1.0179792642593384, train_x1_loss=0.32778528332710266, train_x2_loss=0.6901951432228088\n",
      "INFO:absl:[95] val_loss=1.637722134590149\n",
      "INFO:absl:[95] test_loss=1.6773545742034912\n",
      "INFO:absl:[96] train_loss=1.0164016485214233, train_x1_loss=0.32800155878067017, train_x2_loss=0.6883969306945801\n",
      "INFO:absl:[96] val_loss=1.6304023265838623\n",
      "INFO:absl:[96] test_loss=1.6700302362442017\n",
      "INFO:absl:[97] train_loss=1.015586495399475, train_x1_loss=0.3232233226299286, train_x2_loss=0.6923642158508301\n",
      "INFO:absl:[97] val_loss=1.6336532831192017\n",
      "INFO:absl:[97] test_loss=1.6733859777450562\n",
      "INFO:absl:[98] train_loss=1.0275520086288452, train_x1_loss=0.3328048288822174, train_x2_loss=0.6947466731071472\n",
      "INFO:absl:[98] val_loss=1.6355693340301514\n",
      "INFO:absl:[98] test_loss=1.6753286123275757\n",
      "INFO:absl:[99] train_loss=1.0189793109893799, train_x1_loss=0.32638880610466003, train_x2_loss=0.6925899982452393\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[99] val_loss=1.6287877559661865\n",
      "INFO:absl:[99] test_loss=1.6683918237686157\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'relu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 10, 'dropout_rate': 0.21664822298158384, 'edge_features': (4, 8), 'epochs': 100, 'eval_every_epochs': 1, 'fully_connected_edges': 1, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 1, 'layer_norm': False, 'learning_rate': 2.9648345553652954e-05, 'log_every_epochs': 1, 'max_checkpts_to_keep': 2, 'model': 'MLPGraphNetwork', 'n_blocks': 1, 'n_samples': 5000, 'node_features': (128, 2), 'normalize': True, 'optimizer': 'adam', 'output_delay': 0, 'output_steps': 4, 'sample_buffer': -4, 'seed': 67, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 120, 'timestep_duration': 3, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+-----------+-------+----------+--------+\n",
      "| Name                                   | Shape     | Size  | Mean     | Std    |\n",
      "+----------------------------------------+-----------+-------+----------+--------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (4,)      | 4     | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 4)    | 24    | -0.0819  | 0.36   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (8,)      | 8     | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (4, 8)    | 32    | -0.0639  | 0.439  |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (128,)    | 128   | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (19, 128) | 2,432 | -0.00462 | 0.228  |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)      | 2     | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (128, 2)  | 256   | -0.00622 | 0.0884 |\n",
      "+----------------------------------------+-----------+-------+----------+--------+\n",
      "Total: 2,886\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Checkpoint None does not exist.\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.00s.\n",
      "INFO:absl:Starting training.\n",
      "INFO:absl:[0] train_loss=1.4914394617080688, train_x1_loss=0.6521908640861511, train_x2_loss=0.8392506241798401\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[0] val_loss=1.771560788154602\n",
      "INFO:absl:[0] test_loss=1.8600791692733765\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Created artifact [10] Profile of type ArtifactType.URL and value None.\n",
      "INFO:absl:[1] train_loss=1.1977792978286743, train_x1_loss=0.42837050557136536, train_x2_loss=0.7694074511528015\n",
      "INFO:absl:[1] val_loss=1.671091079711914\n",
      "INFO:absl:[1] test_loss=1.7607457637786865\n",
      "INFO:absl:[2] train_loss=1.1195436716079712, train_x1_loss=0.381025493144989, train_x2_loss=0.738516628742218\n",
      "INFO:absl:[2] val_loss=1.6536945104599\n",
      "INFO:absl:[2] test_loss=1.74385404586792\n",
      "INFO:absl:[3] train_loss=1.0871652364730835, train_x1_loss=0.3604585528373718, train_x2_loss=0.7267091274261475\n",
      "INFO:absl:[3] val_loss=1.628822922706604\n",
      "INFO:absl:[3] test_loss=1.7195907831192017\n",
      "INFO:absl:[4] train_loss=1.0757945775985718, train_x1_loss=0.35332250595092773, train_x2_loss=0.7224706411361694\n",
      "INFO:absl:[4] val_loss=1.613116979598999\n",
      "INFO:absl:[4] test_loss=1.7051403522491455\n",
      "INFO:absl:[5] train_loss=1.063477873802185, train_x1_loss=0.34612521529197693, train_x2_loss=0.717353105545044\n",
      "INFO:absl:[5] val_loss=1.6118978261947632\n",
      "INFO:absl:[5] test_loss=1.703080654144287\n",
      "INFO:absl:[6] train_loss=1.052547812461853, train_x1_loss=0.3407938778400421, train_x2_loss=0.7117526531219482\n",
      "INFO:absl:[6] val_loss=1.6112089157104492\n",
      "INFO:absl:[6] test_loss=1.7027587890625\n",
      "INFO:absl:[7] train_loss=1.051216959953308, train_x1_loss=0.3385677635669708, train_x2_loss=0.7126489877700806\n",
      "INFO:absl:[7] val_loss=1.6091501712799072\n",
      "INFO:absl:[7] test_loss=1.7007713317871094\n",
      "INFO:absl:[8] train_loss=1.0456269979476929, train_x1_loss=0.3362175226211548, train_x2_loss=0.7094090580940247\n",
      "INFO:absl:[8] val_loss=1.6038771867752075\n",
      "INFO:absl:[8] test_loss=1.6956104040145874\n",
      "INFO:absl:[9] train_loss=1.0422981977462769, train_x1_loss=0.3365550935268402, train_x2_loss=0.7057428956031799\n",
      "INFO:absl:[9] val_loss=1.595139741897583\n",
      "INFO:absl:[9] test_loss=1.687547206878662\n",
      "INFO:absl:[10] train_loss=1.0408705472946167, train_x1_loss=0.3313639163970947, train_x2_loss=0.7095075845718384\n",
      "INFO:absl:[10] val_loss=1.5986254215240479\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[10] test_loss=1.6909910440444946\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[11] train_loss=1.0417704582214355, train_x1_loss=0.3333716094493866, train_x2_loss=0.7083993554115295\n",
      "INFO:absl:[11] val_loss=1.5968637466430664\n",
      "INFO:absl:[11] test_loss=1.6893718242645264\n",
      "INFO:absl:[12] train_loss=1.0401990413665771, train_x1_loss=0.3325321078300476, train_x2_loss=0.707669198513031\n",
      "INFO:absl:[12] val_loss=1.593957781791687\n",
      "INFO:absl:[12] test_loss=1.6861516237258911\n",
      "INFO:absl:[13] train_loss=1.0407997369766235, train_x1_loss=0.3348666727542877, train_x2_loss=0.7059307098388672\n",
      "INFO:absl:[13] val_loss=1.596425175666809\n",
      "INFO:absl:[13] test_loss=1.688711404800415\n",
      "INFO:absl:[14] train_loss=1.0361149311065674, train_x1_loss=0.3323338031768799, train_x2_loss=0.703780472278595\n",
      "INFO:absl:[14] val_loss=1.5919365882873535\n",
      "INFO:absl:[14] test_loss=1.6843763589859009\n",
      "INFO:absl:[15] train_loss=1.0342036485671997, train_x1_loss=0.33261024951934814, train_x2_loss=0.7015959024429321\n",
      "INFO:absl:[15] val_loss=1.5847620964050293\n",
      "INFO:absl:[15] test_loss=1.6774680614471436\n",
      "INFO:absl:[16] train_loss=1.0370080471038818, train_x1_loss=0.3330543637275696, train_x2_loss=0.7039540410041809\n",
      "INFO:absl:[16] val_loss=1.5826393365859985\n",
      "INFO:absl:[16] test_loss=1.6758466958999634\n",
      "INFO:absl:[17] train_loss=1.0321849584579468, train_x1_loss=0.3288627862930298, train_x2_loss=0.7033234238624573\n",
      "INFO:absl:[17] val_loss=1.5899300575256348\n",
      "INFO:absl:[17] test_loss=1.6821247339248657\n",
      "INFO:absl:[18] train_loss=1.0347378253936768, train_x1_loss=0.32919150590896606, train_x2_loss=0.7055462002754211\n",
      "INFO:absl:[18] val_loss=1.5918948650360107\n",
      "INFO:absl:[18] test_loss=1.6842725276947021\n",
      "INFO:absl:[19] train_loss=1.036958932876587, train_x1_loss=0.33381128311157227, train_x2_loss=0.7031490802764893\n",
      "INFO:absl:[19] val_loss=1.585913062095642\n",
      "INFO:absl:[19] test_loss=1.6783690452575684\n",
      "INFO:absl:[20] train_loss=1.038938045501709, train_x1_loss=0.3339649736881256, train_x2_loss=0.7049705386161804\n",
      "INFO:absl:[20] val_loss=1.5851171016693115\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[20] test_loss=1.6776998043060303\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[21] train_loss=1.0315452814102173, train_x1_loss=0.32869046926498413, train_x2_loss=0.7028534412384033\n",
      "INFO:absl:[21] val_loss=1.5885586738586426\n",
      "INFO:absl:[21] test_loss=1.6808992624282837\n",
      "INFO:absl:[22] train_loss=1.031194806098938, train_x1_loss=0.32884305715560913, train_x2_loss=0.7023505568504333\n",
      "INFO:absl:[22] val_loss=1.5893365144729614\n",
      "INFO:absl:[22] test_loss=1.6818510293960571\n",
      "INFO:absl:[23] train_loss=1.0298861265182495, train_x1_loss=0.3275570869445801, train_x2_loss=0.7023277282714844\n",
      "INFO:absl:[23] val_loss=1.5882328748703003\n",
      "INFO:absl:[23] test_loss=1.6806765794754028\n",
      "INFO:absl:[24] train_loss=1.0342369079589844, train_x1_loss=0.33119720220565796, train_x2_loss=0.7030413746833801\n",
      "INFO:absl:[24] val_loss=1.5895957946777344\n",
      "INFO:absl:[24] test_loss=1.682515263557434\n",
      "INFO:absl:[25] train_loss=1.0364506244659424, train_x1_loss=0.33322203159332275, train_x2_loss=0.7032281756401062\n",
      "INFO:absl:[25] val_loss=1.5811821222305298\n",
      "INFO:absl:[25] test_loss=1.6737771034240723\n",
      "INFO:absl:[26] train_loss=1.0333069562911987, train_x1_loss=0.32955995202064514, train_x2_loss=0.7037468552589417\n",
      "INFO:absl:[26] val_loss=1.5895655155181885\n",
      "INFO:absl:[26] test_loss=1.6819658279418945\n",
      "INFO:absl:[27] train_loss=1.0305434465408325, train_x1_loss=0.32658568024635315, train_x2_loss=0.7039565443992615\n",
      "INFO:absl:[27] val_loss=1.5909258127212524\n",
      "INFO:absl:[27] test_loss=1.682941198348999\n",
      "INFO:absl:[28] train_loss=1.0324586629867554, train_x1_loss=0.32933759689331055, train_x2_loss=0.7031204104423523\n",
      "INFO:absl:[28] val_loss=1.5853694677352905\n",
      "INFO:absl:[28] test_loss=1.6777260303497314\n",
      "INFO:absl:[29] train_loss=1.0317033529281616, train_x1_loss=0.3291516900062561, train_x2_loss=0.7025519609451294\n",
      "INFO:absl:[29] val_loss=1.5907610654830933\n",
      "INFO:absl:[29] test_loss=1.6830663681030273\n",
      "INFO:absl:[30] train_loss=1.0350146293640137, train_x1_loss=0.32998529076576233, train_x2_loss=0.7050290703773499\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[30] val_loss=1.5846781730651855\n",
      "INFO:absl:[30] test_loss=1.6772977113723755\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[31] train_loss=1.0299267768859863, train_x1_loss=0.3288470208644867, train_x2_loss=0.7010799646377563\n",
      "INFO:absl:[31] val_loss=1.5813648700714111\n",
      "INFO:absl:[31] test_loss=1.6741238832473755\n",
      "INFO:absl:[32] train_loss=1.0363050699234009, train_x1_loss=0.33050769567489624, train_x2_loss=0.7057954668998718\n",
      "INFO:absl:[32] val_loss=1.580219030380249\n",
      "INFO:absl:[32] test_loss=1.6727148294448853\n",
      "INFO:absl:[33] train_loss=1.0332292318344116, train_x1_loss=0.32938361167907715, train_x2_loss=0.7038474678993225\n",
      "INFO:absl:[33] val_loss=1.5895026922225952\n",
      "INFO:absl:[33] test_loss=1.6821587085723877\n",
      "INFO:absl:[34] train_loss=1.0317360162734985, train_x1_loss=0.32915547490119934, train_x2_loss=0.7025814056396484\n",
      "INFO:absl:[34] val_loss=1.5863168239593506\n",
      "INFO:absl:[34] test_loss=1.6788560152053833\n",
      "INFO:absl:[35] train_loss=1.0306388139724731, train_x1_loss=0.3291984796524048, train_x2_loss=0.7014415860176086\n",
      "INFO:absl:[35] val_loss=1.577427625656128\n",
      "INFO:absl:[35] test_loss=1.6702380180358887\n",
      "INFO:absl:[36] train_loss=1.0328326225280762, train_x1_loss=0.3317178189754486, train_x2_loss=0.7011160254478455\n",
      "INFO:absl:[36] val_loss=1.5900591611862183\n",
      "INFO:absl:[36] test_loss=1.682320475578308\n",
      "INFO:absl:[37] train_loss=1.0330884456634521, train_x1_loss=0.33034276962280273, train_x2_loss=0.702744722366333\n",
      "INFO:absl:[37] val_loss=1.5825804471969604\n",
      "INFO:absl:[37] test_loss=1.675498127937317\n",
      "INFO:absl:Setting work unit notes: 2216.8 steps/s, 38.0% (133006/350000), ETA: 1m (1m : 0.0% checkpoint, 8.9% eval)\n",
      "INFO:absl:[133006] steps_per_sec=2216.766261\n",
      "INFO:absl:[38] train_loss=1.0303065776824951, train_x1_loss=0.3280178904533386, train_x2_loss=0.7022891044616699\n",
      "INFO:absl:[38] val_loss=1.5901038646697998\n",
      "INFO:absl:[38] test_loss=1.6822142601013184\n",
      "INFO:absl:[39] train_loss=1.0269314050674438, train_x1_loss=0.32633060216903687, train_x2_loss=0.7005999684333801\n",
      "INFO:absl:[39] val_loss=1.586928367614746\n",
      "INFO:absl:[39] test_loss=1.6796529293060303\n",
      "INFO:absl:[40] train_loss=1.0310012102127075, train_x1_loss=0.32941776514053345, train_x2_loss=0.7015813589096069\n",
      "INFO:absl:[40] val_loss=1.5898525714874268\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[40] test_loss=1.682281255722046\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[41] train_loss=1.03022301197052, train_x1_loss=0.32841256260871887, train_x2_loss=0.7018101811408997\n",
      "INFO:absl:[41] val_loss=1.5926971435546875\n",
      "INFO:absl:[41] test_loss=1.6848081350326538\n",
      "INFO:absl:[42] train_loss=1.0294342041015625, train_x1_loss=0.32723867893218994, train_x2_loss=0.7021955847740173\n",
      "INFO:absl:[42] val_loss=1.5865089893341064\n",
      "INFO:absl:[42] test_loss=1.6790602207183838\n",
      "INFO:absl:[43] train_loss=1.032748818397522, train_x1_loss=0.33031636476516724, train_x2_loss=0.7024339437484741\n",
      "INFO:absl:[43] val_loss=1.5856478214263916\n",
      "INFO:absl:[43] test_loss=1.6784021854400635\n",
      "INFO:absl:[44] train_loss=1.0316632986068726, train_x1_loss=0.3285796344280243, train_x2_loss=0.7030852437019348\n",
      "INFO:absl:[44] val_loss=1.5893763303756714\n",
      "INFO:absl:[44] test_loss=1.6822309494018555\n",
      "INFO:absl:[45] train_loss=1.0300476551055908, train_x1_loss=0.3277539014816284, train_x2_loss=0.7022933959960938\n",
      "INFO:absl:[45] val_loss=1.5860556364059448\n",
      "INFO:absl:[45] test_loss=1.6788409948349\n",
      "INFO:absl:[46] train_loss=1.0256580114364624, train_x1_loss=0.32661405205726624, train_x2_loss=0.6990445256233215\n",
      "INFO:absl:[46] val_loss=1.5910861492156982\n",
      "INFO:absl:[46] test_loss=1.6837010383605957\n",
      "INFO:absl:[47] train_loss=1.02748441696167, train_x1_loss=0.3260183334350586, train_x2_loss=0.701464831829071\n",
      "INFO:absl:[47] val_loss=1.5792505741119385\n",
      "INFO:absl:[47] test_loss=1.6724034547805786\n",
      "INFO:absl:[48] train_loss=1.0287903547286987, train_x1_loss=0.32789358496665955, train_x2_loss=0.7008967399597168\n",
      "INFO:absl:[48] val_loss=1.5848139524459839\n",
      "INFO:absl:[48] test_loss=1.6775245666503906\n",
      "INFO:absl:[49] train_loss=1.0296897888183594, train_x1_loss=0.3281971216201782, train_x2_loss=0.7014914751052856\n",
      "INFO:absl:[49] val_loss=1.5820778608322144\n",
      "INFO:absl:[49] test_loss=1.675387978553772\n",
      "INFO:absl:[50] train_loss=1.024175763130188, train_x1_loss=0.3256829082965851, train_x2_loss=0.6984912753105164\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[50] val_loss=1.5839234590530396\n",
      "INFO:absl:[50] test_loss=1.6765187978744507\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[51] train_loss=1.0331504344940186, train_x1_loss=0.3314206302165985, train_x2_loss=0.7017288208007812\n",
      "INFO:absl:[51] val_loss=1.5805292129516602\n",
      "INFO:absl:[51] test_loss=1.6733176708221436\n",
      "INFO:absl:[52] train_loss=1.0295283794403076, train_x1_loss=0.3299858272075653, train_x2_loss=0.699541449546814\n",
      "INFO:absl:[52] val_loss=1.5862653255462646\n",
      "INFO:absl:[52] test_loss=1.6791558265686035\n",
      "INFO:absl:[53] train_loss=1.030594825744629, train_x1_loss=0.3282833397388458, train_x2_loss=0.7023128271102905\n",
      "INFO:absl:[53] val_loss=1.587196707725525\n",
      "INFO:absl:[53] test_loss=1.6795834302902222\n",
      "INFO:absl:[54] train_loss=1.028947114944458, train_x1_loss=0.32609137892723083, train_x2_loss=0.7028551697731018\n",
      "INFO:absl:[54] val_loss=1.588160753250122\n",
      "INFO:absl:[54] test_loss=1.680385947227478\n",
      "INFO:absl:[55] train_loss=1.0291763544082642, train_x1_loss=0.32718610763549805, train_x2_loss=0.7019901871681213\n",
      "INFO:absl:[55] val_loss=1.5907646417617798\n",
      "INFO:absl:[55] test_loss=1.6830105781555176\n",
      "INFO:absl:[56] train_loss=1.030995488166809, train_x1_loss=0.3301475942134857, train_x2_loss=0.7008451223373413\n",
      "INFO:absl:[56] val_loss=1.5869413614273071\n",
      "INFO:absl:[56] test_loss=1.6792806386947632\n",
      "INFO:absl:[57] train_loss=1.0345039367675781, train_x1_loss=0.3280280530452728, train_x2_loss=0.7064758539199829\n",
      "INFO:absl:[57] val_loss=1.5867689847946167\n",
      "INFO:absl:[57] test_loss=1.6789817810058594\n",
      "INFO:absl:[58] train_loss=1.027683973312378, train_x1_loss=0.32651665806770325, train_x2_loss=0.701167106628418\n",
      "INFO:absl:[58] val_loss=1.586238980293274\n",
      "INFO:absl:[58] test_loss=1.6785541772842407\n",
      "INFO:absl:[59] train_loss=1.0300606489181519, train_x1_loss=0.3297198414802551, train_x2_loss=0.7003423571586609\n",
      "INFO:absl:[59] val_loss=1.5962531566619873\n",
      "INFO:absl:[59] test_loss=1.6885889768600464\n",
      "INFO:absl:[60] train_loss=1.028028964996338, train_x1_loss=0.3288416266441345, train_x2_loss=0.6991868019104004\n",
      "INFO:absl:[60] val_loss=1.589112639427185\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[60] test_loss=1.6814379692077637\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[61] train_loss=1.0302150249481201, train_x1_loss=0.3290785551071167, train_x2_loss=0.7011363506317139\n",
      "INFO:absl:[61] val_loss=1.5863384008407593\n",
      "INFO:absl:[61] test_loss=1.6787409782409668\n",
      "INFO:absl:[62] train_loss=1.031350016593933, train_x1_loss=0.3282833695411682, train_x2_loss=0.7030665874481201\n",
      "INFO:absl:[62] val_loss=1.5828107595443726\n",
      "INFO:absl:[62] test_loss=1.6753416061401367\n",
      "INFO:absl:[63] train_loss=1.0331156253814697, train_x1_loss=0.33006417751312256, train_x2_loss=0.703051745891571\n",
      "INFO:absl:[63] val_loss=1.5842722654342651\n",
      "INFO:absl:[63] test_loss=1.6768263578414917\n",
      "INFO:absl:[64] train_loss=1.0256775617599487, train_x1_loss=0.32561546564102173, train_x2_loss=0.7000625133514404\n",
      "INFO:absl:[64] val_loss=1.5832487344741821\n",
      "INFO:absl:[64] test_loss=1.6758357286453247\n",
      "INFO:absl:[65] train_loss=1.0324691534042358, train_x1_loss=0.33170998096466064, train_x2_loss=0.7007599472999573\n",
      "INFO:absl:[65] val_loss=1.5882461071014404\n",
      "INFO:absl:[65] test_loss=1.6807881593704224\n",
      "INFO:absl:[66] train_loss=1.0344938039779663, train_x1_loss=0.33004555106163025, train_x2_loss=0.7044482827186584\n",
      "INFO:absl:[66] val_loss=1.5858187675476074\n",
      "INFO:absl:[66] test_loss=1.6783275604248047\n",
      "INFO:absl:[67] train_loss=1.0318632125854492, train_x1_loss=0.3284832239151001, train_x2_loss=0.7033800482749939\n",
      "INFO:absl:[67] val_loss=1.5914338827133179\n",
      "INFO:absl:[67] test_loss=1.683605670928955\n",
      "INFO:absl:[68] train_loss=1.0303874015808105, train_x1_loss=0.32783955335617065, train_x2_loss=0.7025483250617981\n",
      "INFO:absl:[68] val_loss=1.5934712886810303\n",
      "INFO:absl:[68] test_loss=1.6857306957244873\n",
      "INFO:absl:[69] train_loss=1.0286600589752197, train_x1_loss=0.327676385641098, train_x2_loss=0.7009817957878113\n",
      "INFO:absl:[69] val_loss=1.5857359170913696\n",
      "INFO:absl:[69] test_loss=1.6777958869934082\n",
      "INFO:absl:[70] train_loss=1.0279104709625244, train_x1_loss=0.32809555530548096, train_x2_loss=0.6998147964477539\n",
      "INFO:absl:[70] val_loss=1.585556983947754\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[70] test_loss=1.677744746208191\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[71] train_loss=1.0275081396102905, train_x1_loss=0.3251771330833435, train_x2_loss=0.7023322582244873\n",
      "INFO:absl:[71] val_loss=1.5880597829818726\n",
      "INFO:absl:[71] test_loss=1.680203914642334\n",
      "INFO:absl:[72] train_loss=1.025351881980896, train_x1_loss=0.32482123374938965, train_x2_loss=0.7005290985107422\n",
      "INFO:absl:[72] val_loss=1.5832396745681763\n",
      "INFO:absl:[72] test_loss=1.6758257150650024\n",
      "INFO:absl:[73] train_loss=1.0279273986816406, train_x1_loss=0.3258175551891327, train_x2_loss=0.7021098136901855\n",
      "INFO:absl:[73] val_loss=1.5908623933792114\n",
      "INFO:absl:[73] test_loss=1.6828598976135254\n",
      "INFO:absl:[74] train_loss=1.0313000679016113, train_x1_loss=0.3288668394088745, train_x2_loss=0.7024335861206055\n",
      "INFO:absl:[74] val_loss=1.5953608751296997\n",
      "INFO:absl:[74] test_loss=1.687495470046997\n",
      "INFO:absl:[75] train_loss=1.0287630558013916, train_x1_loss=0.32733264565467834, train_x2_loss=0.7014302611351013\n",
      "INFO:absl:[75] val_loss=1.583277940750122\n",
      "INFO:absl:[75] test_loss=1.6756948232650757\n",
      "INFO:absl:[76] train_loss=1.029517412185669, train_x1_loss=0.32483989000320435, train_x2_loss=0.7046769261360168\n",
      "INFO:absl:[76] val_loss=1.587093710899353\n",
      "INFO:absl:[76] test_loss=1.6792378425598145\n",
      "INFO:absl:[77] train_loss=1.0277605056762695, train_x1_loss=0.32641294598579407, train_x2_loss=0.7013480067253113\n",
      "INFO:absl:[77] val_loss=1.583565354347229\n",
      "INFO:absl:[77] test_loss=1.6761393547058105\n",
      "INFO:absl:[78] train_loss=1.0280282497406006, train_x1_loss=0.3276037275791168, train_x2_loss=0.7004234790802002\n",
      "INFO:absl:[78] val_loss=1.589645266532898\n",
      "INFO:absl:[78] test_loss=1.68145751953125\n",
      "INFO:absl:[79] train_loss=1.028224229812622, train_x1_loss=0.32780957221984863, train_x2_loss=0.7004145979881287\n",
      "INFO:absl:[79] val_loss=1.5798730850219727\n",
      "INFO:absl:[79] test_loss=1.6722348928451538\n",
      "INFO:absl:[80] train_loss=1.0280942916870117, train_x1_loss=0.32726553082466125, train_x2_loss=0.7008293271064758\n",
      "INFO:absl:[80] val_loss=1.5840669870376587\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[80] test_loss=1.6758893728256226\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Setting work unit notes: 2541.5 steps/s, 81.6% (285495/350000), ETA: 0m (2m : 0.0% checkpoint, 8.8% eval)\n",
      "INFO:absl:[285495] steps_per_sec=2541.473295\n",
      "INFO:absl:[81] train_loss=1.0298112630844116, train_x1_loss=0.32611212134361267, train_x2_loss=0.7036951780319214\n",
      "INFO:absl:[81] val_loss=1.58352792263031\n",
      "INFO:absl:[81] test_loss=1.6758188009262085\n",
      "INFO:absl:[82] train_loss=1.0303484201431274, train_x1_loss=0.3255424201488495, train_x2_loss=0.7048056125640869\n",
      "INFO:absl:[82] val_loss=1.5847872495651245\n",
      "INFO:absl:[82] test_loss=1.6771399974822998\n",
      "INFO:absl:[83] train_loss=1.0270533561706543, train_x1_loss=0.3257397711277008, train_x2_loss=0.7013128399848938\n",
      "INFO:absl:[83] val_loss=1.5869016647338867\n",
      "INFO:absl:[83] test_loss=1.6792876720428467\n",
      "INFO:absl:[84] train_loss=1.0284056663513184, train_x1_loss=0.3260110914707184, train_x2_loss=0.7023928165435791\n",
      "INFO:absl:[84] val_loss=1.5842081308364868\n",
      "INFO:absl:[84] test_loss=1.6766141653060913\n",
      "INFO:absl:[85] train_loss=1.0254014730453491, train_x1_loss=0.3251277506351471, train_x2_loss=0.7002748847007751\n",
      "INFO:absl:[85] val_loss=1.5886390209197998\n",
      "INFO:absl:[85] test_loss=1.6812372207641602\n",
      "INFO:absl:[86] train_loss=1.0281453132629395, train_x1_loss=0.3279435634613037, train_x2_loss=0.7002027034759521\n",
      "INFO:absl:[86] val_loss=1.5884425640106201\n",
      "INFO:absl:[86] test_loss=1.6806814670562744\n",
      "INFO:absl:[87] train_loss=1.0320543050765991, train_x1_loss=0.3294660151004791, train_x2_loss=0.7025870084762573\n",
      "INFO:absl:[87] val_loss=1.5887445211410522\n",
      "INFO:absl:[87] test_loss=1.6810111999511719\n",
      "INFO:absl:[88] train_loss=1.025816798210144, train_x1_loss=0.3232611119747162, train_x2_loss=0.7025561332702637\n",
      "INFO:absl:[88] val_loss=1.5883842706680298\n",
      "INFO:absl:[88] test_loss=1.680548906326294\n",
      "INFO:absl:[89] train_loss=1.027621865272522, train_x1_loss=0.3282960057258606, train_x2_loss=0.6993272304534912\n",
      "INFO:absl:[89] val_loss=1.5845764875411987\n",
      "INFO:absl:[89] test_loss=1.6769323348999023\n",
      "INFO:absl:[90] train_loss=1.0244982242584229, train_x1_loss=0.32550716400146484, train_x2_loss=0.698992908000946\n",
      "INFO:absl:[90] val_loss=1.5896399021148682\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[90] test_loss=1.6819913387298584\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[91] train_loss=1.0278282165527344, train_x1_loss=0.32612714171409607, train_x2_loss=0.7016991972923279\n",
      "INFO:absl:[91] val_loss=1.5870130062103271\n",
      "INFO:absl:[91] test_loss=1.6792901754379272\n",
      "INFO:absl:[92] train_loss=1.0254689455032349, train_x1_loss=0.3243422210216522, train_x2_loss=0.7011259198188782\n",
      "INFO:absl:[92] val_loss=1.585384726524353\n",
      "INFO:absl:[92] test_loss=1.6773905754089355\n",
      "INFO:absl:[93] train_loss=1.0278608798980713, train_x1_loss=0.3252180218696594, train_x2_loss=0.7026425004005432\n",
      "INFO:absl:[93] val_loss=1.5895907878875732\n",
      "INFO:absl:[93] test_loss=1.6819170713424683\n",
      "INFO:absl:[94] train_loss=1.029594898223877, train_x1_loss=0.32796865701675415, train_x2_loss=0.701627254486084\n",
      "INFO:absl:[94] val_loss=1.588535189628601\n",
      "INFO:absl:[94] test_loss=1.680623173713684\n",
      "INFO:absl:[95] train_loss=1.0297075510025024, train_x1_loss=0.32691890001296997, train_x2_loss=0.7027873992919922\n",
      "INFO:absl:[95] val_loss=1.582987904548645\n",
      "INFO:absl:[95] test_loss=1.6753369569778442\n",
      "INFO:absl:[96] train_loss=1.026292324066162, train_x1_loss=0.32731908559799194, train_x2_loss=0.6989724040031433\n",
      "INFO:absl:[96] val_loss=1.5839964151382446\n",
      "INFO:absl:[96] test_loss=1.6764805316925049\n",
      "INFO:absl:[97] train_loss=1.0243663787841797, train_x1_loss=0.3235275447368622, train_x2_loss=0.7008383870124817\n",
      "INFO:absl:[97] val_loss=1.585944652557373\n",
      "INFO:absl:[97] test_loss=1.6783661842346191\n",
      "INFO:absl:[98] train_loss=1.0317530632019043, train_x1_loss=0.32890036702156067, train_x2_loss=0.7028538584709167\n",
      "INFO:absl:[98] val_loss=1.5814964771270752\n",
      "INFO:absl:[98] test_loss=1.6741374731063843\n",
      "INFO:absl:[99] train_loss=1.0312446355819702, train_x1_loss=0.3273768723011017, train_x2_loss=0.7038657069206238\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[99] val_loss=1.5795044898986816\n",
      "INFO:absl:[99] test_loss=1.6722240447998047\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'relu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 10, 'dropout_rate': 0.21664822298158384, 'edge_features': (4, 8), 'epochs': 100, 'eval_every_epochs': 1, 'fully_connected_edges': 1, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 1, 'layer_norm': False, 'learning_rate': 2.9648345553652954e-05, 'log_every_epochs': 1, 'max_checkpts_to_keep': 2, 'model': 'MLPGraphNetwork', 'n_blocks': 1, 'n_samples': 5000, 'node_features': (128, 2), 'normalize': True, 'optimizer': 'adam', 'output_delay': 0, 'output_steps': 4, 'sample_buffer': -4, 'seed': 68, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 120, 'timestep_duration': 3, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+-----------+-------+----------+--------+\n",
      "| Name                                   | Shape     | Size  | Mean     | Std    |\n",
      "+----------------------------------------+-----------+-------+----------+--------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (4,)      | 4     | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 4)    | 24    | -0.0819  | 0.36   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (8,)      | 8     | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (4, 8)    | 32    | -0.0639  | 0.439  |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (128,)    | 128   | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (19, 128) | 2,432 | -0.00462 | 0.228  |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)      | 2     | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (128, 2)  | 256   | -0.00622 | 0.0884 |\n",
      "+----------------------------------------+-----------+-------+----------+--------+\n",
      "Total: 2,886\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Checkpoint None does not exist.\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.00s.\n",
      "INFO:absl:Starting training.\n",
      "INFO:absl:[0] train_loss=1.507524847984314, train_x1_loss=0.6659241914749146, train_x2_loss=0.841599702835083\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[0] val_loss=1.8188669681549072\n",
      "INFO:absl:[0] test_loss=1.8562705516815186\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Created artifact [10] Profile of type ArtifactType.URL and value None.\n",
      "INFO:absl:[1] train_loss=1.201361894607544, train_x1_loss=0.43492722511291504, train_x2_loss=0.7664365768432617\n",
      "INFO:absl:[1] val_loss=1.7022552490234375\n",
      "INFO:absl:[1] test_loss=1.7421658039093018\n",
      "INFO:absl:[2] train_loss=1.125004768371582, train_x1_loss=0.3869853913784027, train_x2_loss=0.7380201816558838\n",
      "INFO:absl:[2] val_loss=1.6921766996383667\n",
      "INFO:absl:[2] test_loss=1.7332199811935425\n",
      "INFO:absl:[3] train_loss=1.0865594148635864, train_x1_loss=0.3623370826244354, train_x2_loss=0.7242220044136047\n",
      "INFO:absl:[3] val_loss=1.666356086730957\n",
      "INFO:absl:[3] test_loss=1.708566665649414\n",
      "INFO:absl:[4] train_loss=1.0727277994155884, train_x1_loss=0.35483747720718384, train_x2_loss=0.7178897261619568\n",
      "INFO:absl:[4] val_loss=1.6500732898712158\n",
      "INFO:absl:[4] test_loss=1.6922788619995117\n",
      "INFO:absl:[5] train_loss=1.0578258037567139, train_x1_loss=0.3485751748085022, train_x2_loss=0.7092507481575012\n",
      "INFO:absl:[5] val_loss=1.6529209613800049\n",
      "INFO:absl:[5] test_loss=1.6958043575286865\n",
      "INFO:absl:[6] train_loss=1.0445653200149536, train_x1_loss=0.34067222476005554, train_x2_loss=0.7038909196853638\n",
      "INFO:absl:[6] val_loss=1.6503652334213257\n",
      "INFO:absl:[6] test_loss=1.6931068897247314\n",
      "INFO:absl:[7] train_loss=1.0481103658676147, train_x1_loss=0.3416629135608673, train_x2_loss=0.7064462900161743\n",
      "INFO:absl:[7] val_loss=1.650005578994751\n",
      "INFO:absl:[7] test_loss=1.6930848360061646\n",
      "INFO:absl:[8] train_loss=1.0381971597671509, train_x1_loss=0.3371482491493225, train_x2_loss=0.7010489106178284\n",
      "INFO:absl:[8] val_loss=1.6400376558303833\n",
      "INFO:absl:[8] test_loss=1.6834352016448975\n",
      "INFO:absl:[9] train_loss=1.0359694957733154, train_x1_loss=0.3357602655887604, train_x2_loss=0.700208306312561\n",
      "INFO:absl:[9] val_loss=1.6375590562820435\n",
      "INFO:absl:[9] test_loss=1.68123197555542\n",
      "INFO:absl:[10] train_loss=1.0397545099258423, train_x1_loss=0.33596450090408325, train_x2_loss=0.7037913203239441\n",
      "INFO:absl:[10] val_loss=1.6391198635101318\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[10] test_loss=1.6826668977737427\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[11] train_loss=1.0398131608963013, train_x1_loss=0.3366633653640747, train_x2_loss=0.7031489014625549\n",
      "INFO:absl:[11] val_loss=1.6469879150390625\n",
      "INFO:absl:[11] test_loss=1.690462350845337\n",
      "INFO:absl:[12] train_loss=1.0298643112182617, train_x1_loss=0.33249470591545105, train_x2_loss=0.6973701119422913\n",
      "INFO:absl:[12] val_loss=1.6315656900405884\n",
      "INFO:absl:[12] test_loss=1.6750752925872803\n",
      "INFO:absl:[13] train_loss=1.0307549238204956, train_x1_loss=0.3346174359321594, train_x2_loss=0.6961376070976257\n",
      "INFO:absl:[13] val_loss=1.6368831396102905\n",
      "INFO:absl:[13] test_loss=1.6802140474319458\n",
      "INFO:absl:[14] train_loss=1.0334081649780273, train_x1_loss=0.33319416642189026, train_x2_loss=0.7002153992652893\n",
      "INFO:absl:[14] val_loss=1.6341508626937866\n",
      "INFO:absl:[14] test_loss=1.6778984069824219\n",
      "INFO:absl:[15] train_loss=1.0322428941726685, train_x1_loss=0.335093230009079, train_x2_loss=0.6971505284309387\n",
      "INFO:absl:[15] val_loss=1.6313804388046265\n",
      "INFO:absl:[15] test_loss=1.675004482269287\n",
      "INFO:absl:[16] train_loss=1.0279141664505005, train_x1_loss=0.3315204381942749, train_x2_loss=0.6963940262794495\n",
      "INFO:absl:[16] val_loss=1.626609444618225\n",
      "INFO:absl:[16] test_loss=1.670832872390747\n",
      "INFO:absl:[17] train_loss=1.0270519256591797, train_x1_loss=0.3281063139438629, train_x2_loss=0.6989449858665466\n",
      "INFO:absl:[17] val_loss=1.6379141807556152\n",
      "INFO:absl:[17] test_loss=1.6816009283065796\n",
      "INFO:absl:[18] train_loss=1.0323572158813477, train_x1_loss=0.33385664224624634, train_x2_loss=0.6985013484954834\n",
      "INFO:absl:[18] val_loss=1.633470058441162\n",
      "INFO:absl:[18] test_loss=1.6772512197494507\n",
      "INFO:absl:[19] train_loss=1.0323078632354736, train_x1_loss=0.3354891836643219, train_x2_loss=0.6968184113502502\n",
      "INFO:absl:[19] val_loss=1.6242456436157227\n",
      "INFO:absl:[19] test_loss=1.668027400970459\n",
      "INFO:absl:[20] train_loss=1.0307844877243042, train_x1_loss=0.3324822187423706, train_x2_loss=0.6983022689819336\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[20] val_loss=1.6239322423934937\n",
      "INFO:absl:[20] test_loss=1.667860984802246\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[21] train_loss=1.0298885107040405, train_x1_loss=0.3307898938655853, train_x2_loss=0.6990979909896851\n",
      "INFO:absl:[21] val_loss=1.631414771080017\n",
      "INFO:absl:[21] test_loss=1.6756638288497925\n",
      "INFO:absl:[22] train_loss=1.0236395597457886, train_x1_loss=0.32827451825141907, train_x2_loss=0.6953645348548889\n",
      "INFO:absl:[22] val_loss=1.6293823719024658\n",
      "INFO:absl:[22] test_loss=1.6734369993209839\n",
      "INFO:absl:[23] train_loss=1.02323579788208, train_x1_loss=0.3281802237033844, train_x2_loss=0.6950553059577942\n",
      "INFO:absl:[23] val_loss=1.6287592649459839\n",
      "INFO:absl:[23] test_loss=1.67274010181427\n",
      "INFO:absl:[24] train_loss=1.0283896923065186, train_x1_loss=0.3332396447658539, train_x2_loss=0.6951490044593811\n",
      "INFO:absl:[24] val_loss=1.6247652769088745\n",
      "INFO:absl:[24] test_loss=1.6690841913223267\n",
      "INFO:absl:[25] train_loss=1.0315732955932617, train_x1_loss=0.3304937183856964, train_x2_loss=0.701079249382019\n",
      "INFO:absl:[25] val_loss=1.628509283065796\n",
      "INFO:absl:[25] test_loss=1.6723660230636597\n",
      "INFO:absl:[26] train_loss=1.0266026258468628, train_x1_loss=0.3300778865814209, train_x2_loss=0.696524441242218\n",
      "INFO:absl:[26] val_loss=1.633203148841858\n",
      "INFO:absl:[26] test_loss=1.677831768989563\n",
      "INFO:absl:[27] train_loss=1.0257488489151, train_x1_loss=0.32864704728126526, train_x2_loss=0.6971012949943542\n",
      "INFO:absl:[27] val_loss=1.6256935596466064\n",
      "INFO:absl:[27] test_loss=1.6701856851577759\n",
      "INFO:absl:[28] train_loss=1.0270118713378906, train_x1_loss=0.32945284247398376, train_x2_loss=0.6975594162940979\n",
      "INFO:absl:[28] val_loss=1.6294736862182617\n",
      "INFO:absl:[28] test_loss=1.6738758087158203\n",
      "INFO:absl:[29] train_loss=1.0310953855514526, train_x1_loss=0.334438294172287, train_x2_loss=0.6966565251350403\n",
      "INFO:absl:[29] val_loss=1.633763313293457\n",
      "INFO:absl:[29] test_loss=1.67806077003479\n",
      "INFO:absl:[30] train_loss=1.0249525308609009, train_x1_loss=0.3278130888938904, train_x2_loss=0.6971402168273926\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[30] val_loss=1.6273367404937744\n",
      "INFO:absl:[30] test_loss=1.671534776687622\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[31] train_loss=1.0273442268371582, train_x1_loss=0.32989680767059326, train_x2_loss=0.69744873046875\n",
      "INFO:absl:[31] val_loss=1.6333250999450684\n",
      "INFO:absl:[31] test_loss=1.6769417524337769\n",
      "INFO:absl:[32] train_loss=1.025955080986023, train_x1_loss=0.33009597659111023, train_x2_loss=0.6958604454994202\n",
      "INFO:absl:[32] val_loss=1.6195683479309082\n",
      "INFO:absl:[32] test_loss=1.663802146911621\n",
      "INFO:absl:[33] train_loss=1.0275189876556396, train_x1_loss=0.3304640054702759, train_x2_loss=0.6970547437667847\n",
      "INFO:absl:[33] val_loss=1.6322875022888184\n",
      "INFO:absl:[33] test_loss=1.6764405965805054\n",
      "INFO:absl:[34] train_loss=1.026512622833252, train_x1_loss=0.32957401871681213, train_x2_loss=0.6969375610351562\n",
      "INFO:absl:[34] val_loss=1.6277049779891968\n",
      "INFO:absl:[34] test_loss=1.671667218208313\n",
      "INFO:absl:[35] train_loss=1.0246303081512451, train_x1_loss=0.3269543945789337, train_x2_loss=0.697676420211792\n",
      "INFO:absl:[35] val_loss=1.6252198219299316\n",
      "INFO:absl:[35] test_loss=1.6694308519363403\n",
      "INFO:absl:Setting work unit notes: 2147.3 steps/s, 36.8% (128839/350000), ETA: 1m (1m : 0.0% checkpoint, 8.7% eval)\n",
      "INFO:absl:[128839] steps_per_sec=2147.307938\n",
      "INFO:absl:[36] train_loss=1.0281809568405151, train_x1_loss=0.3305265009403229, train_x2_loss=0.6976538896560669\n",
      "INFO:absl:[36] val_loss=1.6264723539352417\n",
      "INFO:absl:[36] test_loss=1.6704723834991455\n",
      "INFO:absl:[37] train_loss=1.0248667001724243, train_x1_loss=0.32822346687316895, train_x2_loss=0.6966434121131897\n",
      "INFO:absl:[37] val_loss=1.6275275945663452\n",
      "INFO:absl:[37] test_loss=1.6717966794967651\n",
      "INFO:absl:[38] train_loss=1.027148962020874, train_x1_loss=0.33064255118370056, train_x2_loss=0.6965067386627197\n",
      "INFO:absl:[38] val_loss=1.6285970211029053\n",
      "INFO:absl:[38] test_loss=1.6730011701583862\n",
      "INFO:absl:[39] train_loss=1.026784896850586, train_x1_loss=0.3305148184299469, train_x2_loss=0.6962717175483704\n",
      "INFO:absl:[39] val_loss=1.6321024894714355\n",
      "INFO:absl:[39] test_loss=1.6764726638793945\n",
      "INFO:absl:[40] train_loss=1.0251879692077637, train_x1_loss=0.3296096622943878, train_x2_loss=0.6955786943435669\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[40] val_loss=1.6221632957458496\n",
      "INFO:absl:[40] test_loss=1.6665446758270264\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[41] train_loss=1.0240694284439087, train_x1_loss=0.33083394169807434, train_x2_loss=0.6932365298271179\n",
      "INFO:absl:[41] val_loss=1.6276171207427979\n",
      "INFO:absl:[41] test_loss=1.6717828512191772\n",
      "INFO:absl:[42] train_loss=1.0224450826644897, train_x1_loss=0.32656291127204895, train_x2_loss=0.6958826780319214\n",
      "INFO:absl:[42] val_loss=1.6295874118804932\n",
      "INFO:absl:[42] test_loss=1.673876166343689\n",
      "INFO:absl:[43] train_loss=1.0232759714126587, train_x1_loss=0.32755064964294434, train_x2_loss=0.6957268118858337\n",
      "INFO:absl:[43] val_loss=1.6330288648605347\n",
      "INFO:absl:[43] test_loss=1.676994800567627\n",
      "INFO:absl:[44] train_loss=1.0274192094802856, train_x1_loss=0.33107754588127136, train_x2_loss=0.6963432431221008\n",
      "INFO:absl:[44] val_loss=1.6302181482315063\n",
      "INFO:absl:[44] test_loss=1.6745678186416626\n",
      "INFO:absl:[45] train_loss=1.0249849557876587, train_x1_loss=0.3266489505767822, train_x2_loss=0.698336124420166\n",
      "INFO:absl:[45] val_loss=1.6313629150390625\n",
      "INFO:absl:[45] test_loss=1.6750056743621826\n",
      "INFO:absl:[46] train_loss=1.0273956060409546, train_x1_loss=0.3296465277671814, train_x2_loss=0.6977474093437195\n",
      "INFO:absl:[46] val_loss=1.6367491483688354\n",
      "INFO:absl:[46] test_loss=1.6808727979660034\n",
      "INFO:absl:[47] train_loss=1.0258722305297852, train_x1_loss=0.32799476385116577, train_x2_loss=0.6978794932365417\n",
      "INFO:absl:[47] val_loss=1.6318860054016113\n",
      "INFO:absl:[47] test_loss=1.676221489906311\n",
      "INFO:absl:[48] train_loss=1.026604175567627, train_x1_loss=0.3279057741165161, train_x2_loss=0.6986969709396362\n",
      "INFO:absl:[48] val_loss=1.6254541873931885\n",
      "INFO:absl:[48] test_loss=1.669406771659851\n",
      "INFO:absl:[49] train_loss=1.0279200077056885, train_x1_loss=0.33192288875579834, train_x2_loss=0.6959969997406006\n",
      "INFO:absl:[49] val_loss=1.6228530406951904\n",
      "INFO:absl:[49] test_loss=1.6670316457748413\n",
      "INFO:absl:[50] train_loss=1.0272678136825562, train_x1_loss=0.3276282846927643, train_x2_loss=0.6996425986289978\n",
      "INFO:absl:[50] val_loss=1.6312941312789917\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[50] test_loss=1.674843668937683\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[51] train_loss=1.0298738479614258, train_x1_loss=0.33322596549987793, train_x2_loss=0.6966467499732971\n",
      "INFO:absl:[51] val_loss=1.6329725980758667\n",
      "INFO:absl:[51] test_loss=1.6767863035202026\n",
      "INFO:absl:[52] train_loss=1.0239745378494263, train_x1_loss=0.328981876373291, train_x2_loss=0.6949930191040039\n",
      "INFO:absl:[52] val_loss=1.6247670650482178\n",
      "INFO:absl:[52] test_loss=1.6687124967575073\n",
      "INFO:absl:[53] train_loss=1.021276593208313, train_x1_loss=0.32767540216445923, train_x2_loss=0.6935998797416687\n",
      "INFO:absl:[53] val_loss=1.6336184740066528\n",
      "INFO:absl:[53] test_loss=1.6777362823486328\n",
      "INFO:absl:[54] train_loss=1.0212106704711914, train_x1_loss=0.3254014849662781, train_x2_loss=0.6958091259002686\n",
      "INFO:absl:[54] val_loss=1.6304526329040527\n",
      "INFO:absl:[54] test_loss=1.6744980812072754\n",
      "INFO:absl:[55] train_loss=1.0241279602050781, train_x1_loss=0.3292371928691864, train_x2_loss=0.6948901414871216\n",
      "INFO:absl:[55] val_loss=1.6244456768035889\n",
      "INFO:absl:[55] test_loss=1.6688913106918335\n",
      "INFO:absl:[56] train_loss=1.022512435913086, train_x1_loss=0.32860153913497925, train_x2_loss=0.6939107179641724\n",
      "INFO:absl:[56] val_loss=1.6286033391952515\n",
      "INFO:absl:[56] test_loss=1.672696590423584\n",
      "INFO:absl:[57] train_loss=1.028456449508667, train_x1_loss=0.3312400281429291, train_x2_loss=0.6972162127494812\n",
      "INFO:absl:[57] val_loss=1.6318145990371704\n",
      "INFO:absl:[57] test_loss=1.6753849983215332\n",
      "INFO:absl:[58] train_loss=1.0219354629516602, train_x1_loss=0.32731032371520996, train_x2_loss=0.6946258544921875\n",
      "INFO:absl:[58] val_loss=1.6293323040008545\n",
      "INFO:absl:[58] test_loss=1.673414707183838\n",
      "INFO:absl:[59] train_loss=1.0206886529922485, train_x1_loss=0.3283287584781647, train_x2_loss=0.6923589706420898\n",
      "INFO:absl:[59] val_loss=1.6329752206802368\n",
      "INFO:absl:[59] test_loss=1.6772148609161377\n",
      "INFO:absl:[60] train_loss=1.0260088443756104, train_x1_loss=0.3274468779563904, train_x2_loss=0.6985642910003662\n",
      "INFO:absl:[60] val_loss=1.6263353824615479\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[60] test_loss=1.6706489324569702\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[61] train_loss=1.0239808559417725, train_x1_loss=0.3267184793949127, train_x2_loss=0.6972649693489075\n",
      "INFO:absl:[61] val_loss=1.6287075281143188\n",
      "INFO:absl:[61] test_loss=1.672813892364502\n",
      "INFO:absl:[62] train_loss=1.0241389274597168, train_x1_loss=0.32907968759536743, train_x2_loss=0.6950585246086121\n",
      "INFO:absl:[62] val_loss=1.6357489824295044\n",
      "INFO:absl:[62] test_loss=1.6799983978271484\n",
      "INFO:absl:[63] train_loss=1.0247243642807007, train_x1_loss=0.32907018065452576, train_x2_loss=0.6956540942192078\n",
      "INFO:absl:[63] val_loss=1.6232670545578003\n",
      "INFO:absl:[63] test_loss=1.6673452854156494\n",
      "INFO:absl:[64] train_loss=1.0313374996185303, train_x1_loss=0.33139461278915405, train_x2_loss=0.6999416351318359\n",
      "INFO:absl:[64] val_loss=1.626897931098938\n",
      "INFO:absl:[64] test_loss=1.6704164743423462\n",
      "INFO:absl:[65] train_loss=1.028016209602356, train_x1_loss=0.33087804913520813, train_x2_loss=0.6971387267112732\n",
      "INFO:absl:[65] val_loss=1.6257165670394897\n",
      "INFO:absl:[65] test_loss=1.6695964336395264\n",
      "INFO:absl:[66] train_loss=1.0272878408432007, train_x1_loss=0.32857784628868103, train_x2_loss=0.6987104415893555\n",
      "INFO:absl:[66] val_loss=1.6272779703140259\n",
      "INFO:absl:[66] test_loss=1.6713863611221313\n",
      "INFO:absl:[67] train_loss=1.02471125125885, train_x1_loss=0.32865872979164124, train_x2_loss=0.696051836013794\n",
      "INFO:absl:[67] val_loss=1.6255844831466675\n",
      "INFO:absl:[67] test_loss=1.6693719625473022\n",
      "INFO:absl:[68] train_loss=1.026628851890564, train_x1_loss=0.329912006855011, train_x2_loss=0.6967170834541321\n",
      "INFO:absl:[68] val_loss=1.626893401145935\n",
      "INFO:absl:[68] test_loss=1.6712405681610107\n",
      "INFO:absl:[69] train_loss=1.0234171152114868, train_x1_loss=0.32851341366767883, train_x2_loss=0.6949054002761841\n",
      "INFO:absl:[69] val_loss=1.6181331872940063\n",
      "INFO:absl:[69] test_loss=1.6625109910964966\n",
      "INFO:absl:[70] train_loss=1.023905873298645, train_x1_loss=0.32798895239830017, train_x2_loss=0.6959163546562195\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[70] val_loss=1.6303009986877441\n",
      "INFO:absl:[70] test_loss=1.6748474836349487\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[71] train_loss=1.022324562072754, train_x1_loss=0.3284657299518585, train_x2_loss=0.6938590407371521\n",
      "INFO:absl:[71] val_loss=1.6267896890640259\n",
      "INFO:absl:[71] test_loss=1.6708366870880127\n",
      "INFO:absl:[72] train_loss=1.0261305570602417, train_x1_loss=0.3285733759403229, train_x2_loss=0.6975580453872681\n",
      "INFO:absl:[72] val_loss=1.6293385028839111\n",
      "INFO:absl:[72] test_loss=1.6735409498214722\n",
      "INFO:absl:[73] train_loss=1.02226722240448, train_x1_loss=0.3285682201385498, train_x2_loss=0.6936995387077332\n",
      "INFO:absl:[73] val_loss=1.6325002908706665\n",
      "INFO:absl:[73] test_loss=1.6764904260635376\n",
      "INFO:absl:[74] train_loss=1.024154782295227, train_x1_loss=0.3289647102355957, train_x2_loss=0.6951903104782104\n",
      "INFO:absl:[74] val_loss=1.6230474710464478\n",
      "INFO:absl:[74] test_loss=1.6672132015228271\n",
      "INFO:absl:[75] train_loss=1.0238415002822876, train_x1_loss=0.32624417543411255, train_x2_loss=0.6975961327552795\n",
      "INFO:absl:[75] val_loss=1.6261718273162842\n",
      "INFO:absl:[75] test_loss=1.6702975034713745\n",
      "INFO:absl:[76] train_loss=1.0251619815826416, train_x1_loss=0.33113566040992737, train_x2_loss=0.6940246224403381\n",
      "INFO:absl:[76] val_loss=1.626882553100586\n",
      "INFO:absl:[76] test_loss=1.6710636615753174\n",
      "INFO:absl:[77] train_loss=1.0194227695465088, train_x1_loss=0.3244709074497223, train_x2_loss=0.6949524879455566\n",
      "INFO:absl:[77] val_loss=1.6310235261917114\n",
      "INFO:absl:[77] test_loss=1.675510048866272\n",
      "INFO:absl:[78] train_loss=1.0243865251541138, train_x1_loss=0.32598018646240234, train_x2_loss=0.6984061598777771\n",
      "INFO:absl:[78] val_loss=1.625930666923523\n",
      "INFO:absl:[78] test_loss=1.6700421571731567\n",
      "INFO:absl:Setting work unit notes: 2461.4 steps/s, 79.0% (276526/350000), ETA: 0m (2m : 0.0% checkpoint, 8.8% eval)\n",
      "INFO:absl:[276526] steps_per_sec=2461.436307\n",
      "INFO:absl:[79] train_loss=1.0259978771209717, train_x1_loss=0.33071964979171753, train_x2_loss=0.6952776312828064\n",
      "INFO:absl:[79] val_loss=1.6229867935180664\n",
      "INFO:absl:[79] test_loss=1.667280673980713\n",
      "INFO:absl:[80] train_loss=1.022829532623291, train_x1_loss=0.3264285624027252, train_x2_loss=0.6964017152786255\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[80] val_loss=1.6181474924087524\n",
      "INFO:absl:[80] test_loss=1.6621524095535278\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[81] train_loss=1.022000789642334, train_x1_loss=0.32653453946113586, train_x2_loss=0.695467472076416\n",
      "INFO:absl:[81] val_loss=1.6245837211608887\n",
      "INFO:absl:[81] test_loss=1.668351411819458\n",
      "INFO:absl:[82] train_loss=1.0218157768249512, train_x1_loss=0.32722970843315125, train_x2_loss=0.6945852637290955\n",
      "INFO:absl:[82] val_loss=1.6258070468902588\n",
      "INFO:absl:[82] test_loss=1.6693623065948486\n",
      "INFO:absl:[83] train_loss=1.0206310749053955, train_x1_loss=0.32540076971054077, train_x2_loss=0.6952304244041443\n",
      "INFO:absl:[83] val_loss=1.6284730434417725\n",
      "INFO:absl:[83] test_loss=1.6723281145095825\n",
      "INFO:absl:[84] train_loss=1.0232784748077393, train_x1_loss=0.3278963565826416, train_x2_loss=0.6953840255737305\n",
      "INFO:absl:[84] val_loss=1.6240394115447998\n",
      "INFO:absl:[84] test_loss=1.6679648160934448\n",
      "INFO:absl:[85] train_loss=1.0217939615249634, train_x1_loss=0.3254660964012146, train_x2_loss=0.6963293552398682\n",
      "INFO:absl:[85] val_loss=1.632200002670288\n",
      "INFO:absl:[85] test_loss=1.6758686304092407\n",
      "INFO:absl:[86] train_loss=1.0259865522384644, train_x1_loss=0.3284352719783783, train_x2_loss=0.6975516080856323\n",
      "INFO:absl:[86] val_loss=1.633190631866455\n",
      "INFO:absl:[86] test_loss=1.6773089170455933\n",
      "INFO:absl:[87] train_loss=1.024342656135559, train_x1_loss=0.3289501368999481, train_x2_loss=0.6953928470611572\n",
      "INFO:absl:[87] val_loss=1.6344146728515625\n",
      "INFO:absl:[87] test_loss=1.6785955429077148\n",
      "INFO:absl:[88] train_loss=1.0249319076538086, train_x1_loss=0.32809850573539734, train_x2_loss=0.6968331336975098\n",
      "INFO:absl:[88] val_loss=1.6273659467697144\n",
      "INFO:absl:[88] test_loss=1.6713916063308716\n",
      "INFO:absl:[89] train_loss=1.0208439826965332, train_x1_loss=0.32673370838165283, train_x2_loss=0.6941106915473938\n",
      "INFO:absl:[89] val_loss=1.6294982433319092\n",
      "INFO:absl:[89] test_loss=1.6738698482513428\n",
      "INFO:absl:[90] train_loss=1.0229694843292236, train_x1_loss=0.32732802629470825, train_x2_loss=0.6956422924995422\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[90] val_loss=1.6318873167037964\n",
      "INFO:absl:[90] test_loss=1.675908088684082\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[91] train_loss=1.0234332084655762, train_x1_loss=0.32917675375938416, train_x2_loss=0.6942560076713562\n",
      "INFO:absl:[91] val_loss=1.627736210823059\n",
      "INFO:absl:[91] test_loss=1.6718703508377075\n",
      "INFO:absl:[92] train_loss=1.0201737880706787, train_x1_loss=0.3254428505897522, train_x2_loss=0.6947318911552429\n",
      "INFO:absl:[92] val_loss=1.626392126083374\n",
      "INFO:absl:[92] test_loss=1.670337438583374\n",
      "INFO:absl:[93] train_loss=1.0250345468521118, train_x1_loss=0.32839691638946533, train_x2_loss=0.6966383457183838\n",
      "INFO:absl:[93] val_loss=1.6361534595489502\n",
      "INFO:absl:[93] test_loss=1.679766058921814\n",
      "INFO:absl:[94] train_loss=1.0172010660171509, train_x1_loss=0.324814110994339, train_x2_loss=0.6923885941505432\n",
      "INFO:absl:[94] val_loss=1.6272755861282349\n",
      "INFO:absl:[94] test_loss=1.6717841625213623\n",
      "INFO:absl:[95] train_loss=1.0196534395217896, train_x1_loss=0.32518860697746277, train_x2_loss=0.6944637894630432\n",
      "INFO:absl:[95] val_loss=1.6273859739303589\n",
      "INFO:absl:[95] test_loss=1.67171049118042\n",
      "INFO:absl:[96] train_loss=1.022334098815918, train_x1_loss=0.3267297148704529, train_x2_loss=0.6956036686897278\n",
      "INFO:absl:[96] val_loss=1.6350643634796143\n",
      "INFO:absl:[96] test_loss=1.6792222261428833\n",
      "INFO:absl:[97] train_loss=1.021877408027649, train_x1_loss=0.324669748544693, train_x2_loss=0.6972096562385559\n",
      "INFO:absl:[97] val_loss=1.634111762046814\n",
      "INFO:absl:[97] test_loss=1.6778827905654907\n",
      "INFO:absl:[98] train_loss=1.0269346237182617, train_x1_loss=0.32926446199417114, train_x2_loss=0.697670042514801\n",
      "INFO:absl:[98] val_loss=1.6229788064956665\n",
      "INFO:absl:[98] test_loss=1.667310357093811\n",
      "INFO:absl:[99] train_loss=1.019592046737671, train_x1_loss=0.3247482180595398, train_x2_loss=0.6948449611663818\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[99] val_loss=1.6209197044372559\n",
      "INFO:absl:[99] test_loss=1.665271520614624\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'relu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 10, 'dropout_rate': 0.21664822298158384, 'edge_features': (4, 8), 'epochs': 100, 'eval_every_epochs': 1, 'fully_connected_edges': 1, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 1, 'layer_norm': False, 'learning_rate': 2.9648345553652954e-05, 'log_every_epochs': 1, 'max_checkpts_to_keep': 2, 'model': 'MLPGraphNetwork', 'n_blocks': 1, 'n_samples': 5000, 'node_features': (128, 2), 'normalize': True, 'optimizer': 'adam', 'output_delay': 0, 'output_steps': 4, 'sample_buffer': -4, 'seed': 69, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 120, 'timestep_duration': 3, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+-----------+-------+----------+--------+\n",
      "| Name                                   | Shape     | Size  | Mean     | Std    |\n",
      "+----------------------------------------+-----------+-------+----------+--------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (4,)      | 4     | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 4)    | 24    | -0.0819  | 0.36   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (8,)      | 8     | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (4, 8)    | 32    | -0.0639  | 0.439  |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (128,)    | 128   | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (19, 128) | 2,432 | -0.00462 | 0.228  |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)      | 2     | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (128, 2)  | 256   | -0.00622 | 0.0884 |\n",
      "+----------------------------------------+-----------+-------+----------+--------+\n",
      "Total: 2,886\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Checkpoint None does not exist.\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.00s.\n",
      "INFO:absl:Starting training.\n",
      "INFO:absl:[0] train_loss=1.499811053276062, train_x1_loss=0.6589179635047913, train_x2_loss=0.8408899307250977\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[0] val_loss=1.8395787477493286\n",
      "INFO:absl:[0] test_loss=1.7208510637283325\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Created artifact [10] Profile of type ArtifactType.URL and value None.\n",
      "INFO:absl:[1] train_loss=1.1988290548324585, train_x1_loss=0.42967769503593445, train_x2_loss=0.7691516280174255\n",
      "INFO:absl:[1] val_loss=1.7491315603256226\n",
      "INFO:absl:[1] test_loss=1.6315395832061768\n",
      "INFO:absl:[2] train_loss=1.1281989812850952, train_x1_loss=0.38407063484191895, train_x2_loss=0.7441278100013733\n",
      "INFO:absl:[2] val_loss=1.7199573516845703\n",
      "INFO:absl:[2] test_loss=1.6031802892684937\n",
      "INFO:absl:[3] train_loss=1.0934884548187256, train_x1_loss=0.363508015871048, train_x2_loss=0.729979395866394\n",
      "INFO:absl:[3] val_loss=1.7102986574172974\n",
      "INFO:absl:[3] test_loss=1.5943437814712524\n",
      "INFO:absl:[4] train_loss=1.0742933750152588, train_x1_loss=0.3541920781135559, train_x2_loss=0.7201021909713745\n",
      "INFO:absl:[4] val_loss=1.6985746622085571\n",
      "INFO:absl:[4] test_loss=1.5831831693649292\n",
      "INFO:absl:[5] train_loss=1.0654200315475464, train_x1_loss=0.346145898103714, train_x2_loss=0.7192745804786682\n",
      "INFO:absl:[5] val_loss=1.6851389408111572\n",
      "INFO:absl:[5] test_loss=1.5698230266571045\n",
      "INFO:absl:[6] train_loss=1.056227207183838, train_x1_loss=0.3407869338989258, train_x2_loss=0.7154402732849121\n",
      "INFO:absl:[6] val_loss=1.6948429346084595\n",
      "INFO:absl:[6] test_loss=1.579719066619873\n",
      "INFO:absl:[7] train_loss=1.0548747777938843, train_x1_loss=0.3402705192565918, train_x2_loss=0.7146037220954895\n",
      "INFO:absl:[7] val_loss=1.6859391927719116\n",
      "INFO:absl:[7] test_loss=1.5706398487091064\n",
      "INFO:absl:[8] train_loss=1.0518416166305542, train_x1_loss=0.3402826189994812, train_x2_loss=0.7115585207939148\n",
      "INFO:absl:[8] val_loss=1.6766067743301392\n",
      "INFO:absl:[8] test_loss=1.5621206760406494\n",
      "INFO:absl:[9] train_loss=1.0492582321166992, train_x1_loss=0.3377096354961395, train_x2_loss=0.7115479707717896\n",
      "INFO:absl:[9] val_loss=1.67799973487854\n",
      "INFO:absl:[9] test_loss=1.5626187324523926\n",
      "INFO:absl:[10] train_loss=1.0464247465133667, train_x1_loss=0.33608385920524597, train_x2_loss=0.7103397250175476\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[10] val_loss=1.6718868017196655\n",
      "INFO:absl:[10] test_loss=1.5564329624176025\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[11] train_loss=1.048111081123352, train_x1_loss=0.33615031838417053, train_x2_loss=0.7119607925415039\n",
      "INFO:absl:[11] val_loss=1.674233078956604\n",
      "INFO:absl:[11] test_loss=1.558943271636963\n",
      "INFO:absl:[12] train_loss=1.038355827331543, train_x1_loss=0.33209356665611267, train_x2_loss=0.7062626481056213\n",
      "INFO:absl:[12] val_loss=1.671156883239746\n",
      "INFO:absl:[12] test_loss=1.5559110641479492\n",
      "INFO:absl:[13] train_loss=1.0421794652938843, train_x1_loss=0.33428531885147095, train_x2_loss=0.7078931927680969\n",
      "INFO:absl:[13] val_loss=1.6692794561386108\n",
      "INFO:absl:[13] test_loss=1.553968906402588\n",
      "INFO:absl:[14] train_loss=1.0399858951568604, train_x1_loss=0.33288222551345825, train_x2_loss=0.7071031928062439\n",
      "INFO:absl:[14] val_loss=1.6662635803222656\n",
      "INFO:absl:[14] test_loss=1.550526738166809\n",
      "INFO:absl:[15] train_loss=1.0379565954208374, train_x1_loss=0.332882821559906, train_x2_loss=0.7050732970237732\n",
      "INFO:absl:[15] val_loss=1.6614080667495728\n",
      "INFO:absl:[15] test_loss=1.5458173751831055\n",
      "INFO:absl:[16] train_loss=1.0373810529708862, train_x1_loss=0.33448752760887146, train_x2_loss=0.7028939723968506\n",
      "INFO:absl:[16] val_loss=1.6635040044784546\n",
      "INFO:absl:[16] test_loss=1.5484631061553955\n",
      "INFO:absl:[17] train_loss=1.038176417350769, train_x1_loss=0.3302692472934723, train_x2_loss=0.7079057693481445\n",
      "INFO:absl:[17] val_loss=1.6681020259857178\n",
      "INFO:absl:[17] test_loss=1.5522624254226685\n",
      "INFO:absl:[18] train_loss=1.0394542217254639, train_x1_loss=0.33418789505958557, train_x2_loss=0.7052675485610962\n",
      "INFO:absl:[18] val_loss=1.668911337852478\n",
      "INFO:absl:[18] test_loss=1.5534011125564575\n",
      "INFO:absl:[19] train_loss=1.0385483503341675, train_x1_loss=0.33438923954963684, train_x2_loss=0.7041592597961426\n",
      "INFO:absl:[19] val_loss=1.6670888662338257\n",
      "INFO:absl:[19] test_loss=1.5512306690216064\n",
      "INFO:absl:[20] train_loss=1.0358405113220215, train_x1_loss=0.331653892993927, train_x2_loss=0.704187273979187\n",
      "INFO:absl:[20] val_loss=1.6689538955688477\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[20] test_loss=1.5530943870544434\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[21] train_loss=1.0395054817199707, train_x1_loss=0.3338490426540375, train_x2_loss=0.7056558132171631\n",
      "INFO:absl:[21] val_loss=1.663416862487793\n",
      "INFO:absl:[21] test_loss=1.548147201538086\n",
      "INFO:absl:[22] train_loss=1.0331379175186157, train_x1_loss=0.3292942941188812, train_x2_loss=0.703845202922821\n",
      "INFO:absl:[22] val_loss=1.6607365608215332\n",
      "INFO:absl:[22] test_loss=1.5450892448425293\n",
      "INFO:absl:[23] train_loss=1.0360993146896362, train_x1_loss=0.33238470554351807, train_x2_loss=0.7037137746810913\n",
      "INFO:absl:[23] val_loss=1.666326642036438\n",
      "INFO:absl:[23] test_loss=1.55089271068573\n",
      "INFO:absl:[24] train_loss=1.0359394550323486, train_x1_loss=0.3302837908267975, train_x2_loss=0.7056549191474915\n",
      "INFO:absl:[24] val_loss=1.66399347782135\n",
      "INFO:absl:[24] test_loss=1.5482827425003052\n",
      "INFO:absl:[25] train_loss=1.036479115486145, train_x1_loss=0.3331214487552643, train_x2_loss=0.7033569812774658\n",
      "INFO:absl:[25] val_loss=1.6561859846115112\n",
      "INFO:absl:[25] test_loss=1.5410155057907104\n",
      "INFO:absl:[26] train_loss=1.0369154214859009, train_x1_loss=0.33178022503852844, train_x2_loss=0.7051345109939575\n",
      "INFO:absl:[26] val_loss=1.6633797883987427\n",
      "INFO:absl:[26] test_loss=1.5475300550460815\n",
      "INFO:absl:[27] train_loss=1.0324432849884033, train_x1_loss=0.32948464155197144, train_x2_loss=0.7029574513435364\n",
      "INFO:absl:[27] val_loss=1.6642391681671143\n",
      "INFO:absl:[27] test_loss=1.548473834991455\n",
      "INFO:absl:[28] train_loss=1.0341850519180298, train_x1_loss=0.32911157608032227, train_x2_loss=0.7050725221633911\n",
      "INFO:absl:[28] val_loss=1.6642605066299438\n",
      "INFO:absl:[28] test_loss=1.5487309694290161\n",
      "INFO:absl:[29] train_loss=1.0371525287628174, train_x1_loss=0.33208781480789185, train_x2_loss=0.7050628662109375\n",
      "INFO:absl:[29] val_loss=1.660737156867981\n",
      "INFO:absl:[29] test_loss=1.5454133749008179\n",
      "INFO:absl:[30] train_loss=1.0371332168579102, train_x1_loss=0.3338875472545624, train_x2_loss=0.7032449841499329\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[30] val_loss=1.6644290685653687\n",
      "INFO:absl:[30] test_loss=1.5484455823898315\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[31] train_loss=1.0358740091323853, train_x1_loss=0.3309585452079773, train_x2_loss=0.7049146294593811\n",
      "INFO:absl:[31] val_loss=1.6670658588409424\n",
      "INFO:absl:[31] test_loss=1.5508143901824951\n",
      "INFO:absl:[32] train_loss=1.035512924194336, train_x1_loss=0.3311294913291931, train_x2_loss=0.7043840885162354\n",
      "INFO:absl:[32] val_loss=1.6626825332641602\n",
      "INFO:absl:[32] test_loss=1.5467480421066284\n",
      "INFO:absl:[33] train_loss=1.0316071510314941, train_x1_loss=0.32915523648262024, train_x2_loss=0.7024521231651306\n",
      "INFO:absl:[33] val_loss=1.663400411605835\n",
      "INFO:absl:[33] test_loss=1.5479527711868286\n",
      "INFO:absl:[34] train_loss=1.036684513092041, train_x1_loss=0.33255985379219055, train_x2_loss=0.7041254639625549\n",
      "INFO:absl:[34] val_loss=1.6610771417617798\n",
      "INFO:absl:[34] test_loss=1.5456030368804932\n",
      "INFO:absl:[35] train_loss=1.0304129123687744, train_x1_loss=0.32702282071113586, train_x2_loss=0.7033895254135132\n",
      "INFO:absl:[35] val_loss=1.6595479249954224\n",
      "INFO:absl:[35] test_loss=1.5437172651290894\n",
      "INFO:absl:[36] train_loss=1.0378079414367676, train_x1_loss=0.33182597160339355, train_x2_loss=0.7059825658798218\n",
      "INFO:absl:[36] val_loss=1.6625093221664429\n",
      "INFO:absl:[36] test_loss=1.546846628189087\n",
      "INFO:absl:[37] train_loss=1.035230040550232, train_x1_loss=0.3296775221824646, train_x2_loss=0.7055520415306091\n",
      "INFO:absl:[37] val_loss=1.6652212142944336\n",
      "INFO:absl:[37] test_loss=1.5496063232421875\n",
      "INFO:absl:Setting work unit notes: 2222.9 steps/s, 38.1% (133373/350000), ETA: 1m (1m : 0.0% checkpoint, 9.4% eval)\n",
      "INFO:absl:[133373] steps_per_sec=2222.873847\n",
      "INFO:absl:[38] train_loss=1.0332313776016235, train_x1_loss=0.32980456948280334, train_x2_loss=0.7034264802932739\n",
      "INFO:absl:[38] val_loss=1.6614505052566528\n",
      "INFO:absl:[38] test_loss=1.5464357137680054\n",
      "INFO:absl:[39] train_loss=1.0307774543762207, train_x1_loss=0.32650744915008545, train_x2_loss=0.7042695879936218\n",
      "INFO:absl:[39] val_loss=1.6663904190063477\n",
      "INFO:absl:[39] test_loss=1.5512925386428833\n",
      "INFO:absl:[40] train_loss=1.0322387218475342, train_x1_loss=0.3288956880569458, train_x2_loss=0.7033450603485107\n",
      "INFO:absl:[40] val_loss=1.6643123626708984\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[40] test_loss=1.548785924911499\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[41] train_loss=1.0347340106964111, train_x1_loss=0.32974955439567566, train_x2_loss=0.7049850225448608\n",
      "INFO:absl:[41] val_loss=1.6636431217193604\n",
      "INFO:absl:[41] test_loss=1.5480716228485107\n",
      "INFO:absl:[42] train_loss=1.0318042039871216, train_x1_loss=0.3291069269180298, train_x2_loss=0.7026988863945007\n",
      "INFO:absl:[42] val_loss=1.665070652961731\n",
      "INFO:absl:[42] test_loss=1.549479365348816\n",
      "INFO:absl:[43] train_loss=1.032408595085144, train_x1_loss=0.328037828207016, train_x2_loss=0.7043703198432922\n",
      "INFO:absl:[43] val_loss=1.6644624471664429\n",
      "INFO:absl:[43] test_loss=1.5495898723602295\n",
      "INFO:absl:[44] train_loss=1.032122015953064, train_x1_loss=0.32876476645469666, train_x2_loss=0.7033577561378479\n",
      "INFO:absl:[44] val_loss=1.6614398956298828\n",
      "INFO:absl:[44] test_loss=1.5461301803588867\n",
      "INFO:absl:[45] train_loss=1.0302817821502686, train_x1_loss=0.32745984196662903, train_x2_loss=0.7028219699859619\n",
      "INFO:absl:[45] val_loss=1.666988730430603\n",
      "INFO:absl:[45] test_loss=1.551783561706543\n",
      "INFO:absl:[46] train_loss=1.0316706895828247, train_x1_loss=0.32835376262664795, train_x2_loss=0.7033163905143738\n",
      "INFO:absl:[46] val_loss=1.662807583808899\n",
      "INFO:absl:[46] test_loss=1.5477088689804077\n",
      "INFO:absl:[47] train_loss=1.0349562168121338, train_x1_loss=0.32887202501296997, train_x2_loss=0.7060856819152832\n",
      "INFO:absl:[47] val_loss=1.6688841581344604\n",
      "INFO:absl:[47] test_loss=1.5532819032669067\n",
      "INFO:absl:[48] train_loss=1.0302151441574097, train_x1_loss=0.32806968688964844, train_x2_loss=0.7021448016166687\n",
      "INFO:absl:[48] val_loss=1.6551259756088257\n",
      "INFO:absl:[48] test_loss=1.5394964218139648\n",
      "INFO:absl:[49] train_loss=1.0335019826889038, train_x1_loss=0.3293684422969818, train_x2_loss=0.7041326761245728\n",
      "INFO:absl:[49] val_loss=1.6637436151504517\n",
      "INFO:absl:[49] test_loss=1.5481280088424683\n",
      "INFO:absl:[50] train_loss=1.030019998550415, train_x1_loss=0.3281470537185669, train_x2_loss=0.7018740177154541\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[50] val_loss=1.66061270236969\n",
      "INFO:absl:[50] test_loss=1.5452370643615723\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[51] train_loss=1.0331435203552246, train_x1_loss=0.32899990677833557, train_x2_loss=0.704142689704895\n",
      "INFO:absl:[51] val_loss=1.6600970029830933\n",
      "INFO:absl:[51] test_loss=1.5444529056549072\n",
      "INFO:absl:[52] train_loss=1.0328469276428223, train_x1_loss=0.3296140432357788, train_x2_loss=0.7032333016395569\n",
      "INFO:absl:[52] val_loss=1.6627790927886963\n",
      "INFO:absl:[52] test_loss=1.5471357107162476\n",
      "INFO:absl:[53] train_loss=1.0313222408294678, train_x1_loss=0.32625120878219604, train_x2_loss=0.7050707936286926\n",
      "INFO:absl:[53] val_loss=1.6603114604949951\n",
      "INFO:absl:[53] test_loss=1.54460871219635\n",
      "INFO:absl:[54] train_loss=1.0324190855026245, train_x1_loss=0.3288953900337219, train_x2_loss=0.7035240530967712\n",
      "INFO:absl:[54] val_loss=1.6654287576675415\n",
      "INFO:absl:[54] test_loss=1.549735188484192\n",
      "INFO:absl:[55] train_loss=1.0307127237319946, train_x1_loss=0.3284090459346771, train_x2_loss=0.7023025751113892\n",
      "INFO:absl:[55] val_loss=1.6593995094299316\n",
      "INFO:absl:[55] test_loss=1.5438414812088013\n",
      "INFO:absl:[56] train_loss=1.0353293418884277, train_x1_loss=0.33138954639434814, train_x2_loss=0.7039397358894348\n",
      "INFO:absl:[56] val_loss=1.660325288772583\n",
      "INFO:absl:[56] test_loss=1.5446878671646118\n",
      "INFO:absl:[57] train_loss=1.0320647954940796, train_x1_loss=0.3300355076789856, train_x2_loss=0.7020277380943298\n",
      "INFO:absl:[57] val_loss=1.6629853248596191\n",
      "INFO:absl:[57] test_loss=1.547482967376709\n",
      "INFO:absl:[58] train_loss=1.0268291234970093, train_x1_loss=0.32665830850601196, train_x2_loss=0.7001712918281555\n",
      "INFO:absl:[58] val_loss=1.6646615266799927\n",
      "INFO:absl:[58] test_loss=1.5493208169937134\n",
      "INFO:absl:[59] train_loss=1.0315656661987305, train_x1_loss=0.3266911208629608, train_x2_loss=0.7048734426498413\n",
      "INFO:absl:[59] val_loss=1.6699060201644897\n",
      "INFO:absl:[59] test_loss=1.5545897483825684\n",
      "INFO:absl:[60] train_loss=1.033944845199585, train_x1_loss=0.3288068473339081, train_x2_loss=0.7051377892494202\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[60] val_loss=1.6642910242080688\n",
      "INFO:absl:[60] test_loss=1.5486241579055786\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[61] train_loss=1.033021092414856, train_x1_loss=0.32888665795326233, train_x2_loss=0.7041340470314026\n",
      "INFO:absl:[61] val_loss=1.6632412672042847\n",
      "INFO:absl:[61] test_loss=1.5476627349853516\n",
      "INFO:absl:[62] train_loss=1.0317515134811401, train_x1_loss=0.3279561996459961, train_x2_loss=0.7037959694862366\n",
      "INFO:absl:[62] val_loss=1.6602439880371094\n",
      "INFO:absl:[62] test_loss=1.5446646213531494\n",
      "INFO:absl:[63] train_loss=1.0320385694503784, train_x1_loss=0.33019888401031494, train_x2_loss=0.7018404603004456\n",
      "INFO:absl:[63] val_loss=1.6629174947738647\n",
      "INFO:absl:[63] test_loss=1.5475014448165894\n",
      "INFO:absl:[64] train_loss=1.0361382961273193, train_x1_loss=0.32851722836494446, train_x2_loss=0.7076213955879211\n",
      "INFO:absl:[64] val_loss=1.6595304012298584\n",
      "INFO:absl:[64] test_loss=1.5441007614135742\n",
      "INFO:absl:[65] train_loss=1.0325802564620972, train_x1_loss=0.32860836386680603, train_x2_loss=0.7039740085601807\n",
      "INFO:absl:[65] val_loss=1.6655312776565552\n",
      "INFO:absl:[65] test_loss=1.5496983528137207\n",
      "INFO:absl:[66] train_loss=1.032006025314331, train_x1_loss=0.32788753509521484, train_x2_loss=0.7041189074516296\n",
      "INFO:absl:[66] val_loss=1.6652038097381592\n",
      "INFO:absl:[66] test_loss=1.5500621795654297\n",
      "INFO:absl:[67] train_loss=1.0316487550735474, train_x1_loss=0.32650110125541687, train_x2_loss=0.7051474452018738\n",
      "INFO:absl:[67] val_loss=1.6594675779342651\n",
      "INFO:absl:[67] test_loss=1.5443311929702759\n",
      "INFO:absl:[68] train_loss=1.0303831100463867, train_x1_loss=0.3289567530155182, train_x2_loss=0.7014262676239014\n",
      "INFO:absl:[68] val_loss=1.6605225801467896\n",
      "INFO:absl:[68] test_loss=1.5452308654785156\n",
      "INFO:absl:[69] train_loss=1.033772587776184, train_x1_loss=0.33138787746429443, train_x2_loss=0.7023845314979553\n",
      "INFO:absl:[69] val_loss=1.6640970706939697\n",
      "INFO:absl:[69] test_loss=1.5489732027053833\n",
      "INFO:absl:[70] train_loss=1.0312749147415161, train_x1_loss=0.32826656103134155, train_x2_loss=0.703008770942688\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[70] val_loss=1.6624144315719604\n",
      "INFO:absl:[70] test_loss=1.5472906827926636\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[71] train_loss=1.028092384338379, train_x1_loss=0.3261801302433014, train_x2_loss=0.7019115686416626\n",
      "INFO:absl:[71] val_loss=1.6626039743423462\n",
      "INFO:absl:[71] test_loss=1.54750657081604\n",
      "INFO:absl:[72] train_loss=1.0351684093475342, train_x1_loss=0.3291836977005005, train_x2_loss=0.7059835195541382\n",
      "INFO:absl:[72] val_loss=1.666548728942871\n",
      "INFO:absl:[72] test_loss=1.551207423210144\n",
      "INFO:absl:[73] train_loss=1.032656192779541, train_x1_loss=0.32850611209869385, train_x2_loss=0.704147458076477\n",
      "INFO:absl:[73] val_loss=1.6670688390731812\n",
      "INFO:absl:[73] test_loss=1.5519402027130127\n",
      "INFO:absl:[74] train_loss=1.0346561670303345, train_x1_loss=0.3297986090183258, train_x2_loss=0.704856276512146\n",
      "INFO:absl:[74] val_loss=1.659549593925476\n",
      "INFO:absl:[74] test_loss=1.5442973375320435\n",
      "INFO:absl:[75] train_loss=1.0316174030303955, train_x1_loss=0.3280937373638153, train_x2_loss=0.7035270929336548\n",
      "INFO:absl:[75] val_loss=1.6578807830810547\n",
      "INFO:absl:[75] test_loss=1.5425471067428589\n",
      "INFO:absl:[76] train_loss=1.0289158821105957, train_x1_loss=0.32613497972488403, train_x2_loss=0.7027825117111206\n",
      "INFO:absl:[76] val_loss=1.6617063283920288\n",
      "INFO:absl:[76] test_loss=1.5464307069778442\n",
      "INFO:absl:[77] train_loss=1.0321952104568481, train_x1_loss=0.3271735906600952, train_x2_loss=0.7050232291221619\n",
      "INFO:absl:[77] val_loss=1.664183497428894\n",
      "INFO:absl:[77] test_loss=1.5489152669906616\n",
      "INFO:absl:[78] train_loss=1.0320050716400146, train_x1_loss=0.32731205224990845, train_x2_loss=0.7046923041343689\n",
      "INFO:absl:[78] val_loss=1.672411322593689\n",
      "INFO:absl:[78] test_loss=1.556914210319519\n",
      "INFO:absl:[79] train_loss=1.0312947034835815, train_x1_loss=0.3300691246986389, train_x2_loss=0.701227605342865\n",
      "INFO:absl:[79] val_loss=1.6620078086853027\n",
      "INFO:absl:[79] test_loss=1.5463818311691284\n",
      "INFO:absl:[80] train_loss=1.0342103242874146, train_x1_loss=0.32888519763946533, train_x2_loss=0.7053251266479492\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[80] val_loss=1.6684765815734863\n",
      "INFO:absl:[80] test_loss=1.553046703338623\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[81] train_loss=1.0337340831756592, train_x1_loss=0.327007919549942, train_x2_loss=0.7067255973815918\n",
      "INFO:absl:[81] val_loss=1.6586002111434937\n",
      "INFO:absl:Setting work unit notes: 2556.5 steps/s, 82.0% (287000/350000), ETA: 0m (2m : 0.0% checkpoint, 9.2% eval)\n",
      "INFO:absl:[81] test_loss=1.543209433555603\n",
      "INFO:absl:[287000] steps_per_sec=2556.489324\n",
      "INFO:absl:[82] train_loss=1.031101942062378, train_x1_loss=0.3274613320827484, train_x2_loss=0.7036391496658325\n",
      "INFO:absl:[82] val_loss=1.6630208492279053\n",
      "INFO:absl:[82] test_loss=1.5472958087921143\n",
      "INFO:absl:[83] train_loss=1.0294618606567383, train_x1_loss=0.3257629871368408, train_x2_loss=0.7037001848220825\n",
      "INFO:absl:[83] val_loss=1.6665507555007935\n",
      "INFO:absl:[83] test_loss=1.5508543252944946\n",
      "INFO:absl:[84] train_loss=1.0295578241348267, train_x1_loss=0.3281949758529663, train_x2_loss=0.7013640403747559\n",
      "INFO:absl:[84] val_loss=1.660492181777954\n",
      "INFO:absl:[84] test_loss=1.545112133026123\n",
      "INFO:absl:[85] train_loss=1.0324077606201172, train_x1_loss=0.3289068341255188, train_x2_loss=0.703499972820282\n",
      "INFO:absl:[85] val_loss=1.6575610637664795\n",
      "INFO:absl:[85] test_loss=1.5421464443206787\n",
      "INFO:absl:[86] train_loss=1.0331141948699951, train_x1_loss=0.3282087743282318, train_x2_loss=0.7049046158790588\n",
      "INFO:absl:[86] val_loss=1.6621028184890747\n",
      "INFO:absl:[86] test_loss=1.5465065240859985\n",
      "INFO:absl:[87] train_loss=1.032680630683899, train_x1_loss=0.3291989266872406, train_x2_loss=0.7034806609153748\n",
      "INFO:absl:[87] val_loss=1.6629513502120972\n",
      "INFO:absl:[87] test_loss=1.5477324724197388\n",
      "INFO:absl:[88] train_loss=1.0267671346664429, train_x1_loss=0.32471033930778503, train_x2_loss=0.7020580172538757\n",
      "INFO:absl:[88] val_loss=1.664573073387146\n",
      "INFO:absl:[88] test_loss=1.5490376949310303\n",
      "INFO:absl:[89] train_loss=1.0348474979400635, train_x1_loss=0.32932913303375244, train_x2_loss=0.7055193781852722\n",
      "INFO:absl:[89] val_loss=1.6633992195129395\n",
      "INFO:absl:[89] test_loss=1.5476888418197632\n",
      "INFO:absl:[90] train_loss=1.0293864011764526, train_x1_loss=0.3263480067253113, train_x2_loss=0.7030385136604309\n",
      "INFO:absl:[90] val_loss=1.6607924699783325\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[90] test_loss=1.5458706617355347\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[91] train_loss=1.0346649885177612, train_x1_loss=0.3309473395347595, train_x2_loss=0.7037189602851868\n",
      "INFO:absl:[91] val_loss=1.6650949716567993\n",
      "INFO:absl:[91] test_loss=1.5496273040771484\n",
      "INFO:absl:[92] train_loss=1.0358577966690063, train_x1_loss=0.3305132985115051, train_x2_loss=0.7053453326225281\n",
      "INFO:absl:[92] val_loss=1.665425181388855\n",
      "INFO:absl:[92] test_loss=1.549930214881897\n",
      "INFO:absl:[93] train_loss=1.0347177982330322, train_x1_loss=0.3308810293674469, train_x2_loss=0.7038382291793823\n",
      "INFO:absl:[93] val_loss=1.6620187759399414\n",
      "INFO:absl:[93] test_loss=1.5466877222061157\n",
      "INFO:absl:[94] train_loss=1.0281602144241333, train_x1_loss=0.3252319097518921, train_x2_loss=0.7029295563697815\n",
      "INFO:absl:[94] val_loss=1.6593937873840332\n",
      "INFO:absl:[94] test_loss=1.5435508489608765\n",
      "INFO:absl:[95] train_loss=1.029941439628601, train_x1_loss=0.3266705870628357, train_x2_loss=0.7032699584960938\n",
      "INFO:absl:[95] val_loss=1.6630806922912598\n",
      "INFO:absl:[95] test_loss=1.547463059425354\n",
      "INFO:absl:[96] train_loss=1.0297746658325195, train_x1_loss=0.3287322223186493, train_x2_loss=0.701042115688324\n",
      "INFO:absl:[96] val_loss=1.6570028066635132\n",
      "INFO:absl:[96] test_loss=1.5416061878204346\n",
      "INFO:absl:[97] train_loss=1.0292693376541138, train_x1_loss=0.3259514570236206, train_x2_loss=0.7033197283744812\n",
      "INFO:absl:[97] val_loss=1.657833218574524\n",
      "INFO:absl:[97] test_loss=1.5422685146331787\n",
      "INFO:absl:[98] train_loss=1.0348292589187622, train_x1_loss=0.33119142055511475, train_x2_loss=0.7036371231079102\n",
      "INFO:absl:[98] val_loss=1.6630276441574097\n",
      "INFO:absl:[98] test_loss=1.5472986698150635\n",
      "INFO:absl:[99] train_loss=1.0297192335128784, train_x1_loss=0.32463932037353516, train_x2_loss=0.7050805687904358\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[99] val_loss=1.6577274799346924\n",
      "INFO:absl:[99] test_loss=1.5425167083740234\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'relu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 10, 'dropout_rate': 0.21664822298158384, 'edge_features': (4, 8), 'epochs': 100, 'eval_every_epochs': 1, 'fully_connected_edges': 1, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 1, 'layer_norm': False, 'learning_rate': 2.9648345553652954e-05, 'log_every_epochs': 1, 'max_checkpts_to_keep': 2, 'model': 'MLPGraphNetwork', 'n_blocks': 1, 'n_samples': 5000, 'node_features': (128, 2), 'normalize': True, 'optimizer': 'adam', 'output_delay': 0, 'output_steps': 4, 'sample_buffer': -4, 'seed': 70, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 120, 'timestep_duration': 3, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+-----------+-------+----------+--------+\n",
      "| Name                                   | Shape     | Size  | Mean     | Std    |\n",
      "+----------------------------------------+-----------+-------+----------+--------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (4,)      | 4     | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 4)    | 24    | -0.0819  | 0.36   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (8,)      | 8     | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (4, 8)    | 32    | -0.0639  | 0.439  |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (128,)    | 128   | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (19, 128) | 2,432 | -0.00462 | 0.228  |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)      | 2     | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (128, 2)  | 256   | -0.00622 | 0.0884 |\n",
      "+----------------------------------------+-----------+-------+----------+--------+\n",
      "Total: 2,886\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Checkpoint None does not exist.\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.01s.\n",
      "INFO:absl:Starting training.\n",
      "INFO:absl:[0] train_loss=1.4940556287765503, train_x1_loss=0.6523528695106506, train_x2_loss=0.8417031168937683\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[0] val_loss=1.8304771184921265\n",
      "INFO:absl:[0] test_loss=1.891510009765625\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Created artifact [10] Profile of type ArtifactType.URL and value None.\n",
      "INFO:absl:[1] train_loss=1.1907010078430176, train_x1_loss=0.4286988079547882, train_x2_loss=0.7620033025741577\n",
      "INFO:absl:[1] val_loss=1.724969506263733\n",
      "INFO:absl:[1] test_loss=1.7794791460037231\n",
      "INFO:absl:[2] train_loss=1.1211820840835571, train_x1_loss=0.381580650806427, train_x2_loss=0.7396018505096436\n",
      "INFO:absl:[2] val_loss=1.709723949432373\n",
      "INFO:absl:[2] test_loss=1.7642827033996582\n",
      "INFO:absl:[3] train_loss=1.0851831436157227, train_x1_loss=0.36041778326034546, train_x2_loss=0.7247657775878906\n",
      "INFO:absl:[3] val_loss=1.695008397102356\n",
      "INFO:absl:[3] test_loss=1.7497895956039429\n",
      "INFO:absl:[4] train_loss=1.0744249820709229, train_x1_loss=0.35435640811920166, train_x2_loss=0.7200684547424316\n",
      "INFO:absl:[4] val_loss=1.6822654008865356\n",
      "INFO:absl:[4] test_loss=1.7364789247512817\n",
      "INFO:absl:[5] train_loss=1.0654430389404297, train_x1_loss=0.3491094410419464, train_x2_loss=0.7163324356079102\n",
      "INFO:absl:[5] val_loss=1.6800408363342285\n",
      "INFO:absl:[5] test_loss=1.7348445653915405\n",
      "INFO:absl:[6] train_loss=1.05808424949646, train_x1_loss=0.3432372808456421, train_x2_loss=0.7148464918136597\n",
      "INFO:absl:[6] val_loss=1.6832101345062256\n",
      "INFO:absl:[6] test_loss=1.737633466720581\n",
      "INFO:absl:[7] train_loss=1.0523384809494019, train_x1_loss=0.34059542417526245, train_x2_loss=0.7117412090301514\n",
      "INFO:absl:[7] val_loss=1.6698863506317139\n",
      "INFO:absl:[7] test_loss=1.7239493131637573\n",
      "INFO:absl:[8] train_loss=1.0469387769699097, train_x1_loss=0.337566614151001, train_x2_loss=0.7093726396560669\n",
      "INFO:absl:[8] val_loss=1.672582745552063\n",
      "INFO:absl:[8] test_loss=1.726777195930481\n",
      "INFO:absl:[9] train_loss=1.045867919921875, train_x1_loss=0.3368591070175171, train_x2_loss=0.7090083956718445\n",
      "INFO:absl:[9] val_loss=1.657392978668213\n",
      "INFO:absl:[9] test_loss=1.7108526229858398\n",
      "INFO:absl:[10] train_loss=1.0422061681747437, train_x1_loss=0.3333551585674286, train_x2_loss=0.7088495492935181\n",
      "INFO:absl:[10] val_loss=1.6653653383255005\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[10] test_loss=1.7195074558258057\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[11] train_loss=1.0451524257659912, train_x1_loss=0.3376588225364685, train_x2_loss=0.7074927687644958\n",
      "INFO:absl:[11] val_loss=1.6628401279449463\n",
      "INFO:absl:[11] test_loss=1.716581106185913\n",
      "INFO:absl:[12] train_loss=1.0356863737106323, train_x1_loss=0.3316810131072998, train_x2_loss=0.7040062546730042\n",
      "INFO:absl:[12] val_loss=1.6556369066238403\n",
      "INFO:absl:[12] test_loss=1.7094485759735107\n",
      "INFO:absl:[13] train_loss=1.0336698293685913, train_x1_loss=0.3300545811653137, train_x2_loss=0.7036166191101074\n",
      "INFO:absl:[13] val_loss=1.6559582948684692\n",
      "INFO:absl:[13] test_loss=1.7099741697311401\n",
      "INFO:absl:[14] train_loss=1.0398378372192383, train_x1_loss=0.3333788216114044, train_x2_loss=0.7064597010612488\n",
      "INFO:absl:[14] val_loss=1.6555217504501343\n",
      "INFO:absl:[14] test_loss=1.7095916271209717\n",
      "INFO:absl:[15] train_loss=1.0382319688796997, train_x1_loss=0.3344488739967346, train_x2_loss=0.7037843465805054\n",
      "INFO:absl:[15] val_loss=1.6509912014007568\n",
      "INFO:absl:[15] test_loss=1.704322338104248\n",
      "INFO:absl:[16] train_loss=1.040842056274414, train_x1_loss=0.33719298243522644, train_x2_loss=0.7036494016647339\n",
      "INFO:absl:[16] val_loss=1.649205207824707\n",
      "INFO:absl:[16] test_loss=1.7023911476135254\n",
      "INFO:absl:[17] train_loss=1.0344642400741577, train_x1_loss=0.32963258028030396, train_x2_loss=0.7048323154449463\n",
      "INFO:absl:[17] val_loss=1.654148817062378\n",
      "INFO:absl:[17] test_loss=1.707737684249878\n",
      "INFO:absl:[18] train_loss=1.0342066287994385, train_x1_loss=0.3314559757709503, train_x2_loss=0.7027508616447449\n",
      "INFO:absl:[18] val_loss=1.6478477716445923\n",
      "INFO:absl:[18] test_loss=1.7015630006790161\n",
      "INFO:absl:[19] train_loss=1.0317668914794922, train_x1_loss=0.3287616968154907, train_x2_loss=0.7030045986175537\n",
      "INFO:absl:[19] val_loss=1.6534209251403809\n",
      "INFO:absl:[19] test_loss=1.7068768739700317\n",
      "INFO:absl:[20] train_loss=1.0356351137161255, train_x1_loss=0.33403879404067993, train_x2_loss=0.701596736907959\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[20] val_loss=1.6497974395751953\n",
      "INFO:absl:[20] test_loss=1.703330397605896\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[21] train_loss=1.0283010005950928, train_x1_loss=0.32855895161628723, train_x2_loss=0.6997414827346802\n",
      "INFO:absl:[21] val_loss=1.6514321565628052\n",
      "INFO:absl:[21] test_loss=1.705051064491272\n",
      "INFO:absl:[22] train_loss=1.0329285860061646, train_x1_loss=0.3299667537212372, train_x2_loss=0.7029612064361572\n",
      "INFO:absl:[22] val_loss=1.6591100692749023\n",
      "INFO:absl:[22] test_loss=1.7128502130508423\n",
      "INFO:absl:[23] train_loss=1.03485107421875, train_x1_loss=0.3308611512184143, train_x2_loss=0.7039889097213745\n",
      "INFO:absl:[23] val_loss=1.6522623300552368\n",
      "INFO:absl:[23] test_loss=1.7061388492584229\n",
      "INFO:absl:[24] train_loss=1.0335383415222168, train_x1_loss=0.330782413482666, train_x2_loss=0.7027546763420105\n",
      "INFO:absl:[24] val_loss=1.6505928039550781\n",
      "INFO:absl:[24] test_loss=1.7044577598571777\n",
      "INFO:absl:[25] train_loss=1.03500497341156, train_x1_loss=0.3310074210166931, train_x2_loss=0.7039971351623535\n",
      "INFO:absl:[25] val_loss=1.6471076011657715\n",
      "INFO:absl:[25] test_loss=1.701061487197876\n",
      "INFO:absl:[26] train_loss=1.0292539596557617, train_x1_loss=0.33071237802505493, train_x2_loss=0.6985421776771545\n",
      "INFO:absl:[26] val_loss=1.64561927318573\n",
      "INFO:absl:[26] test_loss=1.699601650238037\n",
      "INFO:absl:[27] train_loss=1.0331528186798096, train_x1_loss=0.33050233125686646, train_x2_loss=0.7026517391204834\n",
      "INFO:absl:[27] val_loss=1.6499220132827759\n",
      "INFO:absl:[27] test_loss=1.7039029598236084\n",
      "INFO:absl:[28] train_loss=1.0327644348144531, train_x1_loss=0.3298681080341339, train_x2_loss=0.7028965353965759\n",
      "INFO:absl:[28] val_loss=1.6553958654403687\n",
      "INFO:absl:[28] test_loss=1.7091350555419922\n",
      "INFO:absl:[29] train_loss=1.0353033542633057, train_x1_loss=0.33317500352859497, train_x2_loss=0.7021258473396301\n",
      "INFO:absl:[29] val_loss=1.6532409191131592\n",
      "INFO:absl:[29] test_loss=1.7074495553970337\n",
      "INFO:absl:[30] train_loss=1.030396580696106, train_x1_loss=0.32874009013175964, train_x2_loss=0.7016602158546448\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[30] val_loss=1.6468756198883057\n",
      "INFO:absl:[30] test_loss=1.7007800340652466\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[31] train_loss=1.0328956842422485, train_x1_loss=0.3335131108760834, train_x2_loss=0.6993825435638428\n",
      "INFO:absl:[31] val_loss=1.6490565538406372\n",
      "INFO:absl:[31] test_loss=1.7030839920043945\n",
      "INFO:absl:[32] train_loss=1.0283334255218506, train_x1_loss=0.3292433023452759, train_x2_loss=0.6990898847579956\n",
      "INFO:absl:[32] val_loss=1.6420073509216309\n",
      "INFO:absl:[32] test_loss=1.6956768035888672\n",
      "INFO:absl:[33] train_loss=1.0302610397338867, train_x1_loss=0.3300457298755646, train_x2_loss=0.7002167701721191\n",
      "INFO:absl:[33] val_loss=1.651033639907837\n",
      "INFO:absl:[33] test_loss=1.704864740371704\n",
      "INFO:absl:[34] train_loss=1.0321236848831177, train_x1_loss=0.3298093378543854, train_x2_loss=0.7023158073425293\n",
      "INFO:absl:[34] val_loss=1.6514009237289429\n",
      "INFO:absl:[34] test_loss=1.7057026624679565\n",
      "INFO:absl:[35] train_loss=1.0286381244659424, train_x1_loss=0.32771676778793335, train_x2_loss=0.7009196877479553\n",
      "INFO:absl:[35] val_loss=1.6482261419296265\n",
      "INFO:absl:[35] test_loss=1.7020915746688843\n",
      "INFO:absl:[36] train_loss=1.032771110534668, train_x1_loss=0.33066290616989136, train_x2_loss=0.7021099328994751\n",
      "INFO:absl:[36] val_loss=1.6531261205673218\n",
      "INFO:absl:[36] test_loss=1.7065941095352173\n",
      "INFO:absl:Setting work unit notes: 2170.9 steps/s, 37.2% (130257/350000), ETA: 1m (1m : 0.0% checkpoint, 9.0% eval)\n",
      "INFO:absl:[130257] steps_per_sec=2170.943780\n",
      "INFO:absl:[37] train_loss=1.0294162034988403, train_x1_loss=0.3296567499637604, train_x2_loss=0.6997607946395874\n",
      "INFO:absl:[37] val_loss=1.6514873504638672\n",
      "INFO:absl:[37] test_loss=1.7056515216827393\n",
      "INFO:absl:[38] train_loss=1.0266611576080322, train_x1_loss=0.3272158205509186, train_x2_loss=0.6994473338127136\n",
      "INFO:absl:[38] val_loss=1.6462379693984985\n",
      "INFO:absl:[38] test_loss=1.7003564834594727\n",
      "INFO:absl:[39] train_loss=1.028874158859253, train_x1_loss=0.3290453255176544, train_x2_loss=0.6998304128646851\n",
      "INFO:absl:[39] val_loss=1.6542251110076904\n",
      "INFO:absl:[39] test_loss=1.7085232734680176\n",
      "INFO:absl:[40] train_loss=1.0318588018417358, train_x1_loss=0.32867103815078735, train_x2_loss=0.7031891942024231\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[40] val_loss=1.6576414108276367\n",
      "INFO:absl:[40] test_loss=1.7122637033462524\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[41] train_loss=1.030975103378296, train_x1_loss=0.32844555377960205, train_x2_loss=0.7025288343429565\n",
      "INFO:absl:[41] val_loss=1.6525044441223145\n",
      "INFO:absl:[41] test_loss=1.7070682048797607\n",
      "INFO:absl:[42] train_loss=1.0259205102920532, train_x1_loss=0.32711201906204224, train_x2_loss=0.6988090872764587\n",
      "INFO:absl:[42] val_loss=1.6455094814300537\n",
      "INFO:absl:[42] test_loss=1.7000467777252197\n",
      "INFO:absl:[43] train_loss=1.0300137996673584, train_x1_loss=0.3293376863002777, train_x2_loss=0.7006750106811523\n",
      "INFO:absl:[43] val_loss=1.6532336473464966\n",
      "INFO:absl:[43] test_loss=1.7077083587646484\n",
      "INFO:absl:[44] train_loss=1.0291889905929565, train_x1_loss=0.3285849988460541, train_x2_loss=0.700603723526001\n",
      "INFO:absl:[44] val_loss=1.645108699798584\n",
      "INFO:absl:[44] test_loss=1.6988743543624878\n",
      "INFO:absl:[45] train_loss=1.0325497388839722, train_x1_loss=0.328414648771286, train_x2_loss=0.7041351795196533\n",
      "INFO:absl:[45] val_loss=1.6501836776733398\n",
      "INFO:absl:[45] test_loss=1.7046064138412476\n",
      "INFO:absl:[46] train_loss=1.0284851789474487, train_x1_loss=0.32792821526527405, train_x2_loss=0.7005571722984314\n",
      "INFO:absl:[46] val_loss=1.653456449508667\n",
      "INFO:absl:[46] test_loss=1.7076157331466675\n",
      "INFO:absl:[47] train_loss=1.0311976671218872, train_x1_loss=0.33020874857902527, train_x2_loss=0.7009890675544739\n",
      "INFO:absl:[47] val_loss=1.650651216506958\n",
      "INFO:absl:[47] test_loss=1.7046096324920654\n",
      "INFO:absl:[48] train_loss=1.0275568962097168, train_x1_loss=0.32655924558639526, train_x2_loss=0.7009977698326111\n",
      "INFO:absl:[48] val_loss=1.6422361135482788\n",
      "INFO:absl:[48] test_loss=1.6964296102523804\n",
      "INFO:absl:[49] train_loss=1.0304702520370483, train_x1_loss=0.3280477821826935, train_x2_loss=0.702421247959137\n",
      "INFO:absl:[49] val_loss=1.6493905782699585\n",
      "INFO:absl:[49] test_loss=1.7031996250152588\n",
      "INFO:absl:[50] train_loss=1.0245574712753296, train_x1_loss=0.3267859220504761, train_x2_loss=0.6977725028991699\n",
      "INFO:absl:[50] val_loss=1.637526273727417\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[50] test_loss=1.6918138265609741\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[51] train_loss=1.0340023040771484, train_x1_loss=0.33187198638916016, train_x2_loss=0.7021315097808838\n",
      "INFO:absl:[51] val_loss=1.6478970050811768\n",
      "INFO:absl:[51] test_loss=1.7017536163330078\n",
      "INFO:absl:[52] train_loss=1.0321195125579834, train_x1_loss=0.33050021529197693, train_x2_loss=0.7016200423240662\n",
      "INFO:absl:[52] val_loss=1.6448391675949097\n",
      "INFO:absl:[52] test_loss=1.6991249322891235\n",
      "INFO:absl:[53] train_loss=1.0322479009628296, train_x1_loss=0.32973286509513855, train_x2_loss=0.7025158405303955\n",
      "INFO:absl:[53] val_loss=1.6483570337295532\n",
      "INFO:absl:[53] test_loss=1.7023749351501465\n",
      "INFO:absl:[54] train_loss=1.0291210412979126, train_x1_loss=0.3274313509464264, train_x2_loss=0.7016915678977966\n",
      "INFO:absl:[54] val_loss=1.6453049182891846\n",
      "INFO:absl:[54] test_loss=1.6996116638183594\n",
      "INFO:absl:[55] train_loss=1.0291213989257812, train_x1_loss=0.3272055387496948, train_x2_loss=0.7019152641296387\n",
      "INFO:absl:[55] val_loss=1.6446926593780518\n",
      "INFO:absl:[55] test_loss=1.6988118886947632\n",
      "INFO:absl:[56] train_loss=1.0303142070770264, train_x1_loss=0.3285214602947235, train_x2_loss=0.7017927169799805\n",
      "INFO:absl:[56] val_loss=1.6499284505844116\n",
      "INFO:absl:[56] test_loss=1.7043261528015137\n",
      "INFO:absl:[57] train_loss=1.0327510833740234, train_x1_loss=0.33195677399635315, train_x2_loss=0.700794517993927\n",
      "INFO:absl:[57] val_loss=1.6483650207519531\n",
      "INFO:absl:[57] test_loss=1.7029842138290405\n",
      "INFO:absl:[58] train_loss=1.029682993888855, train_x1_loss=0.3284553587436676, train_x2_loss=0.7012287974357605\n",
      "INFO:absl:[58] val_loss=1.646855354309082\n",
      "INFO:absl:[58] test_loss=1.701032280921936\n",
      "INFO:absl:[59] train_loss=1.026513934135437, train_x1_loss=0.32863491773605347, train_x2_loss=0.6978791356086731\n",
      "INFO:absl:[59] val_loss=1.6473561525344849\n",
      "INFO:absl:[59] test_loss=1.7014461755752563\n",
      "INFO:absl:[60] train_loss=1.0327776670455933, train_x1_loss=0.3297247588634491, train_x2_loss=0.7030526399612427\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[60] val_loss=1.6468696594238281\n",
      "INFO:absl:[60] test_loss=1.701119065284729\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[61] train_loss=1.0294159650802612, train_x1_loss=0.3274400532245636, train_x2_loss=0.7019757032394409\n",
      "INFO:absl:[61] val_loss=1.6414034366607666\n",
      "INFO:absl:[61] test_loss=1.6956584453582764\n",
      "INFO:absl:[62] train_loss=1.0255314111709595, train_x1_loss=0.32543107867240906, train_x2_loss=0.7001006603240967\n",
      "INFO:absl:[62] val_loss=1.6500816345214844\n",
      "INFO:absl:[62] test_loss=1.7044768333435059\n",
      "INFO:absl:[63] train_loss=1.0306951999664307, train_x1_loss=0.33035996556282043, train_x2_loss=0.7003359794616699\n",
      "INFO:absl:[63] val_loss=1.644916296005249\n",
      "INFO:absl:[63] test_loss=1.6988413333892822\n",
      "INFO:absl:[64] train_loss=1.0296595096588135, train_x1_loss=0.32864275574684143, train_x2_loss=0.7010149359703064\n",
      "INFO:absl:[64] val_loss=1.6532198190689087\n",
      "INFO:absl:[64] test_loss=1.7079623937606812\n",
      "INFO:absl:[65] train_loss=1.0351760387420654, train_x1_loss=0.33095407485961914, train_x2_loss=0.7042236924171448\n",
      "INFO:absl:[65] val_loss=1.6492087841033936\n",
      "INFO:absl:[65] test_loss=1.70294189453125\n",
      "INFO:absl:[66] train_loss=1.028753638267517, train_x1_loss=0.3295355439186096, train_x2_loss=0.6992174983024597\n",
      "INFO:absl:[66] val_loss=1.6549832820892334\n",
      "INFO:absl:[66] test_loss=1.7090859413146973\n",
      "INFO:absl:[67] train_loss=1.0312838554382324, train_x1_loss=0.32958105206489563, train_x2_loss=0.7017036080360413\n",
      "INFO:absl:[67] val_loss=1.6505374908447266\n",
      "INFO:absl:[67] test_loss=1.7044216394424438\n",
      "INFO:absl:[68] train_loss=1.0301152467727661, train_x1_loss=0.3294731676578522, train_x2_loss=0.7006420493125916\n",
      "INFO:absl:[68] val_loss=1.6493723392486572\n",
      "INFO:absl:[68] test_loss=1.7034419775009155\n",
      "INFO:absl:[69] train_loss=1.0309606790542603, train_x1_loss=0.32928937673568726, train_x2_loss=0.7016717195510864\n",
      "INFO:absl:[69] val_loss=1.6539952754974365\n",
      "INFO:absl:[69] test_loss=1.707828402519226\n",
      "INFO:absl:[70] train_loss=1.0279988050460815, train_x1_loss=0.33070582151412964, train_x2_loss=0.6972947120666504\n",
      "INFO:absl:[70] val_loss=1.6547492742538452\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[70] test_loss=1.7086679935455322\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[71] train_loss=1.0263230800628662, train_x1_loss=0.32824522256851196, train_x2_loss=0.698079526424408\n",
      "INFO:absl:[71] val_loss=1.6438746452331543\n",
      "INFO:absl:[71] test_loss=1.6976642608642578\n",
      "INFO:absl:[72] train_loss=1.0318567752838135, train_x1_loss=0.32943812012672424, train_x2_loss=0.7024193406105042\n",
      "INFO:absl:[72] val_loss=1.6474790573120117\n",
      "INFO:absl:[72] test_loss=1.701277732849121\n",
      "INFO:absl:[73] train_loss=1.0280930995941162, train_x1_loss=0.3280957341194153, train_x2_loss=0.699997067451477\n",
      "INFO:absl:[73] val_loss=1.6489381790161133\n",
      "INFO:absl:[73] test_loss=1.702758550643921\n",
      "INFO:absl:[74] train_loss=1.030877709388733, train_x1_loss=0.32888931035995483, train_x2_loss=0.7019890546798706\n",
      "INFO:absl:[74] val_loss=1.650354266166687\n",
      "INFO:absl:[74] test_loss=1.704214334487915\n",
      "INFO:absl:[75] train_loss=1.034104347229004, train_x1_loss=0.3302565813064575, train_x2_loss=0.7038483023643494\n",
      "INFO:absl:[75] val_loss=1.6457483768463135\n",
      "INFO:absl:[75] test_loss=1.6997064352035522\n",
      "INFO:absl:[76] train_loss=1.026978850364685, train_x1_loss=0.32592862844467163, train_x2_loss=0.7010498642921448\n",
      "INFO:absl:[76] val_loss=1.6509555578231812\n",
      "INFO:absl:[76] test_loss=1.704939842224121\n",
      "INFO:absl:[77] train_loss=1.0288887023925781, train_x1_loss=0.3279956877231598, train_x2_loss=0.7008932828903198\n",
      "INFO:absl:[77] val_loss=1.645833134651184\n",
      "INFO:absl:[77] test_loss=1.6998579502105713\n",
      "INFO:absl:[78] train_loss=1.0284255743026733, train_x1_loss=0.32797276973724365, train_x2_loss=0.700453519821167\n",
      "INFO:absl:[78] val_loss=1.6464946269989014\n",
      "INFO:absl:[78] test_loss=1.700399398803711\n",
      "INFO:absl:[79] train_loss=1.0276405811309814, train_x1_loss=0.3275090754032135, train_x2_loss=0.7001312375068665\n",
      "INFO:absl:[79] val_loss=1.6467729806900024\n",
      "INFO:absl:[79] test_loss=1.700770616531372\n",
      "INFO:absl:Setting work unit notes: 2547.0 steps/s, 80.9% (283079/350000), ETA: 0m (2m : 0.0% checkpoint, 8.9% eval)\n",
      "INFO:absl:[283079] steps_per_sec=2547.030874\n",
      "INFO:absl:[80] train_loss=1.0285472869873047, train_x1_loss=0.32736364006996155, train_x2_loss=0.7011839151382446\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[80] val_loss=1.6486082077026367\n",
      "INFO:absl:[80] test_loss=1.7025823593139648\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[81] train_loss=1.0270700454711914, train_x1_loss=0.3266100585460663, train_x2_loss=0.7004600167274475\n",
      "INFO:absl:[81] val_loss=1.6442561149597168\n",
      "INFO:absl:[81] test_loss=1.6982980966567993\n",
      "INFO:absl:[82] train_loss=1.0270929336547852, train_x1_loss=0.3275453746318817, train_x2_loss=0.6995477080345154\n",
      "INFO:absl:[82] val_loss=1.649930715560913\n",
      "INFO:absl:[82] test_loss=1.7039055824279785\n",
      "INFO:absl:[83] train_loss=1.024960994720459, train_x1_loss=0.3270013928413391, train_x2_loss=0.6979600787162781\n",
      "INFO:absl:[83] val_loss=1.6473432779312134\n",
      "INFO:absl:[83] test_loss=1.7008436918258667\n",
      "INFO:absl:[84] train_loss=1.0274327993392944, train_x1_loss=0.3275551497936249, train_x2_loss=0.6998763680458069\n",
      "INFO:absl:[84] val_loss=1.6522111892700195\n",
      "INFO:absl:[84] test_loss=1.7055968046188354\n",
      "INFO:absl:[85] train_loss=1.0277214050292969, train_x1_loss=0.32704269886016846, train_x2_loss=0.7006799578666687\n",
      "INFO:absl:[85] val_loss=1.6500428915023804\n",
      "INFO:absl:[85] test_loss=1.7042746543884277\n",
      "INFO:absl:[86] train_loss=1.0313763618469238, train_x1_loss=0.33025649189949036, train_x2_loss=0.7011203169822693\n",
      "INFO:absl:[86] val_loss=1.6519790887832642\n",
      "INFO:absl:[86] test_loss=1.7061266899108887\n",
      "INFO:absl:[87] train_loss=1.0274548530578613, train_x1_loss=0.3269703984260559, train_x2_loss=0.7004844546318054\n",
      "INFO:absl:[87] val_loss=1.6498267650604248\n",
      "INFO:absl:[87] test_loss=1.7035197019577026\n",
      "INFO:absl:[88] train_loss=1.0252587795257568, train_x1_loss=0.3243841230869293, train_x2_loss=0.7008768320083618\n",
      "INFO:absl:[88] val_loss=1.6541152000427246\n",
      "INFO:absl:[88] test_loss=1.7080769538879395\n",
      "INFO:absl:[89] train_loss=1.0273499488830566, train_x1_loss=0.326479971408844, train_x2_loss=0.70087069272995\n",
      "INFO:absl:[89] val_loss=1.6424404382705688\n",
      "INFO:absl:[89] test_loss=1.69606614112854\n",
      "INFO:absl:[90] train_loss=1.0285217761993408, train_x1_loss=0.32855936884880066, train_x2_loss=0.6999620795249939\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[90] val_loss=1.656887412071228\n",
      "INFO:absl:[90] test_loss=1.7109036445617676\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[91] train_loss=1.0314997434616089, train_x1_loss=0.33017498254776, train_x2_loss=0.7013260126113892\n",
      "INFO:absl:[91] val_loss=1.65337336063385\n",
      "INFO:absl:[91] test_loss=1.7067378759384155\n",
      "INFO:absl:[92] train_loss=1.0227577686309814, train_x1_loss=0.3262450397014618, train_x2_loss=0.6965146064758301\n",
      "INFO:absl:[92] val_loss=1.6422975063323975\n",
      "INFO:absl:[92] test_loss=1.6960207223892212\n",
      "INFO:absl:[93] train_loss=1.0328352451324463, train_x1_loss=0.33214282989501953, train_x2_loss=0.7006937861442566\n",
      "INFO:absl:[93] val_loss=1.6502352952957153\n",
      "INFO:absl:[93] test_loss=1.7035139799118042\n",
      "INFO:absl:[94] train_loss=1.027558445930481, train_x1_loss=0.3270999491214752, train_x2_loss=0.7004594206809998\n",
      "INFO:absl:[94] val_loss=1.6460078954696655\n",
      "INFO:absl:[94] test_loss=1.699864387512207\n",
      "INFO:absl:[95] train_loss=1.026106357574463, train_x1_loss=0.3251064419746399, train_x2_loss=0.7010002732276917\n",
      "INFO:absl:[95] val_loss=1.6444122791290283\n",
      "INFO:absl:[95] test_loss=1.6988791227340698\n",
      "INFO:absl:[96] train_loss=1.0298657417297363, train_x1_loss=0.32773005962371826, train_x2_loss=0.7021366357803345\n",
      "INFO:absl:[96] val_loss=1.6539933681488037\n",
      "INFO:absl:[96] test_loss=1.7078229188919067\n",
      "INFO:absl:[97] train_loss=1.0315921306610107, train_x1_loss=0.32730188965797424, train_x2_loss=0.7042891979217529\n",
      "INFO:absl:[97] val_loss=1.6547327041625977\n",
      "INFO:absl:[97] test_loss=1.708700180053711\n",
      "INFO:absl:[98] train_loss=1.0320137739181519, train_x1_loss=0.3305092453956604, train_x2_loss=0.7015031576156616\n",
      "INFO:absl:[98] val_loss=1.6417930126190186\n",
      "INFO:absl:[98] test_loss=1.6954251527786255\n",
      "INFO:absl:[99] train_loss=1.0278925895690918, train_x1_loss=0.3248859941959381, train_x2_loss=0.7030060887336731\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[99] val_loss=1.6530356407165527\n",
      "INFO:absl:[99] test_loss=1.7068575620651245\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'relu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 10, 'dropout_rate': 0.21664822298158384, 'edge_features': (4, 8), 'epochs': 100, 'eval_every_epochs': 1, 'fully_connected_edges': 1, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 1, 'layer_norm': False, 'learning_rate': 2.9648345553652954e-05, 'log_every_epochs': 1, 'max_checkpts_to_keep': 2, 'model': 'MLPGraphNetwork', 'n_blocks': 1, 'n_samples': 5000, 'node_features': (128, 2), 'normalize': True, 'optimizer': 'adam', 'output_delay': 0, 'output_steps': 4, 'sample_buffer': -4, 'seed': 71, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 120, 'timestep_duration': 3, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+-----------+-------+----------+--------+\n",
      "| Name                                   | Shape     | Size  | Mean     | Std    |\n",
      "+----------------------------------------+-----------+-------+----------+--------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (4,)      | 4     | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 4)    | 24    | -0.0819  | 0.36   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (8,)      | 8     | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (4, 8)    | 32    | -0.0639  | 0.439  |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (128,)    | 128   | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (19, 128) | 2,432 | -0.00462 | 0.228  |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)      | 2     | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (128, 2)  | 256   | -0.00622 | 0.0884 |\n",
      "+----------------------------------------+-----------+-------+----------+--------+\n",
      "Total: 2,886\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Checkpoint None does not exist.\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.00s.\n",
      "INFO:absl:Starting training.\n",
      "INFO:absl:[0] train_loss=1.4940556287765503, train_x1_loss=0.6523528695106506, train_x2_loss=0.8417031168937683\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[0] val_loss=1.8304771184921265\n",
      "INFO:absl:[0] test_loss=1.891510009765625\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Created artifact [10] Profile of type ArtifactType.URL and value None.\n",
      "INFO:absl:[1] train_loss=1.1907010078430176, train_x1_loss=0.4286988079547882, train_x2_loss=0.7620033025741577\n",
      "INFO:absl:[1] val_loss=1.724969506263733\n",
      "INFO:absl:[1] test_loss=1.7794791460037231\n",
      "INFO:absl:[2] train_loss=1.1211820840835571, train_x1_loss=0.381580650806427, train_x2_loss=0.7396018505096436\n",
      "INFO:absl:[2] val_loss=1.709723949432373\n",
      "INFO:absl:[2] test_loss=1.7642827033996582\n",
      "INFO:absl:[3] train_loss=1.0851831436157227, train_x1_loss=0.36041778326034546, train_x2_loss=0.7247657775878906\n",
      "INFO:absl:[3] val_loss=1.695008397102356\n",
      "INFO:absl:[3] test_loss=1.7497895956039429\n",
      "INFO:absl:[4] train_loss=1.0744249820709229, train_x1_loss=0.35435640811920166, train_x2_loss=0.7200684547424316\n",
      "INFO:absl:[4] val_loss=1.6822654008865356\n",
      "INFO:absl:[4] test_loss=1.7364789247512817\n",
      "INFO:absl:[5] train_loss=1.0654430389404297, train_x1_loss=0.3491094410419464, train_x2_loss=0.7163324356079102\n",
      "INFO:absl:[5] val_loss=1.6800408363342285\n",
      "INFO:absl:[5] test_loss=1.7348445653915405\n",
      "INFO:absl:[6] train_loss=1.05808424949646, train_x1_loss=0.3432372808456421, train_x2_loss=0.7148464918136597\n",
      "INFO:absl:[6] val_loss=1.6832101345062256\n",
      "INFO:absl:[6] test_loss=1.737633466720581\n",
      "INFO:absl:[7] train_loss=1.0523384809494019, train_x1_loss=0.34059542417526245, train_x2_loss=0.7117412090301514\n",
      "INFO:absl:[7] val_loss=1.6698863506317139\n",
      "INFO:absl:[7] test_loss=1.7239493131637573\n",
      "INFO:absl:[8] train_loss=1.0469387769699097, train_x1_loss=0.337566614151001, train_x2_loss=0.7093726396560669\n",
      "INFO:absl:[8] val_loss=1.672582745552063\n",
      "INFO:absl:[8] test_loss=1.726777195930481\n",
      "INFO:absl:[9] train_loss=1.045867919921875, train_x1_loss=0.3368591070175171, train_x2_loss=0.7090083956718445\n",
      "INFO:absl:[9] val_loss=1.657392978668213\n",
      "INFO:absl:[9] test_loss=1.7108526229858398\n",
      "INFO:absl:[10] train_loss=1.0422061681747437, train_x1_loss=0.3333551585674286, train_x2_loss=0.7088495492935181\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[10] val_loss=1.6653653383255005\n",
      "INFO:absl:[10] test_loss=1.7195074558258057\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[11] train_loss=1.0451524257659912, train_x1_loss=0.3376588225364685, train_x2_loss=0.7074927687644958\n",
      "INFO:absl:[11] val_loss=1.6628401279449463\n",
      "INFO:absl:[11] test_loss=1.716581106185913\n",
      "INFO:absl:[12] train_loss=1.0356863737106323, train_x1_loss=0.3316810131072998, train_x2_loss=0.7040062546730042\n",
      "INFO:absl:[12] val_loss=1.6556369066238403\n",
      "INFO:absl:[12] test_loss=1.7094485759735107\n",
      "INFO:absl:[13] train_loss=1.0336698293685913, train_x1_loss=0.3300545811653137, train_x2_loss=0.7036166191101074\n",
      "INFO:absl:[13] val_loss=1.6559582948684692\n",
      "INFO:absl:[13] test_loss=1.7099741697311401\n",
      "INFO:absl:[14] train_loss=1.0398378372192383, train_x1_loss=0.3333788216114044, train_x2_loss=0.7064597010612488\n",
      "INFO:absl:[14] val_loss=1.6555217504501343\n",
      "INFO:absl:[14] test_loss=1.7095916271209717\n",
      "INFO:absl:[15] train_loss=1.0382319688796997, train_x1_loss=0.3344488739967346, train_x2_loss=0.7037843465805054\n",
      "INFO:absl:[15] val_loss=1.6509912014007568\n",
      "INFO:absl:[15] test_loss=1.704322338104248\n",
      "INFO:absl:[16] train_loss=1.040842056274414, train_x1_loss=0.33719298243522644, train_x2_loss=0.7036494016647339\n",
      "INFO:absl:[16] val_loss=1.649205207824707\n",
      "INFO:absl:[16] test_loss=1.7023911476135254\n",
      "INFO:absl:[17] train_loss=1.0344642400741577, train_x1_loss=0.32963258028030396, train_x2_loss=0.7048323154449463\n",
      "INFO:absl:[17] val_loss=1.654148817062378\n",
      "INFO:absl:[17] test_loss=1.707737684249878\n",
      "INFO:absl:[18] train_loss=1.0342066287994385, train_x1_loss=0.3314559757709503, train_x2_loss=0.7027508616447449\n",
      "INFO:absl:[18] val_loss=1.6478477716445923\n",
      "INFO:absl:[18] test_loss=1.7015630006790161\n",
      "INFO:absl:[19] train_loss=1.0317668914794922, train_x1_loss=0.3287616968154907, train_x2_loss=0.7030045986175537\n",
      "INFO:absl:[19] val_loss=1.6534209251403809\n",
      "INFO:absl:[19] test_loss=1.7068768739700317\n",
      "INFO:absl:[20] train_loss=1.0356351137161255, train_x1_loss=0.33403879404067993, train_x2_loss=0.701596736907959\n",
      "INFO:absl:[20] val_loss=1.6497974395751953\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[20] test_loss=1.703330397605896\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[21] train_loss=1.0283010005950928, train_x1_loss=0.32855895161628723, train_x2_loss=0.6997414827346802\n",
      "INFO:absl:[21] val_loss=1.6514321565628052\n",
      "INFO:absl:[21] test_loss=1.705051064491272\n",
      "INFO:absl:[22] train_loss=1.0329285860061646, train_x1_loss=0.3299667537212372, train_x2_loss=0.7029612064361572\n",
      "INFO:absl:[22] val_loss=1.6591100692749023\n",
      "INFO:absl:[22] test_loss=1.7128502130508423\n",
      "INFO:absl:[23] train_loss=1.03485107421875, train_x1_loss=0.3308611512184143, train_x2_loss=0.7039889097213745\n",
      "INFO:absl:[23] val_loss=1.6522623300552368\n",
      "INFO:absl:[23] test_loss=1.7061388492584229\n",
      "INFO:absl:[24] train_loss=1.0335383415222168, train_x1_loss=0.330782413482666, train_x2_loss=0.7027546763420105\n",
      "INFO:absl:[24] val_loss=1.6505928039550781\n",
      "INFO:absl:[24] test_loss=1.7044577598571777\n",
      "INFO:absl:[25] train_loss=1.03500497341156, train_x1_loss=0.3310074210166931, train_x2_loss=0.7039971351623535\n",
      "INFO:absl:[25] val_loss=1.6471076011657715\n",
      "INFO:absl:[25] test_loss=1.701061487197876\n",
      "INFO:absl:[26] train_loss=1.0292539596557617, train_x1_loss=0.33071237802505493, train_x2_loss=0.6985421776771545\n",
      "INFO:absl:[26] val_loss=1.64561927318573\n",
      "INFO:absl:[26] test_loss=1.699601650238037\n",
      "INFO:absl:[27] train_loss=1.0331528186798096, train_x1_loss=0.33050233125686646, train_x2_loss=0.7026517391204834\n",
      "INFO:absl:[27] val_loss=1.6499220132827759\n",
      "INFO:absl:[27] test_loss=1.7039029598236084\n",
      "INFO:absl:[28] train_loss=1.0327644348144531, train_x1_loss=0.3298681080341339, train_x2_loss=0.7028965353965759\n",
      "INFO:absl:[28] val_loss=1.6553958654403687\n",
      "INFO:absl:[28] test_loss=1.7091350555419922\n",
      "INFO:absl:[29] train_loss=1.0353033542633057, train_x1_loss=0.33317500352859497, train_x2_loss=0.7021258473396301\n",
      "INFO:absl:[29] val_loss=1.6532409191131592\n",
      "INFO:absl:[29] test_loss=1.7074495553970337\n",
      "INFO:absl:[30] train_loss=1.030396580696106, train_x1_loss=0.32874009013175964, train_x2_loss=0.7016602158546448\n",
      "INFO:absl:[30] val_loss=1.6468756198883057\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[30] test_loss=1.7007800340652466\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[31] train_loss=1.0328956842422485, train_x1_loss=0.3335131108760834, train_x2_loss=0.6993825435638428\n",
      "INFO:absl:[31] val_loss=1.6490565538406372\n",
      "INFO:absl:[31] test_loss=1.7030839920043945\n",
      "INFO:absl:[32] train_loss=1.0283334255218506, train_x1_loss=0.3292433023452759, train_x2_loss=0.6990898847579956\n",
      "INFO:absl:[32] val_loss=1.6420073509216309\n",
      "INFO:absl:[32] test_loss=1.6956768035888672\n",
      "INFO:absl:[33] train_loss=1.0302610397338867, train_x1_loss=0.3300457298755646, train_x2_loss=0.7002167701721191\n",
      "INFO:absl:[33] val_loss=1.651033639907837\n",
      "INFO:absl:[33] test_loss=1.704864740371704\n",
      "INFO:absl:[34] train_loss=1.0321236848831177, train_x1_loss=0.3298093378543854, train_x2_loss=0.7023158073425293\n",
      "INFO:absl:[34] val_loss=1.6514009237289429\n",
      "INFO:absl:[34] test_loss=1.7057026624679565\n",
      "INFO:absl:[35] train_loss=1.0286381244659424, train_x1_loss=0.32771676778793335, train_x2_loss=0.7009196877479553\n",
      "INFO:absl:[35] val_loss=1.6482261419296265\n",
      "INFO:absl:[35] test_loss=1.7020915746688843\n",
      "INFO:absl:[36] train_loss=1.032771110534668, train_x1_loss=0.33066290616989136, train_x2_loss=0.7021099328994751\n",
      "INFO:absl:[36] val_loss=1.6531261205673218\n",
      "INFO:absl:[36] test_loss=1.7065941095352173\n",
      "INFO:absl:[37] train_loss=1.0294162034988403, train_x1_loss=0.3296567499637604, train_x2_loss=0.6997607946395874\n",
      "INFO:absl:[37] val_loss=1.6514873504638672\n",
      "INFO:absl:[37] test_loss=1.7056515216827393\n",
      "INFO:absl:[38] train_loss=1.0266611576080322, train_x1_loss=0.3272158205509186, train_x2_loss=0.6994473338127136\n",
      "INFO:absl:[38] val_loss=1.6462379693984985\n",
      "INFO:absl:[38] test_loss=1.7003564834594727\n",
      "INFO:absl:[39] train_loss=1.028874158859253, train_x1_loss=0.3290453255176544, train_x2_loss=0.6998304128646851\n",
      "INFO:absl:[39] val_loss=1.6542251110076904\n",
      "INFO:absl:[39] test_loss=1.7085232734680176\n",
      "INFO:absl:Setting work unit notes: 2370.6 steps/s, 40.6% (142235/350000), ETA: 1m (1m : 0.0% checkpoint, 9.1% eval)\n",
      "INFO:absl:[142235] steps_per_sec=2370.580328\n",
      "INFO:absl:[40] train_loss=1.0318588018417358, train_x1_loss=0.32867103815078735, train_x2_loss=0.7031891942024231\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[40] val_loss=1.6576414108276367\n",
      "INFO:absl:[40] test_loss=1.7122637033462524\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[41] train_loss=1.030975103378296, train_x1_loss=0.32844555377960205, train_x2_loss=0.7025288343429565\n",
      "INFO:absl:[41] val_loss=1.6525044441223145\n",
      "INFO:absl:[41] test_loss=1.7070682048797607\n",
      "INFO:absl:[42] train_loss=1.0259205102920532, train_x1_loss=0.32711201906204224, train_x2_loss=0.6988090872764587\n",
      "INFO:absl:[42] val_loss=1.6455094814300537\n",
      "INFO:absl:[42] test_loss=1.7000467777252197\n",
      "INFO:absl:[43] train_loss=1.0300137996673584, train_x1_loss=0.3293376863002777, train_x2_loss=0.7006750106811523\n",
      "INFO:absl:[43] val_loss=1.6532336473464966\n",
      "INFO:absl:[43] test_loss=1.7077083587646484\n",
      "INFO:absl:[44] train_loss=1.0291889905929565, train_x1_loss=0.3285849988460541, train_x2_loss=0.700603723526001\n",
      "INFO:absl:[44] val_loss=1.645108699798584\n",
      "INFO:absl:[44] test_loss=1.6988743543624878\n",
      "INFO:absl:[45] train_loss=1.0325497388839722, train_x1_loss=0.328414648771286, train_x2_loss=0.7041351795196533\n",
      "INFO:absl:[45] val_loss=1.6501836776733398\n",
      "INFO:absl:[45] test_loss=1.7046064138412476\n",
      "INFO:absl:[46] train_loss=1.0284851789474487, train_x1_loss=0.32792821526527405, train_x2_loss=0.7005571722984314\n",
      "INFO:absl:[46] val_loss=1.653456449508667\n",
      "INFO:absl:[46] test_loss=1.7076157331466675\n",
      "INFO:absl:[47] train_loss=1.0311976671218872, train_x1_loss=0.33020874857902527, train_x2_loss=0.7009890675544739\n",
      "INFO:absl:[47] val_loss=1.650651216506958\n",
      "INFO:absl:[47] test_loss=1.7046096324920654\n",
      "INFO:absl:[48] train_loss=1.0275568962097168, train_x1_loss=0.32655924558639526, train_x2_loss=0.7009977698326111\n",
      "INFO:absl:[48] val_loss=1.6422361135482788\n",
      "INFO:absl:[48] test_loss=1.6964296102523804\n",
      "INFO:absl:[49] train_loss=1.0304702520370483, train_x1_loss=0.3280477821826935, train_x2_loss=0.702421247959137\n",
      "INFO:absl:[49] val_loss=1.6493905782699585\n",
      "INFO:absl:[49] test_loss=1.7031996250152588\n",
      "INFO:absl:[50] train_loss=1.0245574712753296, train_x1_loss=0.3267859220504761, train_x2_loss=0.6977725028991699\n",
      "INFO:absl:[50] val_loss=1.637526273727417\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[50] test_loss=1.6918138265609741\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[51] train_loss=1.0340023040771484, train_x1_loss=0.33187198638916016, train_x2_loss=0.7021315097808838\n",
      "INFO:absl:[51] val_loss=1.6478970050811768\n",
      "INFO:absl:[51] test_loss=1.7017536163330078\n",
      "INFO:absl:[52] train_loss=1.0321195125579834, train_x1_loss=0.33050021529197693, train_x2_loss=0.7016200423240662\n",
      "INFO:absl:[52] val_loss=1.6448391675949097\n",
      "INFO:absl:[52] test_loss=1.6991249322891235\n",
      "INFO:absl:[53] train_loss=1.0322479009628296, train_x1_loss=0.32973286509513855, train_x2_loss=0.7025158405303955\n",
      "INFO:absl:[53] val_loss=1.6483570337295532\n",
      "INFO:absl:[53] test_loss=1.7023749351501465\n",
      "INFO:absl:[54] train_loss=1.0291210412979126, train_x1_loss=0.3274313509464264, train_x2_loss=0.7016915678977966\n",
      "INFO:absl:[54] val_loss=1.6453049182891846\n",
      "INFO:absl:[54] test_loss=1.6996116638183594\n",
      "INFO:absl:[55] train_loss=1.0291213989257812, train_x1_loss=0.3272055387496948, train_x2_loss=0.7019152641296387\n",
      "INFO:absl:[55] val_loss=1.6446926593780518\n",
      "INFO:absl:[55] test_loss=1.6988118886947632\n",
      "INFO:absl:[56] train_loss=1.0303142070770264, train_x1_loss=0.3285214602947235, train_x2_loss=0.7017927169799805\n",
      "INFO:absl:[56] val_loss=1.6499284505844116\n",
      "INFO:absl:[56] test_loss=1.7043261528015137\n",
      "INFO:absl:[57] train_loss=1.0327510833740234, train_x1_loss=0.33195677399635315, train_x2_loss=0.700794517993927\n",
      "INFO:absl:[57] val_loss=1.6483650207519531\n",
      "INFO:absl:[57] test_loss=1.7029842138290405\n",
      "INFO:absl:[58] train_loss=1.029682993888855, train_x1_loss=0.3284553587436676, train_x2_loss=0.7012287974357605\n",
      "INFO:absl:[58] val_loss=1.646855354309082\n",
      "INFO:absl:[58] test_loss=1.701032280921936\n",
      "INFO:absl:[59] train_loss=1.026513934135437, train_x1_loss=0.32863491773605347, train_x2_loss=0.6978791356086731\n",
      "INFO:absl:[59] val_loss=1.6473561525344849\n",
      "INFO:absl:[59] test_loss=1.7014461755752563\n",
      "INFO:absl:[60] train_loss=1.0327776670455933, train_x1_loss=0.3297247588634491, train_x2_loss=0.7030526399612427\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[60] val_loss=1.6468696594238281\n",
      "INFO:absl:[60] test_loss=1.701119065284729\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[61] train_loss=1.0294159650802612, train_x1_loss=0.3274400532245636, train_x2_loss=0.7019757032394409\n",
      "INFO:absl:[61] val_loss=1.6414034366607666\n",
      "INFO:absl:[61] test_loss=1.6956584453582764\n",
      "INFO:absl:[62] train_loss=1.0255314111709595, train_x1_loss=0.32543107867240906, train_x2_loss=0.7001006603240967\n",
      "INFO:absl:[62] val_loss=1.6500816345214844\n",
      "INFO:absl:[62] test_loss=1.7044768333435059\n",
      "INFO:absl:[63] train_loss=1.0306951999664307, train_x1_loss=0.33035996556282043, train_x2_loss=0.7003359794616699\n",
      "INFO:absl:[63] val_loss=1.644916296005249\n",
      "INFO:absl:[63] test_loss=1.6988413333892822\n",
      "INFO:absl:[64] train_loss=1.0296595096588135, train_x1_loss=0.32864275574684143, train_x2_loss=0.7010149359703064\n",
      "INFO:absl:[64] val_loss=1.6532198190689087\n",
      "INFO:absl:[64] test_loss=1.7079623937606812\n",
      "INFO:absl:[65] train_loss=1.0351760387420654, train_x1_loss=0.33095407485961914, train_x2_loss=0.7042236924171448\n",
      "INFO:absl:[65] val_loss=1.6492087841033936\n",
      "INFO:absl:[65] test_loss=1.70294189453125\n",
      "INFO:absl:[66] train_loss=1.028753638267517, train_x1_loss=0.3295355439186096, train_x2_loss=0.6992174983024597\n",
      "INFO:absl:[66] val_loss=1.6549832820892334\n",
      "INFO:absl:[66] test_loss=1.7090859413146973\n",
      "INFO:absl:[67] train_loss=1.0312838554382324, train_x1_loss=0.32958105206489563, train_x2_loss=0.7017036080360413\n",
      "INFO:absl:[67] val_loss=1.6505374908447266\n",
      "INFO:absl:[67] test_loss=1.7044216394424438\n",
      "INFO:absl:[68] train_loss=1.0301152467727661, train_x1_loss=0.3294731676578522, train_x2_loss=0.7006420493125916\n",
      "INFO:absl:[68] val_loss=1.6493723392486572\n",
      "INFO:absl:[68] test_loss=1.7034419775009155\n",
      "INFO:absl:[69] train_loss=1.0309606790542603, train_x1_loss=0.32928937673568726, train_x2_loss=0.7016717195510864\n",
      "INFO:absl:[69] val_loss=1.6539952754974365\n",
      "INFO:absl:[69] test_loss=1.707828402519226\n",
      "INFO:absl:[70] train_loss=1.0279988050460815, train_x1_loss=0.33070582151412964, train_x2_loss=0.6972947120666504\n",
      "INFO:absl:[70] val_loss=1.6547492742538452\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[70] test_loss=1.7086679935455322\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[71] train_loss=1.0263230800628662, train_x1_loss=0.32824522256851196, train_x2_loss=0.698079526424408\n",
      "INFO:absl:[71] val_loss=1.6438746452331543\n",
      "INFO:absl:[71] test_loss=1.6976642608642578\n",
      "INFO:absl:[72] train_loss=1.0318567752838135, train_x1_loss=0.32943812012672424, train_x2_loss=0.7024193406105042\n",
      "INFO:absl:[72] val_loss=1.6474790573120117\n",
      "INFO:absl:[72] test_loss=1.701277732849121\n",
      "INFO:absl:[73] train_loss=1.0280930995941162, train_x1_loss=0.3280957341194153, train_x2_loss=0.699997067451477\n",
      "INFO:absl:[73] val_loss=1.6489381790161133\n",
      "INFO:absl:[73] test_loss=1.702758550643921\n",
      "INFO:absl:[74] train_loss=1.030877709388733, train_x1_loss=0.32888931035995483, train_x2_loss=0.7019890546798706\n",
      "INFO:absl:[74] val_loss=1.650354266166687\n",
      "INFO:absl:[74] test_loss=1.704214334487915\n",
      "INFO:absl:[75] train_loss=1.034104347229004, train_x1_loss=0.3302565813064575, train_x2_loss=0.7038483023643494\n",
      "INFO:absl:[75] val_loss=1.6457483768463135\n",
      "INFO:absl:[75] test_loss=1.6997064352035522\n",
      "INFO:absl:[76] train_loss=1.026978850364685, train_x1_loss=0.32592862844467163, train_x2_loss=0.7010498642921448\n",
      "INFO:absl:[76] val_loss=1.6509555578231812\n",
      "INFO:absl:[76] test_loss=1.704939842224121\n",
      "INFO:absl:[77] train_loss=1.0288887023925781, train_x1_loss=0.3279956877231598, train_x2_loss=0.7008932828903198\n",
      "INFO:absl:[77] val_loss=1.645833134651184\n",
      "INFO:absl:[77] test_loss=1.6998579502105713\n",
      "INFO:absl:[78] train_loss=1.0284255743026733, train_x1_loss=0.32797276973724365, train_x2_loss=0.700453519821167\n",
      "INFO:absl:[78] val_loss=1.6464946269989014\n",
      "INFO:absl:[78] test_loss=1.700399398803711\n",
      "INFO:absl:[79] train_loss=1.0276405811309814, train_x1_loss=0.3275090754032135, train_x2_loss=0.7001312375068665\n",
      "INFO:absl:[79] val_loss=1.6467729806900024\n",
      "INFO:absl:[79] test_loss=1.700770616531372\n",
      "INFO:absl:[80] train_loss=1.0285472869873047, train_x1_loss=0.32736364006996155, train_x2_loss=0.7011839151382446\n",
      "INFO:absl:[80] val_loss=1.6486082077026367\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[80] test_loss=1.7025823593139648\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[81] train_loss=1.0270700454711914, train_x1_loss=0.3266100585460663, train_x2_loss=0.7004600167274475\n",
      "INFO:absl:[81] val_loss=1.6442561149597168\n",
      "INFO:absl:[81] test_loss=1.6982980966567993\n",
      "INFO:absl:[82] train_loss=1.0270929336547852, train_x1_loss=0.3275453746318817, train_x2_loss=0.6995477080345154\n",
      "INFO:absl:[82] val_loss=1.649930715560913\n",
      "INFO:absl:[82] test_loss=1.7039055824279785\n",
      "INFO:absl:[83] train_loss=1.024960994720459, train_x1_loss=0.3270013928413391, train_x2_loss=0.6979600787162781\n",
      "INFO:absl:[83] val_loss=1.6473432779312134\n",
      "INFO:absl:[83] test_loss=1.7008436918258667\n",
      "INFO:absl:Setting work unit notes: 2585.5 steps/s, 85.0% (297364/350000), ETA: 0m (2m : 0.0% checkpoint, 9.1% eval)\n",
      "INFO:absl:[297364] steps_per_sec=2585.479892\n",
      "INFO:absl:[84] train_loss=1.0274327993392944, train_x1_loss=0.3275551497936249, train_x2_loss=0.6998763680458069\n",
      "INFO:absl:[84] val_loss=1.6522111892700195\n",
      "INFO:absl:[84] test_loss=1.7055968046188354\n",
      "INFO:absl:[85] train_loss=1.0277214050292969, train_x1_loss=0.32704269886016846, train_x2_loss=0.7006799578666687\n",
      "INFO:absl:[85] val_loss=1.6500428915023804\n",
      "INFO:absl:[85] test_loss=1.7042746543884277\n",
      "INFO:absl:[86] train_loss=1.0313763618469238, train_x1_loss=0.33025649189949036, train_x2_loss=0.7011203169822693\n",
      "INFO:absl:[86] val_loss=1.6519790887832642\n",
      "INFO:absl:[86] test_loss=1.7061266899108887\n",
      "INFO:absl:[87] train_loss=1.0274548530578613, train_x1_loss=0.3269703984260559, train_x2_loss=0.7004844546318054\n",
      "INFO:absl:[87] val_loss=1.6498267650604248\n",
      "INFO:absl:[87] test_loss=1.7035197019577026\n",
      "INFO:absl:[88] train_loss=1.0252587795257568, train_x1_loss=0.3243841230869293, train_x2_loss=0.7008768320083618\n",
      "INFO:absl:[88] val_loss=1.6541152000427246\n",
      "INFO:absl:[88] test_loss=1.7080769538879395\n",
      "INFO:absl:[89] train_loss=1.0273499488830566, train_x1_loss=0.326479971408844, train_x2_loss=0.70087069272995\n",
      "INFO:absl:[89] val_loss=1.6424404382705688\n",
      "INFO:absl:[89] test_loss=1.69606614112854\n",
      "INFO:absl:[90] train_loss=1.0285217761993408, train_x1_loss=0.32855936884880066, train_x2_loss=0.6999620795249939\n",
      "INFO:absl:[90] val_loss=1.656887412071228\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[90] test_loss=1.7109036445617676\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[91] train_loss=1.0314997434616089, train_x1_loss=0.33017498254776, train_x2_loss=0.7013260126113892\n",
      "INFO:absl:[91] val_loss=1.65337336063385\n",
      "INFO:absl:[91] test_loss=1.7067378759384155\n",
      "INFO:absl:[92] train_loss=1.0227577686309814, train_x1_loss=0.3262450397014618, train_x2_loss=0.6965146064758301\n",
      "INFO:absl:[92] val_loss=1.6422975063323975\n",
      "INFO:absl:[92] test_loss=1.6960207223892212\n",
      "INFO:absl:[93] train_loss=1.0328352451324463, train_x1_loss=0.33214282989501953, train_x2_loss=0.7006937861442566\n",
      "INFO:absl:[93] val_loss=1.6502352952957153\n",
      "INFO:absl:[93] test_loss=1.7035139799118042\n",
      "INFO:absl:[94] train_loss=1.027558445930481, train_x1_loss=0.3270999491214752, train_x2_loss=0.7004594206809998\n",
      "INFO:absl:[94] val_loss=1.6460078954696655\n",
      "INFO:absl:[94] test_loss=1.699864387512207\n",
      "INFO:absl:[95] train_loss=1.026106357574463, train_x1_loss=0.3251064419746399, train_x2_loss=0.7010002732276917\n",
      "INFO:absl:[95] val_loss=1.6444122791290283\n",
      "INFO:absl:[95] test_loss=1.6988791227340698\n",
      "INFO:absl:[96] train_loss=1.0298657417297363, train_x1_loss=0.32773005962371826, train_x2_loss=0.7021366357803345\n",
      "INFO:absl:[96] val_loss=1.6539933681488037\n",
      "INFO:absl:[96] test_loss=1.7078229188919067\n",
      "INFO:absl:[97] train_loss=1.0315921306610107, train_x1_loss=0.32730188965797424, train_x2_loss=0.7042891979217529\n",
      "INFO:absl:[97] val_loss=1.6547327041625977\n",
      "INFO:absl:[97] test_loss=1.708700180053711\n",
      "INFO:absl:[98] train_loss=1.0320137739181519, train_x1_loss=0.3305092453956604, train_x2_loss=0.7015031576156616\n",
      "INFO:absl:[98] val_loss=1.6417930126190186\n",
      "INFO:absl:[98] test_loss=1.6954251527786255\n",
      "INFO:absl:[99] train_loss=1.0278925895690918, train_x1_loss=0.3248859941959381, train_x2_loss=0.7030060887336731\n",
      "INFO:absl:[99] val_loss=1.6530356407165527\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[99] test_loss=1.7068575620651245\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'relu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 10, 'dropout_rate': 0.21664822298158384, 'edge_features': (4, 8), 'epochs': 100, 'eval_every_epochs': 1, 'fully_connected_edges': 1, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 1, 'layer_norm': False, 'learning_rate': 2.9648345553652954e-05, 'log_every_epochs': 1, 'max_checkpts_to_keep': 2, 'model': 'MLPGraphNetwork', 'n_blocks': 1, 'n_samples': 5000, 'node_features': (128, 2), 'normalize': True, 'optimizer': 'adam', 'output_delay': 0, 'output_steps': 4, 'sample_buffer': -4, 'seed': 72, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 120, 'timestep_duration': 3, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+-----------+-------+----------+--------+\n",
      "| Name                                   | Shape     | Size  | Mean     | Std    |\n",
      "+----------------------------------------+-----------+-------+----------+--------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (4,)      | 4     | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 4)    | 24    | -0.0819  | 0.36   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (8,)      | 8     | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (4, 8)    | 32    | -0.0639  | 0.439  |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (128,)    | 128   | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (19, 128) | 2,432 | -0.00462 | 0.228  |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)      | 2     | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (128, 2)  | 256   | -0.00622 | 0.0884 |\n",
      "+----------------------------------------+-----------+-------+----------+--------+\n",
      "Total: 2,886\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Checkpoint None does not exist.\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.00s.\n",
      "INFO:absl:Starting training.\n",
      "INFO:absl:[0] train_loss=1.4854623079299927, train_x1_loss=0.6524946689605713, train_x2_loss=0.8329669833183289\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[0] val_loss=1.8973891735076904\n",
      "INFO:absl:[0] test_loss=1.8753061294555664\n",
      "INFO:absl:Checkpoint.save() finished after 0.03s.\n",
      "INFO:absl:Created artifact [10] Profile of type ArtifactType.URL and value None.\n",
      "INFO:absl:[1] train_loss=1.179032564163208, train_x1_loss=0.42587360739707947, train_x2_loss=0.7531577944755554\n",
      "INFO:absl:[1] val_loss=1.79709792137146\n",
      "INFO:absl:[1] test_loss=1.7652924060821533\n",
      "INFO:absl:[2] train_loss=1.1081422567367554, train_x1_loss=0.37821102142333984, train_x2_loss=0.729932427406311\n",
      "INFO:absl:[2] val_loss=1.782370686531067\n",
      "INFO:absl:[2] test_loss=1.7499072551727295\n",
      "INFO:absl:[3] train_loss=1.0772751569747925, train_x1_loss=0.3586958944797516, train_x2_loss=0.7185778617858887\n",
      "INFO:absl:[3] val_loss=1.7752562761306763\n",
      "INFO:absl:[3] test_loss=1.7409636974334717\n",
      "INFO:absl:[4] train_loss=1.057521104812622, train_x1_loss=0.34758758544921875, train_x2_loss=0.7099329233169556\n",
      "INFO:absl:[4] val_loss=1.7533009052276611\n",
      "INFO:absl:[4] test_loss=1.718606948852539\n",
      "INFO:absl:[5] train_loss=1.0524265766143799, train_x1_loss=0.34336891770362854, train_x2_loss=0.7090569138526917\n",
      "INFO:absl:[5] val_loss=1.760399580001831\n",
      "INFO:absl:[5] test_loss=1.7264460325241089\n",
      "INFO:absl:[6] train_loss=1.0441559553146362, train_x1_loss=0.3412517011165619, train_x2_loss=0.7029045820236206\n",
      "INFO:absl:[6] val_loss=1.751632809638977\n",
      "INFO:absl:[6] test_loss=1.7162162065505981\n",
      "INFO:absl:[7] train_loss=1.034481406211853, train_x1_loss=0.3384515643119812, train_x2_loss=0.6960299015045166\n",
      "INFO:absl:[7] val_loss=1.743170976638794\n",
      "INFO:absl:[7] test_loss=1.7061185836791992\n",
      "INFO:absl:[8] train_loss=1.0321296453475952, train_x1_loss=0.3346834182739258, train_x2_loss=0.6974461674690247\n",
      "INFO:absl:[8] val_loss=1.7458726167678833\n",
      "INFO:absl:[8] test_loss=1.710484504699707\n",
      "INFO:absl:[9] train_loss=1.0284934043884277, train_x1_loss=0.3326381742954254, train_x2_loss=0.6958563923835754\n",
      "INFO:absl:[9] val_loss=1.73898446559906\n",
      "INFO:absl:[9] test_loss=1.7024481296539307\n",
      "INFO:absl:[10] train_loss=1.0314433574676514, train_x1_loss=0.3341222107410431, train_x2_loss=0.6973200440406799\n",
      "INFO:absl:[10] val_loss=1.7436246871948242\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[10] test_loss=1.7072917222976685\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[11] train_loss=1.0284302234649658, train_x1_loss=0.3320629894733429, train_x2_loss=0.6963672041893005\n",
      "INFO:absl:[11] val_loss=1.73158860206604\n",
      "INFO:absl:[11] test_loss=1.6947036981582642\n",
      "INFO:absl:[12] train_loss=1.0237315893173218, train_x1_loss=0.32883742451667786, train_x2_loss=0.6948949694633484\n",
      "INFO:absl:[12] val_loss=1.735648512840271\n",
      "INFO:absl:[12] test_loss=1.6995269060134888\n",
      "INFO:absl:[13] train_loss=1.0229345560073853, train_x1_loss=0.3275811970233917, train_x2_loss=0.6953525543212891\n",
      "INFO:absl:[13] val_loss=1.7331527471542358\n",
      "INFO:absl:[13] test_loss=1.6970797777175903\n",
      "INFO:absl:[14] train_loss=1.0271998643875122, train_x1_loss=0.3299173414707184, train_x2_loss=0.6972819566726685\n",
      "INFO:absl:[14] val_loss=1.7316261529922485\n",
      "INFO:absl:[14] test_loss=1.695438027381897\n",
      "INFO:absl:[15] train_loss=1.0236690044403076, train_x1_loss=0.3323734998703003, train_x2_loss=0.691295325756073\n",
      "INFO:absl:[15] val_loss=1.726867914199829\n",
      "INFO:absl:[15] test_loss=1.6898128986358643\n",
      "INFO:absl:[16] train_loss=1.0243252515792847, train_x1_loss=0.3324716091156006, train_x2_loss=0.6918526291847229\n",
      "INFO:absl:[16] val_loss=1.729459524154663\n",
      "INFO:absl:[16] test_loss=1.6927661895751953\n",
      "INFO:absl:[17] train_loss=1.0197982788085938, train_x1_loss=0.32636308670043945, train_x2_loss=0.6934351325035095\n",
      "INFO:absl:[17] val_loss=1.7318856716156006\n",
      "INFO:absl:[17] test_loss=1.6950459480285645\n",
      "INFO:absl:[18] train_loss=1.0242664813995361, train_x1_loss=0.3308083713054657, train_x2_loss=0.6934590339660645\n",
      "INFO:absl:[18] val_loss=1.730269432067871\n",
      "INFO:absl:[18] test_loss=1.6940191984176636\n",
      "INFO:absl:[19] train_loss=1.0235991477966309, train_x1_loss=0.3288971483707428, train_x2_loss=0.6947020292282104\n",
      "INFO:absl:[19] val_loss=1.734520673751831\n",
      "INFO:absl:[19] test_loss=1.6986899375915527\n",
      "INFO:absl:Setting work unit notes: 1175.7 steps/s, 20.2% (70540/350000), ETA: 3m (1m : 0.1% checkpoint, 44.4% eval)\n",
      "INFO:absl:[70540] steps_per_sec=1175.665074\n",
      "INFO:absl:[20] train_loss=1.024642825126648, train_x1_loss=0.33050811290740967, train_x2_loss=0.6941342949867249\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[20] val_loss=1.723102331161499\n",
      "INFO:absl:[20] test_loss=1.686219334602356\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[21] train_loss=1.0200966596603394, train_x1_loss=0.3280101716518402, train_x2_loss=0.6920857429504395\n",
      "INFO:absl:[21] val_loss=1.7323687076568604\n",
      "INFO:absl:[21] test_loss=1.6956822872161865\n",
      "INFO:absl:[22] train_loss=1.0221378803253174, train_x1_loss=0.3278425335884094, train_x2_loss=0.6942949891090393\n",
      "INFO:absl:[22] val_loss=1.7265888452529907\n",
      "INFO:absl:[22] test_loss=1.6898856163024902\n",
      "INFO:absl:[23] train_loss=1.018473505973816, train_x1_loss=0.32564693689346313, train_x2_loss=0.6928279399871826\n",
      "INFO:absl:[23] val_loss=1.7288873195648193\n",
      "INFO:absl:[23] test_loss=1.6917695999145508\n",
      "INFO:absl:[24] train_loss=1.0207730531692505, train_x1_loss=0.3284742534160614, train_x2_loss=0.6922985911369324\n",
      "INFO:absl:[24] val_loss=1.7318576574325562\n",
      "INFO:absl:[24] test_loss=1.6950591802597046\n",
      "INFO:absl:[25] train_loss=1.0166778564453125, train_x1_loss=0.3272567391395569, train_x2_loss=0.6894205212593079\n",
      "INFO:absl:[25] val_loss=1.7256898880004883\n",
      "INFO:absl:[25] test_loss=1.6886767148971558\n",
      "INFO:absl:[26] train_loss=1.0190094709396362, train_x1_loss=0.3259966969490051, train_x2_loss=0.6930137276649475\n",
      "INFO:absl:[26] val_loss=1.7379757165908813\n",
      "INFO:absl:[26] test_loss=1.7010207176208496\n",
      "INFO:absl:[27] train_loss=1.0194168090820312, train_x1_loss=0.32525503635406494, train_x2_loss=0.694162130355835\n",
      "INFO:absl:[27] val_loss=1.7255380153656006\n",
      "INFO:absl:[27] test_loss=1.6902556419372559\n",
      "INFO:absl:[28] train_loss=1.0151162147521973, train_x1_loss=0.325040727853775, train_x2_loss=0.6900750994682312\n",
      "INFO:absl:[28] val_loss=1.730333685874939\n",
      "INFO:absl:[28] test_loss=1.6946760416030884\n",
      "INFO:absl:[29] train_loss=1.0207147598266602, train_x1_loss=0.32806894183158875, train_x2_loss=0.6926465630531311\n",
      "INFO:absl:[29] val_loss=1.722861409187317\n",
      "INFO:absl:[29] test_loss=1.6864286661148071\n",
      "INFO:absl:[30] train_loss=1.0125194787979126, train_x1_loss=0.3266943097114563, train_x2_loss=0.6858246922492981\n",
      "INFO:absl:[30] val_loss=1.722280502319336\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[30] test_loss=1.6850547790527344\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[31] train_loss=1.0197157859802246, train_x1_loss=0.32863420248031616, train_x2_loss=0.691080629825592\n",
      "INFO:absl:[31] val_loss=1.7235422134399414\n",
      "INFO:absl:[31] test_loss=1.6862244606018066\n",
      "INFO:absl:[32] train_loss=1.0193902254104614, train_x1_loss=0.327961802482605, train_x2_loss=0.6914281249046326\n",
      "INFO:absl:[32] val_loss=1.727274775505066\n",
      "INFO:absl:[32] test_loss=1.6907238960266113\n",
      "INFO:absl:[33] train_loss=1.020881175994873, train_x1_loss=0.32692721486091614, train_x2_loss=0.6939529776573181\n",
      "INFO:absl:[33] val_loss=1.728320598602295\n",
      "INFO:absl:[33] test_loss=1.6916966438293457\n",
      "INFO:absl:[34] train_loss=1.017783522605896, train_x1_loss=0.3258267045021057, train_x2_loss=0.6919583082199097\n",
      "INFO:absl:[34] val_loss=1.7262234687805176\n",
      "INFO:absl:[34] test_loss=1.6901777982711792\n",
      "INFO:absl:[35] train_loss=1.0167181491851807, train_x1_loss=0.3244462311267853, train_x2_loss=0.6922721266746521\n",
      "INFO:absl:[35] val_loss=1.729435682296753\n",
      "INFO:absl:[35] test_loss=1.6937304735183716\n",
      "INFO:absl:[36] train_loss=1.016378402709961, train_x1_loss=0.32702940702438354, train_x2_loss=0.6893491148948669\n",
      "INFO:absl:[36] val_loss=1.722113847732544\n",
      "INFO:absl:[36] test_loss=1.6848986148834229\n",
      "INFO:absl:[37] train_loss=1.012802004814148, train_x1_loss=0.32410693168640137, train_x2_loss=0.688694953918457\n",
      "INFO:absl:[37] val_loss=1.7278597354888916\n",
      "INFO:absl:[37] test_loss=1.6906721591949463\n",
      "INFO:absl:[38] train_loss=1.0168789625167847, train_x1_loss=0.3256473243236542, train_x2_loss=0.6912295818328857\n",
      "INFO:absl:[38] val_loss=1.7220618724822998\n",
      "INFO:absl:[38] test_loss=1.6850754022598267\n",
      "INFO:absl:[39] train_loss=1.0142496824264526, train_x1_loss=0.3256572186946869, train_x2_loss=0.6885905861854553\n",
      "INFO:absl:[39] val_loss=1.719655156135559\n",
      "INFO:absl:[39] test_loss=1.6825158596038818\n",
      "INFO:absl:[40] train_loss=1.0144511461257935, train_x1_loss=0.3246850371360779, train_x2_loss=0.6897664666175842\n",
      "INFO:absl:[40] val_loss=1.7281235456466675\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[40] test_loss=1.6920205354690552\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[41] train_loss=1.0142263174057007, train_x1_loss=0.3259274959564209, train_x2_loss=0.6882984638214111\n",
      "INFO:absl:[41] val_loss=1.7219197750091553\n",
      "INFO:absl:[41] test_loss=1.6850926876068115\n",
      "INFO:absl:[42] train_loss=1.0139747858047485, train_x1_loss=0.32458046078681946, train_x2_loss=0.6893970966339111\n",
      "INFO:absl:[42] val_loss=1.7248722314834595\n",
      "INFO:absl:[42] test_loss=1.6882292032241821\n",
      "INFO:absl:[43] train_loss=1.01448392868042, train_x1_loss=0.32358434796333313, train_x2_loss=0.6908996105194092\n",
      "INFO:absl:[43] val_loss=1.7260472774505615\n",
      "INFO:absl:[43] test_loss=1.6905747652053833\n",
      "INFO:absl:[44] train_loss=1.0196093320846558, train_x1_loss=0.3259749114513397, train_x2_loss=0.6936348080635071\n",
      "INFO:absl:[44] val_loss=1.7278474569320679\n",
      "INFO:absl:[44] test_loss=1.6917368173599243\n",
      "INFO:absl:[45] train_loss=1.0153326988220215, train_x1_loss=0.3245093822479248, train_x2_loss=0.6908217668533325\n",
      "INFO:absl:[45] val_loss=1.7308160066604614\n",
      "INFO:absl:[45] test_loss=1.694843053817749\n",
      "INFO:absl:[46] train_loss=1.0176029205322266, train_x1_loss=0.32624176144599915, train_x2_loss=0.6913602352142334\n",
      "INFO:absl:[46] val_loss=1.7289589643478394\n",
      "INFO:absl:[46] test_loss=1.692725658416748\n",
      "INFO:absl:[47] train_loss=1.0155099630355835, train_x1_loss=0.3250940144062042, train_x2_loss=0.6904184818267822\n",
      "INFO:absl:[47] val_loss=1.7266076803207397\n",
      "INFO:absl:[47] test_loss=1.6903642416000366\n",
      "INFO:absl:[48] train_loss=1.0168405771255493, train_x1_loss=0.32453638315200806, train_x2_loss=0.6923063397407532\n",
      "INFO:absl:[48] val_loss=1.7204365730285645\n",
      "INFO:absl:[48] test_loss=1.6842082738876343\n",
      "INFO:absl:[49] train_loss=1.0161399841308594, train_x1_loss=0.32407572865486145, train_x2_loss=0.6920636296272278\n",
      "INFO:absl:[49] val_loss=1.7264385223388672\n",
      "INFO:absl:[49] test_loss=1.6895931959152222\n",
      "INFO:absl:[50] train_loss=1.016280174255371, train_x1_loss=0.3252650797367096, train_x2_loss=0.6910151839256287\n",
      "INFO:absl:[50] val_loss=1.7263484001159668\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[50] test_loss=1.6901986598968506\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[51] train_loss=1.0182150602340698, train_x1_loss=0.32688066363334656, train_x2_loss=0.6913357973098755\n",
      "INFO:absl:[51] val_loss=1.7319035530090332\n",
      "INFO:absl:[51] test_loss=1.6954265832901\n",
      "INFO:absl:[52] train_loss=1.0192855596542358, train_x1_loss=0.32907891273498535, train_x2_loss=0.6902077198028564\n",
      "INFO:absl:[52] val_loss=1.7250229120254517\n",
      "INFO:absl:[52] test_loss=1.688629150390625\n",
      "INFO:absl:[53] train_loss=1.0176920890808105, train_x1_loss=0.3260313868522644, train_x2_loss=0.6916613578796387\n",
      "INFO:absl:[53] val_loss=1.7284996509552002\n",
      "INFO:absl:[53] test_loss=1.6926151514053345\n",
      "INFO:absl:[54] train_loss=1.014427661895752, train_x1_loss=0.3232654929161072, train_x2_loss=0.691164493560791\n",
      "INFO:absl:[54] val_loss=1.7249046564102173\n",
      "INFO:absl:[54] test_loss=1.689300298690796\n",
      "INFO:absl:[55] train_loss=1.014946699142456, train_x1_loss=0.3239080011844635, train_x2_loss=0.6910377144813538\n",
      "INFO:absl:[55] val_loss=1.722090244293213\n",
      "INFO:absl:[55] test_loss=1.6853203773498535\n",
      "INFO:absl:[56] train_loss=1.016055941581726, train_x1_loss=0.32642465829849243, train_x2_loss=0.6896317005157471\n",
      "INFO:absl:[56] val_loss=1.7315603494644165\n",
      "INFO:absl:[56] test_loss=1.6952126026153564\n",
      "INFO:absl:[57] train_loss=1.015732765197754, train_x1_loss=0.32559484243392944, train_x2_loss=0.690138578414917\n",
      "INFO:absl:[57] val_loss=1.7295570373535156\n",
      "INFO:absl:[57] test_loss=1.6934936046600342\n",
      "INFO:absl:[58] train_loss=1.0142688751220703, train_x1_loss=0.32501277327537537, train_x2_loss=0.689256489276886\n",
      "INFO:absl:[58] val_loss=1.7227075099945068\n",
      "INFO:absl:[58] test_loss=1.6859660148620605\n",
      "INFO:absl:[59] train_loss=1.0168465375900269, train_x1_loss=0.3248973488807678, train_x2_loss=0.6919499039649963\n",
      "INFO:absl:[59] val_loss=1.7281880378723145\n",
      "INFO:absl:[59] test_loss=1.6923657655715942\n",
      "INFO:absl:[60] train_loss=1.0146976709365845, train_x1_loss=0.3241652846336365, train_x2_loss=0.6905321478843689\n",
      "INFO:absl:[60] val_loss=1.7244627475738525\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[60] test_loss=1.687558889389038\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[61] train_loss=1.0146362781524658, train_x1_loss=0.324404776096344, train_x2_loss=0.6902325749397278\n",
      "INFO:absl:[61] val_loss=1.725705862045288\n",
      "INFO:absl:[61] test_loss=1.6893411874771118\n",
      "INFO:absl:[62] train_loss=1.013843297958374, train_x1_loss=0.3266376554965973, train_x2_loss=0.6872062087059021\n",
      "INFO:absl:[62] val_loss=1.7253187894821167\n",
      "INFO:absl:[62] test_loss=1.6879349946975708\n",
      "INFO:absl:Setting work unit notes: 2540.0 steps/s, 63.7% (222938/350000), ETA: 0m (2m : 0.1% checkpoint, 26.9% eval)\n",
      "INFO:absl:[222938] steps_per_sec=2539.963709\n",
      "INFO:absl:[63] train_loss=1.0151060819625854, train_x1_loss=0.32663750648498535, train_x2_loss=0.6884690523147583\n",
      "INFO:absl:[63] val_loss=1.7222951650619507\n",
      "INFO:absl:[63] test_loss=1.6851673126220703\n",
      "INFO:absl:[64] train_loss=1.0176360607147217, train_x1_loss=0.3256586790084839, train_x2_loss=0.6919767260551453\n",
      "INFO:absl:[64] val_loss=1.7241740226745605\n",
      "INFO:absl:[64] test_loss=1.688521385192871\n",
      "INFO:absl:[65] train_loss=1.0181304216384888, train_x1_loss=0.3268384039402008, train_x2_loss=0.6912912130355835\n",
      "INFO:absl:[65] val_loss=1.721433162689209\n",
      "INFO:absl:[65] test_loss=1.6839447021484375\n",
      "INFO:absl:[66] train_loss=1.0168251991271973, train_x1_loss=0.326645165681839, train_x2_loss=0.6901810169219971\n",
      "INFO:absl:[66] val_loss=1.7227030992507935\n",
      "INFO:absl:[66] test_loss=1.6856672763824463\n",
      "INFO:absl:[67] train_loss=1.0166114568710327, train_x1_loss=0.32411906123161316, train_x2_loss=0.6924929618835449\n",
      "INFO:absl:[67] val_loss=1.7229467630386353\n",
      "INFO:absl:[67] test_loss=1.6864702701568604\n",
      "INFO:absl:[68] train_loss=1.0172683000564575, train_x1_loss=0.32556599378585815, train_x2_loss=0.6917015910148621\n",
      "INFO:absl:[68] val_loss=1.7219544649124146\n",
      "INFO:absl:[68] test_loss=1.6862224340438843\n",
      "INFO:absl:[69] train_loss=1.0147180557250977, train_x1_loss=0.3242599666118622, train_x2_loss=0.6904584169387817\n",
      "INFO:absl:[69] val_loss=1.7225730419158936\n",
      "INFO:absl:[69] test_loss=1.6859434843063354\n",
      "INFO:absl:[70] train_loss=1.0170527696609497, train_x1_loss=0.32508552074432373, train_x2_loss=0.6919658184051514\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[70] val_loss=1.7295259237289429\n",
      "INFO:absl:[70] test_loss=1.6929510831832886\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[71] train_loss=1.0143555402755737, train_x1_loss=0.3241267800331116, train_x2_loss=0.6902270913124084\n",
      "INFO:absl:[71] val_loss=1.7188256978988647\n",
      "INFO:absl:[71] test_loss=1.68242609500885\n",
      "INFO:absl:[72] train_loss=1.016054391860962, train_x1_loss=0.3248942792415619, train_x2_loss=0.6911605000495911\n",
      "INFO:absl:[72] val_loss=1.7307133674621582\n",
      "INFO:absl:[72] test_loss=1.6938108205795288\n",
      "INFO:absl:[73] train_loss=1.013480544090271, train_x1_loss=0.32343927025794983, train_x2_loss=0.6900413632392883\n",
      "INFO:absl:[73] val_loss=1.7212210893630981\n",
      "INFO:absl:[73] test_loss=1.684317708015442\n",
      "INFO:absl:[74] train_loss=1.0135929584503174, train_x1_loss=0.3255629539489746, train_x2_loss=0.6880323886871338\n",
      "INFO:absl:[74] val_loss=1.725070834159851\n",
      "INFO:absl:[74] test_loss=1.6878414154052734\n",
      "INFO:absl:[75] train_loss=1.0187478065490723, train_x1_loss=0.32546430826187134, train_x2_loss=0.6932855248451233\n",
      "INFO:absl:[75] val_loss=1.7198598384857178\n",
      "INFO:absl:[75] test_loss=1.683648943901062\n",
      "INFO:absl:[76] train_loss=1.0188671350479126, train_x1_loss=0.3258110582828522, train_x2_loss=0.6930527091026306\n",
      "INFO:absl:[76] val_loss=1.734601616859436\n",
      "INFO:absl:[76] test_loss=1.6980626583099365\n",
      "INFO:absl:[77] train_loss=1.016878366470337, train_x1_loss=0.32611334323883057, train_x2_loss=0.6907647252082825\n",
      "INFO:absl:[77] val_loss=1.7290936708450317\n",
      "INFO:absl:[77] test_loss=1.6922534704208374\n",
      "INFO:absl:[78] train_loss=1.0191608667373657, train_x1_loss=0.3258812427520752, train_x2_loss=0.6932814717292786\n",
      "INFO:absl:[78] val_loss=1.7322781085968018\n",
      "INFO:absl:[78] test_loss=1.6965547800064087\n",
      "INFO:absl:[79] train_loss=1.0150744915008545, train_x1_loss=0.3257788419723511, train_x2_loss=0.6892954707145691\n",
      "INFO:absl:[79] val_loss=1.7234207391738892\n",
      "INFO:absl:[79] test_loss=1.6865395307540894\n",
      "INFO:absl:[80] train_loss=1.0107417106628418, train_x1_loss=0.32377034425735474, train_x2_loss=0.6869684457778931\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[80] val_loss=1.7181378602981567\n",
      "INFO:absl:[80] test_loss=1.6809080839157104\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[81] train_loss=1.0162616968154907, train_x1_loss=0.32463982701301575, train_x2_loss=0.691620945930481\n",
      "INFO:absl:[81] val_loss=1.723649263381958\n",
      "INFO:absl:[81] test_loss=1.6867092847824097\n",
      "INFO:absl:[82] train_loss=1.0141377449035645, train_x1_loss=0.3233669102191925, train_x2_loss=0.6907715797424316\n",
      "INFO:absl:[82] val_loss=1.722764015197754\n",
      "INFO:absl:[82] test_loss=1.6868782043457031\n",
      "INFO:absl:[83] train_loss=1.0137625932693481, train_x1_loss=0.3238725960254669, train_x2_loss=0.6898905038833618\n",
      "INFO:absl:[83] val_loss=1.7341511249542236\n",
      "INFO:absl:[83] test_loss=1.6972920894622803\n",
      "INFO:absl:[84] train_loss=1.0122729539871216, train_x1_loss=0.32149678468704224, train_x2_loss=0.690777599811554\n",
      "INFO:absl:[84] val_loss=1.7307488918304443\n",
      "INFO:absl:[84] test_loss=1.6937010288238525\n",
      "INFO:absl:[85] train_loss=1.013956904411316, train_x1_loss=0.32504701614379883, train_x2_loss=0.6889092922210693\n",
      "INFO:absl:[85] val_loss=1.7247869968414307\n",
      "INFO:absl:[85] test_loss=1.6879496574401855\n",
      "INFO:absl:[86] train_loss=1.013701319694519, train_x1_loss=0.3240355849266052, train_x2_loss=0.6896632313728333\n",
      "INFO:absl:[86] val_loss=1.7334107160568237\n",
      "INFO:absl:[86] test_loss=1.6975566148757935\n",
      "INFO:absl:[87] train_loss=1.0177584886550903, train_x1_loss=0.3265759348869324, train_x2_loss=0.6911829113960266\n",
      "INFO:absl:[87] val_loss=1.730647087097168\n",
      "INFO:absl:[87] test_loss=1.694563865661621\n",
      "INFO:absl:[88] train_loss=1.0151612758636475, train_x1_loss=0.3252944350242615, train_x2_loss=0.6898666024208069\n",
      "INFO:absl:[88] val_loss=1.7290055751800537\n",
      "INFO:absl:[88] test_loss=1.692150592803955\n",
      "INFO:absl:[89] train_loss=1.0175386667251587, train_x1_loss=0.32623252272605896, train_x2_loss=0.6913058757781982\n",
      "INFO:absl:[89] val_loss=1.7297688722610474\n",
      "INFO:absl:[89] test_loss=1.6937819719314575\n",
      "INFO:absl:[90] train_loss=1.0122798681259155, train_x1_loss=0.3212836682796478, train_x2_loss=0.6909986734390259\n",
      "INFO:absl:[90] val_loss=1.7252376079559326\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[90] test_loss=1.688656210899353\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[91] train_loss=1.017045497894287, train_x1_loss=0.327131450176239, train_x2_loss=0.6899145245552063\n",
      "INFO:absl:[91] val_loss=1.731938123703003\n",
      "INFO:absl:[91] test_loss=1.6943285465240479\n",
      "INFO:absl:[92] train_loss=1.0155694484710693, train_x1_loss=0.32387545704841614, train_x2_loss=0.6916953325271606\n",
      "INFO:absl:[92] val_loss=1.7280930280685425\n",
      "INFO:absl:[92] test_loss=1.6918892860412598\n",
      "INFO:absl:[93] train_loss=1.016121745109558, train_x1_loss=0.32525452971458435, train_x2_loss=0.6908664107322693\n",
      "INFO:absl:[93] val_loss=1.7283382415771484\n",
      "INFO:absl:[93] test_loss=1.691416621208191\n",
      "INFO:absl:[94] train_loss=1.0145658254623413, train_x1_loss=0.32371005415916443, train_x2_loss=0.6908541321754456\n",
      "INFO:absl:[94] val_loss=1.7235527038574219\n",
      "INFO:absl:[94] test_loss=1.6869854927062988\n",
      "INFO:absl:[95] train_loss=1.016950249671936, train_x1_loss=0.32510218024253845, train_x2_loss=0.691846489906311\n",
      "INFO:absl:[95] val_loss=1.7248024940490723\n",
      "INFO:absl:[95] test_loss=1.688769817352295\n",
      "INFO:absl:[96] train_loss=1.0127590894699097, train_x1_loss=0.3246622085571289, train_x2_loss=0.6880956888198853\n",
      "INFO:absl:[96] val_loss=1.722308874130249\n",
      "INFO:absl:[96] test_loss=1.6852349042892456\n",
      "INFO:absl:[97] train_loss=1.012661099433899, train_x1_loss=0.3222818374633789, train_x2_loss=0.6903783679008484\n",
      "INFO:absl:[97] val_loss=1.7254183292388916\n",
      "INFO:absl:[97] test_loss=1.68948233127594\n",
      "INFO:absl:[98] train_loss=1.016457200050354, train_x1_loss=0.3247067928314209, train_x2_loss=0.6917517185211182\n",
      "INFO:absl:[98] val_loss=1.721022605895996\n",
      "INFO:absl:[98] test_loss=1.6849364042282104\n",
      "INFO:absl:[99] train_loss=1.0130339860916138, train_x1_loss=0.321188360452652, train_x2_loss=0.6918456554412842\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[99] val_loss=1.7155741453170776\n",
      "INFO:absl:[99] test_loss=1.6786924600601196\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'relu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 10, 'dropout_rate': 0.21664822298158384, 'edge_features': (4, 8), 'epochs': 100, 'eval_every_epochs': 1, 'fully_connected_edges': 1, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 1, 'layer_norm': False, 'learning_rate': 2.9648345553652954e-05, 'log_every_epochs': 1, 'max_checkpts_to_keep': 2, 'model': 'MLPGraphNetwork', 'n_blocks': 1, 'n_samples': 5000, 'node_features': (128, 2), 'normalize': True, 'optimizer': 'adam', 'output_delay': 0, 'output_steps': 4, 'sample_buffer': -4, 'seed': 73, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 120, 'timestep_duration': 3, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+-----------+-------+----------+--------+\n",
      "| Name                                   | Shape     | Size  | Mean     | Std    |\n",
      "+----------------------------------------+-----------+-------+----------+--------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (4,)      | 4     | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 4)    | 24    | -0.0819  | 0.36   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (8,)      | 8     | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (4, 8)    | 32    | -0.0639  | 0.439  |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (128,)    | 128   | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (19, 128) | 2,432 | -0.00462 | 0.228  |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)      | 2     | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (128, 2)  | 256   | -0.00622 | 0.0884 |\n",
      "+----------------------------------------+-----------+-------+----------+--------+\n",
      "Total: 2,886\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Checkpoint None does not exist.\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.01s.\n",
      "INFO:absl:Starting training.\n",
      "INFO:absl:[0] train_loss=1.4893174171447754, train_x1_loss=0.6513827443122864, train_x2_loss=0.8379331827163696\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[0] val_loss=1.761391282081604\n",
      "INFO:absl:[0] test_loss=1.8691552877426147\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Created artifact [10] Profile of type ArtifactType.URL and value None.\n",
      "INFO:absl:[1] train_loss=1.1897379159927368, train_x1_loss=0.4280783236026764, train_x2_loss=0.7616614103317261\n",
      "INFO:absl:[1] val_loss=1.6573057174682617\n",
      "INFO:absl:[1] test_loss=1.7651296854019165\n",
      "INFO:absl:[2] train_loss=1.1180580854415894, train_x1_loss=0.38169416785240173, train_x2_loss=0.7363646626472473\n",
      "INFO:absl:[2] val_loss=1.6422780752182007\n",
      "INFO:absl:[2] test_loss=1.7486542463302612\n",
      "INFO:absl:[3] train_loss=1.089658260345459, train_x1_loss=0.3648965656757355, train_x2_loss=0.7247630953788757\n",
      "INFO:absl:[3] val_loss=1.6187151670455933\n",
      "INFO:absl:[3] test_loss=1.7270301580429077\n",
      "INFO:absl:[4] train_loss=1.0672831535339355, train_x1_loss=0.35167646408081055, train_x2_loss=0.7156067490577698\n",
      "INFO:absl:[4] val_loss=1.6071516275405884\n",
      "INFO:absl:[4] test_loss=1.7146774530410767\n",
      "INFO:absl:[5] train_loss=1.064841628074646, train_x1_loss=0.3503488004207611, train_x2_loss=0.714492678642273\n",
      "INFO:absl:[5] val_loss=1.6159813404083252\n",
      "INFO:absl:[5] test_loss=1.7228493690490723\n",
      "INFO:absl:[6] train_loss=1.0508873462677002, train_x1_loss=0.34217146039009094, train_x2_loss=0.7087171673774719\n",
      "INFO:absl:[6] val_loss=1.6012029647827148\n",
      "INFO:absl:[6] test_loss=1.7097402811050415\n",
      "INFO:absl:[7] train_loss=1.0496474504470825, train_x1_loss=0.34146934747695923, train_x2_loss=0.7081778049468994\n",
      "INFO:absl:[7] val_loss=1.6106914281845093\n",
      "INFO:absl:[7] test_loss=1.7190710306167603\n",
      "INFO:absl:[8] train_loss=1.0455090999603271, train_x1_loss=0.33909282088279724, train_x2_loss=0.7064163684844971\n",
      "INFO:absl:[8] val_loss=1.6018011569976807\n",
      "INFO:absl:[8] test_loss=1.709728479385376\n",
      "INFO:absl:[9] train_loss=1.0419285297393799, train_x1_loss=0.3379620909690857, train_x2_loss=0.703966498374939\n",
      "INFO:absl:[9] val_loss=1.6047146320343018\n",
      "INFO:absl:[9] test_loss=1.7128726243972778\n",
      "INFO:absl:[10] train_loss=1.0414295196533203, train_x1_loss=0.336940199136734, train_x2_loss=0.7044891715049744\n",
      "INFO:absl:[10] val_loss=1.6039812564849854\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[10] test_loss=1.7123745679855347\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[11] train_loss=1.039480209350586, train_x1_loss=0.3373735845088959, train_x2_loss=0.7021070122718811\n",
      "INFO:absl:[11] val_loss=1.5905280113220215\n",
      "INFO:absl:[11] test_loss=1.6989980936050415\n",
      "INFO:absl:[12] train_loss=1.0375666618347168, train_x1_loss=0.3361215591430664, train_x2_loss=0.7014457583427429\n",
      "INFO:absl:[12] val_loss=1.5800620317459106\n",
      "INFO:absl:[12] test_loss=1.6889361143112183\n",
      "INFO:absl:[13] train_loss=1.0340404510498047, train_x1_loss=0.3335295021533966, train_x2_loss=0.7005100846290588\n",
      "INFO:absl:[13] val_loss=1.5822452306747437\n",
      "INFO:absl:[13] test_loss=1.6914775371551514\n",
      "INFO:absl:[14] train_loss=1.037360429763794, train_x1_loss=0.33389154076576233, train_x2_loss=0.7034686207771301\n",
      "INFO:absl:[14] val_loss=1.5815322399139404\n",
      "INFO:absl:[14] test_loss=1.6909898519515991\n",
      "INFO:absl:[15] train_loss=1.033639907836914, train_x1_loss=0.3347196877002716, train_x2_loss=0.6989206671714783\n",
      "INFO:absl:[15] val_loss=1.5770595073699951\n",
      "INFO:absl:[15] test_loss=1.6863205432891846\n",
      "INFO:absl:[16] train_loss=1.0355494022369385, train_x1_loss=0.3351394832134247, train_x2_loss=0.700410783290863\n",
      "INFO:absl:[16] val_loss=1.579817771911621\n",
      "INFO:absl:[16] test_loss=1.6887344121932983\n",
      "INFO:absl:[17] train_loss=1.030551791191101, train_x1_loss=0.3310120403766632, train_x2_loss=0.6995409727096558\n",
      "INFO:absl:[17] val_loss=1.5796953439712524\n",
      "INFO:absl:[17] test_loss=1.6892884969711304\n",
      "INFO:absl:[18] train_loss=1.034386396408081, train_x1_loss=0.3344634771347046, train_x2_loss=0.6999231576919556\n",
      "INFO:absl:[18] val_loss=1.58270263671875\n",
      "INFO:absl:[18] test_loss=1.6913864612579346\n",
      "INFO:absl:[19] train_loss=1.033191442489624, train_x1_loss=0.3336075246334076, train_x2_loss=0.6995837092399597\n",
      "INFO:absl:[19] val_loss=1.5841712951660156\n",
      "INFO:absl:[19] test_loss=1.6934655904769897\n",
      "INFO:absl:[20] train_loss=1.0355944633483887, train_x1_loss=0.3357847332954407, train_x2_loss=0.6998107433319092\n",
      "INFO:absl:[20] val_loss=1.5804343223571777\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[20] test_loss=1.689795732498169\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[21] train_loss=1.0306535959243774, train_x1_loss=0.3314268887042999, train_x2_loss=0.699225664138794\n",
      "INFO:absl:[21] val_loss=1.579079031944275\n",
      "INFO:absl:[21] test_loss=1.68907630443573\n",
      "INFO:absl:[22] train_loss=1.0305392742156982, train_x1_loss=0.33279669284820557, train_x2_loss=0.6977417469024658\n",
      "INFO:absl:[22] val_loss=1.5808227062225342\n",
      "INFO:absl:[22] test_loss=1.6908552646636963\n",
      "INFO:absl:[23] train_loss=1.0314878225326538, train_x1_loss=0.33417847752571106, train_x2_loss=0.6973093748092651\n",
      "INFO:absl:[23] val_loss=1.583008885383606\n",
      "INFO:absl:[23] test_loss=1.6927043199539185\n",
      "INFO:absl:[24] train_loss=1.0315865278244019, train_x1_loss=0.333833247423172, train_x2_loss=0.6977527737617493\n",
      "INFO:absl:[24] val_loss=1.5730488300323486\n",
      "INFO:absl:[24] test_loss=1.6831790208816528\n",
      "INFO:absl:[25] train_loss=1.0318180322647095, train_x1_loss=0.33391326665878296, train_x2_loss=0.697904109954834\n",
      "INFO:absl:[25] val_loss=1.5712677240371704\n",
      "INFO:absl:[25] test_loss=1.6811922788619995\n",
      "INFO:absl:[26] train_loss=1.0286024808883667, train_x1_loss=0.33081793785095215, train_x2_loss=0.697784960269928\n",
      "INFO:absl:[26] val_loss=1.5799793004989624\n",
      "INFO:absl:[26] test_loss=1.6901869773864746\n",
      "INFO:absl:[27] train_loss=1.031365156173706, train_x1_loss=0.3324766755104065, train_x2_loss=0.6988877058029175\n",
      "INFO:absl:[27] val_loss=1.5879335403442383\n",
      "INFO:absl:[27] test_loss=1.6971184015274048\n",
      "INFO:absl:[28] train_loss=1.0291491746902466, train_x1_loss=0.32917094230651855, train_x2_loss=0.699978232383728\n",
      "INFO:absl:[28] val_loss=1.584754228591919\n",
      "INFO:absl:[28] test_loss=1.694191575050354\n",
      "INFO:absl:[29] train_loss=1.0330501794815063, train_x1_loss=0.3341658413410187, train_x2_loss=0.6988857984542847\n",
      "INFO:absl:[29] val_loss=1.5805108547210693\n",
      "INFO:absl:[29] test_loss=1.6902059316635132\n",
      "INFO:absl:[30] train_loss=1.0323233604431152, train_x1_loss=0.332933634519577, train_x2_loss=0.6993908882141113\n",
      "INFO:absl:[30] val_loss=1.5787832736968994\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[30] test_loss=1.6877546310424805\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:[31] train_loss=1.0290446281433105, train_x1_loss=0.3318602740764618, train_x2_loss=0.6971839666366577\n",
      "INFO:absl:[31] val_loss=1.585202932357788\n",
      "INFO:absl:[31] test_loss=1.695600152015686\n",
      "INFO:absl:[32] train_loss=1.0324697494506836, train_x1_loss=0.33339640498161316, train_x2_loss=0.6990740895271301\n",
      "INFO:absl:[32] val_loss=1.5818414688110352\n",
      "INFO:absl:[32] test_loss=1.6913312673568726\n",
      "INFO:absl:[33] train_loss=1.02454674243927, train_x1_loss=0.3283326029777527, train_x2_loss=0.6962140798568726\n",
      "INFO:absl:[33] val_loss=1.5748655796051025\n",
      "INFO:absl:[33] test_loss=1.6846879720687866\n",
      "INFO:absl:[34] train_loss=1.0266613960266113, train_x1_loss=0.3314554691314697, train_x2_loss=0.6952047348022461\n",
      "INFO:absl:[34] val_loss=1.5805505514144897\n",
      "INFO:absl:[34] test_loss=1.6902259588241577\n",
      "INFO:absl:[35] train_loss=1.032684326171875, train_x1_loss=0.3329186737537384, train_x2_loss=0.6997653245925903\n",
      "INFO:absl:[35] val_loss=1.5832957029342651\n",
      "INFO:absl:[35] test_loss=1.6930742263793945\n",
      "INFO:absl:[36] train_loss=1.0292049646377563, train_x1_loss=0.33123743534088135, train_x2_loss=0.6979663968086243\n",
      "INFO:absl:[36] val_loss=1.5746465921401978\n",
      "INFO:absl:[36] test_loss=1.6836692094802856\n",
      "INFO:absl:Setting work unit notes: 2211.5 steps/s, 37.9% (132693/350000), ETA: 1m (1m : 0.0% checkpoint, 9.1% eval)\n",
      "INFO:absl:[132693] steps_per_sec=2211.546951\n",
      "INFO:absl:[37] train_loss=1.02901029586792, train_x1_loss=0.331121027469635, train_x2_loss=0.6978895664215088\n",
      "INFO:absl:[37] val_loss=1.5839838981628418\n",
      "INFO:absl:[37] test_loss=1.6931817531585693\n",
      "INFO:absl:[38] train_loss=1.0287538766860962, train_x1_loss=0.33200234174728394, train_x2_loss=0.6967535614967346\n",
      "INFO:absl:[38] val_loss=1.5745258331298828\n",
      "INFO:absl:[38] test_loss=1.684248447418213\n",
      "INFO:absl:[39] train_loss=1.0278948545455933, train_x1_loss=0.33052340149879456, train_x2_loss=0.69737309217453\n",
      "INFO:absl:[39] val_loss=1.579572319984436\n",
      "INFO:absl:[39] test_loss=1.6891003847122192\n",
      "INFO:absl:[40] train_loss=1.0224506855010986, train_x1_loss=0.32728421688079834, train_x2_loss=0.6951676607131958\n",
      "INFO:absl:[40] val_loss=1.5691040754318237\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[40] test_loss=1.6795368194580078\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[41] train_loss=1.0302033424377441, train_x1_loss=0.33288267254829407, train_x2_loss=0.6973214745521545\n",
      "INFO:absl:[41] val_loss=1.5784519910812378\n",
      "INFO:absl:[41] test_loss=1.6880760192871094\n",
      "INFO:absl:[42] train_loss=1.0263850688934326, train_x1_loss=0.32950836420059204, train_x2_loss=0.6968770027160645\n",
      "INFO:absl:[42] val_loss=1.5788205862045288\n",
      "INFO:absl:[42] test_loss=1.6890660524368286\n",
      "INFO:absl:[43] train_loss=1.0292474031448364, train_x1_loss=0.33150264620780945, train_x2_loss=0.6977428793907166\n",
      "INFO:absl:[43] val_loss=1.5810348987579346\n",
      "INFO:absl:[43] test_loss=1.690475583076477\n",
      "INFO:absl:[44] train_loss=1.0278664827346802, train_x1_loss=0.3310488164424896, train_x2_loss=0.6968177556991577\n",
      "INFO:absl:[44] val_loss=1.5786439180374146\n",
      "INFO:absl:[44] test_loss=1.6878488063812256\n",
      "INFO:absl:[45] train_loss=1.02688729763031, train_x1_loss=0.32955968379974365, train_x2_loss=0.6973267793655396\n",
      "INFO:absl:[45] val_loss=1.578142523765564\n",
      "INFO:absl:[45] test_loss=1.6882871389389038\n",
      "INFO:absl:[46] train_loss=1.0299783945083618, train_x1_loss=0.33032387495040894, train_x2_loss=0.699655294418335\n",
      "INFO:absl:[46] val_loss=1.5826486349105835\n",
      "INFO:absl:[46] test_loss=1.6924115419387817\n",
      "INFO:absl:[47] train_loss=1.0311100482940674, train_x1_loss=0.3296073377132416, train_x2_loss=0.7014998197555542\n",
      "INFO:absl:[47] val_loss=1.5797317028045654\n",
      "INFO:absl:[47] test_loss=1.6899303197860718\n",
      "INFO:absl:[48] train_loss=1.0280121564865112, train_x1_loss=0.32933175563812256, train_x2_loss=0.6986805200576782\n",
      "INFO:absl:[48] val_loss=1.5857610702514648\n",
      "INFO:absl:[48] test_loss=1.694777011871338\n",
      "INFO:absl:[49] train_loss=1.027214527130127, train_x1_loss=0.32994022965431213, train_x2_loss=0.6972731351852417\n",
      "INFO:absl:[49] val_loss=1.56927490234375\n",
      "INFO:absl:[49] test_loss=1.6787822246551514\n",
      "INFO:absl:[50] train_loss=1.029188632965088, train_x1_loss=0.32975029945373535, train_x2_loss=0.6994362473487854\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[50] val_loss=1.5823845863342285\n",
      "INFO:absl:[50] test_loss=1.6930334568023682\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[51] train_loss=1.0311126708984375, train_x1_loss=0.33164870738983154, train_x2_loss=0.6994627714157104\n",
      "INFO:absl:[51] val_loss=1.5773683786392212\n",
      "INFO:absl:[51] test_loss=1.6869816780090332\n",
      "INFO:absl:[52] train_loss=1.0306214094161987, train_x1_loss=0.3317272663116455, train_x2_loss=0.6988940238952637\n",
      "INFO:absl:[52] val_loss=1.58151113986969\n",
      "INFO:absl:[52] test_loss=1.691247582435608\n",
      "INFO:absl:[53] train_loss=1.029557228088379, train_x1_loss=0.33051007986068726, train_x2_loss=0.6990468502044678\n",
      "INFO:absl:[53] val_loss=1.5806657075881958\n",
      "INFO:absl:[53] test_loss=1.6910046339035034\n",
      "INFO:absl:[54] train_loss=1.0285696983337402, train_x1_loss=0.32941269874572754, train_x2_loss=0.6991589069366455\n",
      "INFO:absl:[54] val_loss=1.5757594108581543\n",
      "INFO:absl:[54] test_loss=1.6860507726669312\n",
      "INFO:absl:[55] train_loss=1.0264841318130493, train_x1_loss=0.32783448696136475, train_x2_loss=0.6986504197120667\n",
      "INFO:absl:[55] val_loss=1.5848848819732666\n",
      "INFO:absl:[55] test_loss=1.6939074993133545\n",
      "INFO:absl:[56] train_loss=1.0272470712661743, train_x1_loss=0.32900679111480713, train_x2_loss=0.6982409954071045\n",
      "INFO:absl:[56] val_loss=1.5741335153579712\n",
      "INFO:absl:[56] test_loss=1.6835968494415283\n",
      "INFO:absl:[57] train_loss=1.0284076929092407, train_x1_loss=0.33072179555892944, train_x2_loss=0.6976853609085083\n",
      "INFO:absl:[57] val_loss=1.578345537185669\n",
      "INFO:absl:[57] test_loss=1.6882174015045166\n",
      "INFO:absl:[58] train_loss=1.0264536142349243, train_x1_loss=0.3301195502281189, train_x2_loss=0.6963334679603577\n",
      "INFO:absl:[58] val_loss=1.572737216949463\n",
      "INFO:absl:[58] test_loss=1.6824848651885986\n",
      "INFO:absl:[59] train_loss=1.0264511108398438, train_x1_loss=0.32877177000045776, train_x2_loss=0.6976786255836487\n",
      "INFO:absl:[59] val_loss=1.5911635160446167\n",
      "INFO:absl:[59] test_loss=1.7001922130584717\n",
      "INFO:absl:[60] train_loss=1.0316965579986572, train_x1_loss=0.3313932716846466, train_x2_loss=0.7003034353256226\n",
      "INFO:absl:[60] val_loss=1.574213981628418\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[60] test_loss=1.6837819814682007\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[61] train_loss=1.027531623840332, train_x1_loss=0.32984745502471924, train_x2_loss=0.697684109210968\n",
      "INFO:absl:[61] val_loss=1.5739108324050903\n",
      "INFO:absl:[61] test_loss=1.6844964027404785\n",
      "INFO:absl:[62] train_loss=1.0270519256591797, train_x1_loss=0.3309263288974762, train_x2_loss=0.6961265802383423\n",
      "INFO:absl:[62] val_loss=1.582058310508728\n",
      "INFO:absl:[62] test_loss=1.6922403573989868\n",
      "INFO:absl:[63] train_loss=1.026342511177063, train_x1_loss=0.3308941423892975, train_x2_loss=0.6954468488693237\n",
      "INFO:absl:[63] val_loss=1.5824706554412842\n",
      "INFO:absl:[63] test_loss=1.6915526390075684\n",
      "INFO:absl:[64] train_loss=1.0273258686065674, train_x1_loss=0.33144041895866394, train_x2_loss=0.6958855986595154\n",
      "INFO:absl:[64] val_loss=1.571705937385559\n",
      "INFO:absl:[64] test_loss=1.6817612648010254\n",
      "INFO:absl:[65] train_loss=1.0270556211471558, train_x1_loss=0.3293958008289337, train_x2_loss=0.6976606249809265\n",
      "INFO:absl:[65] val_loss=1.5793864727020264\n",
      "INFO:absl:[65] test_loss=1.6892149448394775\n",
      "INFO:absl:[66] train_loss=1.0254414081573486, train_x1_loss=0.32901108264923096, train_x2_loss=0.6964278221130371\n",
      "INFO:absl:[66] val_loss=1.5829758644104004\n",
      "INFO:absl:[66] test_loss=1.6919881105422974\n",
      "INFO:absl:[67] train_loss=1.0235942602157593, train_x1_loss=0.3263442814350128, train_x2_loss=0.6972507238388062\n",
      "INFO:absl:[67] val_loss=1.574857234954834\n",
      "INFO:absl:[67] test_loss=1.6844557523727417\n",
      "INFO:absl:[68] train_loss=1.0273544788360596, train_x1_loss=0.3307842016220093, train_x2_loss=0.6965686678886414\n",
      "INFO:absl:[68] val_loss=1.5861557722091675\n",
      "INFO:absl:[68] test_loss=1.6949301958084106\n",
      "INFO:absl:[69] train_loss=1.026678442955017, train_x1_loss=0.32805687189102173, train_x2_loss=0.6986220479011536\n",
      "INFO:absl:[69] val_loss=1.568345069885254\n",
      "INFO:absl:[69] test_loss=1.6780476570129395\n",
      "INFO:absl:[70] train_loss=1.0283560752868652, train_x1_loss=0.33137428760528564, train_x2_loss=0.6969810724258423\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[70] val_loss=1.57124662399292\n",
      "INFO:absl:[70] test_loss=1.6814769506454468\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[71] train_loss=1.0214612483978271, train_x1_loss=0.32707804441452026, train_x2_loss=0.694383442401886\n",
      "INFO:absl:[71] val_loss=1.5764518976211548\n",
      "INFO:absl:[71] test_loss=1.6865499019622803\n",
      "INFO:absl:[72] train_loss=1.0260270833969116, train_x1_loss=0.3283446431159973, train_x2_loss=0.6976838111877441\n",
      "INFO:absl:[72] val_loss=1.5810893774032593\n",
      "INFO:absl:[72] test_loss=1.6911890506744385\n",
      "INFO:absl:[73] train_loss=1.0269438028335571, train_x1_loss=0.3305118978023529, train_x2_loss=0.696431577205658\n",
      "INFO:absl:[73] val_loss=1.5754663944244385\n",
      "INFO:absl:[73] test_loss=1.6854403018951416\n",
      "INFO:absl:[74] train_loss=1.0269289016723633, train_x1_loss=0.3290427327156067, train_x2_loss=0.6978867053985596\n",
      "INFO:absl:[74] val_loss=1.579186201095581\n",
      "INFO:absl:[74] test_loss=1.6888889074325562\n",
      "INFO:absl:[75] train_loss=1.0309652090072632, train_x1_loss=0.33028778433799744, train_x2_loss=0.700677752494812\n",
      "INFO:absl:[75] val_loss=1.5827237367630005\n",
      "INFO:absl:[75] test_loss=1.6923229694366455\n",
      "INFO:absl:[76] train_loss=1.0244261026382446, train_x1_loss=0.3260241448879242, train_x2_loss=0.6984007358551025\n",
      "INFO:absl:[76] val_loss=1.5762226581573486\n",
      "INFO:absl:[76] test_loss=1.685725450515747\n",
      "INFO:absl:[77] train_loss=1.0257641077041626, train_x1_loss=0.3296625316143036, train_x2_loss=0.6960995197296143\n",
      "INFO:absl:[77] val_loss=1.581931471824646\n",
      "INFO:absl:[77] test_loss=1.6920613050460815\n",
      "INFO:absl:[78] train_loss=1.0276869535446167, train_x1_loss=0.3300507068634033, train_x2_loss=0.6976348161697388\n",
      "INFO:absl:[78] val_loss=1.5757323503494263\n",
      "INFO:absl:[78] test_loss=1.685375452041626\n",
      "INFO:absl:[79] train_loss=1.0304384231567383, train_x1_loss=0.33160996437072754, train_x2_loss=0.6988283395767212\n",
      "INFO:absl:[79] val_loss=1.5736638307571411\n",
      "INFO:absl:[79] test_loss=1.6833999156951904\n",
      "INFO:absl:[80] train_loss=1.027408242225647, train_x1_loss=0.32908862829208374, train_x2_loss=0.6983187794685364\n",
      "INFO:absl:[80] val_loss=1.577134370803833\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[80] test_loss=1.6861896514892578\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Setting work unit notes: 2538.7 steps/s, 81.4% (285017/350000), ETA: 0m (2m : 0.0% checkpoint, 9.1% eval)\n",
      "INFO:absl:[285017] steps_per_sec=2538.720935\n",
      "INFO:absl:[81] train_loss=1.026064157485962, train_x1_loss=0.327777624130249, train_x2_loss=0.6982859373092651\n",
      "INFO:absl:[81] val_loss=1.5850352048873901\n",
      "INFO:absl:[81] test_loss=1.6946263313293457\n",
      "INFO:absl:[82] train_loss=1.0257048606872559, train_x1_loss=0.32871758937835693, train_x2_loss=0.6969863176345825\n",
      "INFO:absl:[82] val_loss=1.5734694004058838\n",
      "INFO:absl:[82] test_loss=1.6837352514266968\n",
      "INFO:absl:[83] train_loss=1.0244684219360352, train_x1_loss=0.3282483220100403, train_x2_loss=0.696219265460968\n",
      "INFO:absl:[83] val_loss=1.5847861766815186\n",
      "INFO:absl:[83] test_loss=1.69479238986969\n",
      "INFO:absl:[84] train_loss=1.026166558265686, train_x1_loss=0.3289363384246826, train_x2_loss=0.6972302794456482\n",
      "INFO:absl:[84] val_loss=1.5770901441574097\n",
      "INFO:absl:[84] test_loss=1.6866064071655273\n",
      "INFO:absl:[85] train_loss=1.0283437967300415, train_x1_loss=0.3283785879611969, train_x2_loss=0.6999659538269043\n",
      "INFO:absl:[85] val_loss=1.5766276121139526\n",
      "INFO:absl:[85] test_loss=1.6872152090072632\n",
      "INFO:absl:[86] train_loss=1.0289947986602783, train_x1_loss=0.328557550907135, train_x2_loss=0.7004356980323792\n",
      "INFO:absl:[86] val_loss=1.5891540050506592\n",
      "INFO:absl:[86] test_loss=1.6989153623580933\n",
      "INFO:absl:[87] train_loss=1.0263420343399048, train_x1_loss=0.3302469551563263, train_x2_loss=0.6960960030555725\n",
      "INFO:absl:[87] val_loss=1.575028896331787\n",
      "INFO:absl:[87] test_loss=1.6853238344192505\n",
      "INFO:absl:[88] train_loss=1.0249650478363037, train_x1_loss=0.32773175835609436, train_x2_loss=0.6972329616546631\n",
      "INFO:absl:[88] val_loss=1.583168864250183\n",
      "INFO:absl:[88] test_loss=1.6927000284194946\n",
      "INFO:absl:[89] train_loss=1.026543140411377, train_x1_loss=0.3292248249053955, train_x2_loss=0.6973196268081665\n",
      "INFO:absl:[89] val_loss=1.57623291015625\n",
      "INFO:absl:[89] test_loss=1.6865228414535522\n",
      "INFO:absl:[90] train_loss=1.0278186798095703, train_x1_loss=0.32921355962753296, train_x2_loss=0.6986048817634583\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[90] val_loss=1.5833548307418823\n",
      "INFO:absl:[90] test_loss=1.6930700540542603\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[91] train_loss=1.0281983613967896, train_x1_loss=0.33023977279663086, train_x2_loss=0.6979598999023438\n",
      "INFO:absl:[91] val_loss=1.5758143663406372\n",
      "INFO:absl:[91] test_loss=1.6858131885528564\n",
      "INFO:absl:[92] train_loss=1.0247000455856323, train_x1_loss=0.3285878300666809, train_x2_loss=0.6961104273796082\n",
      "INFO:absl:[92] val_loss=1.5812044143676758\n",
      "INFO:absl:[92] test_loss=1.6909667253494263\n",
      "INFO:absl:[93] train_loss=1.028671145439148, train_x1_loss=0.3293287754058838, train_x2_loss=0.6993433237075806\n",
      "INFO:absl:[93] val_loss=1.5785419940948486\n",
      "INFO:absl:[93] test_loss=1.688527226448059\n",
      "INFO:absl:[94] train_loss=1.0280766487121582, train_x1_loss=0.3301028907299042, train_x2_loss=0.697971522808075\n",
      "INFO:absl:[94] val_loss=1.5796864032745361\n",
      "INFO:absl:[94] test_loss=1.689711570739746\n",
      "INFO:absl:[95] train_loss=1.0280797481536865, train_x1_loss=0.33059626817703247, train_x2_loss=0.6974850296974182\n",
      "INFO:absl:[95] val_loss=1.5804531574249268\n",
      "INFO:absl:[95] test_loss=1.6896822452545166\n",
      "INFO:absl:[96] train_loss=1.022361397743225, train_x1_loss=0.3264634907245636, train_x2_loss=0.6958945989608765\n",
      "INFO:absl:[96] val_loss=1.5790934562683105\n",
      "INFO:absl:[96] test_loss=1.6885298490524292\n",
      "INFO:absl:[97] train_loss=1.0287938117980957, train_x1_loss=0.3317292034626007, train_x2_loss=0.6970627903938293\n",
      "INFO:absl:[97] val_loss=1.5758018493652344\n",
      "INFO:absl:[97] test_loss=1.6851894855499268\n",
      "INFO:absl:[98] train_loss=1.030922770500183, train_x1_loss=0.33191344141960144, train_x2_loss=0.6990095376968384\n",
      "INFO:absl:[98] val_loss=1.5865345001220703\n",
      "INFO:absl:[98] test_loss=1.6961345672607422\n",
      "INFO:absl:[99] train_loss=1.0248602628707886, train_x1_loss=0.3273581564426422, train_x2_loss=0.6975026726722717\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[99] val_loss=1.5729306936264038\n",
      "INFO:absl:[99] test_loss=1.6825414896011353\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Hyperparameters: {'F': 8, 'K': 36, 'activation': 'relu', 'b': 10, 'c': 10, 'checkpoint_every_epochs': 10, 'dropout_rate': 0.21664822298158384, 'edge_features': (4, 8), 'epochs': 100, 'eval_every_epochs': 1, 'fully_connected_edges': 1, 'global_features': None, 'h': 1, 'init_buffer_samples': 0, 'input_steps': 1, 'layer_norm': False, 'learning_rate': 2.9648345553652954e-05, 'log_every_epochs': 1, 'max_checkpts_to_keep': 2, 'model': 'MLPGraphNetwork', 'n_blocks': 1, 'n_samples': 5000, 'node_features': (128, 2), 'normalize': True, 'optimizer': 'adam', 'output_delay': 0, 'output_steps': 4, 'sample_buffer': -4, 'seed': 74, 'share_params': False, 'skip_connections': False, 'test_pct': 0.1, 'time_resolution': 120, 'timestep_duration': 3, 'train_pct': 0.7, 'val_pct': 0.2}\n",
      "INFO:absl:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+-----------+-------+----------+--------+\n",
      "| Name                                   | Shape     | Size  | Mean     | Std    |\n",
      "+----------------------------------------+-----------+-------+----------+--------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (4,)      | 4     | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 4)    | 24    | -0.0819  | 0.36   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (8,)      | 8     | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (4, 8)    | 32    | -0.0639  | 0.439  |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (128,)    | 128   | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (19, 128) | 2,432 | -0.00462 | 0.228  |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)      | 2     | 0.0      | 0.0    |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (128, 2)  | 256   | -0.00622 | 0.0884 |\n",
      "+----------------------------------------+-----------+-------+----------+--------+\n",
      "Total: 2,886\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:No checkpoint specified. Restore the latest checkpoint.\n",
      "INFO:absl:Checkpoint None does not exist.\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:Checkpoint.save() finished after 0.00s.\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.01s.\n",
      "INFO:absl:Starting training.\n",
      "INFO:absl:[0] train_loss=1.5012390613555908, train_x1_loss=0.6580597162246704, train_x2_loss=0.8431798815727234\n",
      "INFO:absl:[0] val_loss=1.7984062433242798\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[0] test_loss=1.7711350917816162\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:Created artifact [10] Profile of type ArtifactType.URL and value None.\n",
      "INFO:absl:[1] train_loss=1.1972506046295166, train_x1_loss=0.4291161894798279, train_x2_loss=0.7681326866149902\n",
      "INFO:absl:[1] val_loss=1.6977477073669434\n",
      "INFO:absl:[1] test_loss=1.6677346229553223\n",
      "INFO:absl:[2] train_loss=1.1298654079437256, train_x1_loss=0.3830956816673279, train_x2_loss=0.7467681765556335\n",
      "INFO:absl:[2] val_loss=1.6857548952102661\n",
      "INFO:absl:[2] test_loss=1.6540977954864502\n",
      "INFO:absl:[3] train_loss=1.090941071510315, train_x1_loss=0.3615161180496216, train_x2_loss=0.7294228076934814\n",
      "INFO:absl:[3] val_loss=1.6645967960357666\n",
      "INFO:absl:[3] test_loss=1.6324725151062012\n",
      "INFO:absl:[4] train_loss=1.0760681629180908, train_x1_loss=0.3529905080795288, train_x2_loss=0.7230775952339172\n",
      "INFO:absl:[4] val_loss=1.6558995246887207\n",
      "INFO:absl:[4] test_loss=1.623082160949707\n",
      "INFO:absl:[5] train_loss=1.0657628774642944, train_x1_loss=0.3466883599758148, train_x2_loss=0.7190744876861572\n",
      "INFO:absl:[5] val_loss=1.6579406261444092\n",
      "INFO:absl:[5] test_loss=1.6242852210998535\n",
      "INFO:absl:[6] train_loss=1.0558658838272095, train_x1_loss=0.33969756960868835, train_x2_loss=0.7161684036254883\n",
      "INFO:absl:[6] val_loss=1.652166724205017\n",
      "INFO:absl:[6] test_loss=1.6187999248504639\n",
      "INFO:absl:[7] train_loss=1.052730679512024, train_x1_loss=0.3390183448791504, train_x2_loss=0.7137115001678467\n",
      "INFO:absl:[7] val_loss=1.646633267402649\n",
      "INFO:absl:[7] test_loss=1.6135778427124023\n",
      "INFO:absl:[8] train_loss=1.0455949306488037, train_x1_loss=0.33571138978004456, train_x2_loss=0.7098833918571472\n",
      "INFO:absl:[8] val_loss=1.6438730955123901\n",
      "INFO:absl:[8] test_loss=1.6102644205093384\n",
      "INFO:absl:[9] train_loss=1.0454351902008057, train_x1_loss=0.33600956201553345, train_x2_loss=0.7094259858131409\n",
      "INFO:absl:[9] val_loss=1.6377638578414917\n",
      "INFO:absl:[9] test_loss=1.6047605276107788\n",
      "INFO:absl:[10] train_loss=1.0450478792190552, train_x1_loss=0.333753764629364, train_x2_loss=0.7112939357757568\n",
      "INFO:absl:[10] val_loss=1.6404856443405151\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[10] test_loss=1.6072112321853638\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[11] train_loss=1.0406514406204224, train_x1_loss=0.33332404494285583, train_x2_loss=0.707327127456665\n",
      "INFO:absl:[11] val_loss=1.636143684387207\n",
      "INFO:absl:[11] test_loss=1.6026971340179443\n",
      "INFO:absl:[12] train_loss=1.035523533821106, train_x1_loss=0.32987868785858154, train_x2_loss=0.7056459188461304\n",
      "INFO:absl:[12] val_loss=1.6331216096878052\n",
      "INFO:absl:[12] test_loss=1.5995309352874756\n",
      "INFO:absl:[13] train_loss=1.039594292640686, train_x1_loss=0.33173611760139465, train_x2_loss=0.7078579068183899\n",
      "INFO:absl:[13] val_loss=1.6337040662765503\n",
      "INFO:absl:[13] test_loss=1.5998854637145996\n",
      "INFO:absl:[14] train_loss=1.040575623512268, train_x1_loss=0.3322991728782654, train_x2_loss=0.7082775235176086\n",
      "INFO:absl:[14] val_loss=1.6311050653457642\n",
      "INFO:absl:[14] test_loss=1.597463846206665\n",
      "INFO:absl:[15] train_loss=1.037144422531128, train_x1_loss=0.3324175179004669, train_x2_loss=0.7047286033630371\n",
      "INFO:absl:[15] val_loss=1.6299101114273071\n",
      "INFO:absl:[15] test_loss=1.5963833332061768\n",
      "INFO:absl:[16] train_loss=1.0417628288269043, train_x1_loss=0.3332441747188568, train_x2_loss=0.708520233631134\n",
      "INFO:absl:[16] val_loss=1.62128484249115\n",
      "INFO:absl:[16] test_loss=1.587918758392334\n",
      "INFO:absl:[17] train_loss=1.0327287912368774, train_x1_loss=0.32904815673828125, train_x2_loss=0.7036809921264648\n",
      "INFO:absl:[17] val_loss=1.6225465536117554\n",
      "INFO:absl:[17] test_loss=1.5889455080032349\n",
      "INFO:absl:[18] train_loss=1.034828782081604, train_x1_loss=0.3308696746826172, train_x2_loss=0.7039601802825928\n",
      "INFO:absl:[18] val_loss=1.624629020690918\n",
      "INFO:absl:[18] test_loss=1.5910028219223022\n",
      "INFO:absl:[19] train_loss=1.0405454635620117, train_x1_loss=0.3330253064632416, train_x2_loss=0.7075201869010925\n",
      "INFO:absl:[19] val_loss=1.6340748071670532\n",
      "INFO:absl:[19] test_loss=1.6006156206130981\n",
      "INFO:absl:[20] train_loss=1.0397591590881348, train_x1_loss=0.3315834701061249, train_x2_loss=0.7081769704818726\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[20] val_loss=1.6356397867202759\n",
      "INFO:absl:[20] test_loss=1.6018668413162231\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[21] train_loss=1.034508466720581, train_x1_loss=0.32940351963043213, train_x2_loss=0.7051039338111877\n",
      "INFO:absl:[21] val_loss=1.627806544303894\n",
      "INFO:absl:[21] test_loss=1.5940121412277222\n",
      "INFO:absl:[22] train_loss=1.0290369987487793, train_x1_loss=0.32769060134887695, train_x2_loss=0.7013465166091919\n",
      "INFO:absl:[22] val_loss=1.6249244213104248\n",
      "INFO:absl:[22] test_loss=1.59091317653656\n",
      "INFO:absl:[23] train_loss=1.035677194595337, train_x1_loss=0.3289245665073395, train_x2_loss=0.7067531943321228\n",
      "INFO:absl:[23] val_loss=1.6268776655197144\n",
      "INFO:absl:[23] test_loss=1.5932475328445435\n",
      "INFO:absl:[24] train_loss=1.0377840995788574, train_x1_loss=0.33123916387557983, train_x2_loss=0.7065449357032776\n",
      "INFO:absl:[24] val_loss=1.6323024034500122\n",
      "INFO:absl:[24] test_loss=1.59856379032135\n",
      "INFO:absl:[25] train_loss=1.0337491035461426, train_x1_loss=0.32908666133880615, train_x2_loss=0.7046634554862976\n",
      "INFO:absl:[25] val_loss=1.6193727254867554\n",
      "INFO:absl:[25] test_loss=1.5856925249099731\n",
      "INFO:absl:[26] train_loss=1.0349698066711426, train_x1_loss=0.3289702534675598, train_x2_loss=0.7059984803199768\n",
      "INFO:absl:[26] val_loss=1.6265932321548462\n",
      "INFO:absl:[26] test_loss=1.5931718349456787\n",
      "INFO:absl:[27] train_loss=1.031972050666809, train_x1_loss=0.3268575370311737, train_x2_loss=0.7051135897636414\n",
      "INFO:absl:[27] val_loss=1.626215934753418\n",
      "INFO:absl:[27] test_loss=1.5923867225646973\n",
      "INFO:absl:[28] train_loss=1.0347578525543213, train_x1_loss=0.3284180164337158, train_x2_loss=0.7063384056091309\n",
      "INFO:absl:[28] val_loss=1.631617546081543\n",
      "INFO:absl:[28] test_loss=1.597637414932251\n",
      "INFO:absl:[29] train_loss=1.0350947380065918, train_x1_loss=0.3293479382991791, train_x2_loss=0.7057456970214844\n",
      "INFO:absl:[29] val_loss=1.6243780851364136\n",
      "INFO:absl:[29] test_loss=1.5908561944961548\n",
      "INFO:absl:[30] train_loss=1.0324697494506836, train_x1_loss=0.3287375867366791, train_x2_loss=0.7037317156791687\n",
      "INFO:absl:[30] val_loss=1.6280664205551147\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[30] test_loss=1.5941814184188843\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[31] train_loss=1.0319490432739258, train_x1_loss=0.3291931450366974, train_x2_loss=0.702755868434906\n",
      "INFO:absl:[31] val_loss=1.6272212266921997\n",
      "INFO:absl:[31] test_loss=1.5934573411941528\n",
      "INFO:absl:[32] train_loss=1.0381524562835693, train_x1_loss=0.3320409953594208, train_x2_loss=0.7061116099357605\n",
      "INFO:absl:[32] val_loss=1.622604250907898\n",
      "INFO:absl:[32] test_loss=1.588791847229004\n",
      "INFO:absl:[33] train_loss=1.031225323677063, train_x1_loss=0.32787224650382996, train_x2_loss=0.7033538818359375\n",
      "INFO:absl:[33] val_loss=1.6314631700515747\n",
      "INFO:absl:[33] test_loss=1.5974810123443604\n",
      "INFO:absl:[34] train_loss=1.0297750234603882, train_x1_loss=0.32795220613479614, train_x2_loss=0.7018235921859741\n",
      "INFO:absl:[34] val_loss=1.6278250217437744\n",
      "INFO:absl:[34] test_loss=1.5935704708099365\n",
      "INFO:absl:[35] train_loss=1.0294287204742432, train_x1_loss=0.3253788352012634, train_x2_loss=0.7040495276451111\n",
      "INFO:absl:[35] val_loss=1.6229760646820068\n",
      "INFO:absl:[35] test_loss=1.5891146659851074\n",
      "INFO:absl:[36] train_loss=1.031461238861084, train_x1_loss=0.32841095328330994, train_x2_loss=0.7030499577522278\n",
      "INFO:absl:[36] val_loss=1.6256499290466309\n",
      "INFO:absl:[36] test_loss=1.5921211242675781\n",
      "INFO:absl:Setting work unit notes: 2180.2 steps/s, 37.4% (130811/350000), ETA: 1m (1m : 0.0% checkpoint, 9.1% eval)\n",
      "INFO:absl:[130811] steps_per_sec=2180.178465\n",
      "INFO:absl:[37] train_loss=1.0332380533218384, train_x1_loss=0.3297622501850128, train_x2_loss=0.7034751176834106\n",
      "INFO:absl:[37] val_loss=1.6273208856582642\n",
      "INFO:absl:[37] test_loss=1.5930134057998657\n",
      "INFO:absl:[38] train_loss=1.0291054248809814, train_x1_loss=0.32545560598373413, train_x2_loss=0.7036494016647339\n",
      "INFO:absl:[38] val_loss=1.6256479024887085\n",
      "INFO:absl:[38] test_loss=1.5915000438690186\n",
      "INFO:absl:[39] train_loss=1.0327867269515991, train_x1_loss=0.32817134261131287, train_x2_loss=0.7046144008636475\n",
      "INFO:absl:[39] val_loss=1.6289408206939697\n",
      "INFO:absl:[39] test_loss=1.5948411226272583\n",
      "INFO:absl:[40] train_loss=1.0282824039459229, train_x1_loss=0.32537636160850525, train_x2_loss=0.702904999256134\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[40] val_loss=1.619762897491455\n",
      "INFO:absl:[40] test_loss=1.5856764316558838\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[41] train_loss=1.0344349145889282, train_x1_loss=0.33005964756011963, train_x2_loss=0.7043750286102295\n",
      "INFO:absl:[41] val_loss=1.6282545328140259\n",
      "INFO:absl:[41] test_loss=1.5945405960083008\n",
      "INFO:absl:[42] train_loss=1.0329554080963135, train_x1_loss=0.3304390013217926, train_x2_loss=0.7025154829025269\n",
      "INFO:absl:[42] val_loss=1.6203560829162598\n",
      "INFO:absl:[42] test_loss=1.5865634679794312\n",
      "INFO:absl:[43] train_loss=1.0301209688186646, train_x1_loss=0.32711532711982727, train_x2_loss=0.7030070424079895\n",
      "INFO:absl:[43] val_loss=1.6275137662887573\n",
      "INFO:absl:[43] test_loss=1.5933440923690796\n",
      "INFO:absl:[44] train_loss=1.0335912704467773, train_x1_loss=0.3292005658149719, train_x2_loss=0.7043905258178711\n",
      "INFO:absl:[44] val_loss=1.6218688488006592\n",
      "INFO:absl:[44] test_loss=1.5876942873001099\n",
      "INFO:absl:[45] train_loss=1.0297212600708008, train_x1_loss=0.3271283209323883, train_x2_loss=0.7025941014289856\n",
      "INFO:absl:[45] val_loss=1.6213786602020264\n",
      "INFO:absl:[45] test_loss=1.587689995765686\n",
      "INFO:absl:[46] train_loss=1.0335286855697632, train_x1_loss=0.3289022445678711, train_x2_loss=0.7046268582344055\n",
      "INFO:absl:[46] val_loss=1.6235549449920654\n",
      "INFO:absl:[46] test_loss=1.5892618894577026\n",
      "INFO:absl:[47] train_loss=1.0300403833389282, train_x1_loss=0.3270615339279175, train_x2_loss=0.702979326248169\n",
      "INFO:absl:[47] val_loss=1.6262686252593994\n",
      "INFO:absl:[47] test_loss=1.5920788049697876\n",
      "INFO:absl:[48] train_loss=1.0320419073104858, train_x1_loss=0.3248975872993469, train_x2_loss=0.7071442604064941\n",
      "INFO:absl:[48] val_loss=1.623876690864563\n",
      "INFO:absl:[48] test_loss=1.5893839597702026\n",
      "INFO:absl:[49] train_loss=1.0300838947296143, train_x1_loss=0.32858166098594666, train_x2_loss=0.7015036940574646\n",
      "INFO:absl:[49] val_loss=1.6328938007354736\n",
      "INFO:absl:[49] test_loss=1.5987515449523926\n",
      "INFO:absl:[50] train_loss=1.0301463603973389, train_x1_loss=0.32665374875068665, train_x2_loss=0.7034950256347656\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[50] val_loss=1.6272343397140503\n",
      "INFO:absl:[50] test_loss=1.592673659324646\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[51] train_loss=1.0295634269714355, train_x1_loss=0.3276430666446686, train_x2_loss=0.7019197940826416\n",
      "INFO:absl:[51] val_loss=1.6187653541564941\n",
      "INFO:absl:[51] test_loss=1.5851123332977295\n",
      "INFO:absl:[52] train_loss=1.0338435173034668, train_x1_loss=0.33023494482040405, train_x2_loss=0.7036084532737732\n",
      "INFO:absl:[52] val_loss=1.6278198957443237\n",
      "INFO:absl:[52] test_loss=1.5934895277023315\n",
      "INFO:absl:[53] train_loss=1.0311468839645386, train_x1_loss=0.3269425928592682, train_x2_loss=0.7042040228843689\n",
      "INFO:absl:[53] val_loss=1.6177780628204346\n",
      "INFO:absl:[53] test_loss=1.5838795900344849\n",
      "INFO:absl:[54] train_loss=1.0290218591690063, train_x1_loss=0.3257841467857361, train_x2_loss=0.7032372355461121\n",
      "INFO:absl:[54] val_loss=1.6289981603622437\n",
      "INFO:absl:[54] test_loss=1.5947835445404053\n",
      "INFO:absl:[55] train_loss=1.0314522981643677, train_x1_loss=0.32696372270584106, train_x2_loss=0.7044879794120789\n",
      "INFO:absl:[55] val_loss=1.6311726570129395\n",
      "INFO:absl:[55] test_loss=1.596985936164856\n",
      "INFO:absl:[56] train_loss=1.0303637981414795, train_x1_loss=0.3278445899486542, train_x2_loss=0.7025200724601746\n",
      "INFO:absl:[56] val_loss=1.6288020610809326\n",
      "INFO:absl:[56] test_loss=1.5946893692016602\n",
      "INFO:absl:[57] train_loss=1.0317180156707764, train_x1_loss=0.32786938548088074, train_x2_loss=0.7038488984107971\n",
      "INFO:absl:[57] val_loss=1.62315833568573\n",
      "INFO:absl:[57] test_loss=1.5883220434188843\n",
      "INFO:absl:[58] train_loss=1.0310999155044556, train_x1_loss=0.32665905356407166, train_x2_loss=0.7044427990913391\n",
      "INFO:absl:[58] val_loss=1.6269041299819946\n",
      "INFO:absl:[58] test_loss=1.5926249027252197\n",
      "INFO:absl:[59] train_loss=1.0321084260940552, train_x1_loss=0.32790660858154297, train_x2_loss=0.7042006254196167\n",
      "INFO:absl:[59] val_loss=1.62442946434021\n",
      "INFO:absl:[59] test_loss=1.590219497680664\n",
      "INFO:absl:[60] train_loss=1.0313236713409424, train_x1_loss=0.3279515206813812, train_x2_loss=0.7033756971359253\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[60] val_loss=1.631987452507019\n",
      "INFO:absl:[60] test_loss=1.5975974798202515\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[61] train_loss=1.0309110879898071, train_x1_loss=0.32574692368507385, train_x2_loss=0.7051645517349243\n",
      "INFO:absl:[61] val_loss=1.624577283859253\n",
      "INFO:absl:[61] test_loss=1.5899394750595093\n",
      "INFO:absl:[62] train_loss=1.0287078619003296, train_x1_loss=0.3245784640312195, train_x2_loss=0.7041286826133728\n",
      "INFO:absl:[62] val_loss=1.621662974357605\n",
      "INFO:absl:[62] test_loss=1.5873773097991943\n",
      "INFO:absl:[63] train_loss=1.0313153266906738, train_x1_loss=0.3282391428947449, train_x2_loss=0.7030768990516663\n",
      "INFO:absl:[63] val_loss=1.6237297058105469\n",
      "INFO:absl:[63] test_loss=1.5895179510116577\n",
      "INFO:absl:[64] train_loss=1.0299943685531616, train_x1_loss=0.32658350467681885, train_x2_loss=0.703409731388092\n",
      "INFO:absl:[64] val_loss=1.619897484779358\n",
      "INFO:absl:[64] test_loss=1.5857527256011963\n",
      "INFO:absl:[65] train_loss=1.0346187353134155, train_x1_loss=0.3297671377658844, train_x2_loss=0.7048512697219849\n",
      "INFO:absl:[65] val_loss=1.6240551471710205\n",
      "INFO:absl:[65] test_loss=1.5900590419769287\n",
      "INFO:absl:[66] train_loss=1.0321124792099, train_x1_loss=0.32882964611053467, train_x2_loss=0.7032821774482727\n",
      "INFO:absl:[66] val_loss=1.6293330192565918\n",
      "INFO:absl:[66] test_loss=1.5951424837112427\n",
      "INFO:absl:[67] train_loss=1.029852032661438, train_x1_loss=0.32646429538726807, train_x2_loss=0.7033881545066833\n",
      "INFO:absl:[67] val_loss=1.6217445135116577\n",
      "INFO:absl:[67] test_loss=1.5876519680023193\n",
      "INFO:absl:[68] train_loss=1.0325915813446045, train_x1_loss=0.3280707001686096, train_x2_loss=0.704520583152771\n",
      "INFO:absl:[68] val_loss=1.6244299411773682\n",
      "INFO:absl:[68] test_loss=1.5903562307357788\n",
      "INFO:absl:[69] train_loss=1.0321698188781738, train_x1_loss=0.33081385493278503, train_x2_loss=0.701356053352356\n",
      "INFO:absl:[69] val_loss=1.6235333681106567\n",
      "INFO:absl:[69] test_loss=1.589621901512146\n",
      "INFO:absl:[70] train_loss=1.0354968309402466, train_x1_loss=0.32923415303230286, train_x2_loss=0.7062620520591736\n",
      "INFO:absl:[70] val_loss=1.6311094760894775\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[70] test_loss=1.596622109413147\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[71] train_loss=1.0302457809448242, train_x1_loss=0.32723236083984375, train_x2_loss=0.7030126452445984\n",
      "INFO:absl:[71] val_loss=1.627306342124939\n",
      "INFO:absl:[71] test_loss=1.592909574508667\n",
      "INFO:absl:[72] train_loss=1.0297446250915527, train_x1_loss=0.32518142461776733, train_x2_loss=0.7045655250549316\n",
      "INFO:absl:[72] val_loss=1.6213091611862183\n",
      "INFO:absl:[72] test_loss=1.587258219718933\n",
      "INFO:absl:[73] train_loss=1.029708981513977, train_x1_loss=0.32595962285995483, train_x2_loss=0.7037496566772461\n",
      "INFO:absl:[73] val_loss=1.6285209655761719\n",
      "INFO:absl:[73] test_loss=1.5945067405700684\n",
      "INFO:absl:[74] train_loss=1.0347729921340942, train_x1_loss=0.3287696838378906, train_x2_loss=0.7060030698776245\n",
      "INFO:absl:[74] val_loss=1.6252800226211548\n",
      "INFO:absl:[74] test_loss=1.591127872467041\n",
      "INFO:absl:[75] train_loss=1.0311250686645508, train_x1_loss=0.3266233205795288, train_x2_loss=0.704502522945404\n",
      "INFO:absl:[75] val_loss=1.6247204542160034\n",
      "INFO:absl:[75] test_loss=1.5907760858535767\n",
      "INFO:absl:[76] train_loss=1.032065987586975, train_x1_loss=0.3281781077384949, train_x2_loss=0.7038888335227966\n",
      "INFO:absl:[76] val_loss=1.6219927072525024\n",
      "INFO:absl:[76] test_loss=1.5880647897720337\n",
      "INFO:absl:[77] train_loss=1.032711148262024, train_x1_loss=0.3275434672832489, train_x2_loss=0.7051679491996765\n",
      "INFO:absl:[77] val_loss=1.6290074586868286\n",
      "INFO:absl:[77] test_loss=1.5948095321655273\n",
      "INFO:absl:[78] train_loss=1.0319408178329468, train_x1_loss=0.3292894661426544, train_x2_loss=0.7026512026786804\n",
      "INFO:absl:[78] val_loss=1.6305091381072998\n",
      "INFO:absl:[78] test_loss=1.5960917472839355\n",
      "INFO:absl:[79] train_loss=1.028052568435669, train_x1_loss=0.3266357481479645, train_x2_loss=0.7014172077178955\n",
      "INFO:absl:[79] val_loss=1.6266621351242065\n",
      "INFO:absl:[79] test_loss=1.5922212600708008\n",
      "INFO:absl:Setting work unit notes: 2495.0 steps/s, 80.1% (280513/350000), ETA: 0m (2m : 0.0% checkpoint, 9.1% eval)\n",
      "INFO:absl:[280513] steps_per_sec=2495.019949\n",
      "INFO:absl:[80] train_loss=1.0334689617156982, train_x1_loss=0.3286276161670685, train_x2_loss=0.7048422694206238\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[80] val_loss=1.6275832653045654\n",
      "INFO:absl:[80] test_loss=1.5930613279342651\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[81] train_loss=1.029897689819336, train_x1_loss=0.3246265947818756, train_x2_loss=0.7052721977233887\n",
      "INFO:absl:[81] val_loss=1.6261755228042603\n",
      "INFO:absl:[81] test_loss=1.5914781093597412\n",
      "INFO:absl:[82] train_loss=1.0291080474853516, train_x1_loss=0.3260137736797333, train_x2_loss=0.7030940055847168\n",
      "INFO:absl:[82] val_loss=1.628589391708374\n",
      "INFO:absl:[82] test_loss=1.5946844816207886\n",
      "INFO:absl:[83] train_loss=1.0286680459976196, train_x1_loss=0.3230906128883362, train_x2_loss=0.7055771350860596\n",
      "INFO:absl:[83] val_loss=1.6254925727844238\n",
      "INFO:absl:[83] test_loss=1.5914896726608276\n",
      "INFO:absl:[84] train_loss=1.0322706699371338, train_x1_loss=0.3272002041339874, train_x2_loss=0.7050721049308777\n",
      "INFO:absl:[84] val_loss=1.6246925592422485\n",
      "INFO:absl:[84] test_loss=1.5905131101608276\n",
      "INFO:absl:[85] train_loss=1.0316747426986694, train_x1_loss=0.32541266083717346, train_x2_loss=0.7062616944313049\n",
      "INFO:absl:[85] val_loss=1.6297177076339722\n",
      "INFO:absl:[85] test_loss=1.595595121383667\n",
      "INFO:absl:[86] train_loss=1.0277516841888428, train_x1_loss=0.32526519894599915, train_x2_loss=0.702486515045166\n",
      "INFO:absl:[86] val_loss=1.623032569885254\n",
      "INFO:absl:[86] test_loss=1.589125394821167\n",
      "INFO:absl:[87] train_loss=1.0318886041641235, train_x1_loss=0.32806122303009033, train_x2_loss=0.7038272023200989\n",
      "INFO:absl:[87] val_loss=1.6249200105667114\n",
      "INFO:absl:[87] test_loss=1.5903611183166504\n",
      "INFO:absl:[88] train_loss=1.0314323902130127, train_x1_loss=0.3249804675579071, train_x2_loss=0.7064506411552429\n",
      "INFO:absl:[88] val_loss=1.6254290342330933\n",
      "INFO:absl:[88] test_loss=1.5914877653121948\n",
      "INFO:absl:[89] train_loss=1.0307339429855347, train_x1_loss=0.32841938734054565, train_x2_loss=0.7023143768310547\n",
      "INFO:absl:[89] val_loss=1.6277505159378052\n",
      "INFO:absl:[89] test_loss=1.593276858329773\n",
      "INFO:absl:[90] train_loss=1.0283178091049194, train_x1_loss=0.3249916434288025, train_x2_loss=0.7033248543739319\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[90] val_loss=1.6213918924331665\n",
      "INFO:absl:[90] test_loss=1.5874950885772705\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n",
      "INFO:absl:[91] train_loss=1.0333055257797241, train_x1_loss=0.32746124267578125, train_x2_loss=0.7058454751968384\n",
      "INFO:absl:[91] val_loss=1.6339435577392578\n",
      "INFO:absl:[91] test_loss=1.5997810363769531\n",
      "INFO:absl:[92] train_loss=1.025820255279541, train_x1_loss=0.3236774504184723, train_x2_loss=0.7021439671516418\n",
      "INFO:absl:[92] val_loss=1.6271148920059204\n",
      "INFO:absl:[92] test_loss=1.593241572380066\n",
      "INFO:absl:[93] train_loss=1.0287526845932007, train_x1_loss=0.32649728655815125, train_x2_loss=0.7022578120231628\n",
      "INFO:absl:[93] val_loss=1.6282851696014404\n",
      "INFO:absl:[93] test_loss=1.593843936920166\n",
      "INFO:absl:[94] train_loss=1.0315583944320679, train_x1_loss=0.327022910118103, train_x2_loss=0.7045348286628723\n",
      "INFO:absl:[94] val_loss=1.6279369592666626\n",
      "INFO:absl:[94] test_loss=1.5936092138290405\n",
      "INFO:absl:[95] train_loss=1.0336252450942993, train_x1_loss=0.32726621627807617, train_x2_loss=0.7063571214675903\n",
      "INFO:absl:[95] val_loss=1.622103214263916\n",
      "INFO:absl:[95] test_loss=1.5879195928573608\n",
      "INFO:absl:[96] train_loss=1.0265921354293823, train_x1_loss=0.3255908191204071, train_x2_loss=0.7010011672973633\n",
      "INFO:absl:[96] val_loss=1.6299041509628296\n",
      "INFO:absl:[96] test_loss=1.5959522724151611\n",
      "INFO:absl:[97] train_loss=1.0291748046875, train_x1_loss=0.32535094022750854, train_x2_loss=0.7038242220878601\n",
      "INFO:absl:[97] val_loss=1.6247297525405884\n",
      "INFO:absl:[97] test_loss=1.5905084609985352\n",
      "INFO:absl:[98] train_loss=1.0304251909255981, train_x1_loss=0.327372670173645, train_x2_loss=0.7030527591705322\n",
      "INFO:absl:[98] val_loss=1.630469560623169\n",
      "INFO:absl:[98] test_loss=1.595932960510254\n",
      "INFO:absl:[99] train_loss=1.029977560043335, train_x1_loss=0.3252049386501312, train_x2_loss=0.7047725915908813\n",
      "INFO:absl:[99] val_loss=1.6319562196731567\n",
      "INFO:absl:Checkpoint.save() ...\n",
      "INFO:absl:[99] test_loss=1.5975967645645142\n",
      "INFO:absl:Checkpoint.save() finished after 0.01s.\n"
     ]
    }
   ],
   "source": [
    "for trial in range(num_trials):\n",
    "    _, _, solo_eval_metric, solo_epoch_losses = train_and_evaluate_with_data(\n",
    "        config=solo_configs[trial], workdir=solo_workdirs[trial], datasets=all_solo_datasets[trial])\n",
    "    \n",
    "    solo_eval_metrics.append(solo_eval_metric)\n",
    "    \n",
    "    all_solo_epoch_losses.append(solo_epoch_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we will take the average of the last 10 of 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_last_epochs(average_after, epoch_losses):\n",
    "    filtered_losses = epoch_losses[average_after:]\n",
    "    avg_loss = np.mean(filtered_losses)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miamirabelli/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/miamirabelli/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "avg_connected_7_losses = []\n",
    "avg_connected_5_losses = []\n",
    "avg_connected_3_losses = []\n",
    "avg_solo_losses = []\n",
    "average_after_epoch = solo_configs[0].epochs - 10 # we want 10 epochs total, so we subtract to get where we should start averaging\n",
    "for trial in range(num_trials):\n",
    "    avg_connected_7_loss = average_last_epochs(average_after_epoch, all_connected_7_epoch_losses[trial])\n",
    "\n",
    "    avg_connected_5_loss = average_last_epochs(average_after_epoch, all_connected_5_epoch_losses[trial])\n",
    "\n",
    "    avg_connected_3_loss = average_last_epochs(average_after_epoch, all_connected_3_epoch_losses[trial])\n",
    "\n",
    "    avg_solo_loss = average_last_epochs(average_after_epoch, all_solo_epoch_losses[trial])\n",
    "\n",
    "    avg_connected_7_losses.append(avg_connected_7_loss)\n",
    "    avg_connected_5_losses.append(avg_connected_5_loss)\n",
    "    avg_connected_3_losses.append(avg_connected_3_loss)\n",
    "    avg_solo_losses.append(avg_solo_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2870419, 1.2871668, 1.240414, 1.2709004, 1.3081582, 1.3009611, 1.3009611, 1.3854215, 1.2018278, 1.2814478]\n",
      "[0.70512, 0.6808599, 0.7043725, 0.67967993, 0.6658358, 0.7045995, 0.7045995, 0.80254614, 0.6710659, 0.7026099]\n",
      "[nan, 1.8927606, 1.8568245, 1.8899324, 1.9256098, 1.9185213, 1.9185213, 2.0000525, 1.8285433, 1.8854145]\n",
      "[1.6429927, 1.6342242, 1.5854094, 1.6289905, 1.6611397, 1.6496769, 1.6496769, 1.7246287, 1.5793415, 1.6277835]\n"
     ]
    }
   ],
   "source": [
    "print(avg_connected_7_losses)\n",
    "print(avg_connected_5_losses)\n",
    "print(avg_connected_3_losses)\n",
    "print(avg_solo_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have gathered the data for the statistical tests. First, we check that the variances are roughly equal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0020235658 0.0013348466 0.0013348466 0.0014742501\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "print(np.var(avg_connected_7_losses), np.var(avg_connected_5_losses), np.var(avg_connected_5_losses), np.var(avg_solo_losses))\n",
    "# variances are very similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will perform a paired T test for each connection type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Connected vs Solo\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-17.85301044586341, pvalue=6.77488979271964e-13)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"7 Connected vs Solo\")\n",
    "stats.ttest_ind(a=avg_connected_7_losses, b=avg_solo_losses, equal_var=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Connected vs Solo\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-52.99479087108089, pvalue=3.2048448145722525e-21)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"5 Connected vs Solo\")\n",
    "stats.ttest_ind(a=avg_connected_5_losses, b=avg_solo_losses, equal_var=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Connected vs Solo\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=nan, pvalue=nan)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"3 Connected vs Solo\")\n",
    "stats.ttest_ind(a=avg_connected_3_losses, b=avg_solo_losses, equal_var=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Connected vs 7 Connected\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-30.247605480304774, pvalue=6.93333550025058e-17)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"5 Connected vs 7 Connected\")\n",
    "stats.ttest_ind(a=avg_connected_5_losses, b=avg_connected_7_losses, equal_var=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Connected vs 3 Connected\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=nan, pvalue=nan)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"5 Connected vs 3 Connected\")\n",
    "stats.ttest_ind(a=avg_connected_5_losses, b=avg_connected_3_losses, equal_var=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Connected vs 3 Connected\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=nan, pvalue=nan)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"7 Connected vs 3 Connected\")\n",
    "stats.ttest_ind(a=avg_connected_7_losses, b=avg_connected_3_losses, equal_var=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Predictions & Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our results more clearly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.jraph_vis import plot_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initializing network.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint directory: tests/outputs/connected_7/trial-0/checkpoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:\n",
      "+----------------------------------------+---------+------+---------+-------+\n",
      "| Name                                   | Shape   | Size | Mean    | Std   |\n",
      "+----------------------------------------+---------+------+---------+-------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (4,)    | 4    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 4)  | 24   | -0.0819 | 0.36  |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (4,)    | 4    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (4, 4)  | 16   | -0.119  | 0.493 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (8,)    | 8    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (11, 8) | 88   | 0.0253  | 0.286 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)    | 2    | 0.0     | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (8, 2)  | 16   | -0.0227 | 0.399 |\n",
      "+----------------------------------------+---------+------+---------+-------+\n",
      "Total: 162\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:Restoring checkpoint: tests/outputs/connected_7/trial-0/checkpoints/ckpt-12\n",
      "INFO:absl:Restored save_counter=12 restored_checkpoint=tests/outputs/connected_7/trial-0/checkpoints/ckpt-12\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.01s.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/miamirabelli/Desktop/GNN Research/lorenzGNN/best_hparam_edges.ipynb Cell 46\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/best_hparam_edges.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m plot_predictions(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/best_hparam_edges.ipynb#X63sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     config\u001b[39m=\u001b[39;49mconnected_configs_7[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/best_hparam_edges.ipynb#X63sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     workdir\u001b[39m=\u001b[39;49mconnected_workdirs_7[\u001b[39m0\u001b[39;49m], \u001b[39m# for loading checkpoints \u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/best_hparam_edges.ipynb#X63sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     plot_ith_rollout_step\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, \u001b[39m# 0 indexed # for this study, we have a 4-step rollout \u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/best_hparam_edges.ipynb#X63sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     datasets\u001b[39m=\u001b[39;49mall_connected_7_datasets[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/best_hparam_edges.ipynb#X63sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     node\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, \u001b[39m# 0-indexed \u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/best_hparam_edges.ipynb#X63sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     plot_mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m# i.e. \"train\"/\"val\"/\"test\"\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/best_hparam_edges.ipynb#X63sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     plot_days\u001b[39m=\u001b[39;49m\u001b[39m60\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/best_hparam_edges.ipynb#X63sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     title\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mConnected 7 Test Predictions for node 0, rollout step 3\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/best_hparam_edges.ipynb#X63sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/best_hparam_edges.ipynb#X63sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m plot_predictions(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/best_hparam_edges.ipynb#X63sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     config\u001b[39m=\u001b[39mconnected_configs_5[\u001b[39m0\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/best_hparam_edges.ipynb#X63sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     workdir\u001b[39m=\u001b[39mconnected_workdirs_5[\u001b[39m0\u001b[39m], \u001b[39m# for loading checkpoints \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/best_hparam_edges.ipynb#X63sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConnected 5 Test Predictions for node 0, rollout step 3\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/best_hparam_edges.ipynb#X63sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/best_hparam_edges.ipynb#X63sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m plot_predictions(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/best_hparam_edges.ipynb#X63sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     config\u001b[39m=\u001b[39mconnected_configs_3[\u001b[39m0\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/best_hparam_edges.ipynb#X63sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     workdir\u001b[39m=\u001b[39mconnected_workdirs_3[\u001b[39m0\u001b[39m], \u001b[39m# for loading checkpoints \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/best_hparam_edges.ipynb#X63sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConnected 3 Test Predictions for node 0, rollout step 3\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/best_hparam_edges.ipynb#X63sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/utils/jraph_vis.py:199\u001b[0m, in \u001b[0;36mplot_predictions\u001b[0;34m(config, workdir, plot_ith_rollout_step, node, plot_mode, datasets, plot_days, title)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m plot_count:\n\u001b[1;32m    198\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m pred_nodes_list \u001b[39m=\u001b[39m rollout(state\u001b[39m=\u001b[39;49mstate,\n\u001b[1;32m    200\u001b[0m                           input_window_graphs\u001b[39m=\u001b[39;49minput_window_graphs,\n\u001b[1;32m    201\u001b[0m                           n_rollout_steps\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49moutput_steps,\n\u001b[1;32m    202\u001b[0m                         \u001b[39m#   n_rollout_steps=plot_ith_rollout_step+1, # +1 since plot_ith_rollout_step is 0-indexed\u001b[39;49;00m\n\u001b[1;32m    203\u001b[0m                           rngs\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m) \u001b[39m# deterministic during eval\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39m# get the last array of predictions, which will correspond to the ith rollout step that we care about \u001b[39;00m\n\u001b[1;32m    206\u001b[0m ith_rollout_pred \u001b[39m=\u001b[39m pred_nodes_list[plot_ith_rollout_step]\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/utils/jraph_training.py:280\u001b[0m, in \u001b[0;36mrollout\u001b[0;34m(state, input_window_graphs, n_rollout_steps, rngs)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39m# total_loss = 0\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_rollout_steps):\n\u001b[0;32m--> 280\u001b[0m     pred_graphs_list \u001b[39m=\u001b[39m state\u001b[39m.\u001b[39;49mapply_fn(state\u001b[39m.\u001b[39;49mparams, curr_input_window_graphs, rngs\u001b[39m=\u001b[39;49mrngs) \n\u001b[1;32m    281\u001b[0m     pred_graph \u001b[39m=\u001b[39m pred_graphs_list[\u001b[39m0\u001b[39m]\n\u001b[1;32m    283\u001b[0m     \u001b[39m# retrieve the new input window \u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/flax/linen/module.py:1922\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, variables, rngs, method, mutable, capture_intermediates, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1920\u001b[0m   method \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m\n\u001b[1;32m   1921\u001b[0m method \u001b[39m=\u001b[39m _get_unbound_fn(method)\n\u001b[0;32m-> 1922\u001b[0m \u001b[39mreturn\u001b[39;00m apply(\n\u001b[1;32m   1923\u001b[0m     method,\n\u001b[1;32m   1924\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1925\u001b[0m     mutable\u001b[39m=\u001b[39;49mmutable,\n\u001b[1;32m   1926\u001b[0m     capture_intermediates\u001b[39m=\u001b[39;49mcapture_intermediates,\n\u001b[1;32m   1927\u001b[0m )(variables, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs, rngs\u001b[39m=\u001b[39;49mrngs)\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/flax/core/scope.py:1084\u001b[0m, in \u001b[0;36mapply.<locals>.wrapper\u001b[0;34m(variables, rngs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m   \u001b[39mraise\u001b[39;00m errors\u001b[39m.\u001b[39mApplyScopeInvalidVariablesStructureError(variables)\n\u001b[1;32m   1081\u001b[0m \u001b[39mwith\u001b[39;00m bind(\n\u001b[1;32m   1082\u001b[0m     variables, rngs\u001b[39m=\u001b[39mrngs, mutable\u001b[39m=\u001b[39mmutable, flags\u001b[39m=\u001b[39mflags\n\u001b[1;32m   1083\u001b[0m )\u001b[39m.\u001b[39mtemporary() \u001b[39mas\u001b[39;00m root:\n\u001b[0;32m-> 1084\u001b[0m   y \u001b[39m=\u001b[39m fn(root, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m mutable \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m   1086\u001b[0m   \u001b[39mreturn\u001b[39;00m y, root\u001b[39m.\u001b[39mmutable_variables()\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/flax/linen/module.py:2563\u001b[0m, in \u001b[0;36mapply.<locals>.scope_fn\u001b[0;34m(scope, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2561\u001b[0m _context\u001b[39m.\u001b[39mcapture_stack\u001b[39m.\u001b[39mappend(capture_intermediates)\n\u001b[1;32m   2562\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2563\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(module\u001b[39m.\u001b[39;49mclone(parent\u001b[39m=\u001b[39;49mscope, _deep_clone\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m), \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2564\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   2565\u001b[0m   _context\u001b[39m.\u001b[39mcapture_stack\u001b[39m.\u001b[39mpop()\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/flax/linen/module.py:602\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[39mif\u001b[39;00m args \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], Module):\n\u001b[1;32m    601\u001b[0m   \u001b[39mself\u001b[39m, args \u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m], args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 602\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_wrapped_method(fun, args, kwargs)\n\u001b[1;32m    603\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    604\u001b[0m   \u001b[39mreturn\u001b[39;00m fun(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/flax/linen/module.py:1119\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[39mif\u001b[39;00m _use_named_call:\n\u001b[1;32m   1118\u001b[0m   \u001b[39mwith\u001b[39;00m jax\u001b[39m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[39mself\u001b[39m, fun)):\n\u001b[0;32m-> 1119\u001b[0m     y \u001b[39m=\u001b[39m run_fun(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1120\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1121\u001b[0m   y \u001b[39m=\u001b[39m run_fun(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/utils/jraph_models.py:196\u001b[0m, in \u001b[0;36mMLPGraphNetwork.__call__\u001b[0;34m(self, input_window_graphs)\u001b[0m\n\u001b[1;32m    183\u001b[0m         blocks\u001b[39m.\u001b[39mappend(MLPBlock(\n\u001b[1;32m    184\u001b[0m             dropout_rate\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout_rate,\n\u001b[1;32m    185\u001b[0m             skip_connections \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskip_connections,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    191\u001b[0m             activation\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation,   \n\u001b[1;32m    192\u001b[0m         ))\n\u001b[1;32m    193\u001b[0m         \u001b[39m# TODO: check that this create distinct blocks with unshared params\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \n\u001b[1;32m    195\u001b[0m \u001b[39m# Apply a Graph Network once for each message-passing round.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m processed_graphs_list \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mSequential(blocks)(input_window_graphs)\n\u001b[1;32m    197\u001b[0m \u001b[39m# TODO: do we need skip connections or layer_norm here? \u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39mreturn\u001b[39;00m processed_graphs_list\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/flax/linen/module.py:602\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[39mif\u001b[39;00m args \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], Module):\n\u001b[1;32m    601\u001b[0m   \u001b[39mself\u001b[39m, args \u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m], args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 602\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_wrapped_method(fun, args, kwargs)\n\u001b[1;32m    603\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    604\u001b[0m   \u001b[39mreturn\u001b[39;00m fun(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/flax/linen/module.py:1119\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[39mif\u001b[39;00m _use_named_call:\n\u001b[1;32m   1118\u001b[0m   \u001b[39mwith\u001b[39;00m jax\u001b[39m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[39mself\u001b[39m, fun)):\n\u001b[0;32m-> 1119\u001b[0m     y \u001b[39m=\u001b[39m run_fun(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1120\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1121\u001b[0m   y \u001b[39m=\u001b[39m run_fun(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/flax/linen/combinators.py:86\u001b[0m, in \u001b[0;36mSequential.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m     84\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEmpty Sequential module \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 86\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers[\u001b[39m0\u001b[39;49m](\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     87\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[\u001b[39m1\u001b[39m:]:\n\u001b[1;32m     88\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mtuple\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/flax/linen/module.py:602\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[39mif\u001b[39;00m args \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], Module):\n\u001b[1;32m    601\u001b[0m   \u001b[39mself\u001b[39m, args \u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m], args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 602\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_wrapped_method(fun, args, kwargs)\n\u001b[1;32m    603\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    604\u001b[0m   \u001b[39mreturn\u001b[39;00m fun(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/flax/linen/module.py:1119\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[39mif\u001b[39;00m _use_named_call:\n\u001b[1;32m   1118\u001b[0m   \u001b[39mwith\u001b[39;00m jax\u001b[39m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[39mself\u001b[39m, fun)):\n\u001b[0;32m-> 1119\u001b[0m     y \u001b[39m=\u001b[39m run_fun(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1120\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1121\u001b[0m   y \u001b[39m=\u001b[39m run_fun(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/utils/jraph_models.py:120\u001b[0m, in \u001b[0;36mMLPBlock.__call__\u001b[0;34m(self, input_window_graphs)\u001b[0m\n\u001b[1;32m    112\u001b[0m graph_net \u001b[39m=\u001b[39m jraph\u001b[39m.\u001b[39mGraphNetwork(\n\u001b[1;32m    113\u001b[0m     update_node_fn\u001b[39m=\u001b[39mupdate_node_fn,\n\u001b[1;32m    114\u001b[0m     update_edge_fn\u001b[39m=\u001b[39mupdate_edge_fn,\n\u001b[1;32m    115\u001b[0m     update_global_fn\u001b[39m=\u001b[39mupdate_global_fn,\n\u001b[1;32m    116\u001b[0m )\n\u001b[1;32m    118\u001b[0m \u001b[39m# print(\"input graph type:\", type(input_graph))\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[39m# print(input_graph)\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m processed_graphs \u001b[39m=\u001b[39m graph_net(input_graph)\n\u001b[1;32m    121\u001b[0m \u001b[39m# revert edge features to their original values\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[39m# we want the edges to be encoded/processed by the update_edge_fn internally as part of the processing for the node features, but we only use the encoded edges internally and don't want it to affect the actual graph structure of the data because we know that it is fixed \u001b[39;00m\n\u001b[1;32m    123\u001b[0m processed_graphs \u001b[39m=\u001b[39m processed_graphs\u001b[39m.\u001b[39m_replace(edges\u001b[39m=\u001b[39minput_graph\u001b[39m.\u001b[39medges)\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/jraph/_src/models.py:182\u001b[0m, in \u001b[0;36mGraphNetwork.<locals>._ApplyGraphNet\u001b[0;34m(graph)\u001b[0m\n\u001b[1;32m    178\u001b[0m global_edge_attributes \u001b[39m=\u001b[39m tree\u001b[39m.\u001b[39mtree_map(\u001b[39mlambda\u001b[39;00m g: jnp\u001b[39m.\u001b[39mrepeat(\n\u001b[1;32m    179\u001b[0m     g, n_edge, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, total_repeat_length\u001b[39m=\u001b[39msum_n_edge), globals_)\n\u001b[1;32m    181\u001b[0m \u001b[39mif\u001b[39;00m update_edge_fn:\n\u001b[0;32m--> 182\u001b[0m   edges \u001b[39m=\u001b[39m update_edge_fn(edges, sent_attributes, received_attributes,\n\u001b[1;32m    183\u001b[0m                          global_edge_attributes)\n\u001b[1;32m    185\u001b[0m \u001b[39mif\u001b[39;00m attention_logit_fn:\n\u001b[1;32m    186\u001b[0m   logits \u001b[39m=\u001b[39m attention_logit_fn(edges, sent_attributes, received_attributes,\n\u001b[1;32m    187\u001b[0m                               global_edge_attributes)\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/jraph/_src/utils.py:828\u001b[0m, in \u001b[0;36mconcatenated_args.<locals>._decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    826\u001b[0m combined_args \u001b[39m=\u001b[39m tree\u001b[39m.\u001b[39mtree_flatten(args)[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m tree\u001b[39m.\u001b[39mtree_flatten(kwargs)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    827\u001b[0m concat_args \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mconcatenate(combined_args, axis\u001b[39m=\u001b[39maxis)\n\u001b[0;32m--> 828\u001b[0m \u001b[39mreturn\u001b[39;00m f(concat_args)\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/flax/linen/module.py:602\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[39mif\u001b[39;00m args \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], Module):\n\u001b[1;32m    601\u001b[0m   \u001b[39mself\u001b[39m, args \u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m], args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 602\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_wrapped_method(fun, args, kwargs)\n\u001b[1;32m    603\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    604\u001b[0m   \u001b[39mreturn\u001b[39;00m fun(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/flax/linen/module.py:1119\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[39mif\u001b[39;00m _use_named_call:\n\u001b[1;32m   1118\u001b[0m   \u001b[39mwith\u001b[39;00m jax\u001b[39m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[39mself\u001b[39m, fun)):\n\u001b[0;32m-> 1119\u001b[0m     y \u001b[39m=\u001b[39m run_fun(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1120\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1121\u001b[0m   y \u001b[39m=\u001b[39m run_fun(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/utils/jraph_models.py:45\u001b[0m, in \u001b[0;36mMLP.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(rate\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout_rate, \n\u001b[1;32m     42\u001b[0m                    deterministic\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeterministic)(x)\n\u001b[1;32m     44\u001b[0m \u001b[39m# we don't want an activation function like relu on the last layer \u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mDense(features\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_sizes[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])(x)\n\u001b[1;32m     46\u001b[0m x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(rate\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout_rate, \n\u001b[1;32m     47\u001b[0m                 deterministic\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeterministic)(x)\n\u001b[1;32m     49\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/flax/linen/module.py:602\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[39mif\u001b[39;00m args \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], Module):\n\u001b[1;32m    601\u001b[0m   \u001b[39mself\u001b[39m, args \u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m], args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 602\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_wrapped_method(fun, args, kwargs)\n\u001b[1;32m    603\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    604\u001b[0m   \u001b[39mreturn\u001b[39;00m fun(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/flax/linen/module.py:1119\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[39mif\u001b[39;00m _use_named_call:\n\u001b[1;32m   1118\u001b[0m   \u001b[39mwith\u001b[39;00m jax\u001b[39m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[39mself\u001b[39m, fun)):\n\u001b[0;32m-> 1119\u001b[0m     y \u001b[39m=\u001b[39m run_fun(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1120\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1121\u001b[0m   y \u001b[39m=\u001b[39m run_fun(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/flax/linen/linear.py:234\u001b[0m, in \u001b[0;36mDense.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[39m@compact\u001b[39m\n\u001b[1;32m    225\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, inputs: Array) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Array:\n\u001b[1;32m    226\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Applies a linear transformation to the inputs along the last dimension.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \n\u001b[1;32m    228\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39m    The transformed input.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m   kernel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam(\n\u001b[1;32m    235\u001b[0m       \u001b[39m'\u001b[39;49m\u001b[39mkernel\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    236\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_init,\n\u001b[1;32m    237\u001b[0m       (jnp\u001b[39m.\u001b[39;49mshape(inputs)[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures),\n\u001b[1;32m    238\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_dtype,\n\u001b[1;32m    239\u001b[0m   )\n\u001b[1;32m    240\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_bias:\n\u001b[1;32m    241\u001b[0m     bias \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam(\n\u001b[1;32m    242\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias_init, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures,), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_dtype\n\u001b[1;32m    243\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/flax/linen/module.py:1657\u001b[0m, in \u001b[0;36mModule.param\u001b[0;34m(self, name, init_fn, unbox, *init_args)\u001b[0m\n\u001b[1;32m   1655\u001b[0m   \u001b[39mraise\u001b[39;00m errors\u001b[39m.\u001b[39mNameInUseError(\u001b[39m'\u001b[39m\u001b[39mparam\u001b[39m\u001b[39m'\u001b[39m, name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m   1656\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscope \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1657\u001b[0m v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscope\u001b[39m.\u001b[39;49mparam(name, init_fn, \u001b[39m*\u001b[39;49minit_args, unbox\u001b[39m=\u001b[39;49munbox)\n\u001b[1;32m   1658\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state\u001b[39m.\u001b[39mchildren[name] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1659\u001b[0m \u001b[39mreturn\u001b[39;00m v\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/flax/core/scope.py:967\u001b[0m, in \u001b[0;36mScope.param\u001b[0;34m(self, name, init_fn, unbox, *init_args)\u001b[0m\n\u001b[1;32m    961\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_variable(\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m, name)\n\u001b[1;32m    962\u001b[0m \u001b[39m# Validate that the shape of the init_fn output is the same as the shape\u001b[39;00m\n\u001b[1;32m    963\u001b[0m \u001b[39m# of the existing parameter. This is to make sure that the hparams set up\u001b[39;00m\n\u001b[1;32m    964\u001b[0m \u001b[39m# in a Flax Module match the shapes coming in during apply, and if not,\u001b[39;00m\n\u001b[1;32m    965\u001b[0m \u001b[39m# catch it with an error message.\u001b[39;00m\n\u001b[1;32m    966\u001b[0m \u001b[39m# NOTE: We could consider moving this to `self.`\u001b[39;00m\n\u001b[0;32m--> 967\u001b[0m abs_value \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39;49meval_shape(\u001b[39mlambda\u001b[39;49;00m rng: init_fn(rng, \u001b[39m*\u001b[39;49minit_args), abs_rng)\n\u001b[1;32m    968\u001b[0m abs_value_flat \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mtree_util\u001b[39m.\u001b[39mtree_leaves(abs_value)\n\u001b[1;32m    969\u001b[0m value_flat \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mtree_util\u001b[39m.\u001b[39mtree_leaves(value)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/jax/_src/api.py:2830\u001b[0m, in \u001b[0;36meval_shape\u001b[0;34m(fun, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2828\u001b[0m wrapped_fun, out_tree \u001b[39m=\u001b[39m flatten_fun(lu\u001b[39m.\u001b[39mwrap_init(fun), in_tree)\n\u001b[1;32m   2829\u001b[0m debug_info \u001b[39m=\u001b[39m pe\u001b[39m.\u001b[39mdebug_info(fun, in_tree, out_tree, \u001b[39mTrue\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39meval_shape\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2830\u001b[0m out \u001b[39m=\u001b[39m pe\u001b[39m.\u001b[39;49mabstract_eval_fun(wrapped_fun\u001b[39m.\u001b[39;49mcall_wrapped,\n\u001b[1;32m   2831\u001b[0m                            \u001b[39m*\u001b[39;49m\u001b[39mmap\u001b[39;49m(shaped_abstractify, args_flat),\n\u001b[1;32m   2832\u001b[0m                            debug_info\u001b[39m=\u001b[39;49mdebug_info)\n\u001b[1;32m   2833\u001b[0m out \u001b[39m=\u001b[39m [ShapeDtypeStruct(x\u001b[39m.\u001b[39mshape, x\u001b[39m.\u001b[39mdtype, x\u001b[39m.\u001b[39mnamed_shape) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m out]\n\u001b[1;32m   2834\u001b[0m \u001b[39mreturn\u001b[39;00m tree_unflatten(out_tree(), out)\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/jax/_src/interpreters/partial_eval.py:667\u001b[0m, in \u001b[0;36mabstract_eval_fun\u001b[0;34m(fun, debug_info, *avals, **params)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mabstract_eval_fun\u001b[39m(fun, \u001b[39m*\u001b[39mavals, debug_info\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams):\n\u001b[0;32m--> 667\u001b[0m   _, avals_out, _ \u001b[39m=\u001b[39m trace_to_jaxpr_dynamic(\n\u001b[1;32m    668\u001b[0m       lu\u001b[39m.\u001b[39;49mwrap_init(fun, params), avals, debug_info)\n\u001b[1;32m    669\u001b[0m   \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(aval, AbstractValue) \u001b[39mfor\u001b[39;00m aval \u001b[39min\u001b[39;00m avals_out)\n\u001b[1;32m    670\u001b[0m   \u001b[39mreturn\u001b[39;00m avals_out\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/jax/_src/profiler.py:340\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m    338\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    339\u001b[0m   \u001b[39mwith\u001b[39;00m TraceAnnotation(name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 340\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    341\u001b[0m   \u001b[39mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/jax/_src/interpreters/partial_eval.py:2278\u001b[0m, in \u001b[0;36mtrace_to_jaxpr_dynamic\u001b[0;34m(fun, in_avals, debug_info, keep_inputs)\u001b[0m\n\u001b[1;32m   2276\u001b[0m \u001b[39mwith\u001b[39;00m core\u001b[39m.\u001b[39mnew_main(DynamicJaxprTrace, dynamic\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m main:  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   2277\u001b[0m   main\u001b[39m.\u001b[39mjaxpr_stack \u001b[39m=\u001b[39m ()  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m-> 2278\u001b[0m   jaxpr, out_avals, consts \u001b[39m=\u001b[39m trace_to_subjaxpr_dynamic(\n\u001b[1;32m   2279\u001b[0m     fun, main, in_avals, keep_inputs\u001b[39m=\u001b[39;49mkeep_inputs, debug_info\u001b[39m=\u001b[39;49mdebug_info)\n\u001b[1;32m   2280\u001b[0m   \u001b[39mdel\u001b[39;00m main, fun\n\u001b[1;32m   2281\u001b[0m \u001b[39mreturn\u001b[39;00m jaxpr, out_avals, consts\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/jax/_src/interpreters/partial_eval.py:2300\u001b[0m, in \u001b[0;36mtrace_to_subjaxpr_dynamic\u001b[0;34m(fun, main, in_avals, keep_inputs, debug_info)\u001b[0m\n\u001b[1;32m   2298\u001b[0m in_tracers \u001b[39m=\u001b[39m _input_type_to_tracers(trace\u001b[39m.\u001b[39mnew_arg, in_avals)\n\u001b[1;32m   2299\u001b[0m in_tracers_ \u001b[39m=\u001b[39m [t \u001b[39mfor\u001b[39;00m t, keep \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(in_tracers, keep_inputs) \u001b[39mif\u001b[39;00m keep]\n\u001b[0;32m-> 2300\u001b[0m ans \u001b[39m=\u001b[39m fun\u001b[39m.\u001b[39;49mcall_wrapped(\u001b[39m*\u001b[39;49min_tracers_)\n\u001b[1;32m   2301\u001b[0m out_tracers \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(trace\u001b[39m.\u001b[39mfull_raise, ans)\n\u001b[1;32m   2302\u001b[0m jaxpr, consts \u001b[39m=\u001b[39m frame\u001b[39m.\u001b[39mto_jaxpr(out_tracers)\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/jax/_src/linear_util.py:191\u001b[0m, in \u001b[0;36mWrappedFun.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m gen \u001b[39m=\u001b[39m gen_static_args \u001b[39m=\u001b[39m out_store \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 191\u001b[0m   ans \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mdict\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m    192\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m   \u001b[39m# Some transformations yield from inside context managers, so we have to\u001b[39;00m\n\u001b[1;32m    194\u001b[0m   \u001b[39m# interrupt them before reraising the exception. Otherwise they will only\u001b[39;00m\n\u001b[1;32m    195\u001b[0m   \u001b[39m# get garbage-collected at some later time, running their cleanup tasks\u001b[39;00m\n\u001b[1;32m    196\u001b[0m   \u001b[39m# only after this exception is handled, which can corrupt the global\u001b[39;00m\n\u001b[1;32m    197\u001b[0m   \u001b[39m# state.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m   \u001b[39mwhile\u001b[39;00m stack:\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/jax/_src/linear_util.py:191\u001b[0m, in \u001b[0;36mWrappedFun.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m gen \u001b[39m=\u001b[39m gen_static_args \u001b[39m=\u001b[39m out_store \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 191\u001b[0m   ans \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mdict\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m    192\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m   \u001b[39m# Some transformations yield from inside context managers, so we have to\u001b[39;00m\n\u001b[1;32m    194\u001b[0m   \u001b[39m# interrupt them before reraising the exception. Otherwise they will only\u001b[39;00m\n\u001b[1;32m    195\u001b[0m   \u001b[39m# get garbage-collected at some later time, running their cleanup tasks\u001b[39;00m\n\u001b[1;32m    196\u001b[0m   \u001b[39m# only after this exception is handled, which can corrupt the global\u001b[39;00m\n\u001b[1;32m    197\u001b[0m   \u001b[39m# state.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m   \u001b[39mwhile\u001b[39;00m stack:\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/flax/core/scope.py:967\u001b[0m, in \u001b[0;36mScope.param.<locals>.<lambda>\u001b[0;34m(rng)\u001b[0m\n\u001b[1;32m    961\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_variable(\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m, name)\n\u001b[1;32m    962\u001b[0m \u001b[39m# Validate that the shape of the init_fn output is the same as the shape\u001b[39;00m\n\u001b[1;32m    963\u001b[0m \u001b[39m# of the existing parameter. This is to make sure that the hparams set up\u001b[39;00m\n\u001b[1;32m    964\u001b[0m \u001b[39m# in a Flax Module match the shapes coming in during apply, and if not,\u001b[39;00m\n\u001b[1;32m    965\u001b[0m \u001b[39m# catch it with an error message.\u001b[39;00m\n\u001b[1;32m    966\u001b[0m \u001b[39m# NOTE: We could consider moving this to `self.`\u001b[39;00m\n\u001b[0;32m--> 967\u001b[0m abs_value \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39meval_shape(\u001b[39mlambda\u001b[39;00m rng: init_fn(rng, \u001b[39m*\u001b[39;49minit_args), abs_rng)\n\u001b[1;32m    968\u001b[0m abs_value_flat \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mtree_util\u001b[39m.\u001b[39mtree_leaves(abs_value)\n\u001b[1;32m    969\u001b[0m value_flat \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mtree_util\u001b[39m.\u001b[39mtree_leaves(value)\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/jax/_src/nn/initializers.py:330\u001b[0m, in \u001b[0;36mvariance_scaling.<locals>.init\u001b[0;34m(key, shape, dtype)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[39mif\u001b[39;00m jnp\u001b[39m.\u001b[39missubdtype(dtype, jnp\u001b[39m.\u001b[39mfloating):\n\u001b[1;32m    328\u001b[0m   \u001b[39m# constant is stddev of standard normal truncated to (-2, 2)\u001b[39;00m\n\u001b[1;32m    329\u001b[0m   stddev \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39msqrt(variance) \u001b[39m/\u001b[39m jnp\u001b[39m.\u001b[39marray(\u001b[39m.87962566103423978\u001b[39m, dtype)\n\u001b[0;32m--> 330\u001b[0m   \u001b[39mreturn\u001b[39;00m random\u001b[39m.\u001b[39;49mtruncated_normal(key, \u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m, \u001b[39m2\u001b[39;49m, named_shape, dtype) \u001b[39m*\u001b[39m stddev\n\u001b[1;32m    331\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m   \u001b[39m# constant is stddev of complex standard normal truncated to 2\u001b[39;00m\n\u001b[1;32m    333\u001b[0m   stddev \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39msqrt(variance) \u001b[39m/\u001b[39m jnp\u001b[39m.\u001b[39marray(\u001b[39m.95311164380491208\u001b[39m, dtype)\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/jax/_src/random.py:880\u001b[0m, in \u001b[0;36mtruncated_normal\u001b[0;34m(key, lower, upper, shape, dtype)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    879\u001b[0m   shape \u001b[39m=\u001b[39m core\u001b[39m.\u001b[39mas_named_shape(shape)\n\u001b[0;32m--> 880\u001b[0m \u001b[39mreturn\u001b[39;00m _truncated_normal(key, lower, upper, shape, dtype)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/jax/_src/pjit.py:256\u001b[0m, in \u001b[0;36m_cpp_pjit.<locals>.cache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39m@api_boundary\u001b[39m\n\u001b[1;32m    255\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcache_miss\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 256\u001b[0m   outs, out_flat, out_tree, args_flat, jaxpr \u001b[39m=\u001b[39m _python_pjit_helper(\n\u001b[1;32m    257\u001b[0m       fun, infer_params_fn, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    258\u001b[0m   executable \u001b[39m=\u001b[39m _read_most_recent_pjit_call_executable(jaxpr)\n\u001b[1;32m    259\u001b[0m   fastpath_data \u001b[39m=\u001b[39m _get_fastpath_data(executable, out_tree, args_flat, out_flat)\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/jax/_src/pjit.py:167\u001b[0m, in \u001b[0;36m_python_pjit_helper\u001b[0;34m(fun, infer_params_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m   dispatch\u001b[39m.\u001b[39mcheck_arg(arg)\n\u001b[1;32m    166\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m   out_flat \u001b[39m=\u001b[39m pjit_p\u001b[39m.\u001b[39;49mbind(\u001b[39m*\u001b[39;49margs_flat, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    168\u001b[0m \u001b[39mexcept\u001b[39;00m pxla\u001b[39m.\u001b[39mDeviceAssignmentMismatchError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    169\u001b[0m   fails, \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39margs\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/jax/_src/core.py:2656\u001b[0m, in \u001b[0;36mAxisPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m   2652\u001b[0m axis_main \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m((axis_frame(a)\u001b[39m.\u001b[39mmain_trace \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m used_axis_names(\u001b[39mself\u001b[39m, params)),\n\u001b[1;32m   2653\u001b[0m                 default\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m t: \u001b[39mgetattr\u001b[39m(t, \u001b[39m'\u001b[39m\u001b[39mlevel\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m   2654\u001b[0m top_trace \u001b[39m=\u001b[39m (top_trace \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m axis_main \u001b[39mor\u001b[39;00m axis_main\u001b[39m.\u001b[39mlevel \u001b[39m<\u001b[39m top_trace\u001b[39m.\u001b[39mlevel\n\u001b[1;32m   2655\u001b[0m              \u001b[39melse\u001b[39;00m axis_main\u001b[39m.\u001b[39mwith_cur_sublevel())\n\u001b[0;32m-> 2656\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbind_with_trace(top_trace, args, params)\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/jax/_src/core.py:388\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind_with_trace\u001b[39m(\u001b[39mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 388\u001b[0m   out \u001b[39m=\u001b[39m trace\u001b[39m.\u001b[39;49mprocess_primitive(\u001b[39mself\u001b[39;49m, \u001b[39mmap\u001b[39;49m(trace\u001b[39m.\u001b[39;49mfull_raise, args), params)\n\u001b[1;32m    389\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mmap\u001b[39m(full_lower, out) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmultiple_results \u001b[39melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/jax/_src/interpreters/partial_eval.py:1971\u001b[0m, in \u001b[0;36mDynamicJaxprTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m   1969\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_primitive\u001b[39m(\u001b[39mself\u001b[39m, primitive, tracers, params):\n\u001b[1;32m   1970\u001b[0m   \u001b[39mif\u001b[39;00m primitive \u001b[39min\u001b[39;00m custom_staging_rules:\n\u001b[0;32m-> 1971\u001b[0m     \u001b[39mreturn\u001b[39;00m custom_staging_rules[primitive](\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49mtracers, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m   1972\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault_process_primitive(primitive, tracers, params)\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/jax/_src/pjit.py:1332\u001b[0m, in \u001b[0;36mpjit_staging_rule\u001b[0;34m(trace, *args, **params)\u001b[0m\n\u001b[1;32m   1330\u001b[0m   \u001b[39mreturn\u001b[39;00m out_tracers\n\u001b[1;32m   1331\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1332\u001b[0m   \u001b[39mreturn\u001b[39;00m trace\u001b[39m.\u001b[39;49mdefault_process_primitive(pjit_p, args, params)\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/jax/_src/interpreters/partial_eval.py:1984\u001b[0m, in \u001b[0;36mDynamicJaxprTrace.default_process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m   1982\u001b[0m out_avals \u001b[39m=\u001b[39m [out_avals] \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m primitive\u001b[39m.\u001b[39mmultiple_results \u001b[39melse\u001b[39;00m out_avals\n\u001b[1;32m   1983\u001b[0m source_info \u001b[39m=\u001b[39m source_info_util\u001b[39m.\u001b[39mcurrent()\n\u001b[0;32m-> 1984\u001b[0m out_tracers \u001b[39m=\u001b[39m [DynamicJaxprTracer(\u001b[39mself\u001b[39;49m, a, source_info) \u001b[39mfor\u001b[39;49;00m a \u001b[39min\u001b[39;49;00m out_avals]\n\u001b[1;32m   1985\u001b[0m invars \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetvar, tracers)\n\u001b[1;32m   1986\u001b[0m outvars \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmakevar, out_tracers)\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/jax/_src/interpreters/partial_eval.py:1984\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1982\u001b[0m out_avals \u001b[39m=\u001b[39m [out_avals] \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m primitive\u001b[39m.\u001b[39mmultiple_results \u001b[39melse\u001b[39;00m out_avals\n\u001b[1;32m   1983\u001b[0m source_info \u001b[39m=\u001b[39m source_info_util\u001b[39m.\u001b[39mcurrent()\n\u001b[0;32m-> 1984\u001b[0m out_tracers \u001b[39m=\u001b[39m [DynamicJaxprTracer(\u001b[39mself\u001b[39m, a, source_info) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m out_avals]\n\u001b[1;32m   1985\u001b[0m invars \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetvar, tracers)\n\u001b[1;32m   1986\u001b[0m outvars \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmakevar, out_tracers)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "plot_predictions(\n",
    "    config=connected_configs_7[0],\n",
    "    workdir=connected_workdirs_7[0], # for loading checkpoints \n",
    "    plot_ith_rollout_step=5, # 0 indexed # for this study, we have a 4-step rollout \n",
    "    datasets=all_connected_7_datasets[0],\n",
    "    node=0, # 0-indexed \n",
    "    plot_mode=\"test\", # i.e. \"train\"/\"val\"/\"test\"\n",
    "    plot_days=60,\n",
    "    title=\"Connected 7 Test Predictions for node 0, rollout step 5\"\n",
    ")\n",
    "\n",
    "plot_predictions(\n",
    "    config=connected_configs_5[0],\n",
    "    workdir=connected_workdirs_5[0], # for loading checkpoints \n",
    "    plot_ith_rollout_step=5, # 0 indexed # for this study, we have a 4-step rollout \n",
    "    datasets=all_connected_5_datasets[0],\n",
    "    node=0, # 0-indexed \n",
    "    plot_mode=\"test\", # i.e. \"train\"/\"val\"/\"test\"\n",
    "    plot_days=60,\n",
    "    title=\"Connected 5 Test Predictions for node 0, rollout step 5\"\n",
    ")\n",
    "\n",
    "plot_predictions(\n",
    "    config=connected_configs_3[0],\n",
    "    workdir=connected_workdirs_3[0], # for loading checkpoints \n",
    "    plot_ith_rollout_step=5, # 0 indexed # for this study, we have a 4-step rollout \n",
    "    datasets=all_connected_3_datasets[0],\n",
    "    node=0, # 0-indexed \n",
    "    plot_mode=\"test\", # i.e. \"train\"/\"val\"/\"test\"\n",
    "    plot_days=60,\n",
    "    title=\"Connected 3 Test Predictions for node 0, rollout step 5\"\n",
    ")\n",
    "\n",
    "plot_predictions(\n",
    "    config=solo_configs[0],\n",
    "    workdir=solo_workdirs[0], # for loading checkpoints \n",
    "    plot_ith_rollout_step=5, # 0 indexed # for this study, we have a 4-step rollout \n",
    "    datasets=all_solo_datasets[0],\n",
    "    node=0, # 0-indexed \n",
    "    plot_mode=\"test\", # i.e. \"train\"/\"val\"/\"test\"\n",
    "    plot_days=60,\n",
    "    title=\"Solo Test Predictions for node 0, rollout step 5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_over_epoch(all_epoch_losses, num_epochs=None):\n",
    "    if num_epochs is not None:\n",
    "        epoch_losses = [epoch_losses[:num_epochs] for epoch_losses in all_epoch_losses]\n",
    "    else:\n",
    "        epoch_losses = all_epoch_losses\n",
    "        # arbitrarily choses\n",
    "        num_epochs = len(epoch_losses[0][0])\n",
    "        print(num_epochs)\n",
    "        \n",
    "    epochs = jnp.arange(num_epochs)\n",
    "    connection_types = [\"7-connected edges\", \"5-connected edges\", \"3-connected edges\", \"solo edges\"]\n",
    "    colors = [\"red\", \"blue\", \"green\", \"purple\"]\n",
    "    fig = plt.figure(figsize=(11,9))\n",
    "    for i in range(len(epoch_losses)):\n",
    "        \n",
    "        mn = np.mean(epoch_losses[i], axis=0)\n",
    "        sd = np.std(epoch_losses[i], axis=0)\n",
    "\n",
    "        plt.plot(epochs, mn, alpha=0.7, label=connection_types[i], c=colors[i], marker='o')\n",
    "        plt.fill_between(epochs, mn+sd, mn-sd, facecolor=colors[i], alpha=0.3)\n",
    "    \n",
    "\n",
    "    plt.tick_params(axis='both', which='major', labelsize=23)\n",
    "    plt.ylabel(\"Average Epoch Loss\", size=30)\n",
    "    plt.xlabel(\"Epoch Number\", size=30)\n",
    "    plt.title(\"Epoch Losses for Varying Egde Configurations\", size=35)\n",
    "    plt.legend(fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[[], [Array(2.9922237, dtype=float32), Array(2.4935875, dtype=float32), Array(2.1822693, dtype=float32), Array(2.1082468, dtype=float32), Array(2.051237, dtype=float32), Array(1.9984847, dtype=float32), Array(1.9648124, dtype=float32), Array(1.9416674, dtype=float32), Array(1.9426953, dtype=float32), Array(1.9264244, dtype=float32), Array(1.9237448, dtype=float32), Array(1.9191467, dtype=float32), Array(1.9188621, dtype=float32), Array(1.915855, dtype=float32), Array(1.9099625, dtype=float32), Array(1.9164841, dtype=float32), Array(1.9132684, dtype=float32), Array(1.9074101, dtype=float32), Array(1.9125059, dtype=float32), Array(1.9137437, dtype=float32), Array(1.9061681, dtype=float32), Array(1.9138238, dtype=float32), Array(1.9146775, dtype=float32), Array(1.9153011, dtype=float32), Array(1.9107713, dtype=float32), Array(1.9146795, dtype=float32), Array(1.9148825, dtype=float32), Array(1.9143335, dtype=float32), Array(1.9131228, dtype=float32), Array(1.9202404, dtype=float32), Array(1.905595, dtype=float32), Array(1.9134814, dtype=float32), Array(1.917371, dtype=float32), Array(1.9102478, dtype=float32), Array(1.9131126, dtype=float32), Array(1.9126788, dtype=float32), Array(1.9080765, dtype=float32), Array(1.9115081, dtype=float32), Array(1.913961, dtype=float32), Array(1.9082596, dtype=float32), Array(1.9100894, dtype=float32), Array(1.9053439, dtype=float32), Array(1.9081758, dtype=float32), Array(1.9057353, dtype=float32), Array(1.9077824, dtype=float32), Array(1.9048901, dtype=float32), Array(1.9081059, dtype=float32), Array(1.9043306, dtype=float32), Array(1.909206, dtype=float32), Array(1.9061713, dtype=float32), Array(1.9071649, dtype=float32), Array(1.9006283, dtype=float32), Array(1.9029057, dtype=float32), Array(1.9008265, dtype=float32), Array(1.9026779, dtype=float32), Array(1.9073039, dtype=float32), Array(1.9040185, dtype=float32), Array(1.9016402, dtype=float32), Array(1.899096, dtype=float32), Array(1.8992105, dtype=float32), Array(1.8971187, dtype=float32), Array(1.9065752, dtype=float32), Array(1.8991168, dtype=float32), Array(1.8974947, dtype=float32), Array(1.9016464, dtype=float32), Array(1.8974425, dtype=float32), Array(1.9003718, dtype=float32), Array(1.8961821, dtype=float32), Array(1.8969722, dtype=float32), Array(1.8946102, dtype=float32), Array(1.8954228, dtype=float32), Array(1.896134, dtype=float32), Array(1.8945619, dtype=float32), Array(1.8973333, dtype=float32), Array(1.8991898, dtype=float32), Array(1.8936343, dtype=float32), Array(1.8955753, dtype=float32), Array(1.8975589, dtype=float32), Array(1.8957218, dtype=float32), Array(1.8939289, dtype=float32), Array(1.8939117, dtype=float32), Array(1.8952689, dtype=float32), Array(1.8995311, dtype=float32), Array(1.8929296, dtype=float32), Array(1.8934721, dtype=float32), Array(1.8948599, dtype=float32), Array(1.8940896, dtype=float32), Array(1.8949292, dtype=float32), Array(1.8946549, dtype=float32), Array(1.8996783, dtype=float32), Array(1.8885883, dtype=float32), Array(1.8904909, dtype=float32), Array(1.8911313, dtype=float32), Array(1.8903435, dtype=float32), Array(1.8964413, dtype=float32), Array(1.8929448, dtype=float32), Array(1.8988916, dtype=float32), Array(1.8927747, dtype=float32), Array(1.8926075, dtype=float32), Array(1.8933932, dtype=float32)], [Array(3.0026877, dtype=float32), Array(2.5634801, dtype=float32), Array(2.184958, dtype=float32), Array(2.0873027, dtype=float32), Array(2.0305927, dtype=float32), Array(1.972247, dtype=float32), Array(1.9354374, dtype=float32), Array(1.9131813, dtype=float32), Array(1.8978508, dtype=float32), Array(1.8834399, dtype=float32), Array(1.8853451, dtype=float32), Array(1.8723929, dtype=float32), Array(1.8777846, dtype=float32), Array(1.8755516, dtype=float32), Array(1.8725934, dtype=float32), Array(1.8688235, dtype=float32), Array(1.8742468, dtype=float32), Array(1.8749743, dtype=float32), Array(1.874954, dtype=float32), Array(1.8740205, dtype=float32), Array(1.873143, dtype=float32), Array(1.8772296, dtype=float32), Array(1.870282, dtype=float32), Array(1.8685334, dtype=float32), Array(1.8686569, dtype=float32), Array(1.870247, dtype=float32), Array(1.8724241, dtype=float32), Array(1.8675935, dtype=float32), Array(1.8738393, dtype=float32), Array(1.8750452, dtype=float32), Array(1.872899, dtype=float32), Array(1.8710195, dtype=float32), Array(1.8722056, dtype=float32), Array(1.868397, dtype=float32), Array(1.8709396, dtype=float32), Array(1.8710214, dtype=float32), Array(1.8712053, dtype=float32), Array(1.8682604, dtype=float32), Array(1.8761545, dtype=float32), Array(1.8695593, dtype=float32), Array(1.8704064, dtype=float32), Array(1.8759507, dtype=float32), Array(1.866972, dtype=float32), Array(1.8659294, dtype=float32), Array(1.8727385, dtype=float32), Array(1.8716714, dtype=float32), Array(1.8694153, dtype=float32), Array(1.863473, dtype=float32), Array(1.8668535, dtype=float32), Array(1.8715651, dtype=float32), Array(1.8671433, dtype=float32), Array(1.8645371, dtype=float32), Array(1.8657569, dtype=float32), Array(1.8636461, dtype=float32), Array(1.861623, dtype=float32), Array(1.8659706, dtype=float32), Array(1.8657067, dtype=float32), Array(1.8638124, dtype=float32), Array(1.86972, dtype=float32), Array(1.8648509, dtype=float32), Array(1.8658993, dtype=float32), Array(1.8642604, dtype=float32), Array(1.8642979, dtype=float32), Array(1.8633591, dtype=float32), Array(1.8607359, dtype=float32), Array(1.8635576, dtype=float32), Array(1.8617631, dtype=float32), Array(1.8625767, dtype=float32), Array(1.86468, dtype=float32), Array(1.8665675, dtype=float32), Array(1.8622234, dtype=float32), Array(1.8606236, dtype=float32), Array(1.8582027, dtype=float32), Array(1.8646413, dtype=float32), Array(1.8615844, dtype=float32), Array(1.8619635, dtype=float32), Array(1.8608702, dtype=float32), Array(1.8636234, dtype=float32), Array(1.860858, dtype=float32), Array(1.8562646, dtype=float32), Array(1.8600646, dtype=float32), Array(1.8596557, dtype=float32), Array(1.8602102, dtype=float32), Array(1.8574361, dtype=float32), Array(1.8619046, dtype=float32), Array(1.8595467, dtype=float32), Array(1.8625784, dtype=float32), Array(1.8609284, dtype=float32), Array(1.8607622, dtype=float32), Array(1.8578478, dtype=float32), Array(1.8609332, dtype=float32), Array(1.860426, dtype=float32), Array(1.8567019, dtype=float32), Array(1.864394, dtype=float32), Array(1.8569148, dtype=float32), Array(1.8514467, dtype=float32), Array(1.8552895, dtype=float32), Array(1.8584447, dtype=float32), Array(1.8522718, dtype=float32), Array(1.8514204, dtype=float32)], [Array(3.0265958, dtype=float32), Array(2.5718672, dtype=float32), Array(2.2167692, dtype=float32), Array(2.1223397, dtype=float32), Array(2.0717642, dtype=float32), Array(2.00877, dtype=float32), Array(1.9743948, dtype=float32), Array(1.9427165, dtype=float32), Array(1.9264475, dtype=float32), Array(1.9214345, dtype=float32), Array(1.9180081, dtype=float32), Array(1.9160031, dtype=float32), Array(1.9079915, dtype=float32), Array(1.9090891, dtype=float32), Array(1.9088763, dtype=float32), Array(1.9057386, dtype=float32), Array(1.9085586, dtype=float32), Array(1.9065158, dtype=float32), Array(1.9088868, dtype=float32), Array(1.9040825, dtype=float32), Array(1.9029559, dtype=float32), Array(1.9079685, dtype=float32), Array(1.9041334, dtype=float32), Array(1.9042618, dtype=float32), Array(1.9014928, dtype=float32), Array(1.9079015, dtype=float32), Array(1.9101579, dtype=float32), Array(1.9084647, dtype=float32), Array(1.9059241, dtype=float32), Array(1.915185, dtype=float32), Array(1.8987331, dtype=float32), Array(1.9122924, dtype=float32), Array(1.9027133, dtype=float32), Array(1.9079849, dtype=float32), Array(1.902581, dtype=float32), Array(1.9071044, dtype=float32), Array(1.9073838, dtype=float32), Array(1.9053236, dtype=float32), Array(1.9044608, dtype=float32), Array(1.9065655, dtype=float32), Array(1.9012097, dtype=float32), Array(1.9007022, dtype=float32), Array(1.9031715, dtype=float32), Array(1.9004719, dtype=float32), Array(1.9045067, dtype=float32), Array(1.9011147, dtype=float32), Array(1.9012313, dtype=float32), Array(1.9009134, dtype=float32), Array(1.899238, dtype=float32), Array(1.8970883, dtype=float32), Array(1.9021058, dtype=float32), Array(1.8991619, dtype=float32), Array(1.8940291, dtype=float32), Array(1.9015002, dtype=float32), Array(1.8953176, dtype=float32), Array(1.8959416, dtype=float32), Array(1.8950269, dtype=float32), Array(1.8950502, dtype=float32), Array(1.8993312, dtype=float32), Array(1.8935581, dtype=float32), Array(1.8954166, dtype=float32), Array(1.8989655, dtype=float32), Array(1.8948045, dtype=float32), Array(1.8960236, dtype=float32), Array(1.8985355, dtype=float32), Array(1.896967, dtype=float32), Array(1.8915575, dtype=float32), Array(1.8887945, dtype=float32), Array(1.8960639, dtype=float32), Array(1.8916626, dtype=float32), Array(1.8975773, dtype=float32), Array(1.891685, dtype=float32), Array(1.9003369, dtype=float32), Array(1.8972133, dtype=float32), Array(1.8981414, dtype=float32), Array(1.8914292, dtype=float32), Array(1.891856, dtype=float32), Array(1.8957049, dtype=float32), Array(1.8951741, dtype=float32), Array(1.8907597, dtype=float32), Array(1.8912992, dtype=float32), Array(1.8959363, dtype=float32), Array(1.8896229, dtype=float32), Array(1.8873618, dtype=float32), Array(1.8914745, dtype=float32), Array(1.8917125, dtype=float32), Array(1.8896736, dtype=float32), Array(1.8918749, dtype=float32), Array(1.8936505, dtype=float32), Array(1.8899497, dtype=float32), Array(1.8916284, dtype=float32), Array(1.8890828, dtype=float32), Array(1.8895736, dtype=float32), Array(1.8949918, dtype=float32), Array(1.8890257, dtype=float32), Array(1.8874879, dtype=float32), Array(1.8909414, dtype=float32), Array(1.8920293, dtype=float32), Array(1.8909186, dtype=float32), Array(1.883644, dtype=float32)], [Array(3.0179138, dtype=float32), Array(2.5302806, dtype=float32), Array(2.235616, dtype=float32), Array(2.149311, dtype=float32), Array(2.0988762, dtype=float32), Array(2.0368783, dtype=float32), Array(2.0098634, dtype=float32), Array(1.9809815, dtype=float32), Array(1.9687443, dtype=float32), Array(1.9628035, dtype=float32), Array(1.9597864, dtype=float32), Array(1.9514735, dtype=float32), Array(1.9536802, dtype=float32), Array(1.9514589, dtype=float32), Array(1.9484285, dtype=float32), Array(1.9461231, dtype=float32), Array(1.9537908, dtype=float32), Array(1.9486415, dtype=float32), Array(1.9467883, dtype=float32), Array(1.9514881, dtype=float32), Array(1.9496436, dtype=float32), Array(1.9445014, dtype=float32), Array(1.9446865, dtype=float32), Array(1.9506756, dtype=float32), Array(1.9419569, dtype=float32), Array(1.943083, dtype=float32), Array(1.9451336, dtype=float32), Array(1.9437442, dtype=float32), Array(1.9443467, dtype=float32), Array(1.9437909, dtype=float32), Array(1.9463187, dtype=float32), Array(1.9478389, dtype=float32), Array(1.9433581, dtype=float32), Array(1.9478163, dtype=float32), Array(1.9473099, dtype=float32), Array(1.9458433, dtype=float32), Array(1.9460943, dtype=float32), Array(1.9456102, dtype=float32), Array(1.9409829, dtype=float32), Array(1.9445498, dtype=float32), Array(1.9443752, dtype=float32), Array(1.9457207, dtype=float32), Array(1.9410422, dtype=float32), Array(1.9402514, dtype=float32), Array(1.9427502, dtype=float32), Array(1.9417877, dtype=float32), Array(1.940152, dtype=float32), Array(1.941256, dtype=float32), Array(1.9393257, dtype=float32), Array(1.9367884, dtype=float32), Array(1.9333304, dtype=float32), Array(1.9385806, dtype=float32), Array(1.9411608, dtype=float32), Array(1.9342963, dtype=float32), Array(1.932951, dtype=float32), Array(1.9358772, dtype=float32), Array(1.9334053, dtype=float32), Array(1.9348339, dtype=float32), Array(1.9310439, dtype=float32), Array(1.935309, dtype=float32), Array(1.9296037, dtype=float32), Array(1.9365156, dtype=float32), Array(1.93408, dtype=float32), Array(1.9301661, dtype=float32), Array(1.9318063, dtype=float32), Array(1.9369247, dtype=float32), Array(1.9309752, dtype=float32), Array(1.927171, dtype=float32), Array(1.9298211, dtype=float32), Array(1.9286731, dtype=float32), Array(1.9254197, dtype=float32), Array(1.9275006, dtype=float32), Array(1.9305866, dtype=float32), Array(1.9294151, dtype=float32), Array(1.9305271, dtype=float32), Array(1.9274347, dtype=float32), Array(1.9333099, dtype=float32), Array(1.9304006, dtype=float32), Array(1.9293715, dtype=float32), Array(1.9240633, dtype=float32), Array(1.9276487, dtype=float32), Array(1.9287509, dtype=float32), Array(1.9235139, dtype=float32), Array(1.9225866, dtype=float32), Array(1.9222634, dtype=float32), Array(1.9222, dtype=float32), Array(1.9249884, dtype=float32), Array(1.9226722, dtype=float32), Array(1.9282178, dtype=float32), Array(1.9283085, dtype=float32), Array(1.9252719, dtype=float32), Array(1.9220957, dtype=float32), Array(1.9241812, dtype=float32), Array(1.9286307, dtype=float32), Array(1.9272614, dtype=float32), Array(1.9246777, dtype=float32), Array(1.9244171, dtype=float32), Array(1.9274658, dtype=float32), Array(1.9279925, dtype=float32), Array(1.9241052, dtype=float32)], [Array(2.99748, dtype=float32), Array(2.4941285, dtype=float32), Array(2.2092505, dtype=float32), Array(2.1361313, dtype=float32), Array(2.082806, dtype=float32), Array(2.0277941, dtype=float32), Array(2.0018003, dtype=float32), Array(1.9744298, dtype=float32), Array(1.9667602, dtype=float32), Array(1.9534682, dtype=float32), Array(1.9490213, dtype=float32), Array(1.9421688, dtype=float32), Array(1.9443358, dtype=float32), Array(1.939197, dtype=float32), Array(1.9396638, dtype=float32), Array(1.940901, dtype=float32), Array(1.9381849, dtype=float32), Array(1.9382174, dtype=float32), Array(1.9406451, dtype=float32), Array(1.9379121, dtype=float32), Array(1.9363304, dtype=float32), Array(1.9339186, dtype=float32), Array(1.9397585, dtype=float32), Array(1.9371866, dtype=float32), Array(1.9379486, dtype=float32), Array(1.9374093, dtype=float32), Array(1.9355493, dtype=float32), Array(1.9412737, dtype=float32), Array(1.9371349, dtype=float32), Array(1.9440728, dtype=float32), Array(1.9373453, dtype=float32), Array(1.9374477, dtype=float32), Array(1.934324, dtype=float32), Array(1.9382172, dtype=float32), Array(1.9355625, dtype=float32), Array(1.9348484, dtype=float32), Array(1.939479, dtype=float32), Array(1.9376566, dtype=float32), Array(1.9366567, dtype=float32), Array(1.932496, dtype=float32), Array(1.9363846, dtype=float32), Array(1.9373412, dtype=float32), Array(1.931098, dtype=float32), Array(1.9311565, dtype=float32), Array(1.9315137, dtype=float32), Array(1.9351091, dtype=float32), Array(1.9329312, dtype=float32), Array(1.9309583, dtype=float32), Array(1.927648, dtype=float32), Array(1.931773, dtype=float32), Array(1.9279408, dtype=float32), Array(1.9328412, dtype=float32), Array(1.9271797, dtype=float32), Array(1.9270471, dtype=float32), Array(1.9256799, dtype=float32), Array(1.927014, dtype=float32), Array(1.9260626, dtype=float32), Array(1.9249148, dtype=float32), Array(1.9275331, dtype=float32), Array(1.9285336, dtype=float32), Array(1.9238821, dtype=float32), Array(1.9273635, dtype=float32), Array(1.9245242, dtype=float32), Array(1.9223378, dtype=float32), Array(1.9290283, dtype=float32), Array(1.9300106, dtype=float32), Array(1.9278603, dtype=float32), Array(1.9229895, dtype=float32), Array(1.9253681, dtype=float32), Array(1.925253, dtype=float32), Array(1.9218297, dtype=float32), Array(1.917926, dtype=float32), Array(1.9245641, dtype=float32), Array(1.9283519, dtype=float32), Array(1.9275973, dtype=float32), Array(1.9244583, dtype=float32), Array(1.9223331, dtype=float32), Array(1.9216429, dtype=float32), Array(1.919634, dtype=float32), Array(1.9185873, dtype=float32), Array(1.921528, dtype=float32), Array(1.9226228, dtype=float32), Array(1.9200046, dtype=float32), Array(1.9171367, dtype=float32), Array(1.9216532, dtype=float32), Array(1.9174836, dtype=float32), Array(1.9206127, dtype=float32), Array(1.91846, dtype=float32), Array(1.9179955, dtype=float32), Array(1.9208242, dtype=float32), Array(1.9204663, dtype=float32), Array(1.9165783, dtype=float32), Array(1.9149863, dtype=float32), Array(1.9182451, dtype=float32), Array(1.919568, dtype=float32), Array(1.9182152, dtype=float32), Array(1.9226537, dtype=float32), Array(1.9187427, dtype=float32), Array(1.9182328, dtype=float32), Array(1.9175268, dtype=float32)], [Array(2.99748, dtype=float32), Array(2.4941285, dtype=float32), Array(2.2092505, dtype=float32), Array(2.1361313, dtype=float32), Array(2.082806, dtype=float32), Array(2.0277941, dtype=float32), Array(2.0018003, dtype=float32), Array(1.9744298, dtype=float32), Array(1.9667602, dtype=float32), Array(1.9534682, dtype=float32), Array(1.9490213, dtype=float32), Array(1.9421688, dtype=float32), Array(1.9443358, dtype=float32), Array(1.939197, dtype=float32), Array(1.9396638, dtype=float32), Array(1.940901, dtype=float32), Array(1.9381849, dtype=float32), Array(1.9382174, dtype=float32), Array(1.9406451, dtype=float32), Array(1.9379121, dtype=float32), Array(1.9363304, dtype=float32), Array(1.9339186, dtype=float32), Array(1.9397585, dtype=float32), Array(1.9371866, dtype=float32), Array(1.9379486, dtype=float32), Array(1.9374093, dtype=float32), Array(1.9355493, dtype=float32), Array(1.9412737, dtype=float32), Array(1.9371349, dtype=float32), Array(1.9440728, dtype=float32), Array(1.9373453, dtype=float32), Array(1.9374477, dtype=float32), Array(1.934324, dtype=float32), Array(1.9382172, dtype=float32), Array(1.9355625, dtype=float32), Array(1.9348484, dtype=float32), Array(1.939479, dtype=float32), Array(1.9376566, dtype=float32), Array(1.9366567, dtype=float32), Array(1.932496, dtype=float32), Array(1.9363846, dtype=float32), Array(1.9373412, dtype=float32), Array(1.931098, dtype=float32), Array(1.9311565, dtype=float32), Array(1.9315137, dtype=float32), Array(1.9351091, dtype=float32), Array(1.9329312, dtype=float32), Array(1.9309583, dtype=float32), Array(1.927648, dtype=float32), Array(1.931773, dtype=float32), Array(1.9279408, dtype=float32), Array(1.9328412, dtype=float32), Array(1.9271797, dtype=float32), Array(1.9270471, dtype=float32), Array(1.9256799, dtype=float32), Array(1.927014, dtype=float32), Array(1.9260626, dtype=float32), Array(1.9249148, dtype=float32), Array(1.9275331, dtype=float32), Array(1.9285336, dtype=float32), Array(1.9238821, dtype=float32), Array(1.9273635, dtype=float32), Array(1.9245242, dtype=float32), Array(1.9223378, dtype=float32), Array(1.9290283, dtype=float32), Array(1.9300106, dtype=float32), Array(1.9278603, dtype=float32), Array(1.9229895, dtype=float32), Array(1.9253681, dtype=float32), Array(1.925253, dtype=float32), Array(1.9218297, dtype=float32), Array(1.917926, dtype=float32), Array(1.9245641, dtype=float32), Array(1.9283519, dtype=float32), Array(1.9275973, dtype=float32), Array(1.9244583, dtype=float32), Array(1.9223331, dtype=float32), Array(1.9216429, dtype=float32), Array(1.919634, dtype=float32), Array(1.9185873, dtype=float32), Array(1.921528, dtype=float32), Array(1.9226228, dtype=float32), Array(1.9200046, dtype=float32), Array(1.9171367, dtype=float32), Array(1.9216532, dtype=float32), Array(1.9174836, dtype=float32), Array(1.9206127, dtype=float32), Array(1.91846, dtype=float32), Array(1.9179955, dtype=float32), Array(1.9208242, dtype=float32), Array(1.9204663, dtype=float32), Array(1.9165783, dtype=float32), Array(1.9149863, dtype=float32), Array(1.9182451, dtype=float32), Array(1.919568, dtype=float32), Array(1.9182152, dtype=float32), Array(1.9226537, dtype=float32), Array(1.9187427, dtype=float32), Array(1.9182328, dtype=float32), Array(1.9175268, dtype=float32)], [Array(3.1352026, dtype=float32), Array(2.675877, dtype=float32), Array(2.3181484, dtype=float32), Array(2.228233, dtype=float32), Array(2.166826, dtype=float32), Array(2.0981147, dtype=float32), Array(2.0713742, dtype=float32), Array(2.04561, dtype=float32), Array(2.0330112, dtype=float32), Array(2.0281355, dtype=float32), Array(2.0218122, dtype=float32), Array(2.0155425, dtype=float32), Array(2.0187635, dtype=float32), Array(2.014017, dtype=float32), Array(2.0175998, dtype=float32), Array(2.0185475, dtype=float32), Array(2.0178611, dtype=float32), Array(2.0154974, dtype=float32), Array(2.014149, dtype=float32), Array(2.0180802, dtype=float32), Array(2.0174224, dtype=float32), Array(2.0169575, dtype=float32), Array(2.0194786, dtype=float32), Array(2.0177174, dtype=float32), Array(2.0176935, dtype=float32), Array(2.0186987, dtype=float32), Array(2.021036, dtype=float32), Array(2.0206666, dtype=float32), Array(2.0162826, dtype=float32), Array(2.0210037, dtype=float32), Array(2.0138094, dtype=float32), Array(2.0147533, dtype=float32), Array(2.02089, dtype=float32), Array(2.0180547, dtype=float32), Array(2.012351, dtype=float32), Array(2.0148108, dtype=float32), Array(2.0095103, dtype=float32), Array(2.0140514, dtype=float32), Array(2.0191958, dtype=float32), Array(2.0137095, dtype=float32), Array(2.0149963, dtype=float32), Array(2.0107543, dtype=float32), Array(2.0121279, dtype=float32), Array(2.0113142, dtype=float32), Array(2.0138698, dtype=float32), Array(2.0139606, dtype=float32), Array(2.0111325, dtype=float32), Array(2.0095232, dtype=float32), Array(2.0099614, dtype=float32), Array(2.005906, dtype=float32), Array(2.0105815, dtype=float32), Array(2.0082638, dtype=float32), Array(2.0093055, dtype=float32), Array(2.005993, dtype=float32), Array(2.0033722, dtype=float32), Array(2.0097806, dtype=float32), Array(2.0097208, dtype=float32), Array(2.007675, dtype=float32), Array(2.0090106, dtype=float32), Array(2.00462, dtype=float32), Array(2.0064561, dtype=float32), Array(2.00434, dtype=float32), Array(2.0049787, dtype=float32), Array(2.0074816, dtype=float32), Array(2.0048132, dtype=float32), Array(2.0047953, dtype=float32), Array(2.0055733, dtype=float32), Array(2.0033653, dtype=float32), Array(2.006191, dtype=float32), Array(2.003576, dtype=float32), Array(2.0031824, dtype=float32), Array(2.0008526, dtype=float32), Array(2.0001154, dtype=float32), Array(2.002837, dtype=float32), Array(2.0005376, dtype=float32), Array(2.0045347, dtype=float32), Array(2.0058546, dtype=float32), Array(2.0031595, dtype=float32), Array(2.005681, dtype=float32), Array(2.001481, dtype=float32), Array(1.9985753, dtype=float32), Array(2.0038004, dtype=float32), Array(2.0039992, dtype=float32), Array(2.0018947, dtype=float32), Array(2.0064235, dtype=float32), Array(2.0009668, dtype=float32), Array(1.9985231, dtype=float32), Array(2.00126, dtype=float32), Array(2.0066168, dtype=float32), Array(2.0043175, dtype=float32), Array(2.0006268, dtype=float32), Array(2.000129, dtype=float32), Array(1.9973719, dtype=float32), Array(2.0037181, dtype=float32), Array(2.0019987, dtype=float32), Array(2.0032454, dtype=float32), Array(1.9983904, dtype=float32), Array(2.0023463, dtype=float32), Array(1.9981693, dtype=float32), Array(1.9945277, dtype=float32)], [Array(2.9057329, dtype=float32), Array(2.4214165, dtype=float32), Array(2.1103044, dtype=float32), Array(2.0390913, dtype=float32), Array(1.9871122, dtype=float32), Array(1.9317585, dtype=float32), Array(1.9016368, dtype=float32), Array(1.8830713, dtype=float32), Array(1.8676108, dtype=float32), Array(1.8584813, dtype=float32), Array(1.8588386, dtype=float32), Array(1.8520937, dtype=float32), Array(1.847942, dtype=float32), Array(1.8487207, dtype=float32), Array(1.848915, dtype=float32), Array(1.8470147, dtype=float32), Array(1.8450769, dtype=float32), Array(1.8461027, dtype=float32), Array(1.848964, dtype=float32), Array(1.8445531, dtype=float32), Array(1.8486552, dtype=float32), Array(1.8464763, dtype=float32), Array(1.8503819, dtype=float32), Array(1.843456, dtype=float32), Array(1.8436395, dtype=float32), Array(1.848985, dtype=float32), Array(1.8480027, dtype=float32), Array(1.850349, dtype=float32), Array(1.845584, dtype=float32), Array(1.8425357, dtype=float32), Array(1.8458365, dtype=float32), Array(1.8483948, dtype=float32), Array(1.8477693, dtype=float32), Array(1.8445598, dtype=float32), Array(1.8488446, dtype=float32), Array(1.85076, dtype=float32), Array(1.85021, dtype=float32), Array(1.8521712, dtype=float32), Array(1.8514645, dtype=float32), Array(1.844825, dtype=float32), Array(1.8419759, dtype=float32), Array(1.8457067, dtype=float32), Array(1.839794, dtype=float32), Array(1.8428013, dtype=float32), Array(1.8426458, dtype=float32), Array(1.8393126, dtype=float32), Array(1.8486445, dtype=float32), Array(1.8418739, dtype=float32), Array(1.837805, dtype=float32), Array(1.8431512, dtype=float32), Array(1.841341, dtype=float32), Array(1.8384329, dtype=float32), Array(1.8359332, dtype=float32), Array(1.8354067, dtype=float32), Array(1.8425317, dtype=float32), Array(1.8391945, dtype=float32), Array(1.8344839, dtype=float32), Array(1.8402034, dtype=float32), Array(1.8371849, dtype=float32), Array(1.8430629, dtype=float32), Array(1.8340751, dtype=float32), Array(1.8424991, dtype=float32), Array(1.8371514, dtype=float32), Array(1.8361045, dtype=float32), Array(1.8287605, dtype=float32), Array(1.8354031, dtype=float32), Array(1.8335155, dtype=float32), Array(1.8348632, dtype=float32), Array(1.8390146, dtype=float32), Array(1.8267547, dtype=float32), Array(1.8354211, dtype=float32), Array(1.835778, dtype=float32), Array(1.8340899, dtype=float32), Array(1.8360531, dtype=float32), Array(1.8341348, dtype=float32), Array(1.8400531, dtype=float32), Array(1.8396237, dtype=float32), Array(1.833836, dtype=float32), Array(1.8306228, dtype=float32), Array(1.8309578, dtype=float32), Array(1.835926, dtype=float32), Array(1.8315269, dtype=float32), Array(1.8281606, dtype=float32), Array(1.8334517, dtype=float32), Array(1.832342, dtype=float32), Array(1.8369334, dtype=float32), Array(1.8313398, dtype=float32), Array(1.8277338, dtype=float32), Array(1.831304, dtype=float32), Array(1.8308967, dtype=float32), Array(1.8293602, dtype=float32), Array(1.8287741, dtype=float32), Array(1.8265685, dtype=float32), Array(1.8302075, dtype=float32), Array(1.8300836, dtype=float32), Array(1.8319043, dtype=float32), Array(1.8285619, dtype=float32), Array(1.8288113, dtype=float32), Array(1.8285921, dtype=float32), Array(1.8225682, dtype=float32)], [Array(2.9574423, dtype=float32), Array(2.4875062, dtype=float32), Array(2.179495, dtype=float32), Array(2.0995018, dtype=float32), Array(2.0461047, dtype=float32), Array(1.9813654, dtype=float32), Array(1.9476627, dtype=float32), Array(1.925447, dtype=float32), Array(1.9154161, dtype=float32), Array(1.9090868, dtype=float32), Array(1.9062692, dtype=float32), Array(1.9019774, dtype=float32), Array(1.899549, dtype=float32), Array(1.8962927, dtype=float32), Array(1.8997895, dtype=float32), Array(1.897185, dtype=float32), Array(1.904143, dtype=float32), Array(1.8934034, dtype=float32), Array(1.8992562, dtype=float32), Array(1.9004195, dtype=float32), Array(1.89845, dtype=float32), Array(1.8996046, dtype=float32), Array(1.8937328, dtype=float32), Array(1.9006474, dtype=float32), Array(1.897971, dtype=float32), Array(1.8981838, dtype=float32), Array(1.8944173, dtype=float32), Array(1.8966652, dtype=float32), Array(1.8949224, dtype=float32), Array(1.9019625, dtype=float32), Array(1.8952099, dtype=float32), Array(1.8927296, dtype=float32), Array(1.8992296, dtype=float32), Array(1.901692, dtype=float32), Array(1.9000002, dtype=float32), Array(1.8967673, dtype=float32), Array(1.89491, dtype=float32), Array(1.8965806, dtype=float32), Array(1.8978965, dtype=float32), Array(1.8926463, dtype=float32), Array(1.8923684, dtype=float32), Array(1.8952582, dtype=float32), Array(1.892711, dtype=float32), Array(1.8912138, dtype=float32), Array(1.8932627, dtype=float32), Array(1.894715, dtype=float32), Array(1.8962274, dtype=float32), Array(1.8963658, dtype=float32), Array(1.8925579, dtype=float32), Array(1.8929051, dtype=float32), Array(1.8895626, dtype=float32), Array(1.88917, dtype=float32), Array(1.8918033, dtype=float32), Array(1.8856304, dtype=float32), Array(1.8970245, dtype=float32), Array(1.8959341, dtype=float32), Array(1.8900054, dtype=float32), Array(1.8944994, dtype=float32), Array(1.8886797, dtype=float32), Array(1.892844, dtype=float32), Array(1.8858533, dtype=float32), Array(1.8938162, dtype=float32), Array(1.8942382, dtype=float32), Array(1.8877761, dtype=float32), Array(1.8903756, dtype=float32), Array(1.8906411, dtype=float32), Array(1.8898858, dtype=float32), Array(1.8824629, dtype=float32), Array(1.8865769, dtype=float32), Array(1.8892703, dtype=float32), Array(1.8897548, dtype=float32), Array(1.8884175, dtype=float32), Array(1.8930984, dtype=float32), Array(1.89129, dtype=float32), Array(1.8907545, dtype=float32), Array(1.8856677, dtype=float32), Array(1.8850038, dtype=float32), Array(1.890935, dtype=float32), Array(1.8945991, dtype=float32), Array(1.8862677, dtype=float32), Array(1.8899602, dtype=float32), Array(1.8905177, dtype=float32), Array(1.8877605, dtype=float32), Array(1.8824795, dtype=float32), Array(1.8863319, dtype=float32), Array(1.888407, dtype=float32), Array(1.8880309, dtype=float32), Array(1.8918054, dtype=float32), Array(1.8899182, dtype=float32), Array(1.8875417, dtype=float32), Array(1.88302, dtype=float32), Array(1.8885844, dtype=float32), Array(1.8851918, dtype=float32), Array(1.8876543, dtype=float32), Array(1.8847665, dtype=float32), Array(1.882989, dtype=float32), Array(1.8888259, dtype=float32), Array(1.8865957, dtype=float32), Array(1.8865848, dtype=float32), Array(1.8799332, dtype=float32)]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all input arrays must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/miamirabelli/Desktop/GNN Research/lorenzGNN/best_hparam_edges.ipynb Cell 48\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/best_hparam_edges.ipynb#X65sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m all_arr \u001b[39m=\u001b[39m all_connected_3_epoch_losses\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/best_hparam_edges.ipynb#X65sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(all_connected_3_epoch_losses)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/best_hparam_edges.ipynb#X65sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m stacked \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mstack(all_arr)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/best_hparam_edges.ipynb#X65sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m mn3 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(stacked, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miamirabelli/Desktop/GNN%20Research/lorenzGNN/best_hparam_edges.ipynb#X65sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m std3 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstd(stacked, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/GNN Research/lorenzGNN/venv/lib/python3.11/site-packages/numpy/core/shape_base.py:426\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    424\u001b[0m shapes \u001b[39m=\u001b[39m {arr\u001b[39m.\u001b[39mshape \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m arrays}\n\u001b[1;32m    425\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(shapes) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 426\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mall input arrays must have the same shape\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    428\u001b[0m result_ndim \u001b[39m=\u001b[39m arrays[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mndim \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    429\u001b[0m axis \u001b[39m=\u001b[39m normalize_axis_index(axis, result_ndim)\n",
      "\u001b[0;31mValueError\u001b[0m: all input arrays must have the same shape"
     ]
    }
   ],
   "source": [
    "all_arr = all_connected_7_epoch_losses\n",
    "stacked = np.stack(all_arr)\n",
    "mn7 = np.mean(stacked, axis=0)\n",
    "std7 = np.std(stacked, axis=0)\n",
    "\n",
    "all_arr1 = all_connected_5_epoch_losses\n",
    "print(len(all_connected_5_epoch_losses))\n",
    "stacked = np.stack(all_arr1)\n",
    "mn5 = np.mean(stacked, axis=0)\n",
    "#print(len(mn5))\n",
    "std5 = np.std(stacked, axis=0)\n",
    "\n",
    "all_arr = all_connected_3_epoch_losses\n",
    "print(all_connected_3_epoch_losses)\n",
    "stacked = np.stack(all_arr)\n",
    "mn3 = np.mean(stacked, axis=1)\n",
    "std3 = np.std(stacked, axis=1)\n",
    "\n",
    "\n",
    "all_arr = all_solo_epoch_losses\n",
    "stacked = np.stack(all_arr)\n",
    "mn1 = np.mean(stacked, axis=1)\n",
    "std1 = np.std(stacked, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'legend.fontsize': 'xx-large',\n",
    "         'axes.labelsize': 'xx-large',\n",
    "         'axes.titlesize':'xx-large',\n",
    "         'xtick.labelsize':'xx-large',\n",
    "         'ytick.labelsize':'xx-large'}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1100x900 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_epoch_losses = [all_connected_7_epoch_losses[1:], all_connected_5_epoch_losses[1:], all_connected_3_epoch_losses[1:], all_solo_epoch_losses[1:]]\n",
    "plot_loss_over_epoch(all_epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, Iterable, List, Optional, Callable\n",
    "from utils.jraph_training import MSE, MB, ME, RMSE, CRMSE, FB, FE, R, R2, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_metric_list(\n",
    "        metric_funcs: Iterable[Callable],\n",
    "        state: train_state.TrainState, \n",
    "        input_window_graphs: Iterable[jraph.GraphsTuple],\n",
    "        target_window_graphs: Iterable[jraph.GraphsTuple],\n",
    "        n_rollout_steps: int,\n",
    "        rngs: Optional[Dict[str, jnp.ndarray]] = None,\n",
    "    ) -> Dict[str, List[float]]:\n",
    "    \"\"\" Computes a suite of metrics for an n-step rollout. \n",
    "    \n",
    "        Also returns predicted nodes.\n",
    "    \"\"\"\n",
    "    assert n_rollout_steps > 0\n",
    "    assert len(target_window_graphs) == n_rollout_steps, (len(target_window_graphs), n_rollout_steps)\n",
    "\n",
    "    curr_input_window_graphs = input_window_graphs\n",
    "    pred_nodes = []\n",
    "\n",
    "    # initialize metrics dict \n",
    "    metric_totals = {metric.__name__.lower(): [] for metric in metric_funcs}\n",
    "\n",
    "    for i in range(n_rollout_steps):\n",
    "        pred_graphs_list = state.apply_fn(state.params, curr_input_window_graphs, rngs=rngs) \n",
    "        pred_graph = pred_graphs_list[0]\n",
    "\n",
    "        # retrieve the new input window \n",
    "        curr_input_window_graphs = curr_input_window_graphs[1:] + [pred_graph]\n",
    "\n",
    "        preds = pred_graph.nodes\n",
    "\n",
    "        targets = target_window_graphs[i].nodes\n",
    "\n",
    "        # compute metrics\n",
    "        for metric in metric_funcs:\n",
    "            metric_val = metric(targets=targets, preds=preds)\n",
    "            metric_totals[metric.__name__.lower()].append(metric_val) # adds \n",
    "    \n",
    "        pred_nodes.append(preds)\n",
    "\n",
    "    return metric_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_rollout(\n",
    "    state: train_state.TrainState,\n",
    "    n_rollout_steps: int,\n",
    "    datasets: Dict[str, Dict[str, Iterable[jraph.GraphsTuple]]], \n",
    "    # first key = train/test/val, second key = input/target \n",
    "    split: str # e.g. [\"val\", \"test\"],\n",
    ") -> Dict[str, List[float]]:\n",
    "    \"\"\"Evaluates the model on metrics over the specified splits (i.e. modes).\"\"\"\n",
    "\n",
    "    # Loop over each split independently.\n",
    "    eval_metrics_dict = {}\n",
    "    input_data = datasets[split]['inputs']\n",
    "    target_data = datasets[split]['targets']\n",
    "\n",
    "    # Initialize metrics dict\n",
    "    metric_totals = {metric_name: [[] for _ in range(n_rollout_steps)] for metric_name in \n",
    "                     [\"mse\", \"r2\"]}\n",
    "\n",
    "    # Loop over individual windows in the dataset \n",
    "    for (input_window_graphs, target_window_graphs) in zip(input_data, target_data):\n",
    "        split_metrics_update = rollout_metric_list(\n",
    "            metric_funcs=[MSE, R2],\n",
    "            state=state, \n",
    "            n_rollout_steps=n_rollout_steps, \n",
    "            input_window_graphs=input_window_graphs, \n",
    "            target_window_graphs=target_window_graphs)\n",
    "\n",
    "        # Update metrics.\n",
    "        for metric_name, step_values in split_metrics_update.items():\n",
    "            for step, values in enumerate(step_values):\n",
    "                metric_totals[metric_name][step].append(values)\n",
    "\n",
    "    # Compute averages for each step in the rollout \n",
    "    eval_metrics_dict = {metric_name: np.array([np.mean(step_values) for step_values in step_values_list])\n",
    "                         for metric_name, step_values_list in metric_totals.items()}\n",
    "\n",
    "    return eval_metrics_dict  # pytype: disable=bad-return-type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multirollout_metrics_multiple_configs(\n",
    "    configs,\n",
    "    workdirs, # for loading checkpoints \n",
    "    plot_mode, # i.e. \"train\"/\"val\"/\"test\"\n",
    "    all_datasets=None,\n",
    "    all_metrics_dicts=None,\n",
    "    title=''):\n",
    "\n",
    "    assert plot_mode in [\"train\", \"val\", \"test\"]\n",
    "    plt.rcParams['xtick.labelsize'] = 18\n",
    "    # set up plot\n",
    "    # NOTE: remember to update the axes if you include more/less metrics\n",
    "    fig, axes  = plt.subplots(2, 1, figsize=(10, 8))\n",
    "    if title == '':\n",
    "        fig.suptitle(f\"{plot_mode} rollout metrics, averaged across {configs[0].K} nodes\", size=28)\n",
    "    else:\n",
    "        fig.suptitle(title, size=30)\n",
    "    colors = [\"red\", \"blue\", \"green\", \"purple\"]\n",
    "    labels = ['7-connected', \"5-connected\", \"3-connected\", \"solo\"]\n",
    "    titles = [\"Mean Squared Error (MSE)\", \"Determination Coefficent (R^2)\"]\n",
    "    current_plot = None\n",
    "\n",
    "    tot_metrics_dict = []\n",
    "    \n",
    "    for index in range(len(configs)):\n",
    "        config = configs[index]\n",
    "        workdir = workdirs[index]\n",
    "\n",
    "        checkpoint_dir = os.path.join(workdir, 'checkpoints')\n",
    "        assert os.path.exists(checkpoint_dir)\n",
    "\n",
    "        # Get datasets, organized by split.\n",
    "        if all_datasets is None:\n",
    "            logging.info('Generating datasets from config because none provided.')\n",
    "            datasets = create_dataset(config)\n",
    "        else:\n",
    "            datasets = all_datasets[index]\n",
    "\n",
    "        plot_set = datasets[plot_mode]\n",
    "        input_data = plot_set['inputs']\n",
    "        n_rollout_steps = config.output_steps\n",
    "        rollout_nums = np.arange(n_rollout_steps)\n",
    "\n",
    "        # Create the evaluation state, corresponding to a deterministic model.\n",
    "        logging.info('Initializing network.')\n",
    "        rng = jax.random.key(0)\n",
    "        rng, init_rng = jax.random.split(rng)\n",
    "        sample_input_window = input_data[0]\n",
    "        eval_net = create_model(config, deterministic=True)\n",
    "        params = jax.jit(eval_net.init)(init_rng, sample_input_window)\n",
    "        parameter_overview.log_parameter_overview(params) # logs to logging.info\n",
    "\n",
    "        # Create the optimizer and state.\n",
    "        # (we don't actually need the optimizer for evaluation, we just need it to create the state)\n",
    "        tx = create_optimizer(config)\n",
    "        state = train_state.TrainState.create(\n",
    "            apply_fn=eval_net.apply, params=params, tx=tx\n",
    "        )\n",
    "\n",
    "        # load the checkpoint state\n",
    "        ckpt = checkpoint.Checkpoint(checkpoint_dir)\n",
    "        state = ckpt.restore(state) # restore latest checkpoint \n",
    "\n",
    "        # gets all metrics\n",
    "        if all_metrics_dicts is None: \n",
    "            logging.info('Generating metrics from dataset because none provided.')\n",
    "            metrics_dict = evaluate_model_rollout(\n",
    "                state=state,\n",
    "                n_rollout_steps=config.output_steps,\n",
    "                datasets=datasets,\n",
    "                split=plot_mode)\n",
    "        else:\n",
    "            metrics_dict = all_metrics_dicts[index]\n",
    "\n",
    "        tot_metrics_dict.append(metrics_dict)\n",
    "        \n",
    "        plot_num = 0\n",
    "        for key in metrics_dict:\n",
    "            current_plot = axes[plot_num]\n",
    "            current_plot.set_title(titles[plot_num], size=25)\n",
    "            plt.xlabel('Rollout Number', size=20)\n",
    "            # plot rollout targets\n",
    "            current_plot.plot(\n",
    "                rollout_nums,\n",
    "                metrics_dict[key],\n",
    "                alpha=0.8,\n",
    "                linestyle='dashed',\n",
    "                marker='o',\n",
    "                c=colors[index],\n",
    "                label=labels[index])\n",
    "            \n",
    "            current_plot.tick_params(labelsize=22)\n",
    "            plot_num += 1\n",
    "\n",
    "    #fig.legend(labels, bbox_to_anchor=(0.99, 0.5), loc=\"center left\") # labels, loc='lower center', ncol=len(labels)\n",
    "    plt.legend(labels, bbox_to_anchor=(0.5, -0.03), loc=\"lower center\", bbox_transform=fig.transFigure, ncol=len(labels), fontsize=18)\n",
    "    fig.tight_layout()\n",
    "    return tot_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| Name                                   | Shape    | Size | Mean     | Std   |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 8)   | 48   | -0.103   | 0.37  |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (8, 8)   | 64   | -0.0189  | 0.344 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (32,)    | 32   | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (19, 32) | 608  | -0.00843 | 0.219 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)     | 2    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (32, 2)  | 64   | -0.00627 | 0.195 |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "Total: 834\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:Restoring checkpoint: tests/outputs/connected_7/trial-0/checkpoints/ckpt-22\n",
      "INFO:absl:Restored save_counter=22 restored_checkpoint=tests/outputs/connected_7/trial-0/checkpoints/ckpt-22\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.00s.\n",
      "INFO:root:Generating metrics from dataset because none provided.\n",
      "INFO:root:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| Name                                   | Shape    | Size | Mean     | Std   |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 8)   | 48   | -0.103   | 0.37  |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (8, 8)   | 64   | -0.0189  | 0.344 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (32,)    | 32   | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (19, 32) | 608  | -0.00843 | 0.219 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)     | 2    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (32, 2)  | 64   | -0.00627 | 0.195 |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "Total: 834\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:Restoring checkpoint: tests/outputs/connected_5/trial-0/checkpoints/ckpt-22\n",
      "INFO:absl:Restored save_counter=22 restored_checkpoint=tests/outputs/connected_5/trial-0/checkpoints/ckpt-22\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.00s.\n",
      "INFO:root:Generating metrics from dataset because none provided.\n",
      "INFO:root:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| Name                                   | Shape    | Size | Mean     | Std   |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 8)   | 48   | -0.103   | 0.37  |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (8, 8)   | 64   | -0.0189  | 0.344 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (32,)    | 32   | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (19, 32) | 608  | -0.00843 | 0.219 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)     | 2    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (32, 2)  | 64   | -0.00627 | 0.195 |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "Total: 834\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:Restoring checkpoint: tests/outputs/connected_3/trial-0/checkpoints/ckpt-22\n",
      "INFO:absl:Restored save_counter=22 restored_checkpoint=tests/outputs/connected_3/trial-0/checkpoints/ckpt-22\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.00s.\n",
      "INFO:root:Generating metrics from dataset because none provided.\n",
      "INFO:root:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| Name                                   | Shape    | Size | Mean     | Std   |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 8)   | 48   | -0.103   | 0.37  |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (8, 8)   | 64   | -0.0189  | 0.344 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (32,)    | 32   | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (19, 32) | 608  | -0.00843 | 0.219 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)     | 2    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (32, 2)  | 64   | -0.00627 | 0.195 |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "Total: 834\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:Restoring checkpoint: tests/outputs/solo/trial-0/checkpoints/ckpt-22\n",
      "INFO:absl:Restored save_counter=22 restored_checkpoint=tests/outputs/solo/trial-0/checkpoints/ckpt-22\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.00s.\n",
      "INFO:root:Generating metrics from dataset because none provided.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "configs = [connected_configs_7[0], connected_configs_5[0],  connected_configs_3[0], solo_configs[0]]\n",
    "workdirs = [connected_workdirs_7[0], connected_workdirs_5[0], connected_workdirs_3[0], solo_workdirs[0]]\n",
    "multi_datasets = [all_connected_7_datasets[0], all_connected_5_datasets[0], all_connected_3_datasets[0], all_solo_datasets[0]] \n",
    "\n",
    "metrics_dict = plot_multirollout_metrics_multiple_configs(\n",
    "    configs=configs,\n",
    "    workdirs=workdirs,\n",
    "    plot_mode='test',\n",
    "    all_datasets=multi_datasets,\n",
    "    title=\"Rollout Metrics Averaved across 36 Nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| Name                                   | Shape    | Size | Mean     | Std   |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 8)   | 48   | -0.103   | 0.37  |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (8, 8)   | 64   | -0.0189  | 0.344 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (32,)    | 32   | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (19, 32) | 608  | -0.00843 | 0.219 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)     | 2    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (32, 2)  | 64   | -0.00627 | 0.195 |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "Total: 834\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:Restoring checkpoint: tests/outputs/connected_7/trial-0/checkpoints/ckpt-22\n",
      "INFO:absl:Restored save_counter=22 restored_checkpoint=tests/outputs/connected_7/trial-0/checkpoints/ckpt-22\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.00s.\n",
      "INFO:root:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| Name                                   | Shape    | Size | Mean     | Std   |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 8)   | 48   | -0.103   | 0.37  |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (8, 8)   | 64   | -0.0189  | 0.344 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (32,)    | 32   | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (19, 32) | 608  | -0.00843 | 0.219 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)     | 2    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (32, 2)  | 64   | -0.00627 | 0.195 |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "Total: 834\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:Restoring checkpoint: tests/outputs/connected_5/trial-0/checkpoints/ckpt-22\n",
      "INFO:absl:Restored save_counter=22 restored_checkpoint=tests/outputs/connected_5/trial-0/checkpoints/ckpt-22\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.00s.\n",
      "INFO:root:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| Name                                   | Shape    | Size | Mean     | Std   |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 8)   | 48   | -0.103   | 0.37  |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (8, 8)   | 64   | -0.0189  | 0.344 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (32,)    | 32   | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (19, 32) | 608  | -0.00843 | 0.219 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)     | 2    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (32, 2)  | 64   | -0.00627 | 0.195 |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "Total: 834\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:Restoring checkpoint: tests/outputs/connected_3/trial-0/checkpoints/ckpt-22\n",
      "INFO:absl:Restored save_counter=22 restored_checkpoint=tests/outputs/connected_3/trial-0/checkpoints/ckpt-22\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.00s.\n",
      "INFO:root:Initializing network.\n",
      "INFO:absl:\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| Name                                   | Shape    | Size | Mean     | Std   |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_0/kernel | (6, 8)   | 48   | -0.103   | 0.37  |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/bias   | (8,)     | 8    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_0/Dense_1/kernel | (8, 8)   | 64   | -0.0189  | 0.344 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/bias   | (32,)    | 32   | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_0/kernel | (19, 32) | 608  | -0.00843 | 0.219 |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/bias   | (2,)     | 2    | 0.0      | 0.0   |\n",
      "| params/MLPBlock_0/MLP_1/Dense_1/kernel | (32, 2)  | 64   | -0.00627 | 0.195 |\n",
      "+----------------------------------------+----------+------+----------+-------+\n",
      "Total: 834\n",
      "INFO:absl:Checkpoint.restore_or_initialize() ...\n",
      "INFO:absl:Restoring checkpoint: tests/outputs/solo/trial-0/checkpoints/ckpt-22\n",
      "INFO:absl:Restored save_counter=22 restored_checkpoint=tests/outputs/solo/trial-0/checkpoints/ckpt-22\n",
      "INFO:absl:Checkpoint.restore_or_initialize() finished after 0.00s.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics_dict = plot_multirollout_metrics_multiple_configs(\n",
    "    configs=configs,\n",
    "    workdirs=workdirs,\n",
    "    plot_mode='test',\n",
    "    all_datasets=multi_datasets,\n",
    "    all_metrics_dicts=metrics_dict,\n",
    "    title=\"Rollout Metrics Averaged Across 36 Nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Connected vs 7-Connected mse\n",
      "Ttest_indResult(statistic=-0.8499359694320989, pvalue=0.42796716617582886)\n",
      "\n",
      "5-Connected vs 7-Connected r2\n",
      "Ttest_indResult(statistic=0.8577783972434626, pvalue=0.4239486808135643)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in metrics_dict[0]:\n",
    "    print(\"5-Connected vs 7-Connected\", key)\n",
    "    print(stats.ttest_ind(a=metrics_dict[1][key], b=metrics_dict[0][key], equal_var=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Connected vs 3-Connected mse\n",
      "Ttest_indResult(statistic=-1.0628759943645425, pvalue=0.3287334656576039)\n",
      "\n",
      "5-Connected vs 3-Connected r2\n",
      "Ttest_indResult(statistic=1.122865712659042, pvalue=0.3044064327206025)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in metrics_dict[0]:\n",
    "    print(\"5-Connected vs 3-Connected\", key)\n",
    "    print(stats.ttest_ind(a=metrics_dict[1][key], b=metrics_dict[2][key], equal_var=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Connected vs Solo mse\n",
      "Ttest_indResult(statistic=-2.0579013743218293, pvalue=0.08530387701581812)\n",
      "\n",
      "5-Connected vs Solo r2\n",
      "Ttest_indResult(statistic=2.0827101663107115, pvalue=0.08242519206645424)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in metrics_dict[0]:\n",
    "    print(\"5-Connected vs Solo\", key)\n",
    "    print(stats.ttest_ind(a=metrics_dict[1][key], b=metrics_dict[3][key], equal_var=True))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
